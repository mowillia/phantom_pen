{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mowillia_GPT-2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "UJit5UMJYIBA"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pzxl1vYX-1kk",
        "colab_type": "text"
      },
      "source": [
        "Setup:\n",
        "\n",
        "1) Make sure GPU is enabled, go to edit->notebook settings->Hardware Accelerator GPU\n",
        "\n",
        "2) Make a copy to your google drive, click on copy to drive in panel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iW0abT07ZkhZ",
        "colab_type": "text"
      },
      "source": [
        "Note: Colab will reset after 12 hours make sure to save your model checkpoints to google drive around 10-11 hours mark or before, then go to runtime->reset all runtimes. Now copy your train model back into colab and start training again from the previous checkpoint."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLXW02eIYpcB",
        "colab_type": "text"
      },
      "source": [
        "clone and cd into repo, nshepperd's fork https://github.com/nshepperd/gpt-2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICYu3w9hIJkC",
        "colab_type": "code",
        "outputId": "d1b538ff-6abb-499f-99f6-bb6aafff56f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "!git clone https://github.com/nshepperd/gpt-2.git\n",
        "#!git clone https://github.com/mowillia/gpt-2.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'gpt-2'...\n",
            "remote: Enumerating objects: 297, done.\u001b[K\n",
            "remote: Total 297 (delta 0), reused 0 (delta 0), pack-reused 297\u001b[K\n",
            "Receiving objects: 100% (297/297), 4.40 MiB | 14.71 MiB/s, done.\n",
            "Resolving deltas: 100% (162/162), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6eEIs3ApZUVO",
        "colab_type": "code",
        "outputId": "bfc77f71-9452-40d7-c86a-a80713de1fc8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd gpt-2"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gpt-2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMzufBi55Xfb",
        "colab_type": "code",
        "outputId": "3255379d-5f9a-4f2d-eb6e-706a94b1e90c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "#check GPU status\n",
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Jun 27 15:42:44 2019       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 418.67       Driver Version: 410.79       CUDA Version: 10.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   46C    P8    16W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLclw6Y95bG4",
        "colab_type": "code",
        "outputId": "3869d58e-79b2-4f12-f1e4-06c3945c7f03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import torch\n",
        "# Checking if GPU is available\n",
        "work_with_gpu = torch.cuda.is_available()\n",
        "if(work_with_gpu):\n",
        "    print('Using GPU!')\n",
        "else: \n",
        "    print('No GPU available, using CPU; Consider using short texts.')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using GPU!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qtn1qZPgZLb0",
        "colab_type": "text"
      },
      "source": [
        "Install requirements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "434oOx0bZH6J",
        "colab_type": "code",
        "outputId": "2b1e53d6-0507-4d59-c593-31c64d60e9f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582
        }
      },
      "source": [
        "!pip3 install -r requirements.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fire>=0.1.3 (from -r requirements.txt (line 1))\n",
            "  Downloading https://files.pythonhosted.org/packages/5a/b7/205702f348aab198baecd1d8344a90748cb68f53bdcd1cc30cbc08e47d3e/fire-0.1.3.tar.gz\n",
            "Collecting regex==2017.4.5 (from -r requirements.txt (line 2))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/62/c0c0d762ffd4ffaf39f372eb8561b8d491a11ace5a7884610424a8b40f95/regex-2017.04.05.tar.gz (601kB)\n",
            "\u001b[K     |████████████████████████████████| 604kB 9.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests==2.21.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 3)) (2.21.0)\n",
            "Collecting tqdm==4.31.1 (from -r requirements.txt (line 4))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6c/4b/c38b5144cf167c4f52288517436ccafefe9dc01b8d1c190e18a6b154cd4a/tqdm-4.31.1-py2.py3-none-any.whl (48kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 23.5MB/s \n",
            "\u001b[?25hCollecting toposort==1.5 (from -r requirements.txt (line 5))\n",
            "  Downloading https://files.pythonhosted.org/packages/e9/8a/321cd8ea5f4a22a06e3ba30ef31ec33bea11a3443eeb1d89807640ee6ed4/toposort-1.5-py2.py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from fire>=0.1.3->-r requirements.txt (line 1)) (1.12.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (2019.6.16)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (3.0.4)\n",
            "Building wheels for collected packages: fire, regex\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/2a/1a/4d/6b30377c3051e76559d1185c1dbbfff15aed31f87acdd14c22\n",
            "  Building wheel for regex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/75/07/38/3c16b529d50cb4e0cd3dbc7b75cece8a09c132692c74450b01\n",
            "Successfully built fire regex\n",
            "Installing collected packages: fire, regex, tqdm, toposort\n",
            "  Found existing installation: tqdm 4.28.1\n",
            "    Uninstalling tqdm-4.28.1:\n",
            "      Successfully uninstalled tqdm-4.28.1\n",
            "Successfully installed fire-0.1.3 regex-2017.4.5 toposort-1.5 tqdm-4.31.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tqdm"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvUQhgK3PQ4L",
        "colab_type": "text"
      },
      "source": [
        "Mount drive to access google drive for saving and accessing checkpoints later. Have to log in to your google account"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNpf6R4ahYSN",
        "colab_type": "code",
        "outputId": "69649b5f-76df-4c64-e053-a19063ae62c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1hrgeKFYsuE",
        "colab_type": "text"
      },
      "source": [
        "Download the model data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "A498TySgHYyF",
        "outputId": "35e2abe5-98d2-43f8-9b79-ddeb9a4b3bae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "!python3 download_model.py 117M"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\rFetching checkpoint:   0%|                                              | 0.00/77.0 [00:00<?, ?it/s]\rFetching checkpoint: 1.00kit [00:00, 566kit/s]                                                      \n",
            "Fetching encoder.json: 1.04Mit [00:00, 54.7Mit/s]                                                   \n",
            "Fetching hparams.json: 1.00kit [00:00, 923kit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 498Mit [00:09, 52.1Mit/s]                                  \n",
            "Fetching model.ckpt.index: 6.00kit [00:00, 5.53Mit/s]                                               \n",
            "Fetching model.ckpt.meta: 472kit [00:00, 53.5Mit/s]                                                 \n",
            "Fetching vocab.bpe: 457kit [00:00, 52.4Mit/s]                                                       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UDpEGjfO8Q2",
        "colab_type": "code",
        "outputId": "33faac78-57ce-4317-95f5-47371c112ae2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "#!python3 download_model.py 345M"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\rFetching checkpoint:   0%|                                              | 0.00/77.0 [00:00<?, ?it/s]\rFetching checkpoint: 1.00kit [00:00, 814kit/s]                                                      \n",
            "\rFetching encoder.json:   0%|                                           | 0.00/1.04M [00:00<?, ?it/s]\rFetching encoder.json: 1.04Mit [00:00, 54.1Mit/s]                                                   \n",
            "Fetching hparams.json: 1.00kit [00:00, 1.02Mit/s]                                                   \n",
            "Fetching model.ckpt.data-00000-of-00001: 1.42Git [00:21, 65.4Mit/s]                                 \n",
            "Fetching model.ckpt.index: 11.0kit [00:00, 7.13Mit/s]                                               \n",
            "Fetching model.ckpt.meta: 927kit [00:00, 52.4Mit/s]                                                 \n",
            "Fetching vocab.bpe: 457kit [00:00, 46.1Mit/s]                                                       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zq-YwRnNOBYO",
        "colab_type": "text"
      },
      "source": [
        "encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7oJPQtdLbbeK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!export PYTHONIOENCODING=UTF-8"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KzSbAvePgsI",
        "colab_type": "text"
      },
      "source": [
        "Fetch checkpoints if you have them saved in google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_P3KCCx91uG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get full essays\n",
        "!cp -r /content/drive/My\\ Drive/writrly_proj_files/Full_Essays/* /content/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cA2Wk7yIPmS6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fetch the models \n",
        "#!cp -r /content/drive/My\\ Drive/models/ /content/gpt-2/ "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQ_-Css-r5fI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fetch the checkpoints \n",
        "#!cp -r /content/drive/My\\ Drive/checkpoint/ /content/gpt-2/ "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0p--9zwqQRTc",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Let's get our train on! In this case the file is A Tale of Two Cities (Charles Dickens) from Project Gutenberg. To change the dataset GPT-2 models will fine-tune on, change this URL to another .txt file, and change corresponding part of the next cell. Note that you can use small datasets if you want but you will have to be sure not to run the fine-tuning for too long or you will overfit badly. Roughly, expect interesting results within minutes to hours in the 1-10s of megabyte ballpark, and below this you may want to stop the run early as fine-tuning can be very fast."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOCvrs-DHvxa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!wget https://www.gutenberg.org/files/98/98-0.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPfJ5b3CQXqr",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Start training, add --model_name '345M' to use 345 model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDFQZfANYm_T",
        "colab_type": "text"
      },
      "source": [
        "### Training without concatenation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEn_ihcGI00T",
        "colab_type": "code",
        "outputId": "c5d15d28-a8ab-425b-ee82-a3e91638e043",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "## business essays training - with 345\n",
        "!PYTHONPATH=src ./train.py --dataset /content/all_business.txt --run_name 'atlantic_business_345' --model_name '345M'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0627 15:46:08.758351 140659569104768 deprecation_wrapper.py:119] From /content/gpt-2/src/model.py:147: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
            "\n",
            "W0627 15:46:08.766982 140659569104768 deprecation_wrapper.py:119] From /content/gpt-2/src/memory_saving_gradients.py:13: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n",
            "W0627 15:46:08.857946 140659569104768 deprecation_wrapper.py:119] From ./train.py:88: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "W0627 15:46:08.858297 140659569104768 deprecation_wrapper.py:119] From ./train.py:91: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "2019-06-27 15:46:08.876915: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-06-27 15:46:08.879284: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1d65100 executing computations on platform Host. Devices:\n",
            "2019-06-27 15:46:08.879332: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2019-06-27 15:46:08.884603: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\n",
            "2019-06-27 15:46:09.070874: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-27 15:46:09.071443: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1d64840 executing computations on platform CUDA. Devices:\n",
            "2019-06-27 15:46:09.071476: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2019-06-27 15:46:09.071741: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-27 15:46:09.072090: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-06-27 15:46:09.081887: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-06-27 15:46:09.251936: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-06-27 15:46:09.330354: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-06-27 15:46:09.353790: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-06-27 15:46:09.544052: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-06-27 15:46:09.689138: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-06-27 15:46:10.022101: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-06-27 15:46:10.022455: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-27 15:46:10.022971: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-27 15:46:10.023311: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
            "2019-06-27 15:46:10.026436: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-06-27 15:46:10.027589: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-06-27 15:46:10.027625: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n",
            "2019-06-27 15:46:10.027636: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n",
            "2019-06-27 15:46:10.034057: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-27 15:46:10.034530: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-27 15:46:10.034902: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14202 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "W0627 15:46:10.035805 140659569104768 deprecation_wrapper.py:119] From ./train.py:92: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0627 15:46:20.571707 140659569104768 deprecation.py:323] From /content/gpt-2/src/sample.py:65: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "W0627 15:46:20.585787 140659569104768 deprecation.py:323] From /content/gpt-2/src/sample.py:16: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0627 15:46:20.587478 140659569104768 deprecation.py:323] From /content/gpt-2/src/sample.py:70: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.random.categorical` instead.\n",
            "W0627 15:46:20.597584 140659569104768 deprecation_wrapper.py:119] From ./train.py:121: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "W0627 15:46:35.455458 140659569104768 deprecation_wrapper.py:119] From ./train.py:144: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n",
            "W0627 15:46:35.458387 140659569104768 deprecation_wrapper.py:119] From ./train.py:147: The name tf.summary.merge is deprecated. Please use tf.compat.v1.summary.merge instead.\n",
            "\n",
            "W0627 15:46:35.459138 140659569104768 deprecation_wrapper.py:119] From ./train.py:149: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
            "\n",
            "W0627 15:46:35.460095 140659569104768 deprecation_wrapper.py:119] From ./train.py:152: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "Loading checkpoint models/345M/model.ckpt\n",
            "W0627 15:46:49.748164 140659569104768 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "Loading dataset...\n",
            "100% 1/1 [00:02<00:00,  2.91s/it]\n",
            "dataset has 391882 tokens\n",
            "Training...\n",
            "2019-06-27 15:47:06.294597: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n",
            "2019-06-27 15:47:07.145714: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "[1 | 14.30] loss=2.94 avg=2.94\n",
            "[2 | 15.77] loss=2.59 avg=2.76\n",
            "[3 | 17.25] loss=2.87 avg=2.80\n",
            "[4 | 18.72] loss=2.46 avg=2.71\n",
            "[5 | 20.20] loss=2.53 avg=2.67\n",
            "[6 | 21.68] loss=2.92 avg=2.72\n",
            "[7 | 23.16] loss=2.88 avg=2.74\n",
            "[8 | 24.64] loss=2.91 avg=2.76\n",
            "[9 | 26.12] loss=2.89 avg=2.78\n",
            "[10 | 27.60] loss=2.88 avg=2.79\n",
            "[11 | 29.08] loss=2.39 avg=2.75\n",
            "[12 | 30.57] loss=2.44 avg=2.72\n",
            "[13 | 32.06] loss=2.75 avg=2.72\n",
            "[14 | 33.55] loss=2.89 avg=2.74\n",
            "[15 | 35.04] loss=2.72 avg=2.74\n",
            "[16 | 36.53] loss=3.34 avg=2.78\n",
            "[17 | 38.03] loss=2.79 avg=2.78\n",
            "[18 | 39.53] loss=2.73 avg=2.77\n",
            "[19 | 41.02] loss=2.68 avg=2.77\n",
            "[20 | 42.52] loss=2.43 avg=2.75\n",
            "[21 | 44.01] loss=2.73 avg=2.75\n",
            "[22 | 45.51] loss=2.74 avg=2.75\n",
            "[23 | 47.01] loss=2.77 avg=2.75\n",
            "[24 | 48.52] loss=2.85 avg=2.76\n",
            "[25 | 50.02] loss=2.62 avg=2.75\n",
            "[26 | 51.54] loss=2.96 avg=2.76\n",
            "[27 | 53.06] loss=2.76 avg=2.76\n",
            "[28 | 54.57] loss=2.56 avg=2.75\n",
            "[29 | 56.08] loss=2.60 avg=2.74\n",
            "[30 | 57.59] loss=2.95 avg=2.75\n",
            "[31 | 59.11] loss=2.64 avg=2.75\n",
            "[32 | 60.63] loss=2.86 avg=2.75\n",
            "[33 | 62.16] loss=2.73 avg=2.75\n",
            "[34 | 63.68] loss=2.55 avg=2.74\n",
            "[35 | 65.20] loss=2.77 avg=2.74\n",
            "[36 | 66.71] loss=2.87 avg=2.75\n",
            "[37 | 68.24] loss=2.90 avg=2.75\n",
            "[38 | 69.77] loss=2.64 avg=2.75\n",
            "[39 | 71.30] loss=2.69 avg=2.75\n",
            "[40 | 72.83] loss=2.52 avg=2.74\n",
            "[41 | 74.37] loss=2.26 avg=2.73\n",
            "[42 | 75.91] loss=2.68 avg=2.73\n",
            "[43 | 77.45] loss=2.57 avg=2.72\n",
            "[44 | 79.01] loss=2.61 avg=2.72\n",
            "[45 | 80.55] loss=3.21 avg=2.73\n",
            "[46 | 82.10] loss=3.04 avg=2.74\n",
            "[47 | 83.66] loss=2.72 avg=2.74\n",
            "[48 | 85.21] loss=2.65 avg=2.74\n",
            "[49 | 86.77] loss=2.94 avg=2.74\n",
            "[50 | 88.33] loss=2.70 avg=2.74\n",
            "[51 | 89.88] loss=2.57 avg=2.74\n",
            "[52 | 91.42] loss=2.74 avg=2.74\n",
            "[53 | 92.97] loss=2.60 avg=2.73\n",
            "[54 | 94.52] loss=2.38 avg=2.73\n",
            "[55 | 96.08] loss=3.00 avg=2.73\n",
            "[56 | 97.63] loss=2.74 avg=2.73\n",
            "[57 | 99.18] loss=2.60 avg=2.73\n",
            "[58 | 100.73] loss=2.59 avg=2.73\n",
            "[59 | 102.28] loss=2.66 avg=2.73\n",
            "[60 | 103.84] loss=2.88 avg=2.73\n",
            "[61 | 105.39] loss=2.80 avg=2.73\n",
            "[62 | 106.95] loss=2.33 avg=2.72\n",
            "[63 | 108.52] loss=2.71 avg=2.72\n",
            "[64 | 110.07] loss=2.59 avg=2.72\n",
            "[65 | 111.64] loss=3.00 avg=2.72\n",
            "[66 | 113.20] loss=3.02 avg=2.73\n",
            "[67 | 114.77] loss=3.04 avg=2.74\n",
            "[68 | 116.33] loss=2.66 avg=2.74\n",
            "[69 | 117.89] loss=2.41 avg=2.73\n",
            "[70 | 119.46] loss=2.66 avg=2.73\n",
            "[71 | 121.03] loss=2.75 avg=2.73\n",
            "[72 | 122.59] loss=2.59 avg=2.73\n",
            "[73 | 124.15] loss=2.46 avg=2.72\n",
            "[74 | 125.71] loss=2.89 avg=2.72\n",
            "[75 | 127.27] loss=2.78 avg=2.72\n",
            "[76 | 128.83] loss=2.19 avg=2.71\n",
            "[77 | 130.39] loss=2.47 avg=2.71\n",
            "[78 | 131.95] loss=2.73 avg=2.71\n",
            "[79 | 133.52] loss=2.71 avg=2.71\n",
            "[80 | 135.08] loss=2.55 avg=2.71\n",
            "[81 | 136.64] loss=2.88 avg=2.71\n",
            "[82 | 138.20] loss=2.53 avg=2.71\n",
            "[83 | 139.77] loss=2.57 avg=2.70\n",
            "[84 | 141.34] loss=2.66 avg=2.70\n",
            "[85 | 142.90] loss=2.54 avg=2.70\n",
            "[86 | 144.47] loss=2.51 avg=2.70\n",
            "[87 | 146.04] loss=2.73 avg=2.70\n",
            "[88 | 147.61] loss=2.76 avg=2.70\n",
            "[89 | 149.18] loss=2.96 avg=2.70\n",
            "[90 | 150.75] loss=2.59 avg=2.70\n",
            "[91 | 152.33] loss=2.77 avg=2.70\n",
            "[92 | 153.90] loss=3.00 avg=2.71\n",
            "[93 | 155.48] loss=2.79 avg=2.71\n",
            "[94 | 157.06] loss=2.58 avg=2.71\n",
            "[95 | 158.64] loss=2.55 avg=2.70\n",
            "[96 | 160.23] loss=2.54 avg=2.70\n",
            "[97 | 161.81] loss=2.84 avg=2.70\n",
            "[98 | 163.39] loss=2.82 avg=2.71\n",
            "[99 | 164.99] loss=2.82 avg=2.71\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            ", said: \"We are grateful for all the help and support we have received from all sources. However it must be noted that, at this time, the Police Complaints team of South Yorkshire have made no further enquiries related to the case.\"\n",
            "\n",
            "The officer at the centre of the case has resigned from the force.<|endoftext|>\"I wish I could say it came from anyone besides myself, but the entire situation was completely mine,\" says Bélix, who asked not to give his full name because he was scared of a backlash against his friends. \"It was the second time I was raped, and when I came to have my baby, two years earlier, it was because I had come to them in tears and tried to help them.\"\n",
            "\n",
            "He met the young women through Facebook Groups, then went into their homes to make sure they had everything they needed—such as child-rearing equipment that she could use. She was then paid as soon as her baby arrived, although she didn't take the money, which led to abuse. She started drinking regularly, and started leaving her baby to strangers in rural French villages. As she got older, she met more men. \"I could see that we were becoming less and less happy, because it was not happening anymore. My partner was still working, he was still with me. It started affecting my mind, and it was very traumatic, and I felt I couldn't go on.\"\n",
            "\n",
            "But it's not hard to see how Bélix's story was just part of a large cultural problem in France: It's one that disproportionately affects women, ages 20 to 50, of color, and young people of all ages. This is the third time Bélix has called for change in the way that French sex workers and their advocates are treated. The first time had been in 2006, and now these are the third.\n",
            "\n",
            "There's a strong tradition of protection for prostitutes, which has served well in keeping pimps at bay: Police officers in France arrested more traffickers and pimps in 2006 than any other year. Prostitution is regulated by municipal regulations and paid for from a pool of taxpayers' money. Prostitutes, like other employees, receive certain minimum wages, benefits, and security, the same benefits as men. In theory, pimps are protected from lawsuits over their sex trafficking, which is legal. Legal action is expensive, and it must be initiated by prostitutes themselves, as they can be barred from the labor market entirely. For this reason, protection is often done without a complaint, leaving pimps little legal recourse to sue for their clients' exploitation.\n",
            "\n",
            "The French government is, however, on the wrong side of the law in terms of enforcing trafficking laws. For example, pimps can be forced to register as sex workers (though not the victims themselves), and they are not required to register with the police. Prostitutes can also be prosecuted for trafficking, if they are identified or testify at court.\n",
            "\n",
            "Advertisement\n",
            "\n",
            "The French government has also been slow in making progress in addressing the root causes of violence against women in prostitution. Most of the problems stemmed from a long-standing legal trend: Most women in prostitution are already victims of violence (with or without protection from trafficking) and prostitution is a very vulnerable profession that often works in ways that are exploitative and harmful to both the women who work and the communities they live in. This is particularly true for young and inexperienced women, who come to work for the most part knowing nothing about labor-labor policies.\n",
            "\n",
            "It was a big reason why the French government passed a law in 2008 to protect prostituted girls under the age of 16. It does not, however, address trafficking as a crime against nature.\n",
            "\n",
            "The French government is also under pressure to change attitudes to sex work, in part by creating a culture of social responsibility on the part of police officers, as well as to enact laws that would reduce the number of female sex workers, and make it harder for pimps, traffickers, and abuse survivors to sue women for their abuse. The problem is compounded by French laws, like the prostitution law, that penalize trafficking victims for their own well-being.\n",
            "\n",
            "It was not until the fall of 2013 that the government took the first steps that would lead to a major reform: After 10 years of waiting for change, and in the face of intense pressure from the media and advocacy groups, and by a number of French organizations such as Médecins Sans Frontières and the French Network Against Sexual Violence, the French government passed an act requiring all prostitutes to undergo training on how to protect themselves in case of abuse or violence.\n",
            "\n",
            "This will improve the situation in two ways: First, it will make the police more attentive to the lives of pimps and other traffickers. Second, it will help the French government to make the case for sex work. Both are necessary steps, but they are not the only ways for the French government to change its attitude to trafficking.\n",
            "\n",
            "[100 | 192.88] loss=2.51 avg=2.70\n",
            "[101 | 194.49] loss=2.81 avg=2.71\n",
            "[102 | 196.10] loss=2.46 avg=2.70\n",
            "[103 | 197.71] loss=2.66 avg=2.70\n",
            "[104 | 199.32] loss=2.56 avg=2.70\n",
            "[105 | 200.92] loss=2.71 avg=2.70\n",
            "[106 | 202.52] loss=2.76 avg=2.70\n",
            "[107 | 204.12] loss=2.85 avg=2.70\n",
            "[108 | 205.72] loss=2.56 avg=2.70\n",
            "[109 | 207.33] loss=2.63 avg=2.70\n",
            "[110 | 208.94] loss=2.59 avg=2.70\n",
            "[111 | 210.54] loss=2.75 avg=2.70\n",
            "[112 | 212.14] loss=2.25 avg=2.69\n",
            "[113 | 213.74] loss=2.68 avg=2.69\n",
            "[114 | 215.34] loss=2.54 avg=2.69\n",
            "[115 | 216.95] loss=2.69 avg=2.69\n",
            "[116 | 218.55] loss=2.73 avg=2.69\n",
            "[117 | 220.15] loss=2.81 avg=2.69\n",
            "[118 | 221.75] loss=2.42 avg=2.69\n",
            "[119 | 223.36] loss=2.49 avg=2.69\n",
            "[120 | 224.96] loss=2.70 avg=2.69\n",
            "[121 | 226.55] loss=2.95 avg=2.69\n",
            "[122 | 228.16] loss=2.76 avg=2.69\n",
            "[123 | 229.76] loss=2.71 avg=2.69\n",
            "[124 | 231.36] loss=2.99 avg=2.69\n",
            "[125 | 232.95] loss=2.58 avg=2.69\n",
            "[126 | 234.56] loss=2.53 avg=2.69\n",
            "[127 | 236.15] loss=2.54 avg=2.69\n",
            "[128 | 237.76] loss=2.38 avg=2.68\n",
            "[129 | 239.36] loss=2.69 avg=2.68\n",
            "[130 | 240.96] loss=2.79 avg=2.69\n",
            "[131 | 242.57] loss=2.50 avg=2.68\n",
            "[132 | 244.17] loss=2.65 avg=2.68\n",
            "[133 | 245.77] loss=2.65 avg=2.68\n",
            "[134 | 247.38] loss=2.72 avg=2.68\n",
            "[135 | 248.99] loss=2.72 avg=2.68\n",
            "[136 | 250.60] loss=2.72 avg=2.68\n",
            "[137 | 252.21] loss=2.54 avg=2.68\n",
            "[138 | 253.81] loss=2.56 avg=2.68\n",
            "[139 | 255.43] loss=2.82 avg=2.68\n",
            "[140 | 257.04] loss=2.92 avg=2.69\n",
            "[141 | 258.65] loss=2.59 avg=2.68\n",
            "[142 | 260.27] loss=2.50 avg=2.68\n",
            "[143 | 261.88] loss=2.85 avg=2.68\n",
            "[144 | 263.49] loss=2.53 avg=2.68\n",
            "[145 | 265.11] loss=2.49 avg=2.68\n",
            "[146 | 266.74] loss=2.38 avg=2.68\n",
            "[147 | 268.37] loss=2.57 avg=2.67\n",
            "[148 | 269.99] loss=2.77 avg=2.68\n",
            "[149 | 271.63] loss=2.53 avg=2.67\n",
            "[150 | 273.26] loss=2.45 avg=2.67\n",
            "[151 | 274.89] loss=2.54 avg=2.67\n",
            "[152 | 276.53] loss=2.78 avg=2.67\n",
            "[153 | 278.17] loss=2.71 avg=2.67\n",
            "[154 | 279.82] loss=2.61 avg=2.67\n",
            "[155 | 281.47] loss=2.97 avg=2.67\n",
            "[156 | 283.11] loss=2.34 avg=2.67\n",
            "[157 | 284.77] loss=2.73 avg=2.67\n",
            "[158 | 286.42] loss=2.38 avg=2.67\n",
            "[159 | 288.07] loss=2.86 avg=2.67\n",
            "[160 | 289.72] loss=2.87 avg=2.67\n",
            "[161 | 291.36] loss=2.52 avg=2.67\n",
            "[162 | 293.01] loss=2.76 avg=2.67\n",
            "[163 | 294.65] loss=3.03 avg=2.68\n",
            "[164 | 296.29] loss=2.53 avg=2.67\n",
            "[165 | 297.93] loss=2.70 avg=2.67\n",
            "[166 | 299.56] loss=2.90 avg=2.68\n",
            "[167 | 301.20] loss=2.79 avg=2.68\n",
            "[168 | 302.83] loss=2.97 avg=2.68\n",
            "[169 | 304.47] loss=2.67 avg=2.68\n",
            "[170 | 306.10] loss=2.60 avg=2.68\n",
            "[171 | 307.73] loss=2.63 avg=2.68\n",
            "[172 | 309.36] loss=2.67 avg=2.68\n",
            "[173 | 310.99] loss=2.51 avg=2.68\n",
            "[174 | 312.61] loss=2.61 avg=2.68\n",
            "[175 | 314.25] loss=2.32 avg=2.67\n",
            "[176 | 315.87] loss=2.64 avg=2.67\n",
            "[177 | 317.49] loss=2.71 avg=2.67\n",
            "[178 | 319.12] loss=2.70 avg=2.67\n",
            "[179 | 320.75] loss=2.60 avg=2.67\n",
            "[180 | 322.37] loss=2.41 avg=2.67\n",
            "[181 | 323.99] loss=2.57 avg=2.67\n",
            "[182 | 325.61] loss=2.55 avg=2.67\n",
            "[183 | 327.24] loss=2.35 avg=2.66\n",
            "[184 | 328.86] loss=2.49 avg=2.66\n",
            "[185 | 330.48] loss=2.70 avg=2.66\n",
            "[186 | 332.09] loss=2.56 avg=2.66\n",
            "[187 | 333.72] loss=2.57 avg=2.66\n",
            "[188 | 335.33] loss=2.48 avg=2.66\n",
            "[189 | 336.95] loss=2.44 avg=2.65\n",
            "[190 | 338.58] loss=2.73 avg=2.66\n",
            "[191 | 340.20] loss=3.01 avg=2.66\n",
            "[192 | 341.82] loss=2.97 avg=2.66\n",
            "[193 | 343.44] loss=2.28 avg=2.66\n",
            "[194 | 345.06] loss=2.44 avg=2.66\n",
            "[195 | 346.69] loss=2.73 avg=2.66\n",
            "[196 | 348.31] loss=2.56 avg=2.66\n",
            "[197 | 349.92] loss=2.42 avg=2.65\n",
            "[198 | 351.55] loss=2.61 avg=2.65\n",
            "[199 | 353.17] loss=2.39 avg=2.65\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " cooker, or a pizza oven that can be used to toast pizza, to make a healthy meal in minutes.\n",
            "\n",
            "But there are downsides, too. If you're still learning to cook, the food processor and blender can be an intimidating tool, the way it is now, especially if you're not comfortable with some of the more basic tools that come with cooking. Still, a tool like this could take a lot of practice for some people, especially for someone like me who likes to do dishes with a lot of control.\n",
            "\n",
            "The food processor is designed for use in small kitchens: the larger the kitchen, the smaller the food processor. The unit itself measures less than 1 ½ inches tall and weighs about a pound (the dimensions can be changed in the tool catalog). The only disadvantage? The food processor itself is a heavy metal with a rounded face that isn't designed to handle large batches. But it still can be used to peel vegetables, cook rice, and mix vegetables in smaller batches—all things I did with the unit and will do on the other side.\n",
            "\n",
            "The food processor includes a touchscreen display to let users control the cooking process like no other. I use the device to cook my broccoli and carrot on the stovetop in less than a minute, with the help of a few quick steps of my hand. The unit measures 9 by 7 inches, which is about what I need for any kitchen. At the moment, Amazon lists that it is sold mostly exclusively through the company's Prime service. It costs about $29 for the basic model that Amazon introduced earlier this year or $29.95 for the version with the touch screen—about the same price as a blender, but $30 cheaper than a food processor.\n",
            "\n",
            "The unit also comes with a free trial that will keep the food processor on your shopping list, so you can try cooking with the device before you commit. It's currently only available in the US. Amazon did not respond to a request for comment about whether other countries have started selling it. This may go in part to Amazon's increased focus on Prime customers, since the company has also been able to turn a handful of Prime-eligible items into free food. A few weeks ago, a family of four tried out Amazon-launched rice cookers: A box worth roughly $24.99 sold for $39.49, for instance.\n",
            "\n",
            "One final touch: The Amazon-built app is designed to let you set and see recipes. Once finished, it offers a full preview, so you can review your favorite recipes and see what cooking tips you've picked up along the way. You can even bookmark recipes, but the app only shows recipes as they are being cooked. (To prevent accidental over-cooking, you can also simply close the app when it's finished.) The app can use location data to recommend ingredients based on location, as well as to give you tips on when or where to cook new recipes. (The food processor is not currently on the Amazon store.)\n",
            "\n",
            "It's always nice to have a kitchen device that works well with the cookbook—at least if it can use my own kitchen's power to work on a plate.\n",
            "\n",
            "The Amazon Food Processor is still, as of this writing, available at Amazon.com for $49.96. It also makes some sense that most of the reviews there have been positive, since it's the third device from Amazon to come along this year. Amazon's new kitchen kitchen products, such as the new Prime kitchen line, include cooktops, microwaves, and dryers, among other devices. It's now possible to purchase Amazon-produced cookware in the US using preorders available on Amazon directly via the Food Processor site (though the company hasn't published pricing for the device yet). And at time of writing, it wasn't clear when Amazon would begin shipping the device to US households this winter.\n",
            "\n",
            "Amazon has been making some improvements lately regarding its kitchen products. Last week, the company announced that it will be reducing the time it takes to set up a new cookbook and will soon begin offering it to all customers. It also revealed the addition of Amazon Prime members and a new food-delivery business, and said it's currently testing features such as ordering in advance to avoid any delays. Amazon Prime has recently also been gaining popularity, selling more books and other products through the service and now comes with some additional features, such as an upcoming subscription service that will let customers buy new recipes by the week.\n",
            "\n",
            "The Food Processor has yet to be officially approved as a kitchen appliance by the Federal Trade Commission, but this news could have some retailers, such as MyKEA, excited to be making the device. MyKEA, a company that stores products in its warehouse, said in a statement last week that it had partnered with Amazon on a product called the One. That product, which sells for $49.99, will allow customers to order their ingredients pre-made to use as meals (the company has also expanded its\n",
            "\n",
            "[200 | 377.65] loss=2.23 avg=2.64\n",
            "[201 | 379.30] loss=2.64 avg=2.64\n",
            "[202 | 380.96] loss=2.65 avg=2.64\n",
            "[203 | 382.62] loss=2.47 avg=2.64\n",
            "[204 | 384.28] loss=2.48 avg=2.64\n",
            "[205 | 385.94] loss=2.52 avg=2.64\n",
            "[206 | 387.59] loss=2.55 avg=2.64\n",
            "[207 | 389.26] loss=2.89 avg=2.64\n",
            "[208 | 390.91] loss=2.72 avg=2.64\n",
            "[209 | 392.57] loss=2.59 avg=2.64\n",
            "[210 | 394.23] loss=2.54 avg=2.64\n",
            "[211 | 395.88] loss=2.37 avg=2.64\n",
            "[212 | 397.54] loss=3.14 avg=2.64\n",
            "[213 | 399.20] loss=2.65 avg=2.64\n",
            "[214 | 400.85] loss=2.82 avg=2.64\n",
            "[215 | 402.50] loss=2.93 avg=2.65\n",
            "[216 | 404.15] loss=2.49 avg=2.65\n",
            "[217 | 405.79] loss=2.65 avg=2.65\n",
            "[218 | 407.43] loss=2.37 avg=2.64\n",
            "[219 | 409.07] loss=2.79 avg=2.65\n",
            "[220 | 410.71] loss=2.21 avg=2.64\n",
            "[221 | 412.35] loss=3.01 avg=2.64\n",
            "[222 | 413.99] loss=2.69 avg=2.64\n",
            "[223 | 415.62] loss=2.71 avg=2.65\n",
            "[224 | 417.25] loss=2.62 avg=2.65\n",
            "[225 | 418.89] loss=2.61 avg=2.64\n",
            "[226 | 420.52] loss=2.68 avg=2.65\n",
            "[227 | 422.15] loss=2.38 avg=2.64\n",
            "[228 | 423.79] loss=2.53 avg=2.64\n",
            "[229 | 425.42] loss=2.52 avg=2.64\n",
            "[230 | 427.05] loss=2.51 avg=2.64\n",
            "[231 | 428.67] loss=2.37 avg=2.64\n",
            "[232 | 430.30] loss=2.79 avg=2.64\n",
            "[233 | 431.94] loss=2.58 avg=2.64\n",
            "[234 | 433.57] loss=2.53 avg=2.64\n",
            "[235 | 435.20] loss=2.64 avg=2.64\n",
            "[236 | 436.83] loss=2.74 avg=2.64\n",
            "[237 | 438.46] loss=2.52 avg=2.64\n",
            "[238 | 440.07] loss=2.38 avg=2.63\n",
            "[239 | 441.70] loss=3.04 avg=2.64\n",
            "[240 | 443.33] loss=2.33 avg=2.63\n",
            "[241 | 444.96] loss=2.50 avg=2.63\n",
            "[242 | 446.59] loss=2.64 avg=2.63\n",
            "[243 | 448.22] loss=2.42 avg=2.63\n",
            "[244 | 449.84] loss=2.50 avg=2.63\n",
            "[245 | 451.47] loss=2.37 avg=2.63\n",
            "[246 | 453.09] loss=2.44 avg=2.62\n",
            "[247 | 454.72] loss=2.20 avg=2.62\n",
            "[248 | 456.35] loss=2.36 avg=2.62\n",
            "[249 | 457.98] loss=2.29 avg=2.61\n",
            "[250 | 459.61] loss=2.55 avg=2.61\n",
            "[251 | 461.25] loss=2.77 avg=2.61\n",
            "[252 | 462.90] loss=2.27 avg=2.61\n",
            "[253 | 464.54] loss=2.65 avg=2.61\n",
            "[254 | 466.17] loss=2.69 avg=2.61\n",
            "[255 | 467.82] loss=2.70 avg=2.61\n",
            "[256 | 469.48] loss=2.65 avg=2.61\n",
            "[257 | 471.13] loss=2.59 avg=2.61\n",
            "[258 | 472.79] loss=2.61 avg=2.61\n",
            "[259 | 474.45] loss=2.52 avg=2.61\n",
            "[260 | 476.10] loss=2.59 avg=2.61\n",
            "[261 | 477.77] loss=2.39 avg=2.61\n",
            "[262 | 479.44] loss=2.22 avg=2.60\n",
            "[263 | 481.10] loss=2.65 avg=2.61\n",
            "[264 | 482.76] loss=2.85 avg=2.61\n",
            "[265 | 484.43] loss=2.15 avg=2.60\n",
            "[266 | 486.10] loss=2.62 avg=2.60\n",
            "[267 | 487.77] loss=2.52 avg=2.60\n",
            "[268 | 489.44] loss=2.11 avg=2.60\n",
            "[269 | 491.11] loss=2.65 avg=2.60\n",
            "[270 | 492.78] loss=2.65 avg=2.60\n",
            "[271 | 494.45] loss=2.03 avg=2.59\n",
            "[272 | 496.12] loss=2.39 avg=2.59\n",
            "[273 | 497.78] loss=2.52 avg=2.59\n",
            "[274 | 499.44] loss=2.62 avg=2.59\n",
            "[275 | 501.12] loss=2.13 avg=2.58\n",
            "[276 | 502.79] loss=2.48 avg=2.58\n",
            "[277 | 504.46] loss=2.41 avg=2.58\n",
            "[278 | 506.12] loss=2.87 avg=2.58\n",
            "[279 | 507.78] loss=2.76 avg=2.59\n",
            "[280 | 509.43] loss=2.68 avg=2.59\n",
            "[281 | 511.09] loss=2.60 avg=2.59\n",
            "[282 | 512.74] loss=3.05 avg=2.59\n",
            "[283 | 514.38] loss=2.48 avg=2.59\n",
            "[284 | 516.03] loss=2.33 avg=2.59\n",
            "[285 | 517.67] loss=2.46 avg=2.59\n",
            "[286 | 519.31] loss=2.64 avg=2.59\n",
            "[287 | 520.95] loss=2.65 avg=2.59\n",
            "[288 | 522.60] loss=2.79 avg=2.59\n",
            "[289 | 524.24] loss=2.75 avg=2.59\n",
            "[290 | 525.87] loss=2.72 avg=2.59\n",
            "[291 | 527.50] loss=2.68 avg=2.59\n",
            "[292 | 529.13] loss=2.74 avg=2.60\n",
            "[293 | 530.76] loss=3.10 avg=2.60\n",
            "[294 | 532.40] loss=2.35 avg=2.60\n",
            "[295 | 534.03] loss=2.79 avg=2.60\n",
            "[296 | 535.67] loss=2.47 avg=2.60\n",
            "[297 | 537.30] loss=2.68 avg=2.60\n",
            "[298 | 538.92] loss=2.95 avg=2.60\n",
            "[299 | 540.55] loss=2.80 avg=2.61\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " nationalist city, but also one that is in a state of collapse, with its housing projects, offices, and shops empty.\n",
            "\n",
            "Advertisement\n",
            "\n",
            "One such building was used as a public restroom in March, where the owner allegedly threw a rock at the front door, according to a complaint filed, which went unheard by a spokesperson for the nearby district.\n",
            "\n",
            "In February, the owner, a former soldier, reportedly said that he wanted to move out, but was unable to because the district is illegal under the Indian Civil and Political Rights Act, for not paying rent. The police department eventually arrested him for harassment, with a promise from the district that he would be given a place to live in the town. (They declined this, but did give a permit to him to live in the town.) After months of waiting for an official response, on February 28th, the court sent an official notice of eviction to the owner.\n",
            "\n",
            "Another woman, identified in court documents as \"B\" in documents, said that when she complained to the city, the first thing the authorities said was \"I know the city doesn't have a law in place for this problem but I'll do my best to help you with that later.\" However, the city officials said they had no legal response and refused to let her move in. She also said that the company that owns an employee-housing property, which houses several thousand people, refused to provide an apartment, saying that any tenant could be arrested for violating the town's ordinance on noise.\n",
            "\n",
            "Advertisement\n",
            "\n",
            "Despite this, residents continued to pay rent on building materials, and the owners of the other houses told other residents that they wanted to pay their own way, she said. The two women met with a lawyer, who said that the owner could pay the rent, and that he'd find someone else to do the rent collection but the city council would have to take action. When this didn't work, she said, she contacted the police department.\n",
            "\n",
            "The owners of the buildings decided to move out without evicting anyone, but they said they'd have to evict the people who were not paying—since they had no legal recourse, it seemed.\n",
            "\n",
            "Advertisement\n",
            "\n",
            "One such building, called the \"City House\" by the community, is the largest residential project in the district. It was built in 2000 with funds from the Indian Housing and Urban Development Corporation. The project was part of the district's project to expand the area of the district (one of the city's priorities).\n",
            "\n",
            "Advertisement\n",
            "\n",
            "According to the owner, the owners of the building told her that he would pay the rent and that they'd provide her with a place to live, if she paid her own way. When I asked for a statement on his behalf, the owner declined.\n",
            "\n",
            "Eventually, she moved in. I went to meet her in her flat, which had been broken into only once before I arrived. When I asked about the eviction notice she mentioned that her landlord was a soldier and that he had thrown a rock at her door. She replied that he had done this on several occasions, and that she thought he was trying to kill her.\n",
            "\n",
            "I asked for a statement. She gave one through a translator, which she read out. She also asked that I not name the employer. When I asked her again how she made sure the landlord's family never came to their house, she said when one of her brothers went to Afghanistan and they were separated, she put a picture of her on his Facebook page, and she would take the picture of him or something like that to him and she'd be a good friend. Her brother was also in that army.\n",
            "\n",
            "Advertisement\n",
            "\n",
            "The building was torn down in April, along with the town. A day later I returned to check in with the owner. He agreed to move out if I paid his rent. I went into his house and I told him my story. He said he'd do his best for me, but he'd also have to evict the people who were not paying. I told him we wouldn't be able to pay either.\n",
            "\n",
            "To help, I met with the owner, who told me that the district is illegal here for not paying rent as long as you have proof. I asked if he understood what that meant. He said it meant something. When I asked again if the owner knew it was illegal, he told me it was illegal. He told me he couldn't pay because it was illegal.\n",
            "\n",
            "Advertisement\n",
            "\n",
            "Then I asked why he hadn't done it. He assured me that this was his legal right, and he would do nothing wrong.\n",
            "\n",
            "I asked if he would try to find someone who could work the place and pay. He said to me that he wouldn't understand anyone paying rent, and that, I should learn to understand it so that I could help him, just as the children of military families get a decent education for many years.\n",
            "\n",
            "The owner told us that if he could pay, he would have\n",
            "\n",
            "[300 | 564.67] loss=2.44 avg=2.60\n",
            "[301 | 566.31] loss=2.39 avg=2.60\n",
            "[302 | 567.97] loss=2.53 avg=2.60\n",
            "[303 | 569.61] loss=2.76 avg=2.60\n",
            "[304 | 571.26] loss=2.81 avg=2.60\n",
            "[305 | 572.92] loss=2.31 avg=2.60\n",
            "[306 | 574.58] loss=2.54 avg=2.60\n",
            "[307 | 576.24] loss=2.68 avg=2.60\n",
            "[308 | 577.90] loss=2.42 avg=2.60\n",
            "[309 | 579.56] loss=2.46 avg=2.60\n",
            "[310 | 581.23] loss=2.58 avg=2.60\n",
            "[311 | 582.90] loss=2.75 avg=2.60\n",
            "[312 | 584.56] loss=2.37 avg=2.60\n",
            "[313 | 586.22] loss=2.57 avg=2.60\n",
            "[314 | 587.89] loss=2.69 avg=2.60\n",
            "[315 | 589.55] loss=2.57 avg=2.60\n",
            "[316 | 591.21] loss=1.96 avg=2.59\n",
            "[317 | 592.88] loss=2.37 avg=2.59\n",
            "[318 | 594.54] loss=2.59 avg=2.59\n",
            "[319 | 596.21] loss=3.05 avg=2.59\n",
            "[320 | 597.86] loss=2.43 avg=2.59\n",
            "[321 | 599.52] loss=2.44 avg=2.59\n",
            "[322 | 601.18] loss=2.51 avg=2.59\n",
            "[323 | 602.84] loss=2.26 avg=2.59\n",
            "[324 | 604.50] loss=2.78 avg=2.59\n",
            "[325 | 606.15] loss=2.35 avg=2.59\n",
            "[326 | 607.80] loss=2.51 avg=2.58\n",
            "[327 | 609.45] loss=2.18 avg=2.58\n",
            "[328 | 611.10] loss=2.50 avg=2.58\n",
            "[329 | 612.74] loss=2.64 avg=2.58\n",
            "[330 | 614.37] loss=2.59 avg=2.58\n",
            "[331 | 616.02] loss=2.21 avg=2.58\n",
            "[332 | 617.65] loss=2.54 avg=2.58\n",
            "[333 | 619.29] loss=2.80 avg=2.58\n",
            "[334 | 620.93] loss=2.37 avg=2.58\n",
            "[335 | 622.57] loss=2.71 avg=2.58\n",
            "[336 | 624.21] loss=2.61 avg=2.58\n",
            "[337 | 625.84] loss=2.70 avg=2.58\n",
            "[338 | 627.49] loss=2.46 avg=2.58\n",
            "[339 | 629.12] loss=2.70 avg=2.58\n",
            "[340 | 630.75] loss=2.54 avg=2.58\n",
            "[341 | 632.38] loss=2.70 avg=2.58\n",
            "[342 | 634.01] loss=2.67 avg=2.58\n",
            "[343 | 635.64] loss=2.21 avg=2.58\n",
            "[344 | 637.26] loss=2.55 avg=2.58\n",
            "[345 | 638.89] loss=2.60 avg=2.58\n",
            "[346 | 640.53] loss=2.44 avg=2.58\n",
            "[347 | 642.15] loss=2.48 avg=2.57\n",
            "[348 | 643.79] loss=2.22 avg=2.57\n",
            "[349 | 645.42] loss=2.63 avg=2.57\n",
            "[350 | 647.05] loss=2.45 avg=2.57\n",
            "[351 | 648.67] loss=2.41 avg=2.57\n",
            "[352 | 650.30] loss=2.50 avg=2.57\n",
            "[353 | 651.93] loss=2.92 avg=2.57\n",
            "[354 | 653.56] loss=2.37 avg=2.57\n",
            "[355 | 655.20] loss=2.41 avg=2.57\n",
            "[356 | 656.83] loss=2.67 avg=2.57\n",
            "[357 | 658.47] loss=2.20 avg=2.57\n",
            "[358 | 660.11] loss=2.47 avg=2.56\n",
            "[359 | 661.75] loss=2.61 avg=2.56\n",
            "[360 | 663.39] loss=2.68 avg=2.57\n",
            "[361 | 665.03] loss=2.45 avg=2.56\n",
            "[362 | 666.68] loss=2.31 avg=2.56\n",
            "[363 | 668.34] loss=2.43 avg=2.56\n",
            "[364 | 670.00] loss=2.31 avg=2.56\n",
            "[365 | 671.65] loss=2.56 avg=2.56\n",
            "[366 | 673.32] loss=2.34 avg=2.56\n",
            "[367 | 674.98] loss=2.16 avg=2.55\n",
            "[368 | 676.66] loss=2.53 avg=2.55\n",
            "[369 | 678.33] loss=2.94 avg=2.56\n",
            "[370 | 680.00] loss=2.55 avg=2.56\n",
            "[371 | 681.67] loss=2.70 avg=2.56\n",
            "[372 | 683.34] loss=2.40 avg=2.56\n",
            "[373 | 685.01] loss=2.49 avg=2.55\n",
            "[374 | 686.68] loss=2.30 avg=2.55\n",
            "[375 | 688.35] loss=2.51 avg=2.55\n",
            "[376 | 690.02] loss=2.09 avg=2.55\n",
            "[377 | 691.69] loss=2.28 avg=2.54\n",
            "[378 | 693.36] loss=2.23 avg=2.54\n",
            "[379 | 695.03] loss=2.26 avg=2.54\n",
            "[380 | 696.70] loss=2.79 avg=2.54\n",
            "[381 | 698.37] loss=2.27 avg=2.54\n",
            "[382 | 700.04] loss=2.32 avg=2.54\n",
            "[383 | 701.70] loss=2.64 avg=2.54\n",
            "[384 | 703.37] loss=2.60 avg=2.54\n",
            "[385 | 705.03] loss=2.43 avg=2.54\n",
            "[386 | 706.69] loss=2.35 avg=2.53\n",
            "[387 | 708.35] loss=2.21 avg=2.53\n",
            "[388 | 710.00] loss=2.38 avg=2.53\n",
            "[389 | 711.65] loss=2.43 avg=2.53\n",
            "[390 | 713.30] loss=2.61 avg=2.53\n",
            "[391 | 714.95] loss=2.23 avg=2.53\n",
            "[392 | 716.59] loss=3.23 avg=2.53\n",
            "[393 | 718.24] loss=2.82 avg=2.54\n",
            "[394 | 719.88] loss=2.88 avg=2.54\n",
            "[395 | 721.52] loss=2.38 avg=2.54\n",
            "[396 | 723.16] loss=2.58 avg=2.54\n",
            "[397 | 724.79] loss=2.73 avg=2.54\n",
            "[398 | 726.42] loss=1.93 avg=2.53\n",
            "[399 | 728.05] loss=2.49 avg=2.53\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " Brooklyn, and the first person whose car has ever driven at least 50 miles on the New York City subway.\n",
            "\n",
            "The ride is a kind of trip of a lifetime in a car of an average size. For anyone outside Manhattan, the subway will feel a little like a dream—even before you hop on it, you feel like you're out in nature. The city itself becomes a living room, as your eyes follow the trains around the block, then into the tunnels, then up to the next stop.\n",
            "\n",
            "Ticket prices are a little absurd, but they're also the best price of the week. And they mean a lot to people.\n",
            "\n",
            "That's because the subway system helps people around the world get there. Today I'm at the second stop of the new Met-Roux system, which begins at the old Uniondale station, and heads through Manhattan. You can either take a train up Uniondale Ave., along the Hudson River to Penn Station, then take a bus to the train station, or take the subway—all in one ticket. But because this subway line will connect to a larger one running under Brooklyn, the line is also known as the subway extension. Met-Roux serves customers in Manhattan, Brooklyn, Queens, Staten Island, the Bronx, and Chicago.\n",
            "\n",
            "There's a difference between a subway car and a subway train. A train is a vehicle that carries passengers, while subway cars are things that can be pulled along in the shape of a train. There are two types of subway cars, articulated and trackless. A subway train, which runs on a track, is just a regular train that doesn't have an overhead compartment, like a subway car. But the subway is also a car, and for some reason, it has the potential to move faster than our feet can travel.\n",
            "\n",
            "The first Met-Roux train arrives in Brooklyn on September 19th and heads toward Manhattan at 1 p.m., arriving at a station called Brooklyn Battery Park at 2:42 p.m. It then heads towards Manhattan again at 4:26 p.m. For the second-last of the first three stops, the train heads north to Manhattan, arriving just before 6:45 a.m. It then runs through the borough of Staten Island on the way to Manhattan. After that, the train only has to go past Brooklyn Battery Park once. The train stops at JFK International just before 9 a.m. The train will then be back on the same route its last time in Brooklyn.\n",
            "\n",
            "The new train operates at a profit, making roughly $3,300 a minute, all to the MTA. For every trip made on the Met-Roux system, a passenger makes $2.20. But the service doesn't just make money: The cost of transporting customers doesn't seem to add up: When Met-Roux passengers board a car at a station, Met-Roux staff walk over to it and help them out. (There is no room for luggage in the car, after all—that's in the basement.) It's a bit similar to what passengers at some airports get to do: When boarding the plane, they enter through the front door, and a staff member takes their luggage to the back—a move that, when the plane leaves, the luggage is transferred automatically. When passengers board a train, they're supposed to do the same. But this time, they have the car—and they need to help someone with luggage on the platform.\n",
            "\n",
            "It takes less than an hour to get from the Brooklyn Battery Park subway station to JFK, and it takes less than an hour to get there from Manhattan. For the next 6.28 miles, the subway car will travel at a maximum of 62 mph—just enough to get to the stop at Newark Liberty, which is located at the base of a 45-story building. For the last 13 miles, passengers will be traveling at 57 mph. At the end, passengers will ride for around 12.6 miles from Brooklyn and have come nearly 11 miles in the span of about 3 ½ hours. But since the last stop is a big stop, the car leaves Manhattan after around 18,500 miles—that's the maximum distance one can be carried in a subway car.\n",
            "\n",
            "At its heart, Met-Roux is a system of subways and commuter railcars. But it's also a transit management company that connects transit riders with employers and stores where they can buy goods, like products from the local Metro station.\n",
            "\n",
            "When I met up with the Met-Roux team, I asked Met-Roux CEO Josh Voth how the company is changing its business model. \"The key feature we're trying to achieve is better business performance, which translates into more business value, therefore more business volume,\" Voth said. It turns out that one of the main benefits from using fewer cars is increased passenger traffic. This is one of the features that makes Met-Roux\n",
            "\n",
            "[400 | 752.26] loss=2.54 avg=2.53\n",
            "[401 | 753.90] loss=2.91 avg=2.54\n",
            "[402 | 755.54] loss=2.92 avg=2.54\n",
            "[403 | 757.18] loss=2.33 avg=2.54\n",
            "[404 | 758.83] loss=2.73 avg=2.54\n",
            "[405 | 760.47] loss=2.31 avg=2.54\n",
            "[406 | 762.12] loss=2.18 avg=2.54\n",
            "[407 | 763.78] loss=2.71 avg=2.54\n",
            "[408 | 765.43] loss=2.54 avg=2.54\n",
            "[409 | 767.09] loss=2.30 avg=2.54\n",
            "[410 | 768.74] loss=2.46 avg=2.53\n",
            "[411 | 770.40] loss=2.87 avg=2.54\n",
            "[412 | 772.06] loss=2.23 avg=2.53\n",
            "[413 | 773.73] loss=2.36 avg=2.53\n",
            "[414 | 775.40] loss=2.74 avg=2.53\n",
            "[415 | 777.07] loss=2.19 avg=2.53\n",
            "[416 | 778.75] loss=2.83 avg=2.53\n",
            "[417 | 780.42] loss=2.60 avg=2.54\n",
            "[418 | 782.09] loss=2.16 avg=2.53\n",
            "[419 | 783.76] loss=2.32 avg=2.53\n",
            "[420 | 785.44] loss=3.13 avg=2.54\n",
            "[421 | 787.10] loss=2.45 avg=2.53\n",
            "[422 | 788.77] loss=2.52 avg=2.53\n",
            "[423 | 790.44] loss=3.04 avg=2.54\n",
            "[424 | 792.12] loss=2.72 avg=2.54\n",
            "[425 | 793.77] loss=2.68 avg=2.54\n",
            "[426 | 795.44] loss=2.37 avg=2.54\n",
            "[427 | 797.10] loss=2.47 avg=2.54\n",
            "[428 | 798.76] loss=2.56 avg=2.54\n",
            "[429 | 800.41] loss=2.34 avg=2.54\n",
            "[430 | 802.07] loss=2.44 avg=2.54\n",
            "[431 | 803.73] loss=2.53 avg=2.54\n",
            "[432 | 805.38] loss=2.46 avg=2.54\n",
            "[433 | 807.03] loss=2.64 avg=2.54\n",
            "[434 | 808.67] loss=2.37 avg=2.54\n",
            "[435 | 810.32] loss=2.51 avg=2.54\n",
            "[436 | 811.97] loss=2.47 avg=2.54\n",
            "[437 | 813.61] loss=2.48 avg=2.53\n",
            "[438 | 815.25] loss=2.11 avg=2.53\n",
            "[439 | 816.89] loss=2.60 avg=2.53\n",
            "[440 | 818.53] loss=2.49 avg=2.53\n",
            "[441 | 820.17] loss=2.63 avg=2.53\n",
            "[442 | 821.80] loss=3.33 avg=2.54\n",
            "[443 | 823.43] loss=2.45 avg=2.54\n",
            "[444 | 825.06] loss=2.83 avg=2.54\n",
            "[445 | 826.70] loss=2.22 avg=2.54\n",
            "[446 | 828.33] loss=2.62 avg=2.54\n",
            "[447 | 829.96] loss=2.48 avg=2.54\n",
            "[448 | 831.59] loss=2.52 avg=2.54\n",
            "[449 | 833.22] loss=2.66 avg=2.54\n",
            "[450 | 834.85] loss=2.53 avg=2.54\n",
            "[451 | 836.48] loss=2.77 avg=2.54\n",
            "[452 | 838.11] loss=2.07 avg=2.54\n",
            "[453 | 839.74] loss=2.33 avg=2.53\n",
            "[454 | 841.37] loss=2.68 avg=2.54\n",
            "[455 | 842.99] loss=2.09 avg=2.53\n",
            "[456 | 844.63] loss=2.42 avg=2.53\n",
            "[457 | 846.26] loss=2.10 avg=2.53\n",
            "[458 | 847.89] loss=2.43 avg=2.53\n",
            "[459 | 849.52] loss=2.56 avg=2.53\n",
            "[460 | 851.15] loss=2.40 avg=2.52\n",
            "[461 | 852.78] loss=2.79 avg=2.53\n",
            "[462 | 854.42] loss=2.48 avg=2.53\n",
            "[463 | 856.05] loss=2.33 avg=2.52\n",
            "[464 | 857.69] loss=2.17 avg=2.52\n",
            "[465 | 859.33] loss=2.76 avg=2.52\n",
            "[466 | 860.98] loss=2.78 avg=2.53\n",
            "[467 | 862.63] loss=2.23 avg=2.52\n",
            "[468 | 864.28] loss=2.69 avg=2.52\n",
            "[469 | 865.94] loss=2.39 avg=2.52\n",
            "[470 | 867.59] loss=2.18 avg=2.52\n",
            "[471 | 869.26] loss=2.40 avg=2.52\n",
            "[472 | 870.93] loss=2.78 avg=2.52\n",
            "[473 | 872.59] loss=2.27 avg=2.52\n",
            "[474 | 874.26] loss=2.35 avg=2.52\n",
            "[475 | 875.93] loss=2.19 avg=2.51\n",
            "[476 | 877.60] loss=2.40 avg=2.51\n",
            "[477 | 879.26] loss=2.05 avg=2.51\n",
            "[478 | 880.94] loss=2.40 avg=2.51\n",
            "[479 | 882.61] loss=2.82 avg=2.51\n",
            "[480 | 884.28] loss=3.29 avg=2.52\n",
            "[481 | 885.95] loss=2.70 avg=2.52\n",
            "[482 | 887.62] loss=2.29 avg=2.52\n",
            "[483 | 889.29] loss=2.46 avg=2.52\n",
            "[484 | 890.96] loss=2.38 avg=2.52\n",
            "[485 | 892.63] loss=2.49 avg=2.52\n",
            "[486 | 894.30] loss=2.47 avg=2.51\n",
            "[487 | 895.97] loss=1.99 avg=2.51\n",
            "[488 | 897.63] loss=2.32 avg=2.51\n",
            "[489 | 899.29] loss=2.26 avg=2.51\n",
            "[490 | 900.95] loss=2.39 avg=2.50\n",
            "[491 | 902.61] loss=2.30 avg=2.50\n",
            "[492 | 904.27] loss=1.85 avg=2.50\n",
            "[493 | 905.92] loss=2.83 avg=2.50\n",
            "[494 | 907.57] loss=2.89 avg=2.50\n",
            "[495 | 909.22] loss=1.89 avg=2.50\n",
            "[496 | 910.86] loss=2.93 avg=2.50\n",
            "[497 | 912.51] loss=2.51 avg=2.50\n",
            "[498 | 914.15] loss=2.42 avg=2.50\n",
            "[499 | 915.79] loss=2.75 avg=2.50\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " And and all the things that would be hard to do at a college campus. They started to work in the backyards of these neighborhoods, on dirt and weeds, to be able to put up some walls around the neighborhood, in order to make it more stable—and they worked hard. For the most part, I was just teaching them to work, and then seeing them grow up and become doctors. But I was not satisfied with just teaching them to work. I thought, What if I wanted to help them take over and make this neighborhood safe—to rebuild it? I thought they would be better suited to that.\n",
            "\n",
            "BJ: The neighborhoods in Atlanta were particularly tough, because they had been devastated by the country-and-western migrations that swept through the mid-20th-century American South. What was the feeling like when you noticed, in your new neighborhood of Atlanta, African Americans were doing much better?\n",
            "\n",
            "Laurie Brown: There was always something like that in Atlanta, when the blacks moved there. I remember seeing this new sign, along with this statement: \"Atlanta no longer has poverty.\" I think it was because the city hadn't experienced that kind of devastation.\n",
            "\n",
            "I was living out of a van in my spare time. The van was a white Ford Van Coop. I would pick up a girl at a busstop and head to the corner and have sex with her. Or, if I didn't feel like having sex, I would say, \"Let's just drive,\" or pull her over and have a little sexual encounter. The whole time we were both watching the news; she was at the grocery store, I was at work. The whole time we were having sexual experiences, I could be watching the news. I'm saying, \"Let me sit in your car. I think I might have something to tell you.\" I just wanted to see what it was like.\n",
            "\n",
            "Then, in the '80s, the poverty rate dropped very quickly. The area around the bus stop was one of that big losers. They figured out how to give out free bus passes and then had a few black businesses sprouting up, like the old Atlanta Radio Cafe, where they would give out free radio stations that weren't in radio stations. By '94, there were half a million African Americans living in Atlanta. All the stores were closed. Black-owned businesses opened their doors. There were the convenience stores, and you could go to a convenience store for coffee and have it shipped over your home. You could buy whatever you wanted on the bus or take the bus anywhere.\n",
            "\n",
            "Now, you can go down to any convenience store and you can buy anything from a T-shirt to a box of Legos. As far as the housing situation is concerned, the only problem was that I was paying the median income. There were all these middle-aged black families who couldn't afford to buy a house, in neighborhoods I grew up in, trying to figure out what to do next.\n",
            "\n",
            "BJ: You talk about black-owned businesses sprouting in the backyards of communities—did you notice that for the first time? You were living off the land and in neighborhoods not where residents lived.\n",
            "\n",
            "Laurie Brown: I didn't notice that. I lived in two houses that were next to each other—I was also working in my ex-girlfriend's business. When I was working here, I used to go down and get groceries in the mornings and come back the next day and pick up the stuff. I had to work in the backyards of neighbors to pay the rent. It wasn't until I moved to Atlanta from Austin that I realized that I had to have a place to live in this area because I did so little working inside.\n",
            "\n",
            "I went to a bunch of the farmers markets because there were only two of us to share a car. I would come in, drive around the block, and pick out a place for sale. When I left, we stopped in the city; I liked the city. I knew people, and I felt safe. The same goes with the street vendors, who make everything they can. I had this friend, and we started selling music at the farmers market. I felt like a royalty.\n",
            "\n",
            "BJ: You describe this as being in the backyards of a neighborhood.\n",
            "\n",
            "Laurie Brown: Yeah. The neighborhood was just a short stretch of dirt road. It was all black, but we got to know each other—we went to the same church, went to the same community center, and we talked to each other. You could call us brothers or sisters, you could call us friends. But at the grocery store, we felt like we could be any two of us and be accepted.\n",
            "\n",
            "BJ: Who was your friend?\n",
            "\n",
            "Laurie Brown: I think I was a waitress. She was my boss. She was a little older than I was. So she took me to all\n",
            "\n",
            "[500 | 940.03] loss=2.51 avg=2.50\n",
            "[501 | 941.66] loss=2.02 avg=2.50\n",
            "[502 | 943.30] loss=2.02 avg=2.49\n",
            "[503 | 944.94] loss=2.90 avg=2.50\n",
            "[504 | 946.57] loss=1.61 avg=2.49\n",
            "[505 | 948.22] loss=2.40 avg=2.49\n",
            "[506 | 949.86] loss=2.65 avg=2.49\n",
            "[507 | 951.50] loss=2.55 avg=2.49\n",
            "[508 | 953.14] loss=2.47 avg=2.49\n",
            "[509 | 954.78] loss=2.27 avg=2.49\n",
            "[510 | 956.43] loss=2.29 avg=2.48\n",
            "[511 | 958.07] loss=2.28 avg=2.48\n",
            "[512 | 959.72] loss=2.72 avg=2.49\n",
            "[513 | 961.37] loss=2.39 avg=2.48\n",
            "[514 | 963.03] loss=2.15 avg=2.48\n",
            "[515 | 964.68] loss=2.29 avg=2.48\n",
            "[516 | 966.34] loss=2.50 avg=2.48\n",
            "[517 | 968.00] loss=2.53 avg=2.48\n",
            "[518 | 969.67] loss=2.40 avg=2.48\n",
            "[519 | 971.35] loss=2.54 avg=2.48\n",
            "[520 | 973.02] loss=2.99 avg=2.48\n",
            "[521 | 974.69] loss=2.25 avg=2.48\n",
            "[522 | 976.36] loss=2.24 avg=2.48\n",
            "[523 | 978.04] loss=2.53 avg=2.48\n",
            "[524 | 979.71] loss=2.28 avg=2.48\n",
            "[525 | 981.39] loss=2.17 avg=2.48\n",
            "[526 | 983.06] loss=2.38 avg=2.47\n",
            "[527 | 984.73] loss=2.22 avg=2.47\n",
            "[528 | 986.40] loss=2.17 avg=2.47\n",
            "[529 | 988.08] loss=2.61 avg=2.47\n",
            "[530 | 989.75] loss=2.14 avg=2.47\n",
            "[531 | 991.42] loss=2.00 avg=2.46\n",
            "[532 | 993.09] loss=2.22 avg=2.46\n",
            "[533 | 994.75] loss=2.44 avg=2.46\n",
            "[534 | 996.42] loss=2.14 avg=2.46\n",
            "[535 | 998.07] loss=1.96 avg=2.45\n",
            "[536 | 999.72] loss=2.25 avg=2.45\n",
            "[537 | 1001.38] loss=2.75 avg=2.45\n",
            "[538 | 1003.04] loss=2.20 avg=2.45\n",
            "[539 | 1004.69] loss=2.05 avg=2.45\n",
            "[540 | 1006.34] loss=2.68 avg=2.45\n",
            "[541 | 1007.98] loss=2.87 avg=2.45\n",
            "[542 | 1009.62] loss=2.34 avg=2.45\n",
            "[543 | 1011.25] loss=2.35 avg=2.45\n",
            "[544 | 1012.89] loss=2.14 avg=2.45\n",
            "[545 | 1014.53] loss=2.17 avg=2.44\n",
            "[546 | 1016.17] loss=1.47 avg=2.43\n",
            "[547 | 1017.81] loss=2.37 avg=2.43\n",
            "[548 | 1019.44] loss=2.43 avg=2.43\n",
            "[549 | 1021.07] loss=2.21 avg=2.43\n",
            "[550 | 1022.70] loss=2.86 avg=2.44\n",
            "[551 | 1024.33] loss=2.70 avg=2.44\n",
            "[552 | 1025.96] loss=2.24 avg=2.44\n",
            "[553 | 1027.60] loss=2.01 avg=2.43\n",
            "[554 | 1029.23] loss=2.22 avg=2.43\n",
            "[555 | 1030.86] loss=2.01 avg=2.43\n",
            "[556 | 1032.49] loss=2.75 avg=2.43\n",
            "[557 | 1034.12] loss=2.29 avg=2.43\n",
            "[558 | 1035.75] loss=2.34 avg=2.43\n",
            "[559 | 1037.39] loss=1.31 avg=2.42\n",
            "[560 | 1039.02] loss=2.31 avg=2.41\n",
            "[561 | 1040.65] loss=2.93 avg=2.42\n",
            "[562 | 1042.28] loss=2.57 avg=2.42\n",
            "[563 | 1043.92] loss=2.50 avg=2.42\n",
            "[564 | 1045.56] loss=2.61 avg=2.42\n",
            "[565 | 1047.19] loss=2.25 avg=2.42\n",
            "[566 | 1048.83] loss=2.65 avg=2.42\n",
            "[567 | 1050.47] loss=2.28 avg=2.42\n",
            "[568 | 1052.11] loss=2.61 avg=2.42\n",
            "[569 | 1053.75] loss=2.46 avg=2.43\n",
            "[570 | 1055.39] loss=2.67 avg=2.43\n",
            "[571 | 1057.04] loss=2.87 avg=2.43\n",
            "[572 | 1058.68] loss=2.52 avg=2.43\n",
            "[573 | 1060.32] loss=2.06 avg=2.43\n",
            "[574 | 1061.96] loss=2.15 avg=2.43\n",
            "[575 | 1063.62] loss=2.72 avg=2.43\n",
            "[576 | 1065.28] loss=2.35 avg=2.43\n",
            "[577 | 1066.94] loss=3.23 avg=2.44\n",
            "[578 | 1068.60] loss=2.28 avg=2.44\n",
            "[579 | 1070.27] loss=2.57 avg=2.44\n",
            "[580 | 1071.94] loss=2.86 avg=2.44\n",
            "[581 | 1073.62] loss=2.33 avg=2.44\n",
            "[582 | 1075.29] loss=2.25 avg=2.44\n",
            "[583 | 1076.96] loss=2.31 avg=2.44\n",
            "[584 | 1078.63] loss=2.30 avg=2.44\n",
            "[585 | 1080.30] loss=2.32 avg=2.43\n",
            "[586 | 1081.97] loss=2.71 avg=2.44\n",
            "[587 | 1083.66] loss=2.05 avg=2.43\n",
            "[588 | 1085.33] loss=2.70 avg=2.44\n",
            "[589 | 1087.00] loss=2.30 avg=2.43\n",
            "[590 | 1088.67] loss=2.47 avg=2.43\n",
            "[591 | 1090.34] loss=2.01 avg=2.43\n",
            "[592 | 1092.01] loss=2.57 avg=2.43\n",
            "[593 | 1093.68] loss=1.51 avg=2.42\n",
            "[594 | 1095.35] loss=2.70 avg=2.43\n",
            "[595 | 1097.01] loss=2.47 avg=2.43\n",
            "[596 | 1098.68] loss=2.98 avg=2.43\n",
            "[597 | 1100.34] loss=2.07 avg=2.43\n",
            "[598 | 1102.00] loss=2.64 avg=2.43\n",
            "[599 | 1103.66] loss=2.28 avg=2.43\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " the city. It would have required the city to spend an estimated $1 billion on the plan, she said.\n",
            "\n",
            "It turns out the City Council voted to keep the train station in service, too. That didn't sit well with the board. Some worried that the new rail line would cause another traffic jam and cause more traffic jams.\n",
            "\n",
            "But even if the city had paid for the entire $1 billion in capital costs, those funds would still have been a drop in the bucket compared to all the money the city will have to throw at a new downtown train station. Construction will take six to eight years, and the funding for the project will run out in 30 years, as the new stations get older and the transit system is gradually phased out of many of its older trains.\n",
            "\n",
            "The reason it never got built is the same reason it was never built in other cities: the money was way too expensive. The new cost was about 50 percent higher than the cost of building a track and signal replacement, according to a recent report by the Downtown Denver Partnership and the Colorado Transportation Partners Alliance.\n",
            "\n",
            "The new cost of the construction will be paid for largely by fares, according to a recent analysis by the Colorado Transportation Partnership and the Colorado Transportation Partners Alliance. The transit agency doesn't own the train track or signals; the city does. When the state was looking for money to build the project, Denver asked for money through the Transportation Trust Fund, which will eventually replace most state funding.\n",
            "\n",
            "If the federal money weren't available, the rail line probably would have been built, with the city getting a 50 cents to the dollar subsidy, according to the report. It wouldn't have mattered because state funding was set to drop by $40 million between 2017 and 2027, according to the report, due for release Wednesday.\n",
            "\n",
            "The transit agency will eventually have to foot the bill for the new station, said Chris Schlueter, executive director of the CPPA, which represented the city in a federal lawsuit challenging the state's decision to give away a massive portion of federal money. Denver is already facing the same fiscal issues of the rest of the Metro system. Without the new train station, it will have to do even more work to provide service for the growing number of people who now live within walking and biking distance, a task that will take years. The city is also expected to pay into a fund to help pay for the new track.\n",
            "\n",
            "The city also needs more money for new transit improvements, including a new bus rapid transit line and bus rapid transit lanes, with more buses, buses and buses, Schlueter said.\n",
            "\n",
            "Denver will also have to look for $20 million to continue to improve parking in downtown areas. That money was already in the state budget for the Denver Urban Transportation District and would likely be available if the transit plan gets approved for passage in the upcoming session. The state and the city will need to find at least $5 million a year to keep the bus rapid transit lanes and bus rapid transit lanes along the rail line active. The state and city can then figure out how to pay for the other improvements, which would be estimated over the next five to seven years, the CPPA's Schlueter said.\n",
            "\n",
            "The question of how much money and how it will be spent will remain open, Schlueter said. When the project is done, the city will have the money it needs to be successful, he stated.\n",
            "\n",
            "Despite the city's concerns about the price of the project, the Metro board supported building the train station. Its members wanted the new facility to be a \"tremendous success,\" the report states. The metro board members voted unanimously that \"the project can and should serve as a model for developing new transit investments across the region.\"\n",
            "\n",
            "In the last decades, the federal government has made major investments in rail transit, most recently with new tracks at the Denver Transit Center and the Southwest Corridor Extension. While such investments help create new ridership during rush times, they also lead to traffic jams. Metro has already had more than 150 days in which traffic was worse than normal between 9 a.m. and midnight—an even longer period than in 2013.\n",
            "\n",
            "Denver has had its share of train-line closures over the years, as construction has dragged on. Most recently, a major north-south tunnel boring machine was dismantled Oct. 20, 2013, at the beginning of the busiest stretch of construction in years, a major north-south tunnel bored in July 2014 and a major north-south tunnel boring machine shut down for more than a decade for the work. Construction in the next few months could start on the second phase of the Colorado Avenue tunnel.\n",
            "\n",
            "But even the longest-running of Denver's projects, which will have to be repaired and reopened frequently, is an exception in that the tracks are the most important part of the project. Most of the tracks are there to link the existing tracks to the north and south lines. The tracks are also the reason the tracks have sat\n",
            "\n",
            "[600 | 1127.83] loss=2.48 avg=2.43\n",
            "[601 | 1129.47] loss=2.51 avg=2.43\n",
            "[602 | 1131.11] loss=2.24 avg=2.43\n",
            "[603 | 1132.74] loss=2.62 avg=2.43\n",
            "[604 | 1134.37] loss=2.39 avg=2.43\n",
            "[605 | 1136.01] loss=1.16 avg=2.42\n",
            "[606 | 1137.65] loss=2.65 avg=2.42\n",
            "[607 | 1139.28] loss=2.30 avg=2.42\n",
            "[608 | 1140.91] loss=2.50 avg=2.42\n",
            "[609 | 1142.54] loss=2.63 avg=2.42\n",
            "[610 | 1144.18] loss=2.67 avg=2.42\n",
            "[611 | 1145.82] loss=2.57 avg=2.42\n",
            "[612 | 1147.46] loss=2.54 avg=2.43\n",
            "[613 | 1149.10] loss=2.95 avg=2.43\n",
            "[614 | 1150.74] loss=2.27 avg=2.43\n",
            "[615 | 1152.38] loss=2.89 avg=2.43\n",
            "[616 | 1154.03] loss=2.48 avg=2.43\n",
            "[617 | 1155.67] loss=2.48 avg=2.43\n",
            "[618 | 1157.32] loss=2.62 avg=2.44\n",
            "[619 | 1158.96] loss=2.08 avg=2.43\n",
            "[620 | 1160.61] loss=2.38 avg=2.43\n",
            "[621 | 1162.27] loss=2.62 avg=2.43\n",
            "[622 | 1163.93] loss=2.52 avg=2.44\n",
            "[623 | 1165.58] loss=2.19 avg=2.43\n",
            "[624 | 1167.25] loss=2.60 avg=2.43\n",
            "[625 | 1168.91] loss=2.41 avg=2.43\n",
            "[626 | 1170.57] loss=2.74 avg=2.44\n",
            "[627 | 1172.24] loss=2.42 avg=2.44\n",
            "[628 | 1173.91] loss=2.32 avg=2.44\n",
            "[629 | 1175.58] loss=2.68 avg=2.44\n",
            "[630 | 1177.25] loss=2.42 avg=2.44\n",
            "[631 | 1178.92] loss=2.88 avg=2.44\n",
            "[632 | 1180.59] loss=3.00 avg=2.45\n",
            "[633 | 1182.27] loss=2.88 avg=2.45\n",
            "[634 | 1183.95] loss=2.22 avg=2.45\n",
            "[635 | 1185.62] loss=2.29 avg=2.45\n",
            "[636 | 1187.29] loss=2.54 avg=2.45\n",
            "[637 | 1188.95] loss=2.23 avg=2.45\n",
            "[638 | 1190.63] loss=2.22 avg=2.44\n",
            "[639 | 1192.30] loss=2.00 avg=2.44\n",
            "[640 | 1193.97] loss=2.05 avg=2.44\n",
            "[641 | 1195.63] loss=2.38 avg=2.44\n",
            "[642 | 1197.28] loss=2.54 avg=2.44\n",
            "[643 | 1198.95] loss=2.56 avg=2.44\n",
            "[644 | 1200.61] loss=2.28 avg=2.44\n",
            "[645 | 1202.27] loss=2.58 avg=2.44\n",
            "[646 | 1203.92] loss=2.63 avg=2.44\n",
            "[647 | 1205.57] loss=2.35 avg=2.44\n",
            "[648 | 1207.22] loss=2.17 avg=2.44\n",
            "[649 | 1208.87] loss=2.44 avg=2.44\n",
            "[650 | 1210.52] loss=2.46 avg=2.44\n",
            "[651 | 1212.17] loss=2.31 avg=2.44\n",
            "[652 | 1213.82] loss=2.59 avg=2.44\n",
            "[653 | 1215.46] loss=2.15 avg=2.43\n",
            "[654 | 1217.10] loss=2.23 avg=2.43\n",
            "[655 | 1218.73] loss=2.31 avg=2.43\n",
            "[656 | 1220.36] loss=2.66 avg=2.43\n",
            "[657 | 1222.00] loss=2.34 avg=2.43\n",
            "[658 | 1223.63] loss=2.37 avg=2.43\n",
            "[659 | 1225.27] loss=2.62 avg=2.43\n",
            "[660 | 1226.90] loss=2.53 avg=2.43\n",
            "[661 | 1228.52] loss=2.86 avg=2.44\n",
            "[662 | 1230.16] loss=2.33 avg=2.44\n",
            "[663 | 1231.78] loss=2.35 avg=2.44\n",
            "[664 | 1233.41] loss=2.57 avg=2.44\n",
            "[665 | 1235.04] loss=2.26 avg=2.44\n",
            "[666 | 1236.68] loss=2.40 avg=2.44\n",
            "[667 | 1238.31] loss=2.15 avg=2.43\n",
            "[668 | 1239.94] loss=2.33 avg=2.43\n",
            "[669 | 1241.56] loss=2.35 avg=2.43\n",
            "[670 | 1243.19] loss=2.28 avg=2.43\n",
            "[671 | 1244.82] loss=2.14 avg=2.43\n",
            "[672 | 1246.44] loss=2.23 avg=2.43\n",
            "[673 | 1248.08] loss=3.01 avg=2.43\n",
            "[674 | 1249.72] loss=2.23 avg=2.43\n",
            "[675 | 1251.35] loss=2.03 avg=2.42\n",
            "[676 | 1252.99] loss=2.14 avg=2.42\n",
            "[677 | 1254.63] loss=2.29 avg=2.42\n",
            "[678 | 1256.28] loss=2.24 avg=2.42\n",
            "[679 | 1257.92] loss=2.41 avg=2.42\n",
            "[680 | 1259.57] loss=1.98 avg=2.41\n",
            "[681 | 1261.22] loss=1.71 avg=2.41\n",
            "[682 | 1262.87] loss=2.09 avg=2.40\n",
            "[683 | 1264.53] loss=2.07 avg=2.40\n",
            "[684 | 1266.18] loss=3.15 avg=2.41\n",
            "[685 | 1267.85] loss=2.38 avg=2.41\n",
            "[686 | 1269.51] loss=2.11 avg=2.41\n",
            "[687 | 1271.18] loss=2.35 avg=2.40\n",
            "[688 | 1272.85] loss=2.80 avg=2.41\n",
            "[689 | 1274.52] loss=2.23 avg=2.41\n",
            "[690 | 1276.19] loss=2.42 avg=2.41\n",
            "[691 | 1277.86] loss=1.89 avg=2.40\n",
            "[692 | 1279.53] loss=2.40 avg=2.40\n",
            "[693 | 1281.20] loss=1.13 avg=2.39\n",
            "[694 | 1282.87] loss=2.47 avg=2.39\n",
            "[695 | 1284.54] loss=2.72 avg=2.39\n",
            "[696 | 1286.20] loss=2.04 avg=2.39\n",
            "[697 | 1287.88] loss=2.11 avg=2.39\n",
            "[698 | 1289.55] loss=2.39 avg=2.39\n",
            "[699 | 1291.22] loss=3.01 avg=2.39\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " declining business growth, including the closure of some smaller outlets such as The Kitchen, and job losses in some of those areas, including its parent company, Dunkin Donuts, one of the people said. Those people spoke on the condition of anonymity as Dunkin's details the situation in greater detail than the company typically does. (Dunkin Donuts did not send me an official statement addressing these rumors. You can read the whole statement below.)\n",
            "\n",
            "One example that illustrates the challenges that Dunkin Donuts will face comes in the form of technology. At the company's headquarters in Brooklyn, Brooklyn, the majority of the offices look like a garage: boxes in corners, rows of desks, and a staircase leading out to what one person called the \"universe of Dunkin Donuts.\" That's because the company has recently decided to focus its efforts on new markets in the U.S., one person said. Dunkin Donuts, which began as an offshoot in 1939, is the world's fifth-largest bakery company, according to Euromonitor. The company's profits are mostly fueled by food sales, and its bakery business accounted for $40 billion in revenue in 2016.\n",
            "\n",
            "Another area of Dunkin Donuts growth will be in the retail division, where the company plans to sell a range of items from its signature orange Donut Cups and Dunkin Donuts T-shirts to its blue, circular-shaped Donut Bars (also known as the \"L.L. Bean\") that are sold at grocery stores. Customers can purchase the cups and bar as a single package that contains 20 of the cups or as four boxes (the boxes cost $4 each). The cups and bars, which look like colorful plastic cups with a Donut-inspired outline, have six different colors that pop to various levels of orange, black, yellow, green, and red that correspond to their sweet or savory toppings. These Donut Cups feature a sticker that says, \"Our Donut Cups are Better than Most, So We Made Them This Way,\" and the T-shirts sport a \"Best Of Doughnut Shops\" logo with a white circle inside the circle to represent the company itself. According to one person familiar with the plans, the Donut Cups will also be available in various flavors.\n",
            "\n",
            "In terms of the company's business model, a large portion of Dunkin' Donuts customers are millennial-age customers. According to the company, the bulk of the company's growth is coming from millennials, and Dunkin Donuts, which will be renamed Dunkin Sugar, has been investing heavily in marketing its sweets and bars from the youth category. But what that means in the real world is unclear, because the majority of customers are not young enough to have a relationship with the brand.\n",
            "\n",
            "One other possibility is that the company is making progress toward rebranding as Dunkin Donuts—the company is rebranding its entire business model, including its iconic Donut Cup and Dunkin Donuts cups, among other things—which may result in better advertising and brand awareness, more favorable business conditions for the company, and perhaps a larger profit.\n",
            "\n",
            "But the most likely scenario for Dunkin' Donuts is that it wants to cut costs in order to focus on the freshness and convenience of its fresh donuts, while keeping the profits and customer loyalty in its cake-baking business. That could require it to cut more staff (it has close to 1,000 employees) and to shut down locations (such as The Kitchen in Brooklyn, NY), and there are reports that Dunkin' Donuts is considering the sale of the Donut Bar company to a higher bidder.\n",
            "\n",
            "Those closures create uncertainty, particularly at a time when the real estate market in particular is in a crisis condition, and when Dunkin and its owners are being closely watched around the sector for any signs of imminent trouble. The company announced earlier this year that it would move several locations out of New York City, and said it would phase out its Donut Cups. That leaves its only remaining location, along the Brooklyn waterfront, to move.\n",
            "\n",
            "In the long run, though, Dunkin Donuts has a very difficult choice to make: keep investing in its brand, or close down places like the Brooklyn location. These closures may help the business model, but they hurt customers.\n",
            "\n",
            "As the company has tried to focus on fresher donuts, it has eliminated the location by location, eliminating the option of ordering a large bag using a fork and knife. This is a change that, for someone who has owned a Donut Bar, makes sense. But it won't help the bottom lines of customers coming in wanting a way to get a fresh bag of donuts without the headache of leaving the coffee shop. The location was not located within walking distance to food trucks and other food trucks; instead, they had to walk right past the Donut Bar—a convenience that no customer wants to have to use a Dunkin Donuts location.\n",
            "\n",
            "\n",
            "[700 | 1315.43] loss=2.43 avg=2.39\n",
            "[701 | 1317.08] loss=1.99 avg=2.39\n",
            "[702 | 1318.72] loss=2.03 avg=2.39\n",
            "[703 | 1320.36] loss=2.22 avg=2.38\n",
            "[704 | 1321.99] loss=2.58 avg=2.39\n",
            "[705 | 1323.63] loss=2.32 avg=2.39\n",
            "[706 | 1325.27] loss=2.55 avg=2.39\n",
            "[707 | 1326.90] loss=2.45 avg=2.39\n",
            "[708 | 1328.54] loss=2.30 avg=2.39\n",
            "[709 | 1330.17] loss=2.03 avg=2.38\n",
            "[710 | 1331.81] loss=1.21 avg=2.37\n",
            "[711 | 1333.44] loss=2.48 avg=2.37\n",
            "[712 | 1335.06] loss=2.42 avg=2.37\n",
            "[713 | 1336.69] loss=2.13 avg=2.37\n",
            "[714 | 1338.32] loss=2.49 avg=2.37\n",
            "[715 | 1339.95] loss=2.31 avg=2.37\n",
            "[716 | 1341.59] loss=2.49 avg=2.37\n",
            "[717 | 1343.22] loss=2.28 avg=2.37\n",
            "[718 | 1344.85] loss=1.84 avg=2.37\n",
            "[719 | 1346.48] loss=2.56 avg=2.37\n",
            "[720 | 1348.11] loss=2.44 avg=2.37\n",
            "[721 | 1349.75] loss=2.10 avg=2.37\n",
            "[722 | 1351.38] loss=2.68 avg=2.37\n",
            "[723 | 1353.03] loss=1.94 avg=2.36\n",
            "[724 | 1354.67] loss=2.34 avg=2.36\n",
            "[725 | 1356.31] loss=2.41 avg=2.37\n",
            "[726 | 1357.96] loss=2.15 avg=2.36\n",
            "[727 | 1359.61] loss=2.26 avg=2.36\n",
            "[728 | 1361.27] loss=2.29 avg=2.36\n",
            "[729 | 1362.93] loss=2.35 avg=2.36\n",
            "[730 | 1364.60] loss=2.26 avg=2.36\n",
            "[731 | 1366.27] loss=2.21 avg=2.36\n",
            "[732 | 1367.94] loss=1.89 avg=2.35\n",
            "[733 | 1369.61] loss=2.57 avg=2.36\n",
            "[734 | 1371.28] loss=2.12 avg=2.35\n",
            "[735 | 1372.96] loss=2.72 avg=2.36\n",
            "[736 | 1374.63] loss=2.33 avg=2.36\n",
            "[737 | 1376.30] loss=2.72 avg=2.36\n",
            "[738 | 1377.97] loss=2.16 avg=2.36\n",
            "[739 | 1379.64] loss=2.39 avg=2.36\n",
            "[740 | 1381.31] loss=1.95 avg=2.35\n",
            "[741 | 1382.98] loss=2.18 avg=2.35\n",
            "[742 | 1384.65] loss=2.18 avg=2.35\n",
            "[743 | 1386.32] loss=2.38 avg=2.35\n",
            "[744 | 1387.99] loss=2.51 avg=2.35\n",
            "[745 | 1389.66] loss=2.53 avg=2.36\n",
            "[746 | 1391.33] loss=2.97 avg=2.36\n",
            "[747 | 1392.99] loss=2.56 avg=2.36\n",
            "[748 | 1394.65] loss=2.53 avg=2.36\n",
            "[749 | 1396.31] loss=2.55 avg=2.37\n",
            "[750 | 1397.97] loss=2.29 avg=2.37\n",
            "[751 | 1399.62] loss=2.18 avg=2.36\n",
            "[752 | 1401.27] loss=2.21 avg=2.36\n",
            "[753 | 1402.92] loss=2.15 avg=2.36\n",
            "[754 | 1404.56] loss=2.17 avg=2.36\n",
            "[755 | 1406.21] loss=2.00 avg=2.35\n",
            "[756 | 1407.85] loss=2.27 avg=2.35\n",
            "[757 | 1409.49] loss=2.54 avg=2.36\n",
            "[758 | 1411.13] loss=2.45 avg=2.36\n",
            "[759 | 1412.77] loss=2.00 avg=2.35\n",
            "[760 | 1414.41] loss=2.67 avg=2.36\n",
            "[761 | 1416.05] loss=2.37 avg=2.36\n",
            "[762 | 1417.68] loss=2.00 avg=2.35\n",
            "[763 | 1419.31] loss=2.61 avg=2.36\n",
            "[764 | 1420.94] loss=0.82 avg=2.34\n",
            "[765 | 1422.57] loss=2.68 avg=2.34\n",
            "[766 | 1424.20] loss=2.30 avg=2.34\n",
            "[767 | 1425.84] loss=2.28 avg=2.34\n",
            "[768 | 1427.47] loss=2.49 avg=2.34\n",
            "[769 | 1429.11] loss=2.20 avg=2.34\n",
            "[770 | 1430.74] loss=2.07 avg=2.34\n",
            "[771 | 1432.37] loss=2.11 avg=2.34\n",
            "[772 | 1434.00] loss=2.24 avg=2.34\n",
            "[773 | 1435.63] loss=2.28 avg=2.34\n",
            "[774 | 1437.26] loss=1.94 avg=2.33\n",
            "[775 | 1438.90] loss=2.21 avg=2.33\n",
            "[776 | 1440.53] loss=2.48 avg=2.33\n",
            "[777 | 1442.17] loss=2.57 avg=2.33\n",
            "[778 | 1443.81] loss=1.95 avg=2.33\n",
            "[779 | 1445.45] loss=2.23 avg=2.33\n",
            "[780 | 1447.07] loss=1.97 avg=2.33\n",
            "[781 | 1448.72] loss=2.05 avg=2.32\n",
            "[782 | 1450.36] loss=2.36 avg=2.32\n",
            "[783 | 1452.00] loss=2.50 avg=2.33\n",
            "[784 | 1453.65] loss=2.42 avg=2.33\n",
            "[785 | 1455.30] loss=2.11 avg=2.32\n",
            "[786 | 1456.95] loss=2.42 avg=2.33\n",
            "[787 | 1458.61] loss=2.34 avg=2.33\n",
            "[788 | 1460.26] loss=2.62 avg=2.33\n",
            "[789 | 1461.93] loss=2.30 avg=2.33\n",
            "[790 | 1463.59] loss=2.34 avg=2.33\n",
            "[791 | 1465.24] loss=2.05 avg=2.33\n",
            "[792 | 1466.91] loss=2.14 avg=2.32\n",
            "[793 | 1468.59] loss=1.88 avg=2.32\n",
            "[794 | 1470.26] loss=2.14 avg=2.32\n",
            "[795 | 1471.93] loss=2.16 avg=2.32\n",
            "[796 | 1473.61] loss=2.47 avg=2.32\n",
            "[797 | 1475.28] loss=1.28 avg=2.31\n",
            "[798 | 1476.96] loss=2.27 avg=2.31\n",
            "[799 | 1478.65] loss=2.19 avg=2.31\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "ortia was founded in 1991 and has more than 250 employees.\n",
            "\n",
            "—With assistance by Matthew Fiedler, and John Kelly, and Elizabeth Killeen, and Erin O'Toole, and Maryann Koster, and Jessica Y. Sun, and Elizabeth Kwong, and Julia Fung, from The New York Times.\n",
            "\n",
            "READ MORE:\n",
            "\n",
            "In Florida, immigrants are being sent back to countries with high rates of violence, neglect, and exploitation, and with little prospect of success. In the U.S. Virgin Islands, hundreds of young people are waiting to hear the results of their immigration petitions, many of whom are undocumented immigrants with legal status — families like the Cotaes families.\n",
            "\n",
            "By Melissa Boteach, Aaron K. Davis, and James L. Eller, and M. Reisman Chang, from the Washington Post;\n",
            "\n",
            "A month after Hurricane Maria struck Puerto Rico, Cotaes, a father and son, took in a friend who had borrowed money and found a house to rent in the Bronx. The friend asked for help from a social worker. The friend called one of Cotaes and his girlfriend who asked the couple to stay with the social worker for two months so they could work through the emotional tumult of Hurricane Maria. They waited, the girl said, and found her parents could not relocate. “At least in the Bronx, life was good,” her mother told me as they sat in the living room of their home in Ponce City, just north of New York’s biggest city, New York. “I don’t regret it.”\n",
            "\n",
            "This post appears courtesy of CityLab.\n",
            "\n",
            "\n",
            "\n",
            "The United States has a shortage of skilled workers, particularly for high-tech industries. Last year, the Bureau of Labor Statistics announced, there were 7,350 new STEM jobs added, or those in computer science, data visualization, and mathematics. The number of jobs in engineering and mathematics grew by 5,600. These are industries, after all, that have long been seen as the places to be; their employment boomed around the rise of productivity and technology.\n",
            "\n",
            "\n",
            "The United States has also a shortage of nurses, physical therapists, and dental hygienists. While they made up less than 1 percent of the workforce nationally, they accounted for 12 percent of the job growth, according to the Bureau of Labor Statistics. The shortage may have more to do with demographics than health care—there are so few of them, in fact, that there are at least 20,000 registered nurses nationwide, and another 13,000 dentists, physical therapists, and nurses in all. In other words, there are so many, that the country has a mismatch of talent between the very few and the many.\n",
            "\n",
            "At the same time, the government is increasing funding for the physical-health corps (which coordinates medical care for the poor, people with disabilities, and the mentally ill), and investing in mental-health care, which provides better access to mental-health services.\n",
            "\n",
            "The mismatch between the number of medical doctors and the number of nurses might be the leading cause of the occupational death rate—and one of the most pressing policy-makers could have a hand in fixing.\n",
            "\n",
            "The United States has one of the few countries that doesn’t guarantee paid maternity leave or paid paternity leave. The country has one of the few countries that does not guarantee sick and vacation days. The country has one of the few countries that does not guarantee paid family and medical leave. And, unfortunately, it has one of the most punitive—and underfunded—policies for paid sick leave.\n",
            "\n",
            "There is no law in the United States mandating paid sick leave. And while more men are working in certain areas of health and human-services work—for instance, social workers and family mediators—they are still a pretty small percentage of the workforce. Moreover, many of the jobs that are being filled by people who do have advanced degrees and advanced skills are low-paying, and don’t require advanced skills or advanced training. These jobs, after all, tend to pay well, and have a high turnover rate.\n",
            "\n",
            "In the past, paid sick leave was one policy area of particular interest for women. Back in 2012, then-Sen. Kirsten Gillibrand introduced the Payscale Back to Work Act as a way to help women, who lag behind men in many other areas of life, get back on their feet.\n",
            "\n",
            "The legislation would have created three categories of paid sick leave, along with incentives to encourage companies to provide paid time off. It would have required employers with 500 or more workers or more to offer paid sick leave; it would have required employers to offer paid mental-health leave; and it would have required employers to provide paid time off for children.\n",
            "\n",
            "The legislation has largely been met with mixed results. The National Women's Law Center, for instance, has said that women who take\n",
            "\n",
            "[800 | 1502.94] loss=2.18 avg=2.30\n",
            "[801 | 1504.59] loss=2.12 avg=2.30\n",
            "[802 | 1506.24] loss=2.50 avg=2.30\n",
            "[803 | 1507.89] loss=2.29 avg=2.30\n",
            "[804 | 1509.54] loss=1.98 avg=2.30\n",
            "[805 | 1511.18] loss=2.34 avg=2.30\n",
            "[806 | 1512.83] loss=2.23 avg=2.30\n",
            "[807 | 1514.46] loss=2.40 avg=2.30\n",
            "[808 | 1516.10] loss=2.38 avg=2.30\n",
            "[809 | 1517.73] loss=2.53 avg=2.30\n",
            "[810 | 1519.37] loss=2.04 avg=2.30\n",
            "[811 | 1521.00] loss=2.06 avg=2.30\n",
            "[812 | 1522.64] loss=1.99 avg=2.30\n",
            "[813 | 1524.27] loss=2.10 avg=2.29\n",
            "[814 | 1525.91] loss=2.11 avg=2.29\n",
            "[815 | 1527.53] loss=1.95 avg=2.29\n",
            "[816 | 1529.16] loss=2.10 avg=2.29\n",
            "[817 | 1530.79] loss=2.22 avg=2.29\n",
            "[818 | 1532.42] loss=2.34 avg=2.29\n",
            "[819 | 1534.06] loss=2.27 avg=2.29\n",
            "[820 | 1535.69] loss=2.67 avg=2.29\n",
            "[821 | 1537.31] loss=2.50 avg=2.29\n",
            "[822 | 1538.94] loss=2.94 avg=2.30\n",
            "[823 | 1540.58] loss=2.42 avg=2.30\n",
            "[824 | 1542.21] loss=1.93 avg=2.30\n",
            "[825 | 1543.85] loss=2.91 avg=2.30\n",
            "[826 | 1545.49] loss=2.02 avg=2.30\n",
            "[827 | 1547.12] loss=2.12 avg=2.30\n",
            "[828 | 1548.76] loss=2.67 avg=2.30\n",
            "[829 | 1550.40] loss=2.78 avg=2.31\n",
            "[830 | 1552.04] loss=2.44 avg=2.31\n",
            "[831 | 1553.69] loss=2.23 avg=2.31\n",
            "[832 | 1555.33] loss=2.30 avg=2.31\n",
            "[833 | 1556.99] loss=0.67 avg=2.29\n",
            "[834 | 1558.64] loss=2.39 avg=2.29\n",
            "[835 | 1560.29] loss=2.29 avg=2.29\n",
            "[836 | 1561.95] loss=2.08 avg=2.29\n",
            "[837 | 1563.61] loss=2.44 avg=2.29\n",
            "[838 | 1565.26] loss=2.33 avg=2.29\n",
            "[839 | 1566.92] loss=2.24 avg=2.29\n",
            "[840 | 1568.58] loss=2.81 avg=2.30\n",
            "[841 | 1570.25] loss=2.32 avg=2.30\n",
            "[842 | 1571.92] loss=2.00 avg=2.29\n",
            "[843 | 1573.58] loss=2.29 avg=2.29\n",
            "[844 | 1575.25] loss=1.98 avg=2.29\n",
            "[845 | 1576.92] loss=2.39 avg=2.29\n",
            "[846 | 1578.59] loss=2.34 avg=2.29\n",
            "[847 | 1580.27] loss=2.22 avg=2.29\n",
            "[848 | 1581.93] loss=2.24 avg=2.29\n",
            "[849 | 1583.60] loss=1.99 avg=2.29\n",
            "[850 | 1585.28] loss=2.12 avg=2.29\n",
            "[851 | 1586.94] loss=2.62 avg=2.29\n",
            "[852 | 1588.61] loss=2.28 avg=2.29\n",
            "[853 | 1590.28] loss=2.60 avg=2.29\n",
            "[854 | 1591.95] loss=2.49 avg=2.29\n",
            "[855 | 1593.62] loss=2.28 avg=2.29\n",
            "[856 | 1595.29] loss=2.05 avg=2.29\n",
            "[857 | 1596.95] loss=2.21 avg=2.29\n",
            "[858 | 1598.61] loss=2.68 avg=2.29\n",
            "[859 | 1600.27] loss=2.62 avg=2.30\n",
            "[860 | 1601.93] loss=2.09 avg=2.30\n",
            "[861 | 1603.57] loss=2.06 avg=2.29\n",
            "[862 | 1605.22] loss=2.76 avg=2.30\n",
            "[863 | 1606.88] loss=2.60 avg=2.30\n",
            "[864 | 1608.51] loss=2.20 avg=2.30\n",
            "[865 | 1610.16] loss=1.92 avg=2.30\n",
            "[866 | 1611.80] loss=2.30 avg=2.30\n",
            "[867 | 1613.45] loss=1.81 avg=2.29\n",
            "[868 | 1615.09] loss=2.50 avg=2.29\n",
            "[869 | 1616.73] loss=2.11 avg=2.29\n",
            "[870 | 1618.36] loss=2.14 avg=2.29\n",
            "[871 | 1620.00] loss=2.66 avg=2.29\n",
            "[872 | 1621.63] loss=2.28 avg=2.29\n",
            "[873 | 1623.26] loss=2.10 avg=2.29\n",
            "[874 | 1624.90] loss=1.85 avg=2.29\n",
            "[875 | 1626.53] loss=2.74 avg=2.29\n",
            "[876 | 1628.16] loss=1.86 avg=2.29\n",
            "[877 | 1629.78] loss=2.00 avg=2.29\n",
            "[878 | 1631.41] loss=2.22 avg=2.28\n",
            "[879 | 1633.04] loss=2.63 avg=2.29\n",
            "[880 | 1634.67] loss=1.91 avg=2.28\n",
            "[881 | 1636.29] loss=2.70 avg=2.29\n",
            "[882 | 1637.93] loss=2.37 avg=2.29\n",
            "[883 | 1639.56] loss=2.28 avg=2.29\n",
            "[884 | 1641.19] loss=2.46 avg=2.29\n",
            "[885 | 1642.82] loss=2.23 avg=2.29\n",
            "[886 | 1644.45] loss=2.19 avg=2.29\n",
            "[887 | 1646.09] loss=2.74 avg=2.29\n",
            "[888 | 1647.72] loss=2.39 avg=2.29\n",
            "[889 | 1649.36] loss=2.44 avg=2.30\n",
            "[890 | 1651.00] loss=2.22 avg=2.30\n",
            "[891 | 1652.64] loss=2.43 avg=2.30\n",
            "[892 | 1654.29] loss=2.84 avg=2.30\n",
            "[893 | 1655.93] loss=2.11 avg=2.30\n",
            "[894 | 1657.57] loss=1.87 avg=2.30\n",
            "[895 | 1659.22] loss=1.76 avg=2.29\n",
            "[896 | 1660.88] loss=2.27 avg=2.29\n",
            "[897 | 1662.54] loss=2.73 avg=2.29\n",
            "[898 | 1664.20] loss=2.62 avg=2.30\n",
            "[899 | 1665.87] loss=1.91 avg=2.29\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " relationship and trust in an institution of high importance; that is, the public trust in a financial institution, such as a bank. This trust is especially important for institutions that serve as the backbone of a modern economy–be they small businesses, large companies, for-profit firms, or for-profit institutions or businesses. Banks are the bedrock of the American economy and thus need an institution of high reliability. That is, institutions must be trusted with large amounts of public funding. In a recent survey, more than 80% of Americans agreed that the U.S. should have a large and stable government–that is, a government that will not be a threat to the future of the marketplace, business, or the personal.\n",
            "\n",
            "Unfortunately, the Trump Administration appears to have abandoned this principle. Last week, in its first executive order, Donald Trump reversed an Obama Administration regulation that had directed U.S. banks to avoid risky lending practices, in order to protect big institutions. Moreover, in its second executive order, issued at the start of this month, the Trump Administration rescinded the Obama-era rules regulating derivatives. The policy decisions announced last month will be a source of consternation for the global financial markets, and potentially for the banks themselves. In order to protect their own companies and the public, and presumably those of investors, such firms are likely to look for new financial partners.\n",
            "\n",
            "If Trump is serious about protecting the financial markets, his first order of business should be a review of the relationship between institutions and consumers.\n",
            "\n",
            "I am reminded of the story of the old woman in the market. Everyone loves that old lady, she is always buying. The old woman is so loved that she will put her neck in the window and let the birds sing for her. Everyone, even the women making her money, wants to take her for a walk in the park. But sometimes she stops and thinks about her bill of fare, or the time when she used to stand alone at the register, so early in the morning, and pay the person waiting next to her. What should she do, give her a smile? She should say, thank you, but I have a lot of other things to do. The customer, or the buyer? The old woman can't bear to stare into her heart, and goes off to shop. She has been doing so for years. But, the day the Trump Administration said she can't stare in the window, things went horribly, horribly wrong.\n",
            "\n",
            "Here, too, Trump needs to take a close look at the relationship between the banks and Americans. The Department of Treasury has made a number of policy decisions designed to reduce the number of Americans reliant on a few big banks. The most obvious is regulation of the financial industry itself. But the Trump Administration should ask whether America is still as dependable as it thinks it is. The Trump Administration has done all it can to distance itself from Wall Street; but what it has done most to reduce the dependence of Americans on big banks is to take steps to diminish the role of America's largest companies, in banking, in general. These actions are more likely to benefit those firms than the country as a whole.\n",
            "\n",
            "As for the president, he should consider the huge, sometimes perverse incentives he is given to increase the financial sector, whether it is his desire to keep the stock market above $30,000, or to make the government much bigger, or to make banks even bigger, by passing new legislation that permits their merger, or for that matter, by passing an even bigger legislation that disallows it. How can a Trump Administration ensure that a new banking regulation will be passed or that there is no future of such regulations? The only way to see it is through the eyes of a financial behemoth like The New York Times. That means checking on all the big companies, then deciding if they will be merged or broken up or left entirely to their own devices.\n",
            "\n",
            "This story began on the eve of the first anniversary of the crisis, and could end only in the short term. But as we watch with horror, markets have lost nearly every aspect of their former glories. It might well take a very long time for the government to find out if the biggest banks are in a financial collapse, and decide if something needs to change. Or The New York Times might just decide the country, at a stroke, is ready to move on from The Economist and The New York Times.\n",
            "\n",
            "This column does not necessarily reflect the opinion of the editorial board or Bloomberg LP and its owners.\n",
            "\n",
            "To contact the author of this story:\n",
            "\n",
            "Max Fisher at mmfisher3 at aol.com\n",
            "\n",
            "To contact the editor responsible the picture:\n",
            "\n",
            "Alex Bradley at alexmbradford @ aol.com<|endoftext|>Welcome to the fifth and last installment as I look at our first ever Super Bowl viewing party. I wanted to take time to thank every one of you who have been an integral part of helping us promote The Super Bowl in order to date.\n",
            "\n",
            "[900 | 1690.13] loss=2.21 avg=2.29\n",
            "[901 | 1691.79] loss=2.03 avg=2.29\n",
            "[902 | 1693.46] loss=2.44 avg=2.29\n",
            "[903 | 1695.11] loss=2.48 avg=2.29\n",
            "[904 | 1696.77] loss=1.96 avg=2.29\n",
            "[905 | 1698.43] loss=2.66 avg=2.29\n",
            "[906 | 1700.08] loss=1.76 avg=2.29\n",
            "[907 | 1701.73] loss=2.11 avg=2.29\n",
            "[908 | 1703.37] loss=2.04 avg=2.28\n",
            "[909 | 1705.00] loss=1.98 avg=2.28\n",
            "[910 | 1706.65] loss=2.50 avg=2.28\n",
            "[911 | 1708.29] loss=2.01 avg=2.28\n",
            "[912 | 1709.93] loss=2.09 avg=2.28\n",
            "[913 | 1711.56] loss=2.02 avg=2.28\n",
            "[914 | 1713.19] loss=1.76 avg=2.27\n",
            "[915 | 1714.83] loss=2.58 avg=2.27\n",
            "[916 | 1716.47] loss=2.09 avg=2.27\n",
            "[917 | 1718.10] loss=0.64 avg=2.26\n",
            "[918 | 1719.73] loss=1.98 avg=2.25\n",
            "[919 | 1721.36] loss=1.96 avg=2.25\n",
            "[920 | 1722.99] loss=2.70 avg=2.25\n",
            "[921 | 1724.62] loss=2.50 avg=2.26\n",
            "[922 | 1726.24] loss=2.52 avg=2.26\n",
            "[923 | 1727.88] loss=1.75 avg=2.25\n",
            "[924 | 1729.51] loss=2.06 avg=2.25\n",
            "[925 | 1731.14] loss=2.91 avg=2.26\n",
            "[926 | 1732.77] loss=1.82 avg=2.26\n",
            "[927 | 1734.40] loss=2.09 avg=2.25\n",
            "[928 | 1736.03] loss=2.88 avg=2.26\n",
            "[929 | 1737.67] loss=2.55 avg=2.26\n",
            "[930 | 1739.30] loss=2.55 avg=2.27\n",
            "[931 | 1740.93] loss=1.87 avg=2.26\n",
            "[932 | 1742.57] loss=2.12 avg=2.26\n",
            "[933 | 1744.20] loss=1.96 avg=2.26\n",
            "[934 | 1745.84] loss=2.48 avg=2.26\n",
            "[935 | 1747.48] loss=1.98 avg=2.26\n",
            "[936 | 1749.11] loss=1.83 avg=2.25\n",
            "[937 | 1750.75] loss=1.61 avg=2.25\n",
            "[938 | 1752.40] loss=2.48 avg=2.25\n",
            "[939 | 1754.05] loss=2.18 avg=2.25\n",
            "[940 | 1755.70] loss=2.53 avg=2.25\n",
            "[941 | 1757.36] loss=2.44 avg=2.25\n",
            "[942 | 1759.02] loss=2.00 avg=2.25\n",
            "[943 | 1760.68] loss=2.14 avg=2.25\n",
            "[944 | 1762.34] loss=2.29 avg=2.25\n",
            "[945 | 1763.99] loss=2.17 avg=2.25\n",
            "[946 | 1765.66] loss=2.22 avg=2.25\n",
            "[947 | 1767.33] loss=1.96 avg=2.25\n",
            "[948 | 1769.00] loss=2.39 avg=2.25\n",
            "[949 | 1770.67] loss=2.48 avg=2.25\n",
            "[950 | 1772.34] loss=2.18 avg=2.25\n",
            "[951 | 1774.02] loss=2.25 avg=2.25\n",
            "[952 | 1775.70] loss=1.89 avg=2.24\n",
            "[953 | 1777.38] loss=2.69 avg=2.25\n",
            "[954 | 1779.06] loss=2.85 avg=2.26\n",
            "[955 | 1780.73] loss=2.26 avg=2.26\n",
            "[956 | 1782.41] loss=2.08 avg=2.25\n",
            "[957 | 1784.08] loss=2.14 avg=2.25\n",
            "[958 | 1785.75] loss=2.19 avg=2.25\n",
            "[959 | 1787.42] loss=1.99 avg=2.25\n",
            "[960 | 1789.09] loss=2.03 avg=2.25\n",
            "[961 | 1790.76] loss=1.93 avg=2.24\n",
            "[962 | 1792.42] loss=2.11 avg=2.24\n",
            "[963 | 1794.08] loss=2.01 avg=2.24\n",
            "[964 | 1795.74] loss=1.37 avg=2.23\n",
            "[965 | 1797.39] loss=2.92 avg=2.24\n",
            "[966 | 1799.05] loss=2.01 avg=2.24\n",
            "[967 | 1800.70] loss=1.78 avg=2.23\n",
            "[968 | 1802.36] loss=2.04 avg=2.23\n",
            "[969 | 1804.01] loss=1.77 avg=2.22\n",
            "[970 | 1805.65] loss=2.34 avg=2.23\n",
            "[971 | 1807.29] loss=2.21 avg=2.23\n",
            "[972 | 1808.93] loss=1.93 avg=2.22\n",
            "[973 | 1810.57] loss=2.41 avg=2.22\n",
            "[974 | 1812.21] loss=1.78 avg=2.22\n",
            "[975 | 1813.85] loss=2.90 avg=2.23\n",
            "[976 | 1815.49] loss=2.38 avg=2.23\n",
            "[977 | 1817.13] loss=2.11 avg=2.23\n",
            "[978 | 1818.76] loss=2.28 avg=2.23\n",
            "[979 | 1820.39] loss=2.78 avg=2.23\n",
            "[980 | 1822.02] loss=2.25 avg=2.23\n",
            "[981 | 1823.66] loss=1.81 avg=2.23\n",
            "[982 | 1825.29] loss=2.29 avg=2.23\n",
            "[983 | 1826.92] loss=2.48 avg=2.23\n",
            "[984 | 1828.55] loss=2.24 avg=2.23\n",
            "[985 | 1830.18] loss=1.95 avg=2.23\n",
            "[986 | 1831.81] loss=2.32 avg=2.23\n",
            "[987 | 1833.44] loss=1.79 avg=2.23\n",
            "[988 | 1835.07] loss=2.30 avg=2.23\n",
            "[989 | 1836.71] loss=2.56 avg=2.23\n",
            "[990 | 1838.34] loss=2.31 avg=2.23\n",
            "[991 | 1839.97] loss=2.29 avg=2.23\n",
            "[992 | 1841.60] loss=2.79 avg=2.24\n",
            "[993 | 1843.24] loss=2.26 avg=2.24\n",
            "[994 | 1844.88] loss=1.75 avg=2.23\n",
            "[995 | 1846.52] loss=1.97 avg=2.23\n",
            "[996 | 1848.16] loss=2.45 avg=2.23\n",
            "[997 | 1849.80] loss=1.99 avg=2.23\n",
            "[998 | 1851.44] loss=2.12 avg=2.23\n",
            "[999 | 1853.09] loss=2.22 avg=2.23\n",
            "Ran 1010 steps.\n",
            "Saving checkpoint/atlantic_business_345/model-1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axNGwoKZ8UGF",
        "colab_type": "code",
        "outputId": "8222697e-a7ec-4b06-dc4f-99db59ff94a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "## technology essays training  - with 345\n",
        "!PYTHONPATH=src ./train.py --dataset /content/all_technology.txt --run_name 'atlantic_technology_345' --model_name '345M'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0627 16:18:07.355815 140453592807296 deprecation_wrapper.py:119] From /content/gpt-2/src/model.py:147: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
            "\n",
            "W0627 16:18:07.364850 140453592807296 deprecation_wrapper.py:119] From /content/gpt-2/src/memory_saving_gradients.py:13: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n",
            "W0627 16:18:07.454462 140453592807296 deprecation_wrapper.py:119] From ./train.py:88: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "W0627 16:18:07.454832 140453592807296 deprecation_wrapper.py:119] From ./train.py:91: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "2019-06-27 16:18:07.461376: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-06-27 16:18:07.461646: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1803100 executing computations on platform Host. Devices:\n",
            "2019-06-27 16:18:07.461679: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2019-06-27 16:18:07.480693: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\n",
            "2019-06-27 16:18:07.630706: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-27 16:18:07.631456: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1802840 executing computations on platform CUDA. Devices:\n",
            "2019-06-27 16:18:07.631488: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2019-06-27 16:18:07.631811: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-27 16:18:07.632336: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-06-27 16:18:07.632796: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-06-27 16:18:07.634448: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-06-27 16:18:07.636088: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-06-27 16:18:07.636564: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-06-27 16:18:07.638543: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-06-27 16:18:07.639873: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-06-27 16:18:07.643244: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-06-27 16:18:07.643415: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-27 16:18:07.643834: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-27 16:18:07.644191: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
            "2019-06-27 16:18:07.644254: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-06-27 16:18:07.645260: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-06-27 16:18:07.645289: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n",
            "2019-06-27 16:18:07.645300: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n",
            "2019-06-27 16:18:07.645679: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-27 16:18:07.646105: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-27 16:18:07.646586: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14202 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "W0627 16:18:07.647524 140453592807296 deprecation_wrapper.py:119] From ./train.py:92: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0627 16:18:18.261071 140453592807296 deprecation.py:323] From /content/gpt-2/src/sample.py:65: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "W0627 16:18:18.275680 140453592807296 deprecation.py:323] From /content/gpt-2/src/sample.py:16: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0627 16:18:18.277296 140453592807296 deprecation.py:323] From /content/gpt-2/src/sample.py:70: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.random.categorical` instead.\n",
            "W0627 16:18:18.286978 140453592807296 deprecation_wrapper.py:119] From ./train.py:121: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "W0627 16:18:33.210593 140453592807296 deprecation_wrapper.py:119] From ./train.py:144: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n",
            "W0627 16:18:33.213492 140453592807296 deprecation_wrapper.py:119] From ./train.py:147: The name tf.summary.merge is deprecated. Please use tf.compat.v1.summary.merge instead.\n",
            "\n",
            "W0627 16:18:33.214278 140453592807296 deprecation_wrapper.py:119] From ./train.py:149: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
            "\n",
            "W0627 16:18:33.215021 140453592807296 deprecation_wrapper.py:119] From ./train.py:152: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "Loading checkpoint models/345M/model.ckpt\n",
            "W0627 16:18:46.518481 140453592807296 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "Loading dataset...\n",
            "100% 1/1 [00:03<00:00,  3.52s/it]\n",
            "dataset has 461553 tokens\n",
            "Training...\n",
            "2019-06-27 16:19:07.606457: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n",
            "2019-06-27 16:19:08.276938: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "[1 | 13.62] loss=2.93 avg=2.93\n",
            "[2 | 15.14] loss=3.11 avg=3.02\n",
            "[3 | 16.66] loss=2.83 avg=2.95\n",
            "[4 | 18.19] loss=3.01 avg=2.97\n",
            "[5 | 19.72] loss=3.05 avg=2.99\n",
            "[6 | 21.25] loss=2.81 avg=2.96\n",
            "[7 | 22.79] loss=2.61 avg=2.90\n",
            "[8 | 24.33] loss=2.84 avg=2.90\n",
            "[9 | 25.87] loss=2.92 avg=2.90\n",
            "[10 | 27.41] loss=2.66 avg=2.87\n",
            "[11 | 28.96] loss=2.84 avg=2.87\n",
            "[12 | 30.51] loss=3.54 avg=2.93\n",
            "[13 | 32.06] loss=3.18 avg=2.95\n",
            "[14 | 33.62] loss=3.02 avg=2.96\n",
            "[15 | 35.18] loss=3.13 avg=2.97\n",
            "[16 | 36.73] loss=2.93 avg=2.97\n",
            "[17 | 38.29] loss=3.38 avg=2.99\n",
            "[18 | 39.85] loss=2.42 avg=2.96\n",
            "[19 | 41.42] loss=3.24 avg=2.97\n",
            "[20 | 42.98] loss=2.70 avg=2.96\n",
            "[21 | 44.55] loss=2.74 avg=2.95\n",
            "[22 | 46.12] loss=3.06 avg=2.95\n",
            "[23 | 47.69] loss=2.89 avg=2.95\n",
            "[24 | 49.26] loss=2.72 avg=2.94\n",
            "[25 | 50.84] loss=2.85 avg=2.94\n",
            "[26 | 52.41] loss=3.16 avg=2.94\n",
            "[27 | 53.99] loss=3.04 avg=2.95\n",
            "[28 | 55.57] loss=3.10 avg=2.96\n",
            "[29 | 57.14] loss=3.09 avg=2.96\n",
            "[30 | 58.73] loss=3.04 avg=2.96\n",
            "[31 | 60.30] loss=2.81 avg=2.96\n",
            "[32 | 61.88] loss=2.77 avg=2.95\n",
            "[33 | 63.46] loss=2.89 avg=2.95\n",
            "[34 | 65.06] loss=2.82 avg=2.94\n",
            "[35 | 66.64] loss=3.09 avg=2.95\n",
            "[36 | 68.22] loss=2.84 avg=2.95\n",
            "[37 | 69.81] loss=2.95 avg=2.95\n",
            "[38 | 71.39] loss=3.05 avg=2.95\n",
            "[39 | 72.97] loss=3.33 avg=2.96\n",
            "[40 | 74.56] loss=2.96 avg=2.96\n",
            "[41 | 76.14] loss=2.92 avg=2.96\n",
            "[42 | 77.73] loss=2.68 avg=2.95\n",
            "[43 | 79.32] loss=2.96 avg=2.95\n",
            "[44 | 80.91] loss=2.63 avg=2.94\n",
            "[45 | 82.49] loss=2.53 avg=2.93\n",
            "[46 | 84.08] loss=2.34 avg=2.92\n",
            "[47 | 85.66] loss=3.28 avg=2.92\n",
            "[48 | 87.24] loss=2.87 avg=2.92\n",
            "[49 | 88.83] loss=2.70 avg=2.92\n",
            "[50 | 90.41] loss=3.08 avg=2.92\n",
            "[51 | 91.99] loss=2.72 avg=2.92\n",
            "[52 | 93.58] loss=2.85 avg=2.91\n",
            "[53 | 95.16] loss=2.68 avg=2.91\n",
            "[54 | 96.75] loss=3.14 avg=2.91\n",
            "[55 | 98.34] loss=3.24 avg=2.92\n",
            "[56 | 99.92] loss=3.24 avg=2.93\n",
            "[57 | 101.51] loss=3.28 avg=2.94\n",
            "[58 | 103.11] loss=3.11 avg=2.94\n",
            "[59 | 104.70] loss=2.92 avg=2.94\n",
            "[60 | 106.29] loss=2.53 avg=2.93\n",
            "[61 | 107.87] loss=3.26 avg=2.94\n",
            "[62 | 109.46] loss=2.99 avg=2.94\n",
            "[63 | 111.05] loss=3.03 avg=2.94\n",
            "[64 | 112.64] loss=2.91 avg=2.94\n",
            "[65 | 114.23] loss=2.92 avg=2.94\n",
            "[66 | 115.81] loss=3.09 avg=2.94\n",
            "[67 | 117.40] loss=3.46 avg=2.95\n",
            "[68 | 118.98] loss=2.86 avg=2.95\n",
            "[69 | 120.58] loss=3.04 avg=2.95\n",
            "[70 | 122.18] loss=2.90 avg=2.95\n",
            "[71 | 123.77] loss=2.57 avg=2.95\n",
            "[72 | 125.36] loss=2.84 avg=2.94\n",
            "[73 | 126.95] loss=3.15 avg=2.95\n",
            "[74 | 128.54] loss=3.04 avg=2.95\n",
            "[75 | 130.14] loss=2.95 avg=2.95\n",
            "[76 | 131.74] loss=2.62 avg=2.94\n",
            "[77 | 133.34] loss=3.06 avg=2.95\n",
            "[78 | 134.93] loss=2.97 avg=2.95\n",
            "[79 | 136.54] loss=2.65 avg=2.94\n",
            "[80 | 138.15] loss=2.95 avg=2.94\n",
            "[81 | 139.77] loss=3.32 avg=2.95\n",
            "[82 | 141.38] loss=2.92 avg=2.95\n",
            "[83 | 142.99] loss=2.68 avg=2.94\n",
            "[84 | 144.61] loss=2.66 avg=2.94\n",
            "[85 | 146.22] loss=2.61 avg=2.93\n",
            "[86 | 147.84] loss=2.82 avg=2.93\n",
            "[87 | 149.45] loss=3.20 avg=2.93\n",
            "[88 | 151.07] loss=2.74 avg=2.93\n",
            "[89 | 152.69] loss=3.09 avg=2.93\n",
            "[90 | 154.32] loss=2.46 avg=2.93\n",
            "[91 | 155.95] loss=3.22 avg=2.93\n",
            "[92 | 157.58] loss=3.00 avg=2.93\n",
            "[93 | 159.21] loss=2.88 avg=2.93\n",
            "[94 | 160.82] loss=3.18 avg=2.94\n",
            "[95 | 162.45] loss=2.95 avg=2.94\n",
            "[96 | 164.07] loss=2.96 avg=2.94\n",
            "[97 | 165.69] loss=3.33 avg=2.94\n",
            "[98 | 167.32] loss=2.87 avg=2.94\n",
            "[99 | 168.93] loss=2.77 avg=2.94\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " group-theater phenomenon that is the heart of the movie. The movie follows a young girl who arrives in an unfamiliar suburban town in the summer of 1986, where there is no Disney in the family. She starts dating her childhood best friend, a boy named Paul, who is the son of Frank, her father's accountant while she was a high school freshman. They make a name for themselves as actors playing gay characters, and they're also romantically involved but are also deeply involved with one another.\n",
            "\n",
            "For the first time in the directorial career of Steven Spielberg, a gay man, it is one of the biggest movies you have seen.\n",
            "\n",
            "Spielberg had been preparing for some time so that he couldn't be arrested as he was getting ready for the press tour of Close Encounters. The press tour, as you can imagine, was the first thing Spielberg is not supposed to be on at a press conference. He walked into a bar after the tour and was seated in a row with a couple of colleagues from Reuters.\n",
            "\n",
            "\"I'm here for the press thing,\" Spielberg said as he walked toward them.\n",
            "\n",
            "At the time, Spielberg was making a movie about his own father, but he was so embarrassed by the press tour that he was unable to give any additional interviews or to answer most questions, including one about how the family's life has changed since they left the big city for the suburbs.\n",
            "\n",
            "Spielberg told reporters that he was going to play an adult-in-training and that he and Spielberg \"probably couldn't tell you anything more about the relationship except that we've got a baby girl.\" Spielberg and his parents were in town for the press tour, he said, and, as he later admitted, he and Spielberg \"just kind of went for it, and then I said, 'Hey, come look at me out here! Let me talk a little on camera.' He said, 'Don't you want to talk about it in front of everybody?' And I thought, Well, good, then we're finished, and so, that's what we do.\"\n",
            "\n",
            "Then Spielberg said, \"Well, can I get a hug?\" and he let his daughter get a hug on his arm.\n",
            "\n",
            "Spielberg, now the screenwriter of E.T.'s sequel, has become a kind of gay icon: He played an adult in the movie-maker movie, Star Trek. He had appeared in and guest-starred in The Graduate; the young son of the late John Carpenter was once considered one of the two best things John Carpenter ever did. He is a fan of The Twilight Saga franchise, and, as recently as last year, he was thinking of opening a theater near the set of the HBO series, Silicon Valley. He has recently made a movie called The Interview, with Robert Forster as a hacker who becomes a master assassin. But there's another side of him that the new film's audience might never have known existed—a side to his self that might not even be recognizable, and one that has shaped his life and his career.\n",
            "\n",
            "In addition to opening a theater, Spielberg has built two more homes: He and his longtime girlfriend, Cari, have two girls, a boy and girl, and two men in their thirties. They have lived in both Los Angeles and San Diego throughout the years, and they live in different neighborhoods of the world. But in the 1980s, as they were growing up, Cari and Spielberg were trying to get jobs and were living in different cities. They'd go to the same movie theater on a Saturday night; the people there would often take a movie ticket instead of a movie ticket for that evening's special showing of the film.\n",
            "\n",
            "By the time Spielberg hit high school, he was able to buy the ticket for a Sunday morning movie showing, but he had already bought the tickets for many a Sunday night. And so, when they were going to the cinema, the day before the show, he would drive himself out of the neighborhood to a friend's house and use the movie box that a cousin opened up to sell him a ticket. He had always been on the right end of a social circle that included his childhood best friend, who was in his class, and a few close friends and family. \"When you get older, you are a little more sensitive to people's expectations,\" Spielberg told me. \"It didn't matter. When I was in the first row, people didn't look anything like those people.\"\n",
            "\n",
            "They were on the right end of his \"community.\"\n",
            "\n",
            "It wasn't until the third grade when you became involved with an older sibling that you began to take the \"community\" route more generally. You were more isolated, in many ways. As an older brother to that younger brother in the fourth grade, and a friend and peer in the fifth and the sixth-graders, I took the community route.\n",
            "\n",
            "To me, it reflected a bigger thing about them both. Their\n",
            "\n",
            "[100 | 196.58] loss=3.12 avg=2.94\n",
            "[101 | 198.19] loss=3.10 avg=2.94\n",
            "[102 | 199.81] loss=2.99 avg=2.94\n",
            "[103 | 201.42] loss=2.91 avg=2.94\n",
            "[104 | 203.03] loss=3.27 avg=2.95\n",
            "[105 | 204.64] loss=3.09 avg=2.95\n",
            "[106 | 206.25] loss=2.54 avg=2.94\n",
            "[107 | 207.87] loss=3.00 avg=2.95\n",
            "[108 | 209.48] loss=2.77 avg=2.94\n",
            "[109 | 211.09] loss=2.65 avg=2.94\n",
            "[110 | 212.70] loss=2.75 avg=2.94\n",
            "[111 | 214.32] loss=3.13 avg=2.94\n",
            "[112 | 215.93] loss=2.68 avg=2.93\n",
            "[113 | 217.55] loss=2.57 avg=2.93\n",
            "[114 | 219.16] loss=2.90 avg=2.93\n",
            "[115 | 220.79] loss=2.93 avg=2.93\n",
            "[116 | 222.40] loss=3.06 avg=2.93\n",
            "[117 | 224.02] loss=2.82 avg=2.93\n",
            "[118 | 225.64] loss=2.94 avg=2.93\n",
            "[119 | 227.27] loss=2.70 avg=2.93\n",
            "[120 | 228.89] loss=2.63 avg=2.92\n",
            "[121 | 230.52] loss=3.01 avg=2.92\n",
            "[122 | 232.15] loss=3.26 avg=2.93\n",
            "[123 | 233.78] loss=2.88 avg=2.93\n",
            "[124 | 235.41] loss=2.75 avg=2.92\n",
            "[125 | 237.05] loss=2.97 avg=2.93\n",
            "[126 | 238.69] loss=2.66 avg=2.92\n",
            "[127 | 240.34] loss=2.63 avg=2.92\n",
            "[128 | 241.98] loss=2.93 avg=2.92\n",
            "[129 | 243.63] loss=2.67 avg=2.91\n",
            "[130 | 245.28] loss=2.96 avg=2.91\n",
            "[131 | 246.93] loss=3.34 avg=2.92\n",
            "[132 | 248.58] loss=2.77 avg=2.92\n",
            "[133 | 250.23] loss=2.82 avg=2.92\n",
            "[134 | 251.88] loss=2.73 avg=2.91\n",
            "[135 | 253.53] loss=2.79 avg=2.91\n",
            "[136 | 255.18] loss=2.69 avg=2.91\n",
            "[137 | 256.83] loss=2.75 avg=2.91\n",
            "[138 | 258.48] loss=2.57 avg=2.90\n",
            "[139 | 260.13] loss=3.16 avg=2.91\n",
            "[140 | 261.78] loss=2.70 avg=2.90\n",
            "[141 | 263.43] loss=2.60 avg=2.90\n",
            "[142 | 265.07] loss=3.12 avg=2.90\n",
            "[143 | 266.71] loss=3.05 avg=2.91\n",
            "[144 | 268.35] loss=2.77 avg=2.90\n",
            "[145 | 269.99] loss=2.45 avg=2.90\n",
            "[146 | 271.63] loss=3.07 avg=2.90\n",
            "[147 | 273.27] loss=2.48 avg=2.89\n",
            "[148 | 274.91] loss=2.82 avg=2.89\n",
            "[149 | 276.54] loss=2.85 avg=2.89\n",
            "[150 | 278.17] loss=2.95 avg=2.89\n",
            "[151 | 279.80] loss=2.43 avg=2.89\n",
            "[152 | 281.43] loss=2.80 avg=2.89\n",
            "[153 | 283.06] loss=3.09 avg=2.89\n",
            "[154 | 284.69] loss=2.58 avg=2.89\n",
            "[155 | 286.32] loss=2.67 avg=2.88\n",
            "[156 | 287.95] loss=2.47 avg=2.88\n",
            "[157 | 289.58] loss=2.90 avg=2.88\n",
            "[158 | 291.21] loss=3.03 avg=2.88\n",
            "[159 | 292.84] loss=3.00 avg=2.88\n",
            "[160 | 294.46] loss=2.92 avg=2.88\n",
            "[161 | 296.08] loss=3.20 avg=2.89\n",
            "[162 | 297.71] loss=2.32 avg=2.88\n",
            "[163 | 299.33] loss=2.78 avg=2.88\n",
            "[164 | 300.95] loss=2.99 avg=2.88\n",
            "[165 | 302.58] loss=2.70 avg=2.88\n",
            "[166 | 304.20] loss=2.96 avg=2.88\n",
            "[167 | 305.82] loss=2.79 avg=2.88\n",
            "[168 | 307.45] loss=2.97 avg=2.88\n",
            "[169 | 309.07] loss=2.77 avg=2.88\n",
            "[170 | 310.69] loss=2.74 avg=2.87\n",
            "[171 | 312.32] loss=3.23 avg=2.88\n",
            "[172 | 313.95] loss=2.57 avg=2.87\n",
            "[173 | 315.58] loss=3.13 avg=2.88\n",
            "[174 | 317.21] loss=2.75 avg=2.88\n",
            "[175 | 318.84] loss=2.65 avg=2.87\n",
            "[176 | 320.48] loss=2.81 avg=2.87\n",
            "[177 | 322.11] loss=2.66 avg=2.87\n",
            "[178 | 323.74] loss=2.85 avg=2.87\n",
            "[179 | 325.38] loss=2.88 avg=2.87\n",
            "[180 | 327.01] loss=2.71 avg=2.87\n",
            "[181 | 328.64] loss=2.66 avg=2.87\n",
            "[182 | 330.28] loss=2.90 avg=2.87\n",
            "[183 | 331.93] loss=2.66 avg=2.86\n",
            "[184 | 333.58] loss=2.73 avg=2.86\n",
            "[185 | 335.22] loss=3.25 avg=2.87\n",
            "[186 | 336.87] loss=2.57 avg=2.86\n",
            "[187 | 338.52] loss=2.87 avg=2.86\n",
            "[188 | 340.17] loss=3.28 avg=2.87\n",
            "[189 | 341.83] loss=3.03 avg=2.87\n",
            "[190 | 343.50] loss=2.93 avg=2.87\n",
            "[191 | 345.16] loss=2.98 avg=2.87\n",
            "[192 | 346.83] loss=2.87 avg=2.87\n",
            "[193 | 348.49] loss=3.21 avg=2.88\n",
            "[194 | 350.16] loss=3.02 avg=2.88\n",
            "[195 | 351.83] loss=2.55 avg=2.87\n",
            "[196 | 353.49] loss=2.63 avg=2.87\n",
            "[197 | 355.16] loss=2.90 avg=2.87\n",
            "[198 | 356.82] loss=2.69 avg=2.87\n",
            "[199 | 358.49] loss=2.76 avg=2.87\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " change a few seconds on, a new update would land next week. This time around, the new version will feature a higher resolution user interface. It also has a custom chat interface and enhanced messaging features, a change that is sure to please gamers.\n",
            "\n",
            "The update will take place on September 4th, according to Microsoft , so players that are running it during the 10-day beta period won't be able to install it immediately following the end of that period. Microsoft has been updating all of its software updates with a new version to help prevent new updates from crashing.\n",
            "\n",
            "Microsoft has also taken the time to update the Windows 10 Mobile app. As the Microsoft Twitter account tweeted today, the company will be updating \"all major Windows 10 Mobile apps,\" including PC Messenger, a new app that lets you send and receive notifications. The updated app will also let you create custom groups and groups of friends.\n",
            "\n",
            "While these new features are important for users concerned about the security of their PC, they don't appear to have had the biggest impact. As PCMag pointed out, the app update only seems visible to those that have Windows 10 Mobile installed and not necessarily those that installed it themselves. For those who installed it themselves, it appears to be the biggest impact.\n",
            "\n",
            "Users of PCs that are running Windows 10 Mobile versions 11.14.1 and 17.13.0 will get the update as a download, and users running older versions of the OS and the operating system can access the update on their PCs. Those running Windows 10 Mobile versions 7.1 or higher aren't likely to see the update, but those older on 11.14.1 and 17.13.0 were likely to be affected.\n",
            "\n",
            "While the PC app update is welcome, many people have expressed frustration with how Microsoft updates Windows 10 Mobile. People on Twitter noted that this was Microsoft's second major update, and that Windows 10 Mobile could take months to have this much impact, while older versions of Windows had been largely forgotten.\n",
            "\n",
            "While Microsoft has repeatedly said that it plans to \"do more across the board,\" this does little to change the fact that people are taking it in the face with the update. Windows 10 Mobile apps are still very limited at the moment, but as I said at the end of last week, the app update could be the key to Microsoft's continuing efforts to make Windows more accessible.\n",
            "\n",
            "The new update also brings the Start screen back to the center of Windows. It could be an attractive feature, too, because it could make it harder to launch games or other apps without swiping across the Start screen. For those looking to jump on the game-playing bandwagon, the Windows Store is likely a good place to do so.\n",
            "\n",
            "While some of these changes were nice, others were, well, odd. For example, users of the older version of the Windows 10 Mobile app (11.14.1) will be getting an updated version, but users of Windows 10 Mobile version 17.13.0, which was released on September 4th, won't see the update. The Windows Mobile app is currently the most popular Windows 10 Mobile version — Windows Phone users have been waiting for the chance to get their hands on it for quite some time — and some people are likely to miss this major update.\n",
            "\n",
            "Windows 10 Mobile has been a popular operating system among gamers, but Windows 10 Mobile does not appear to be the only version of the OS on the market. Microsoft's other mobile operating system, Windows 10 Mobile, will also get a major update to its operating system starting Monday, September 12th.<|endoftext|>Praise be to Allaah.\n",
            "\n",
            "It is not permissible for a Muslim to leave a place of prayer except with the permission of the imaams or the immaams, or the imaams or the imas. No one is obliged to obey the opinion of anyone who contradicts the opinion of his father or the imaams, or imaams, or the imas except in the case of a group of people, such as the Muslims.\n",
            "\n",
            "What is the opinion of Allaah, and what is the opinion of our brother?\n",
            "\n",
            "The opinion of Allaah was unanimously decided on the night of 'Ahaadeeth'. On this issue the Hanbalis differed a bit. It is true that one of the great scholars of the Ahlul Bayt, Ibn Majaah ibn Muhammad ibn 'Abd al-Wahhab, was killed when fighting during the Banu Amman raid on Banu Ad-Dahhak in 715 CE. However, this was probably to prevent him or his followers from taking the leadership into the next battle. Other scholars are far more consistent and have given the opinion that one of the reasons for the defeat of Banu Ad-Dahhak was because the army of Shaytanah, the companions and the believers of Banu Ad-Dahhak was too strong. [Imam al-Albani]\n",
            "\n",
            "What are the opinions of others?\n",
            "\n",
            "The\n",
            "\n",
            "[200 | 382.79] loss=2.98 avg=2.87\n",
            "[201 | 384.43] loss=2.77 avg=2.87\n",
            "[202 | 386.07] loss=2.95 avg=2.87\n",
            "[203 | 387.70] loss=2.98 avg=2.87\n",
            "[204 | 389.33] loss=2.87 avg=2.87\n",
            "[205 | 390.96] loss=2.29 avg=2.86\n",
            "[206 | 392.60] loss=3.24 avg=2.87\n",
            "[207 | 394.22] loss=2.53 avg=2.86\n",
            "[208 | 395.86] loss=2.74 avg=2.86\n",
            "[209 | 397.49] loss=3.32 avg=2.87\n",
            "[210 | 399.12] loss=3.03 avg=2.87\n",
            "[211 | 400.75] loss=3.12 avg=2.87\n",
            "[212 | 402.38] loss=3.03 avg=2.87\n",
            "[213 | 404.01] loss=2.57 avg=2.87\n",
            "[214 | 405.64] loss=2.71 avg=2.87\n",
            "[215 | 407.28] loss=2.82 avg=2.87\n",
            "[216 | 408.91] loss=2.66 avg=2.87\n",
            "[217 | 410.54] loss=2.97 avg=2.87\n",
            "[218 | 412.17] loss=2.74 avg=2.87\n",
            "[219 | 413.80] loss=3.16 avg=2.87\n",
            "[220 | 415.42] loss=2.93 avg=2.87\n",
            "[221 | 417.04] loss=2.79 avg=2.87\n",
            "[222 | 418.67] loss=3.07 avg=2.87\n",
            "[223 | 420.31] loss=2.41 avg=2.87\n",
            "[224 | 421.94] loss=3.23 avg=2.87\n",
            "[225 | 423.58] loss=2.88 avg=2.87\n",
            "[226 | 425.22] loss=2.90 avg=2.87\n",
            "[227 | 426.86] loss=2.92 avg=2.87\n",
            "[228 | 428.50] loss=3.16 avg=2.87\n",
            "[229 | 430.15] loss=2.72 avg=2.87\n",
            "[230 | 431.80] loss=2.78 avg=2.87\n",
            "[231 | 433.44] loss=3.20 avg=2.88\n",
            "[232 | 435.10] loss=2.84 avg=2.88\n",
            "[233 | 436.76] loss=2.74 avg=2.87\n",
            "[234 | 438.43] loss=2.32 avg=2.87\n",
            "[235 | 440.10] loss=2.55 avg=2.86\n",
            "[236 | 441.77] loss=2.93 avg=2.86\n",
            "[237 | 443.44] loss=2.82 avg=2.86\n",
            "[238 | 445.11] loss=2.95 avg=2.87\n",
            "[239 | 446.78] loss=3.04 avg=2.87\n",
            "[240 | 448.46] loss=2.59 avg=2.86\n",
            "[241 | 450.12] loss=3.33 avg=2.87\n",
            "[242 | 451.80] loss=3.03 avg=2.87\n",
            "[243 | 453.47] loss=2.12 avg=2.86\n",
            "[244 | 455.14] loss=3.15 avg=2.87\n",
            "[245 | 456.81] loss=2.82 avg=2.87\n",
            "[246 | 458.47] loss=2.28 avg=2.86\n",
            "[247 | 460.15] loss=2.84 avg=2.86\n",
            "[248 | 461.81] loss=2.67 avg=2.86\n",
            "[249 | 463.48] loss=2.67 avg=2.85\n",
            "[250 | 465.14] loss=2.62 avg=2.85\n",
            "[251 | 466.80] loss=3.33 avg=2.86\n",
            "[252 | 468.45] loss=3.21 avg=2.86\n",
            "[253 | 470.10] loss=3.01 avg=2.86\n",
            "[254 | 471.76] loss=2.89 avg=2.86\n",
            "[255 | 473.40] loss=2.84 avg=2.86\n",
            "[256 | 475.05] loss=2.90 avg=2.86\n",
            "[257 | 476.68] loss=2.74 avg=2.86\n",
            "[258 | 478.31] loss=2.47 avg=2.86\n",
            "[259 | 479.96] loss=2.98 avg=2.86\n",
            "[260 | 481.60] loss=2.77 avg=2.86\n",
            "[261 | 483.23] loss=2.67 avg=2.86\n",
            "[262 | 484.86] loss=2.91 avg=2.86\n",
            "[263 | 486.49] loss=3.20 avg=2.86\n",
            "[264 | 488.11] loss=2.18 avg=2.85\n",
            "[265 | 489.75] loss=2.80 avg=2.85\n",
            "[266 | 491.38] loss=2.71 avg=2.85\n",
            "[267 | 493.01] loss=2.75 avg=2.85\n",
            "[268 | 494.65] loss=2.81 avg=2.85\n",
            "[269 | 496.28] loss=2.48 avg=2.85\n",
            "[270 | 497.92] loss=2.93 avg=2.85\n",
            "[271 | 499.55] loss=2.66 avg=2.84\n",
            "[272 | 501.18] loss=3.07 avg=2.85\n",
            "[273 | 502.81] loss=2.92 avg=2.85\n",
            "[274 | 504.44] loss=3.31 avg=2.85\n",
            "[275 | 506.08] loss=2.40 avg=2.85\n",
            "[276 | 507.71] loss=2.73 avg=2.85\n",
            "[277 | 509.33] loss=2.87 avg=2.85\n",
            "[278 | 510.96] loss=2.66 avg=2.84\n",
            "[279 | 512.58] loss=2.98 avg=2.85\n",
            "[280 | 514.21] loss=2.67 avg=2.84\n",
            "[281 | 515.85] loss=2.96 avg=2.85\n",
            "[282 | 517.48] loss=2.80 avg=2.85\n",
            "[283 | 519.12] loss=2.80 avg=2.84\n",
            "[284 | 520.76] loss=2.91 avg=2.85\n",
            "[285 | 522.40] loss=3.28 avg=2.85\n",
            "[286 | 524.05] loss=2.79 avg=2.85\n",
            "[287 | 525.69] loss=3.10 avg=2.85\n",
            "[288 | 527.33] loss=2.75 avg=2.85\n",
            "[289 | 528.98] loss=2.96 avg=2.85\n",
            "[290 | 530.63] loss=2.46 avg=2.85\n",
            "[291 | 532.27] loss=3.24 avg=2.85\n",
            "[292 | 533.93] loss=2.82 avg=2.85\n",
            "[293 | 535.59] loss=2.57 avg=2.85\n",
            "[294 | 537.26] loss=2.92 avg=2.85\n",
            "[295 | 538.92] loss=3.00 avg=2.85\n",
            "[296 | 540.59] loss=3.12 avg=2.85\n",
            "[297 | 542.26] loss=2.77 avg=2.85\n",
            "[298 | 543.94] loss=2.47 avg=2.85\n",
            "[299 | 545.61] loss=2.92 avg=2.85\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "rators have argued that Apple's use of data collection and marketing practices is just one element of its broader PR push. While I agree there's an enormous amount of PR, I don't even think Apple has to engage directly. Their PR is, like all PR, an attempt to sell more products. The only reason it doesn't work is because Apple doesn't need to do any PR at all.\n",
            "\n",
            "Apple isn't the only company that does its PR stuff through PR, of course. That's what you read about in the new book on PR by Jeff Jarvis, who was executive editor of the Times and now writes for Forbes. Other PRs will likely rise and fall as the times change, and Apple will likely continue with its model even if the public responds negatively to the company's PR techniques. But the new book, if true, is really quite interesting.\n",
            "\n",
            "Apple's PR has never been particularly impressive. It has been more like a PR bot. But the PR Bot did do a lot of stuff that I'll never understand. Why?\n",
            "\n",
            "When I was a kid, there was a kid on the street who always had these giant yellow plastic toys for his hands. One time, he got mad and stole one of them, and it fell off his finger. Now I can see why he's on these toys when the kids in my neighborhood call them \"scissors.\" But there was always this guy in the neighborhood who always had one of those things. Maybe it was this giant toy that he got when he was small, too.\n",
            "\n",
            "You know what I always ask my kids? Do you ever use these things? Do you ever scratch the floor with your hand in my house when you've got something and there's paper on the floor or something?\n",
            "\n",
            "Apple knows very well it's going to get calls about what they're doing, and some of that attention is deserved, but most of it doesn't work or doesn't reflect the product they've been selling.\n",
            "\n",
            "So I would like to go back and ask you a question about Steve Jobs. As most of us know, when Apple co-counsel David Drummond was first introduced to Steve Jobs in March of 2007, he said there were eight things he loved about the Jobs brand. Then, six months later, when I interviewed Jobs for the cover story in Mac Life that year, Drammond was back out on the scene, saying three things I just don't think anybody else has ever said.\n",
            "\n",
            "He said that Jobs was the most \"human\" person he ever met and that he found Jobs to be \"kind, encouraging, and generous, with a smile on his face.\" He said the Jobs brand was \"more real\" than any other company's or a family-owned one. He said Jobs taught \"a remarkable amount about the art of the job.\" But he also seemed to be saying that he was a product person who was obsessed with selling things, and that the product was the one thing he loved about Jobs.\n",
            "\n",
            "He seemed to be offering a simple answer, the kind of answer that would lead, at a minimum, to a quick resolution to any dispute that might break out in a future Apple case: Apple would never change the behavior of the Jobs brand or its customers. I could go on.\n",
            "\n",
            "If I had to sum up what happened when Jobs got on my phone, the answer would be pretty simple: I said it all, I was just repeating it a little too many times for him to be able to see what I was really getting at. After this exchange, I'm afraid the rest of what he had to say—about his family, about their children and about the things that mattered most to him—probably goes entirely unrecorded. After that, there was an avalanche of other stories about all the things Jobs loved about the Jobs brand (what did he love really?); about the qualities that made the Jobs brand so deeply personal to him; about how Steve was incredibly generous and generous, how Steve was also a great teacher and a wonderful person.\n",
            "\n",
            "And all of it seems to have come from some dude named Dave who just happened to be on my side of the table. I mean, really: This guy just happened to be on your side of the table? How could you possibly have known, or why could the guy have even be there at the wrong time to do a basic reality check on the whole situation?\n",
            "\n",
            "There were so many great things about the Jobs brand that I was trying not to write, but I had to, because I couldn't help myself if I told it all out loud. If I had the opportunity, I think all of the great things that Jobs had to say about the Jobs brand might just have been true.\n",
            "\n",
            "I don't just mean all my things. I mean everything that they were trying to tell me. I was just trying to do something different, to write something, to be different. I thought maybe it would make it easier for Steve to know, you know\n",
            "\n",
            "[300 | 570.08] loss=2.68 avg=2.85\n",
            "[301 | 571.74] loss=2.44 avg=2.84\n",
            "[302 | 573.39] loss=2.65 avg=2.84\n",
            "[303 | 575.04] loss=2.88 avg=2.84\n",
            "[304 | 576.70] loss=2.90 avg=2.84\n",
            "[305 | 578.35] loss=3.03 avg=2.84\n",
            "[306 | 580.00] loss=2.88 avg=2.85\n",
            "[307 | 581.65] loss=2.90 avg=2.85\n",
            "[308 | 583.29] loss=2.42 avg=2.84\n",
            "[309 | 584.94] loss=2.38 avg=2.84\n",
            "[310 | 586.58] loss=3.12 avg=2.84\n",
            "[311 | 588.23] loss=2.42 avg=2.84\n",
            "[312 | 589.87] loss=2.48 avg=2.83\n",
            "[313 | 591.51] loss=3.10 avg=2.83\n",
            "[314 | 593.14] loss=2.73 avg=2.83\n",
            "[315 | 594.77] loss=2.96 avg=2.83\n",
            "[316 | 596.41] loss=2.60 avg=2.83\n",
            "[317 | 598.05] loss=3.04 avg=2.83\n",
            "[318 | 599.68] loss=2.62 avg=2.83\n",
            "[319 | 601.31] loss=3.18 avg=2.84\n",
            "[320 | 602.95] loss=2.85 avg=2.84\n",
            "[321 | 604.58] loss=2.89 avg=2.84\n",
            "[322 | 606.22] loss=2.56 avg=2.83\n",
            "[323 | 607.84] loss=2.97 avg=2.83\n",
            "[324 | 609.47] loss=2.78 avg=2.83\n",
            "[325 | 611.09] loss=2.63 avg=2.83\n",
            "[326 | 612.73] loss=2.76 avg=2.83\n",
            "[327 | 614.36] loss=2.84 avg=2.83\n",
            "[328 | 616.00] loss=2.31 avg=2.83\n",
            "[329 | 617.64] loss=2.86 avg=2.83\n",
            "[330 | 619.27] loss=2.68 avg=2.82\n",
            "[331 | 620.90] loss=2.76 avg=2.82\n",
            "[332 | 622.55] loss=2.67 avg=2.82\n",
            "[333 | 624.19] loss=2.85 avg=2.82\n",
            "[334 | 625.83] loss=2.58 avg=2.82\n",
            "[335 | 627.47] loss=2.63 avg=2.82\n",
            "[336 | 629.11] loss=2.67 avg=2.82\n",
            "[337 | 630.76] loss=2.93 avg=2.82\n",
            "[338 | 632.41] loss=2.88 avg=2.82\n",
            "[339 | 634.07] loss=3.09 avg=2.82\n",
            "[340 | 635.72] loss=2.73 avg=2.82\n",
            "[341 | 637.38] loss=3.22 avg=2.82\n",
            "[342 | 639.04] loss=2.93 avg=2.83\n",
            "[343 | 640.71] loss=2.48 avg=2.82\n",
            "[344 | 642.38] loss=2.65 avg=2.82\n",
            "[345 | 644.06] loss=3.08 avg=2.82\n",
            "[346 | 645.73] loss=3.04 avg=2.83\n",
            "[347 | 647.40] loss=2.78 avg=2.82\n",
            "[348 | 649.07] loss=2.83 avg=2.82\n",
            "[349 | 650.74] loss=2.71 avg=2.82\n",
            "[350 | 652.40] loss=2.80 avg=2.82\n",
            "[351 | 654.08] loss=2.65 avg=2.82\n",
            "[352 | 655.75] loss=2.57 avg=2.82\n",
            "[353 | 657.42] loss=2.80 avg=2.82\n",
            "[354 | 659.09] loss=2.62 avg=2.82\n",
            "[355 | 660.75] loss=2.83 avg=2.82\n",
            "[356 | 662.42] loss=2.88 avg=2.82\n",
            "[357 | 664.08] loss=2.92 avg=2.82\n",
            "[358 | 665.74] loss=3.06 avg=2.82\n",
            "[359 | 667.40] loss=2.86 avg=2.82\n",
            "[360 | 669.06] loss=2.99 avg=2.82\n",
            "[361 | 670.72] loss=2.69 avg=2.82\n",
            "[362 | 672.38] loss=3.40 avg=2.83\n",
            "[363 | 674.03] loss=2.53 avg=2.82\n",
            "[364 | 675.68] loss=2.87 avg=2.83\n",
            "[365 | 677.33] loss=2.23 avg=2.82\n",
            "[366 | 678.97] loss=2.53 avg=2.82\n",
            "[367 | 680.61] loss=3.03 avg=2.82\n",
            "[368 | 682.25] loss=2.33 avg=2.81\n",
            "[369 | 683.90] loss=2.92 avg=2.81\n",
            "[370 | 685.53] loss=2.88 avg=2.81\n",
            "[371 | 687.17] loss=3.03 avg=2.82\n",
            "[372 | 688.81] loss=2.66 avg=2.82\n",
            "[373 | 690.45] loss=3.19 avg=2.82\n",
            "[374 | 692.08] loss=2.82 avg=2.82\n",
            "[375 | 693.71] loss=3.29 avg=2.82\n",
            "[376 | 695.33] loss=2.94 avg=2.83\n",
            "[377 | 696.96] loss=2.44 avg=2.82\n",
            "[378 | 698.59] loss=2.81 avg=2.82\n",
            "[379 | 700.22] loss=2.61 avg=2.82\n",
            "[380 | 701.85] loss=2.72 avg=2.82\n",
            "[381 | 703.48] loss=2.85 avg=2.82\n",
            "[382 | 705.11] loss=2.18 avg=2.81\n",
            "[383 | 706.74] loss=2.66 avg=2.81\n",
            "[384 | 708.37] loss=2.42 avg=2.81\n",
            "[385 | 710.01] loss=3.21 avg=2.81\n",
            "[386 | 711.64] loss=2.35 avg=2.81\n",
            "[387 | 713.28] loss=2.78 avg=2.81\n",
            "[388 | 714.92] loss=2.93 avg=2.81\n",
            "[389 | 716.55] loss=2.86 avg=2.81\n",
            "[390 | 718.18] loss=2.45 avg=2.80\n",
            "[391 | 719.82] loss=2.49 avg=2.80\n",
            "[392 | 721.45] loss=3.07 avg=2.80\n",
            "[393 | 723.10] loss=2.63 avg=2.80\n",
            "[394 | 724.74] loss=3.03 avg=2.80\n",
            "[395 | 726.38] loss=3.06 avg=2.81\n",
            "[396 | 728.04] loss=2.72 avg=2.81\n",
            "[397 | 729.69] loss=2.78 avg=2.81\n",
            "[398 | 731.34] loss=2.94 avg=2.81\n",
            "[399 | 733.00] loss=2.17 avg=2.80\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " and “not for sale,” the site said.\n",
            "\n",
            "The site “is owned by the developer, but this will not affect the location,” it said.\n",
            "\n",
            "A company representative confirmed to HuffPost that the company acquired the website through crowdfunding, but wouldn’t shed any more detail.\n",
            "\n",
            "A spokesperson for the Dallas-based company did not respond to emails seeking comment, but the company’s Twitter handle tweeted about the purchase, saying, “This is a major step forward in the creation of the Dallas restaurant chain, and we’re thrilled to have our doors open. We’re ready to begin construction on our first restaurant in Dallas!”\n",
            "\n",
            "The Dallas restaurant chain has been a dream job creator for the founder of the company, Michael Aronofsky, who is also the creator and producer of the Netflix series Stranger Things. The company recently moved from just a small office downtown to a sprawling campus near downtown Dallas.\n",
            "\n",
            "Aronofsky told reporters in June that he hopes the restaurant chain comes to be considered the food system in general. “It’s a new food system for me, just like the Internet was for me 15 years ago,” he said of the restaurant chain. “I think we can do great things together. I’m looking forward to starting it, not because it’s fun to be a part of, but because it makes this place a better place to be.''\n",
            "\n",
            "But the dream of restaurateurs and entrepreneurs dreaming of restaurants in Dallas, whether it is for the benefit of all those who make it possible or purely for personal gain, is not for everyone.\n",
            "\n",
            "The Dallas Restaurant Association has been concerned about the expansion of the DDA since at least 2015 when the board voted on an unprecedented $2 million investment to build a third DDA location. The association said, in an effort to ensure it remained relevant, that it planned to hold a second vote on the issue in June. The vote drew little enthusiasm and only two members voted in favor -- the board of directors and a representative from the Dallas Bar Association who supported the restaurant industry.\n",
            "\n",
            "“We feel that it is important that all those who support the development have their say on this issue,” said Michelle Johnson-Thompson, an attorney with the Dallas-based Dallas-based law firm of Jones Day. “But our members voted against this investment because they felt this investment was better spent on serving more people, which included the greater downtown area and that was the plan all along, which wasn’t the case.”\n",
            "\n",
            "The investment was opposed by more than 1,000 Dallas restaurants in September.\n",
            "\n",
            "Last month, as part of that campaign, the Dallas Business Chronicle reported that restaurant associations had become even more skeptical of DDA plans. “In the end, they are no better or worse,” it said. “They’re just different.”\n",
            "\n",
            "As of September 1, 2016, the website of the Dallas Restaurant Association did not include any comment on the development project, according to data from the company’s website. It does not appear to have been purchased, and the company did not respond to requests for comment via email or phone.\n",
            "\n",
            "According to the company’s Facebook profile at the time of the sale, the Dallas Restaurant Association is a major fundraiser for the organization, which supports local restaurant owners and employees. The company’s Facebook page has also featured the endorsement and support of a wide range of political leaders, including former President Barack Obama, former Secretary of Defense Robert Gates, former Vice President Al Gore, the former Mayor of San Antonio, and former Secretary of the Air Force Gary Locke.\n",
            "\n",
            "It’s unclear what exactly the Dallas DDA has gained from the proposal to buy the website and website of the Dallas Restaurant Association. The company’s page does have a description of the potential business plan for the company: “New concept concept that will combine fine-dining dining, beer and live entertainment, as well as outdoor patio and beer gardens for a fully realized restaurant destination, which will serve 100+ customers per night at the location (the existing location will serve less than 35 and is the cheapest we currently know of).”\n",
            "\n",
            "The website of the Dallas Restaurant Association does feature a photo of the founder, Michael Aronofsky, who has long been a major supporter of DDA and who is the producer of the Netflix series Stranger Things’.\n",
            "\n",
            "At the time of the campaign for the website, the Dallas Restaurant Association had about 25,000 members.\n",
            "\n",
            "The Dallas Restaurant Association is not the only restaurant association in the country that has been vocal in opposition to the sale of the Dallas DDA website. One of the country’s most prominent restaurants, the Metropolitan Hotel & Restaurant Association, has announced that it will oppose the sale of the restaurant in Philadelphia. The Philadelphia Hotel & Restaurant Association also has\n",
            "\n",
            "[400 | 757.34] loss=2.50 avg=2.80\n",
            "[401 | 759.01] loss=2.97 avg=2.80\n",
            "[402 | 760.69] loss=2.76 avg=2.80\n",
            "[403 | 762.35] loss=2.95 avg=2.80\n",
            "[404 | 764.02] loss=2.72 avg=2.80\n",
            "[405 | 765.68] loss=3.10 avg=2.80\n",
            "[406 | 767.35] loss=3.15 avg=2.81\n",
            "[407 | 769.00] loss=3.35 avg=2.81\n",
            "[408 | 770.66] loss=2.76 avg=2.81\n",
            "[409 | 772.31] loss=2.70 avg=2.81\n",
            "[410 | 773.96] loss=2.75 avg=2.81\n",
            "[411 | 775.61] loss=2.83 avg=2.81\n",
            "[412 | 777.25] loss=3.01 avg=2.81\n",
            "[413 | 778.90] loss=2.91 avg=2.81\n",
            "[414 | 780.54] loss=2.90 avg=2.81\n",
            "[415 | 782.18] loss=2.84 avg=2.81\n",
            "[416 | 783.82] loss=2.62 avg=2.81\n",
            "[417 | 785.46] loss=2.76 avg=2.81\n",
            "[418 | 787.09] loss=2.99 avg=2.81\n",
            "[419 | 788.73] loss=2.25 avg=2.81\n",
            "[420 | 790.36] loss=2.76 avg=2.81\n",
            "[421 | 792.00] loss=2.20 avg=2.80\n",
            "[422 | 793.63] loss=2.68 avg=2.80\n",
            "[423 | 795.26] loss=2.53 avg=2.80\n",
            "[424 | 796.89] loss=2.76 avg=2.80\n",
            "[425 | 798.52] loss=2.87 avg=2.80\n",
            "[426 | 800.15] loss=1.89 avg=2.79\n",
            "[427 | 801.78] loss=2.51 avg=2.79\n",
            "[428 | 803.42] loss=2.86 avg=2.79\n",
            "[429 | 805.05] loss=2.95 avg=2.79\n",
            "[430 | 806.68] loss=2.73 avg=2.79\n",
            "[431 | 808.31] loss=2.58 avg=2.78\n",
            "[432 | 809.94] loss=2.87 avg=2.79\n",
            "[433 | 811.58] loss=2.79 avg=2.79\n",
            "[434 | 813.21] loss=2.30 avg=2.78\n",
            "[435 | 814.84] loss=2.91 avg=2.78\n",
            "[436 | 816.47] loss=2.66 avg=2.78\n",
            "[437 | 818.11] loss=2.44 avg=2.78\n",
            "[438 | 819.75] loss=2.49 avg=2.77\n",
            "[439 | 821.39] loss=3.01 avg=2.78\n",
            "[440 | 823.03] loss=2.64 avg=2.78\n",
            "[441 | 824.67] loss=2.41 avg=2.77\n",
            "[442 | 826.31] loss=3.10 avg=2.78\n",
            "[443 | 827.96] loss=2.86 avg=2.78\n",
            "[444 | 829.62] loss=2.45 avg=2.77\n",
            "[445 | 831.27] loss=2.40 avg=2.77\n",
            "[446 | 832.92] loss=2.46 avg=2.77\n",
            "[447 | 834.58] loss=2.61 avg=2.76\n",
            "[448 | 836.24] loss=2.86 avg=2.77\n",
            "[449 | 837.91] loss=2.46 avg=2.76\n",
            "[450 | 839.57] loss=2.95 avg=2.76\n",
            "[451 | 841.23] loss=2.81 avg=2.76\n",
            "[452 | 842.89] loss=2.58 avg=2.76\n",
            "[453 | 844.56] loss=2.59 avg=2.76\n",
            "[454 | 846.23] loss=2.57 avg=2.76\n",
            "[455 | 847.90] loss=2.42 avg=2.76\n",
            "[456 | 849.57] loss=2.70 avg=2.75\n",
            "[457 | 851.24] loss=2.78 avg=2.76\n",
            "[458 | 852.91] loss=2.71 avg=2.75\n",
            "[459 | 854.58] loss=2.76 avg=2.75\n",
            "[460 | 856.25] loss=2.41 avg=2.75\n",
            "[461 | 857.90] loss=2.90 avg=2.75\n",
            "[462 | 859.57] loss=2.95 avg=2.75\n",
            "[463 | 861.24] loss=2.66 avg=2.75\n",
            "[464 | 862.90] loss=2.66 avg=2.75\n",
            "[465 | 864.57] loss=2.53 avg=2.75\n",
            "[466 | 866.23] loss=3.08 avg=2.75\n",
            "[467 | 867.89] loss=2.88 avg=2.76\n",
            "[468 | 869.55] loss=2.94 avg=2.76\n",
            "[469 | 871.20] loss=2.79 avg=2.76\n",
            "[470 | 872.86] loss=2.53 avg=2.76\n",
            "[471 | 874.51] loss=2.61 avg=2.75\n",
            "[472 | 876.16] loss=2.54 avg=2.75\n",
            "[473 | 877.81] loss=3.13 avg=2.76\n",
            "[474 | 879.46] loss=2.89 avg=2.76\n",
            "[475 | 881.10] loss=2.78 avg=2.76\n",
            "[476 | 882.73] loss=2.43 avg=2.75\n",
            "[477 | 884.37] loss=2.61 avg=2.75\n",
            "[478 | 886.02] loss=2.50 avg=2.75\n",
            "[479 | 887.66] loss=2.47 avg=2.75\n",
            "[480 | 889.29] loss=2.00 avg=2.74\n",
            "[481 | 890.92] loss=2.96 avg=2.74\n",
            "[482 | 892.55] loss=2.60 avg=2.74\n",
            "[483 | 894.18] loss=2.99 avg=2.74\n",
            "[484 | 895.82] loss=2.94 avg=2.74\n",
            "[485 | 897.45] loss=3.22 avg=2.75\n",
            "[486 | 899.08] loss=2.50 avg=2.75\n",
            "[487 | 900.71] loss=3.03 avg=2.75\n",
            "[488 | 902.34] loss=2.62 avg=2.75\n",
            "[489 | 903.97] loss=3.26 avg=2.75\n",
            "[490 | 905.60] loss=2.48 avg=2.75\n",
            "[491 | 907.23] loss=3.05 avg=2.75\n",
            "[492 | 908.86] loss=2.78 avg=2.75\n",
            "[493 | 910.47] loss=2.51 avg=2.75\n",
            "[494 | 912.10] loss=2.74 avg=2.75\n",
            "[495 | 913.73] loss=2.92 avg=2.75\n",
            "[496 | 915.35] loss=2.68 avg=2.75\n",
            "[497 | 916.99] loss=2.55 avg=2.75\n",
            "[498 | 918.62] loss=2.59 avg=2.75\n",
            "[499 | 920.26] loss=2.91 avg=2.75\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " government, but more to protect the public, including families — or at least a representative sample.\n",
            "\n",
            "If we want to find effective responses to climate change, we need to be asking the question: what is the public going to do about it?\n",
            "\n",
            "If the answer to that question is: nothing, then the question has been answered. At best, it reflects our collective failure to do much about climate change. Our inaction has resulted in less storm surge, more droughts, and more waterborne diseases. The loss of species across the globe has been dramatic. And that is to just mention a couple.\n",
            "\n",
            "By contrast, the United States has been making progress on cutting emissions. We are moving away from coal-fired power plants in favor of natural gas, wind, solar, and geothermal. At the same time, the number of acres of forest that our country's forest preserve departments manage has fallen from 717 million acres in 2000 to 482 million acres in 2016. That's a difference of about 8 million acres per year.\n",
            "\n",
            "That number isn't insignificant. But we have been working very hard at it.\n",
            "\n",
            "We had more than 300 million tons of CO2 emitted in 2012. By the end of the century, we're projected to burn another 325 million tons of CO2. More than that is emitted from human activity.\n",
            "\n",
            "That figure includes emissions from land-use change, mining, and deforestation. But also from building, transportation, and manufacturing that has increased energy use and pollution and is now a concern. We also have carbon pollution from vehicle fuels, especially diesel. In 2015, the United States had about 2.47 trillion tons of CO2 in our atmosphere — equal to that figure for China.\n",
            "\n",
            "That may seem huge. So why aren't we doing more?\n",
            ":\n",
            "\n",
            "But the most common reason for inaction has nothing to do with climate change. The most common reason is to protect private and public sector interests.\n",
            "\n",
            "That's right. Private profit interests trump public policy.\n",
            "\n",
            "The reason that companies like Apple and Google pay nothing to the government when they dump carbon pollution in the atmosphere isn't because we want to keep them out. They've built very large companies to do that. Apple and Google, the main ones, are very large people. They are paid almost nowhere near what governments are paying for carbon emissions. The companies have the same interests as anyone else, and they don't like it.\n",
            "\n",
            "If we want to fix climate change, it won't be a matter of cutting regulations. I mean, I'd love to see the European Union go full industrial-scale for a transition to low-carbon energy, and cut pollution by a long shot. But the U.S. has a huge oil industry that doesn't want to cut down and that won't do that as well, either. The U.S. has enormous industrial power, and that means it would have to stop burning oil all together. Even that would be a hard sell.\n",
            "\n",
            "The United States has the biggest financial sector among advanced economies. Many big banks can't get along without a subsidy that gives them incentives for lending — and they don't like that because it could slow economic growth that they didn't start on the assumption it would.\n",
            "\n",
            "The corporate sector isn't doing all that badly. We are still the most competitive economy of any big economy in the world, even if that means that we are the least energy efficient.\n",
            "\n",
            "But for the United States to avoid serious environmental damage is going to require major cuts both in our fossil fuel use and in the number of miles we drive on that fuel. We haven't taken the necessary steps at the national level, either, to address both the air pollution and the climate change problem, so we're stuck.\n",
            "\n",
            "The idea that we should stop oil drilling in the Gulf of Mexico because it could cause a disaster is ludicrous. It's hard to imagine any place that is as vulnerable to climate change as where the Gulf of Mexico is, and where the oil wells and the pipelines are.\n",
            "\n",
            "The idea that there may be a public health risk from fracking is equally absurd. Not only is that a good analogy for where we are right now in terms of cutting pollution, but it's also a statement of priorities that the United States has to make, and that would make the Gulf of Mexico and other places of concern even worse.\n",
            "\n",
            "Our state agencies are not doing enough because they have jobs. Many state agencies still have job openings, even at the federal level, but in the absence of a federal energy strategy, even the most promising job opportunities will leave them a little short. So to get things done, we all need to work for it.\n",
            "\n",
            "So, the most common reason for inaction has nothing to do with climate change; it has to do with protecting corporate and public interests.\n",
            "\n",
            "There are two kinds of actions they can take: what they could have done, and what they didn't do. If we had done the things we\n",
            "\n",
            "[500 | 944.74] loss=2.96 avg=2.75\n",
            "[501 | 946.41] loss=2.96 avg=2.75\n",
            "[502 | 948.09] loss=2.92 avg=2.76\n",
            "[503 | 949.75] loss=2.53 avg=2.75\n",
            "[504 | 951.42] loss=2.81 avg=2.75\n",
            "[505 | 953.09] loss=2.70 avg=2.75\n",
            "[506 | 954.76] loss=2.06 avg=2.75\n",
            "[507 | 956.44] loss=3.03 avg=2.75\n",
            "[508 | 958.10] loss=2.89 avg=2.75\n",
            "[509 | 959.77] loss=2.77 avg=2.75\n",
            "[510 | 961.44] loss=2.17 avg=2.75\n",
            "[511 | 963.10] loss=2.29 avg=2.74\n",
            "[512 | 964.76] loss=3.05 avg=2.74\n",
            "[513 | 966.42] loss=2.24 avg=2.74\n",
            "[514 | 968.06] loss=2.68 avg=2.74\n",
            "[515 | 969.71] loss=2.54 avg=2.74\n",
            "[516 | 971.36] loss=2.89 avg=2.74\n",
            "[517 | 973.02] loss=2.35 avg=2.73\n",
            "[518 | 974.66] loss=2.98 avg=2.74\n",
            "[519 | 976.31] loss=2.34 avg=2.73\n",
            "[520 | 977.96] loss=2.67 avg=2.73\n",
            "[521 | 979.60] loss=2.76 avg=2.73\n",
            "[522 | 981.24] loss=2.73 avg=2.73\n",
            "[523 | 982.88] loss=2.71 avg=2.73\n",
            "[524 | 984.52] loss=2.75 avg=2.73\n",
            "[525 | 986.16] loss=2.78 avg=2.73\n",
            "[526 | 987.80] loss=3.19 avg=2.74\n",
            "[527 | 989.42] loss=2.96 avg=2.74\n",
            "[528 | 991.05] loss=2.41 avg=2.74\n",
            "[529 | 992.69] loss=2.70 avg=2.74\n",
            "[530 | 994.31] loss=2.89 avg=2.74\n",
            "[531 | 995.94] loss=2.62 avg=2.74\n",
            "[532 | 997.57] loss=2.44 avg=2.73\n",
            "[533 | 999.21] loss=2.96 avg=2.74\n",
            "[534 | 1000.84] loss=2.72 avg=2.74\n",
            "[535 | 1002.47] loss=2.57 avg=2.73\n",
            "[536 | 1004.11] loss=2.69 avg=2.73\n",
            "[537 | 1005.74] loss=2.44 avg=2.73\n",
            "[538 | 1007.37] loss=2.64 avg=2.73\n",
            "[539 | 1009.00] loss=2.70 avg=2.73\n",
            "[540 | 1010.63] loss=2.47 avg=2.73\n",
            "[541 | 1012.27] loss=2.40 avg=2.72\n",
            "[542 | 1013.90] loss=2.57 avg=2.72\n",
            "[543 | 1015.53] loss=1.66 avg=2.71\n",
            "[544 | 1017.17] loss=2.69 avg=2.71\n",
            "[545 | 1018.81] loss=2.05 avg=2.70\n",
            "[546 | 1020.45] loss=2.58 avg=2.70\n",
            "[547 | 1022.09] loss=2.57 avg=2.70\n",
            "[548 | 1023.74] loss=2.58 avg=2.70\n",
            "[549 | 1025.38] loss=2.68 avg=2.70\n",
            "[550 | 1027.03] loss=2.47 avg=2.70\n",
            "[551 | 1028.68] loss=2.75 avg=2.70\n",
            "[552 | 1030.33] loss=2.96 avg=2.70\n",
            "[553 | 1031.99] loss=3.01 avg=2.70\n",
            "[554 | 1033.65] loss=2.97 avg=2.71\n",
            "[555 | 1035.32] loss=2.83 avg=2.71\n",
            "[556 | 1036.99] loss=2.57 avg=2.71\n",
            "[557 | 1038.65] loss=3.03 avg=2.71\n",
            "[558 | 1040.32] loss=2.89 avg=2.71\n",
            "[559 | 1042.00] loss=2.34 avg=2.71\n",
            "[560 | 1043.68] loss=2.51 avg=2.71\n",
            "[561 | 1045.35] loss=2.91 avg=2.71\n",
            "[562 | 1047.03] loss=2.81 avg=2.71\n",
            "[563 | 1048.70] loss=2.41 avg=2.71\n",
            "[564 | 1050.37] loss=2.97 avg=2.71\n",
            "[565 | 1052.04] loss=3.22 avg=2.71\n",
            "[566 | 1053.71] loss=2.53 avg=2.71\n",
            "[567 | 1055.39] loss=2.95 avg=2.71\n",
            "[568 | 1057.05] loss=2.66 avg=2.71\n",
            "[569 | 1058.72] loss=3.03 avg=2.72\n",
            "[570 | 1060.39] loss=2.67 avg=2.72\n",
            "[571 | 1062.06] loss=2.77 avg=2.72\n",
            "[572 | 1063.73] loss=2.91 avg=2.72\n",
            "[573 | 1065.39] loss=2.78 avg=2.72\n",
            "[574 | 1067.05] loss=2.77 avg=2.72\n",
            "[575 | 1068.71] loss=3.20 avg=2.72\n",
            "[576 | 1070.37] loss=2.79 avg=2.73\n",
            "[577 | 1072.02] loss=2.76 avg=2.73\n",
            "[578 | 1073.67] loss=2.37 avg=2.72\n",
            "[579 | 1075.32] loss=2.58 avg=2.72\n",
            "[580 | 1076.96] loss=2.83 avg=2.72\n",
            "[581 | 1078.59] loss=2.62 avg=2.72\n",
            "[582 | 1080.23] loss=2.66 avg=2.72\n",
            "[583 | 1081.87] loss=2.54 avg=2.72\n",
            "[584 | 1083.51] loss=2.54 avg=2.72\n",
            "[585 | 1085.15] loss=2.73 avg=2.72\n",
            "[586 | 1086.79] loss=2.80 avg=2.72\n",
            "[587 | 1088.43] loss=2.58 avg=2.72\n",
            "[588 | 1090.07] loss=2.98 avg=2.72\n",
            "[589 | 1091.70] loss=2.74 avg=2.72\n",
            "[590 | 1093.33] loss=2.50 avg=2.72\n",
            "[591 | 1094.96] loss=2.52 avg=2.71\n",
            "[592 | 1096.60] loss=1.98 avg=2.71\n",
            "[593 | 1098.23] loss=2.41 avg=2.70\n",
            "[594 | 1099.86] loss=2.42 avg=2.70\n",
            "[595 | 1101.50] loss=2.85 avg=2.70\n",
            "[596 | 1103.12] loss=2.31 avg=2.70\n",
            "[597 | 1104.76] loss=2.72 avg=2.70\n",
            "[598 | 1106.39] loss=2.61 avg=2.70\n",
            "[599 | 1108.02] loss=2.99 avg=2.70\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "ittie, then the CEO of the company, called his wife, asking to meet her at the office. The phone rang. Her husband was a man named Mark, and he had been hired by Google to manage an anti-harassment committee created to track down the people who complained about harassment in general, and especially about women in tech.\n",
            "\n",
            "Briefly, the two had a cordial conversation. She had worked at Google for nearly a decade, so she knew her husband, and he had also worked at Google. Then the phone rang again. This time, the woman, who was apparently Google's new head of employee support, was a woman named Rachel. She was also a new hire, and had no idea who she was, or any of the people who might actually be at risk of harassment at Google.\n",
            "\n",
            "Her first thought was to give her husband an immediate retort by telling him she didn*ck around, but this was wrong. She had never done such a thing. Her husband knew nothing about women in tech. All his years of dealing with them had taught him to never do that, and it would only make it worse if the situation became public. So he called the head of the Google anti-harassment committee and asked what the problem was. What he got was silence. His colleague, meanwhile, just stood there in stunned silence.\n",
            "\n",
            "A Google spokesperson had also been on the list. He'd been hired in November, so Google had hired a new member just last month. Rachel, who was also a newcomer to the board, would be working with two of his colleagues. And if these were his colleagues, they had nothing to do with the issue.\n",
            "\n",
            "By the time she got off the phone with her husband, the woman was ready to leave. She didn*ck around, but it was just a second or two, as she watched the door close. Her husband looked at the door, tried to look relieved, but he soon realized what had happened and how he should have reacted. He should have called her, or at least gone to her first, and let her ask a question — something like \"Is there more we can do?\" — so that he could tell her and her colleagues directly.\n",
            "\n",
            "He also should have told his wife that he was sorry for what he had said about her in their first conversation, but that he didn’t mean to say anything so hurtful to anyone that it was unfair or insulting. This is the only time in his whole life when he’s heard his wife say it, and he should have said, “It is not my job to tell women to go out and have consensual, healthy relationships,”” he said later. He said that he understood that his word might not be exactly what her husband thought was acceptable — but that he was trying to help her understand what was acceptable. He should have acknowledged that, and gone back to trying to help.\n",
            "\n",
            "After a while, after a few more tries and a few more phone calls, Rachel said it was time to leave. So she walked out, walked right past her husband, and left.\n",
            "\n",
            "So much of what he and his coworkers had been taught about women in tech was wrong, and so little was known about the subject. The Google anti-harassment committee was small, poorly funded, and led by a woman, who had been hired by the company as a consultant. They were also the first to speak up about problematic behavior on the tech company’s team, and so had more direct involvement in the problem. When the committee’s chair, one of the men, suggested that his deputy was uncomfortable with bringing up harassment with his wife, the deputy suggested that maybe she should move on and get a job at an even larger company. And so, the Google anti-harassment committee was launched.\n",
            "\n",
            "And then it disappeared. The only thing they had done was give advice. They also invited the male colleagues they knew were feeling upset and needed to know where to turn, and so they told lots of people. But then their advice was not adopted across the entire company: When they did talk to people, they made sure to make sure there was nothing particularly troubling about what they had said.\n",
            "\n",
            "So why did he not tell Rachel what she had said, or asked her to leave when she asked? “The fact that I’ve been married to my wife for eight years … I feel like … that’s a personal thing that I can’t change,” he said he said to himself, looking back. “For her to say this is not my place to speak about this is like saying, ‘My business is my business.’ I feel like I should be doing my job.”\n",
            "\n",
            "He didn’t want to change his job or make a big deal out of it. But the things he had said and done to her in person had been so unwise.\n",
            "\n",
            "[600 | 1132.54] loss=2.89 avg=2.70\n",
            "[601 | 1134.20] loss=2.78 avg=2.70\n",
            "[602 | 1135.87] loss=3.27 avg=2.71\n",
            "[603 | 1137.54] loss=2.61 avg=2.71\n",
            "[604 | 1139.21] loss=1.76 avg=2.70\n",
            "[605 | 1140.88] loss=3.40 avg=2.71\n",
            "[606 | 1142.55] loss=3.03 avg=2.71\n",
            "[607 | 1144.22] loss=2.71 avg=2.71\n",
            "[608 | 1145.89] loss=2.53 avg=2.71\n",
            "[609 | 1147.56] loss=3.05 avg=2.71\n",
            "[610 | 1149.23] loss=2.93 avg=2.71\n",
            "[611 | 1150.90] loss=1.57 avg=2.70\n",
            "[612 | 1152.57] loss=2.34 avg=2.70\n",
            "[613 | 1154.24] loss=2.71 avg=2.70\n",
            "[614 | 1155.91] loss=2.58 avg=2.70\n",
            "[615 | 1157.58] loss=2.60 avg=2.70\n",
            "[616 | 1159.25] loss=2.39 avg=2.69\n",
            "[617 | 1160.91] loss=2.74 avg=2.69\n",
            "[618 | 1162.57] loss=2.72 avg=2.69\n",
            "[619 | 1164.24] loss=2.61 avg=2.69\n",
            "[620 | 1165.90] loss=3.04 avg=2.70\n",
            "[621 | 1167.55] loss=2.70 avg=2.70\n",
            "[622 | 1169.20] loss=2.47 avg=2.69\n",
            "[623 | 1170.86] loss=2.49 avg=2.69\n",
            "[624 | 1172.51] loss=2.79 avg=2.69\n",
            "[625 | 1174.16] loss=3.05 avg=2.70\n",
            "[626 | 1175.80] loss=2.72 avg=2.70\n",
            "[627 | 1177.45] loss=2.39 avg=2.69\n",
            "[628 | 1179.09] loss=2.90 avg=2.70\n",
            "[629 | 1180.73] loss=2.46 avg=2.69\n",
            "[630 | 1182.37] loss=2.58 avg=2.69\n",
            "[631 | 1184.01] loss=2.96 avg=2.69\n",
            "[632 | 1185.64] loss=2.42 avg=2.69\n",
            "[633 | 1187.28] loss=2.62 avg=2.69\n",
            "[634 | 1188.91] loss=2.79 avg=2.69\n",
            "[635 | 1190.54] loss=2.63 avg=2.69\n",
            "[636 | 1192.17] loss=2.65 avg=2.69\n",
            "[637 | 1193.80] loss=2.47 avg=2.69\n",
            "[638 | 1195.43] loss=2.06 avg=2.68\n",
            "[639 | 1197.06] loss=2.69 avg=2.68\n",
            "[640 | 1198.69] loss=2.86 avg=2.68\n",
            "[641 | 1200.32] loss=2.10 avg=2.68\n",
            "[642 | 1201.94] loss=2.30 avg=2.68\n",
            "[643 | 1203.57] loss=1.85 avg=2.67\n",
            "[644 | 1205.21] loss=2.41 avg=2.66\n",
            "[645 | 1206.83] loss=2.56 avg=2.66\n",
            "[646 | 1208.47] loss=2.93 avg=2.67\n",
            "[647 | 1210.10] loss=2.47 avg=2.66\n",
            "[648 | 1211.74] loss=2.69 avg=2.66\n",
            "[649 | 1213.37] loss=2.55 avg=2.66\n",
            "[650 | 1215.01] loss=2.47 avg=2.66\n",
            "[651 | 1216.65] loss=2.10 avg=2.66\n",
            "[652 | 1218.29] loss=1.87 avg=2.65\n",
            "[653 | 1219.93] loss=2.20 avg=2.64\n",
            "[654 | 1221.58] loss=2.31 avg=2.64\n",
            "[655 | 1223.22] loss=2.59 avg=2.64\n",
            "[656 | 1224.87] loss=2.37 avg=2.64\n",
            "[657 | 1226.52] loss=2.91 avg=2.64\n",
            "[658 | 1228.17] loss=2.76 avg=2.64\n",
            "[659 | 1229.83] loss=2.90 avg=2.64\n",
            "[660 | 1231.49] loss=2.65 avg=2.64\n",
            "[661 | 1233.16] loss=2.44 avg=2.64\n",
            "[662 | 1234.83] loss=2.90 avg=2.64\n",
            "[663 | 1236.49] loss=2.82 avg=2.65\n",
            "[664 | 1238.16] loss=2.94 avg=2.65\n",
            "[665 | 1239.84] loss=3.07 avg=2.65\n",
            "[666 | 1241.51] loss=2.46 avg=2.65\n",
            "[667 | 1243.18] loss=2.92 avg=2.65\n",
            "[668 | 1244.85] loss=2.64 avg=2.65\n",
            "[669 | 1246.53] loss=2.42 avg=2.65\n",
            "[670 | 1248.20] loss=2.55 avg=2.65\n",
            "[671 | 1249.87] loss=2.39 avg=2.65\n",
            "[672 | 1251.54] loss=1.74 avg=2.64\n",
            "[673 | 1253.21] loss=2.57 avg=2.64\n",
            "[674 | 1254.87] loss=2.57 avg=2.64\n",
            "[675 | 1256.54] loss=2.70 avg=2.64\n",
            "[676 | 1258.22] loss=2.62 avg=2.64\n",
            "[677 | 1259.88] loss=2.53 avg=2.64\n",
            "[678 | 1261.54] loss=2.83 avg=2.64\n",
            "[679 | 1263.20] loss=2.65 avg=2.64\n",
            "[680 | 1264.86] loss=1.59 avg=2.63\n",
            "[681 | 1266.51] loss=2.24 avg=2.62\n",
            "[682 | 1268.16] loss=1.91 avg=2.62\n",
            "[683 | 1269.81] loss=2.52 avg=2.62\n",
            "[684 | 1271.46] loss=2.29 avg=2.61\n",
            "[685 | 1273.10] loss=2.58 avg=2.61\n",
            "[686 | 1274.74] loss=3.00 avg=2.62\n",
            "[687 | 1276.38] loss=3.00 avg=2.62\n",
            "[688 | 1278.03] loss=2.67 avg=2.62\n",
            "[689 | 1279.66] loss=2.76 avg=2.62\n",
            "[690 | 1281.29] loss=2.45 avg=2.62\n",
            "[691 | 1282.92] loss=2.37 avg=2.62\n",
            "[692 | 1284.55] loss=2.48 avg=2.62\n",
            "[693 | 1286.19] loss=2.37 avg=2.61\n",
            "[694 | 1287.82] loss=3.13 avg=2.62\n",
            "[695 | 1289.45] loss=2.70 avg=2.62\n",
            "[696 | 1291.09] loss=2.86 avg=2.62\n",
            "[697 | 1292.72] loss=2.56 avg=2.62\n",
            "[698 | 1294.35] loss=2.41 avg=2.62\n",
            "[699 | 1295.98] loss=2.87 avg=2.62\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " screstful. I could probably use that extra hour at the beach or on a vacation. And at times I'm the type of person who would never ask my family for assistance at home. As a result, I can just grab my phone and be on my way, and it doesn't really bother me that I'm spending that much time in an unfamiliar location, on an unfamiliar trip. But I also don’t want to feel like I’re sacrificing, like I’re saying that I’m not as good as the people who worked for me before. And so we’re like, yeah, we’re going to look these other places out. We're going to be so proud of ourselves with that decision.\n",
            "\n",
            "I’ve been using these app on and off since 2014, and it’s definitely been my most prolific use, probably. I have three different accounts. I also have a bunch of different Google docs. I have all of my own social calendars. It’s a huge thing.\n",
            "\n",
            "But I also love what they’ve created for travel, so that’s what I always keep on hand.\n",
            "\n",
            "This interview has been edited and condensed.<|endoftext|>This morning, a federal judge in California announced a ruling that could dramatically shift the way the US government prosecutes Internet troll accounts—a potentially dangerous development that threatens to undermine the legal foundations for an emerging technology that has a hard time even fitting into a computer’s operating system.\n",
            "\n",
            "The name of the victim will be revealed in the lawsuit; the troll—who will likely be identified only as The Defendant—will not. The court’s ruling—PDF—presents a new legal framework that the US government cannot evade while attempting to use the law in ways that it has been unable to do so by claiming that the service is operated for, and in compliance with, the Computer Fraud & Abuse Act, or CFAA. In doing so, it appears to undermine the legal foundation for Tor.\n",
            "\n",
            "The US government does not have statutory authority to prosecute Internet trolls under the CFAA, but a variety of efforts have occurred at the federal level to enforce its anti-troll provisions through the Computer Fraud and Abuse Act (CFAA). As the Wall Street Journal editorial page put it, “[T]his lawsuit could have real consequences for the virtual world of the internet—and it will not happen in the court of public opinion. Instead it will be reported by news agencies, broadcast on television and debated over lunch tables. It will affect every aspect of the American economy.”\n",
            "\n",
            "The ruling against the defendant was unanimous, and it sets a scary precedent that many US internet users will eventually find themselves exposed to as they navigate the legal system under the CFAA. If the decision stands, online users will have to determine which of their online activities fall under the definition of “cybercrimes” under federal law, and which do not. Some of our most personal information will be at risk. And the internet could well suffer.\n",
            "\n",
            "While federal courts around the country have been grappling with whether to apply criminal liability to the conduct of sites like the one running the virtual bank Tor, and to which the defendant refers, there is also “not a single federal statute that expressly covers” any service that uses the internet to store information or conduct without the people or the things of value being involved. On the other hand, most other US state and local laws that attempt to criminalize these actions have not been tested in front of a US court, and are therefore too vague to apply in a criminal climate. The result is that cybercriminals are free to hide behind the guise of freedom in the hope that the law will not catch up to them.\n",
            "\n",
            "The ruling has been praised by technologists, including the founders of Tor, the most prominent website to come under attack by the US government, as well as activists and policymakers. “The ruling is great for freedom of expression, because it makes sure there are a few rules,” says Ryan Noland, CEO of the Internet Policy Project, a think tank. “But also it’s great for freedom of people to be able to use the Tor network to avoid being targeted by a government, because it means your internet connections won’t be monitored by the government.”\n",
            "\n",
            "This case stems from a 2015 incident in which authorities arrested a man in New Jersey on computer-crimes counts, after he was found in possession of child pornography. During a routine search of the suspect’s computers, they discovered an image file hosted on Tor, “a protocol by which websites hidden inside networked computers are identified as operating-system and application-server-related.” The police reportedly found text messages and images depicting child sexual abuse.\n",
            "\n",
            "An initial court-supervised search revealed only one file of child pornography, sent to the suspect from a different ISP. In\n",
            "\n",
            "[700 | 1320.10] loss=2.82 avg=2.62\n",
            "[701 | 1321.75] loss=2.79 avg=2.63\n",
            "[702 | 1323.40] loss=2.62 avg=2.63\n",
            "[703 | 1325.05] loss=2.49 avg=2.62\n",
            "[704 | 1326.71] loss=2.56 avg=2.62\n",
            "[705 | 1328.37] loss=2.44 avg=2.62\n",
            "[706 | 1330.03] loss=2.26 avg=2.62\n",
            "[707 | 1331.70] loss=2.18 avg=2.61\n",
            "[708 | 1333.37] loss=1.17 avg=2.60\n",
            "[709 | 1335.04] loss=2.44 avg=2.60\n",
            "[710 | 1336.71] loss=2.08 avg=2.59\n",
            "[711 | 1338.39] loss=2.63 avg=2.59\n",
            "[712 | 1340.06] loss=2.61 avg=2.59\n",
            "[713 | 1341.73] loss=2.88 avg=2.60\n",
            "[714 | 1343.41] loss=2.49 avg=2.60\n",
            "[715 | 1345.09] loss=2.49 avg=2.59\n",
            "[716 | 1346.75] loss=2.71 avg=2.60\n",
            "[717 | 1348.43] loss=2.31 avg=2.59\n",
            "[718 | 1350.10] loss=2.88 avg=2.60\n",
            "[719 | 1351.79] loss=2.18 avg=2.59\n",
            "[720 | 1353.48] loss=2.43 avg=2.59\n",
            "[721 | 1355.15] loss=2.95 avg=2.59\n",
            "[722 | 1356.82] loss=2.84 avg=2.60\n",
            "[723 | 1358.49] loss=2.53 avg=2.60\n",
            "[724 | 1360.15] loss=2.73 avg=2.60\n",
            "[725 | 1361.82] loss=2.14 avg=2.59\n",
            "[726 | 1363.48] loss=2.98 avg=2.60\n",
            "[727 | 1365.14] loss=3.19 avg=2.60\n",
            "[728 | 1366.79] loss=2.76 avg=2.60\n",
            "[729 | 1368.45] loss=2.37 avg=2.60\n",
            "[730 | 1370.10] loss=2.28 avg=2.60\n",
            "[731 | 1371.75] loss=2.41 avg=2.60\n",
            "[732 | 1373.39] loss=2.73 avg=2.60\n",
            "[733 | 1375.04] loss=2.06 avg=2.59\n",
            "[734 | 1376.68] loss=2.44 avg=2.59\n",
            "[735 | 1378.32] loss=2.56 avg=2.59\n",
            "[736 | 1379.96] loss=2.61 avg=2.59\n",
            "[737 | 1381.61] loss=2.34 avg=2.59\n",
            "[738 | 1383.24] loss=2.26 avg=2.58\n",
            "[739 | 1384.88] loss=0.86 avg=2.57\n",
            "[740 | 1386.51] loss=2.20 avg=2.56\n",
            "[741 | 1388.14] loss=2.62 avg=2.56\n",
            "[742 | 1389.76] loss=2.78 avg=2.57\n",
            "[743 | 1391.40] loss=2.46 avg=2.57\n",
            "[744 | 1393.02] loss=2.41 avg=2.56\n",
            "[745 | 1394.66] loss=2.48 avg=2.56\n",
            "[746 | 1396.29] loss=2.41 avg=2.56\n",
            "[747 | 1397.92] loss=2.25 avg=2.56\n",
            "[748 | 1399.54] loss=2.28 avg=2.56\n",
            "[749 | 1401.15] loss=2.40 avg=2.55\n",
            "[750 | 1402.78] loss=3.03 avg=2.56\n",
            "[751 | 1404.41] loss=2.47 avg=2.56\n",
            "[752 | 1406.04] loss=3.16 avg=2.56\n",
            "[753 | 1407.67] loss=2.70 avg=2.56\n",
            "[754 | 1409.31] loss=2.60 avg=2.57\n",
            "[755 | 1410.92] loss=2.83 avg=2.57\n",
            "[756 | 1412.56] loss=2.59 avg=2.57\n",
            "[757 | 1414.20] loss=2.16 avg=2.56\n",
            "[758 | 1415.84] loss=2.87 avg=2.57\n",
            "[759 | 1417.48] loss=2.59 avg=2.57\n",
            "[760 | 1419.13] loss=2.56 avg=2.57\n",
            "[761 | 1420.78] loss=2.82 avg=2.57\n",
            "[762 | 1422.43] loss=2.61 avg=2.57\n",
            "[763 | 1424.07] loss=2.63 avg=2.57\n",
            "[764 | 1425.73] loss=0.80 avg=2.55\n",
            "[765 | 1427.39] loss=2.50 avg=2.55\n",
            "[766 | 1429.05] loss=2.48 avg=2.55\n",
            "[767 | 1430.71] loss=2.77 avg=2.55\n",
            "[768 | 1432.38] loss=2.49 avg=2.55\n",
            "[769 | 1434.04] loss=2.24 avg=2.55\n",
            "[770 | 1435.71] loss=2.90 avg=2.55\n",
            "[771 | 1437.39] loss=2.25 avg=2.55\n",
            "[772 | 1439.06] loss=2.47 avg=2.55\n",
            "[773 | 1440.73] loss=2.49 avg=2.55\n",
            "[774 | 1442.41] loss=2.79 avg=2.55\n",
            "[775 | 1444.08] loss=2.41 avg=2.55\n",
            "[776 | 1445.75] loss=2.65 avg=2.55\n",
            "[777 | 1447.43] loss=2.68 avg=2.55\n",
            "[778 | 1449.10] loss=2.72 avg=2.55\n",
            "[779 | 1450.77] loss=2.80 avg=2.56\n",
            "[780 | 1452.44] loss=2.87 avg=2.56\n",
            "[781 | 1454.12] loss=2.68 avg=2.56\n",
            "[782 | 1455.79] loss=2.17 avg=2.56\n",
            "[783 | 1457.46] loss=3.35 avg=2.56\n",
            "[784 | 1459.13] loss=2.92 avg=2.57\n",
            "[785 | 1460.80] loss=2.42 avg=2.57\n",
            "[786 | 1462.46] loss=3.09 avg=2.57\n",
            "[787 | 1464.13] loss=2.58 avg=2.57\n",
            "[788 | 1465.79] loss=3.12 avg=2.58\n",
            "[789 | 1467.45] loss=2.65 avg=2.58\n",
            "[790 | 1469.10] loss=2.66 avg=2.58\n",
            "[791 | 1470.76] loss=3.17 avg=2.59\n",
            "[792 | 1472.40] loss=2.36 avg=2.58\n",
            "[793 | 1474.05] loss=1.08 avg=2.57\n",
            "[794 | 1475.69] loss=2.47 avg=2.57\n",
            "[795 | 1477.33] loss=2.89 avg=2.57\n",
            "[796 | 1478.97] loss=2.62 avg=2.57\n",
            "[797 | 1480.61] loss=1.55 avg=2.56\n",
            "[798 | 1482.25] loss=2.95 avg=2.56\n",
            "[799 | 1483.89] loss=2.62 avg=2.56\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " or not to keep doing it.   But to stay within reasonable limits, it is going to be hard and expensive.\n",
            "So while the government was making good on its promise to crack down on the illegal internet sales of guns, the manufacturers of these banned items were trying their best to keep these sales going, at a profit, to the tune of $3.4 million.  And, of course, there was the NRA, which spent tens of millions of dollars in support of House members who tried to shut down the government.\n",
            "As I argued last year , the problem with gun violence is not that someone has a gun, but that someone has a gun. And it remains one of the major arguments that politicians make when it comes to regulating guns—to reduce gun violence, while also reducing gun consumption and gun sales. But there is a reason these manufacturers still have an important legal and political role in gun regulation. And it is that gun prohibition is almost always justified in the name of safety.\n",
            "At least, that is the argument. Gun control advocates never actually mention or even think about the fact that, just like the gun manufactures, they are only serving a very limited, very narrow market.  Many people with personal guns, including the manufacturers, are using their business and the profit stream they generate to make up for this.\n",
            "At the same time, gun prohibition in this country is never truly about safety.  It is always about maximizing profits and the ability of the gun manufacturers to sell to a relatively small, but very profitable, market. \n",
            "In this case, the NRA has worked hard to ensure that the legal gun market remains saturated. \n",
            "That may be convenient for gun manufacturers, but it is a shortsighted view of what gun safety should be about.\n",
            "For decades now, Congress and law-enforcement officials have tried their best to make it more difficult to buy or sell firearms. As part of that effort, Congress and the Obama Administration have required background checks, and some states have adopted gun-control laws. But the gun industry has been very successful at making the process much less expensive and easier to navigate.\n",
            "This isn’t true in every state. Some have laws that limit who can buy guns--even if they can’t demonstrate to a federal agency what kind of gun they’re buying. Some states impose higher costs on sellers because they charge a lot more for background checks. And in some places, like the northern state of Nebraska, the Department of Justice has warned that it would prosecute all gun sellers based on a state with an \"a strong arm statute.\" In other states, law-enforcement sources say even more stringent sales laws and high costs make it cheaper for people to sell their guns in private.\n",
            "In some places, just selling a gun is much more expensive than buying one. In Florida, for example, a new gun owner who wants to add a concealed-weapon permit and buy a firearm can’t do that without a background check or through licensed gun dealers, depending on how one is certified.\n",
            "“It is hard to estimate how many people would have difficulty getting one through a state or federal transfer process,” Chris Cox, a lobbyist for one such state, told me. “That will go down as costs.” But gun-violence researchers say it can probably be done. If every gun-smuggling ring in this country is like a gun-smuggling ring, almost everyone would wind up in a police mugshot.\n",
            "Gunmakers may make an effort to make the process easier, but the overall effect is that the gun companies have been buying up small players as they struggle with their own high costs.\n",
            "The NRA’s own research seems to confirm this.  The organization compiled a survey of 1,000 gun owners between September and December of last year, with the question of what it does to make people aware that the NRA works with federal and state governments on firearms issues, and found that only 20 percent of respondents would say that the NRA had any influence over their purchase--not an increase of any single percentage point.\n",
            "But what about in states with strong gun laws? In Maine, where a law allows carry in all grocery stores but prohibited hunting, that number was 48 percent. And in Alabama, it was 32 percent.\n",
            "The question is not whether gun companies are aware of efforts to lower their prices. It’s whether, in places that have passed strong gun laws, gun manufacturers can keep selling these products--if they get rid of restrictions that make those regulations harder and harder for people to pass.\n",
            "It would be easy to blame the NRA for not being able to raise the overall costs it was charging people for buying guns. Or, at least, to blame the federal government. But as a gun buyer himself, I don’t buy that hypothesis. Rather, I’m buying it because my own experience tells me that the level of ignorance among gun manufacturers and dealers about all of the potential policies that could be\n",
            "\n",
            "[800 | 1508.26] loss=2.52 avg=2.56\n",
            "[801 | 1509.90] loss=2.49 avg=2.56\n",
            "[802 | 1511.53] loss=2.41 avg=2.56\n",
            "[803 | 1513.17] loss=2.55 avg=2.56\n",
            "[804 | 1514.81] loss=2.22 avg=2.56\n",
            "[805 | 1516.44] loss=2.19 avg=2.56\n",
            "[806 | 1518.09] loss=2.24 avg=2.55\n",
            "[807 | 1519.73] loss=2.05 avg=2.55\n",
            "[808 | 1521.38] loss=2.51 avg=2.55\n",
            "[809 | 1523.04] loss=2.56 avg=2.55\n",
            "[810 | 1524.68] loss=2.61 avg=2.55\n",
            "[811 | 1526.34] loss=2.89 avg=2.55\n",
            "[812 | 1528.00] loss=2.48 avg=2.55\n",
            "[813 | 1529.66] loss=2.62 avg=2.55\n",
            "[814 | 1531.33] loss=1.82 avg=2.54\n",
            "[815 | 1533.00] loss=2.71 avg=2.55\n",
            "[816 | 1534.65] loss=2.16 avg=2.54\n",
            "[817 | 1536.32] loss=2.84 avg=2.54\n",
            "[818 | 1537.99] loss=2.54 avg=2.54\n",
            "[819 | 1539.67] loss=3.06 avg=2.55\n",
            "[820 | 1541.35] loss=2.54 avg=2.55\n",
            "[821 | 1543.03] loss=3.18 avg=2.56\n",
            "[822 | 1544.70] loss=2.75 avg=2.56\n",
            "[823 | 1546.38] loss=2.98 avg=2.56\n",
            "[824 | 1548.06] loss=2.51 avg=2.56\n",
            "[825 | 1549.74] loss=2.11 avg=2.56\n",
            "[826 | 1551.42] loss=2.18 avg=2.55\n",
            "[827 | 1553.12] loss=2.76 avg=2.56\n",
            "[828 | 1554.80] loss=2.47 avg=2.55\n",
            "[829 | 1556.48] loss=2.32 avg=2.55\n",
            "[830 | 1558.17] loss=2.61 avg=2.55\n",
            "[831 | 1559.84] loss=2.33 avg=2.55\n",
            "[832 | 1561.51] loss=2.45 avg=2.55\n",
            "[833 | 1563.20] loss=2.71 avg=2.55\n",
            "[834 | 1564.86] loss=2.32 avg=2.55\n",
            "[835 | 1566.54] loss=1.50 avg=2.54\n",
            "[836 | 1568.20] loss=2.62 avg=2.54\n",
            "[837 | 1569.86] loss=2.46 avg=2.54\n",
            "[838 | 1571.51] loss=2.35 avg=2.54\n",
            "[839 | 1573.17] loss=2.59 avg=2.54\n",
            "[840 | 1574.83] loss=2.53 avg=2.54\n",
            "[841 | 1576.48] loss=2.32 avg=2.53\n",
            "[842 | 1578.14] loss=2.75 avg=2.54\n",
            "[843 | 1579.78] loss=3.47 avg=2.55\n",
            "[844 | 1581.43] loss=2.36 avg=2.54\n",
            "[845 | 1583.08] loss=0.59 avg=2.52\n",
            "[846 | 1584.72] loss=2.82 avg=2.53\n",
            "[847 | 1586.35] loss=3.32 avg=2.54\n",
            "[848 | 1587.98] loss=2.01 avg=2.53\n",
            "[849 | 1589.62] loss=2.91 avg=2.53\n",
            "[850 | 1591.25] loss=2.63 avg=2.53\n",
            "[851 | 1592.89] loss=3.10 avg=2.54\n",
            "[852 | 1594.52] loss=2.27 avg=2.54\n",
            "[853 | 1596.15] loss=3.02 avg=2.54\n",
            "[854 | 1597.78] loss=2.51 avg=2.54\n",
            "[855 | 1599.41] loss=2.87 avg=2.55\n",
            "[856 | 1601.04] loss=2.80 avg=2.55\n",
            "[857 | 1602.68] loss=2.32 avg=2.55\n",
            "[858 | 1604.31] loss=2.22 avg=2.54\n",
            "[859 | 1605.94] loss=2.56 avg=2.54\n",
            "[860 | 1607.58] loss=2.17 avg=2.54\n",
            "[861 | 1609.21] loss=3.12 avg=2.54\n",
            "[862 | 1610.85] loss=2.58 avg=2.55\n",
            "[863 | 1612.49] loss=2.13 avg=2.54\n",
            "[864 | 1614.13] loss=2.08 avg=2.54\n",
            "[865 | 1615.76] loss=2.00 avg=2.53\n",
            "[866 | 1617.41] loss=2.78 avg=2.53\n",
            "[867 | 1619.05] loss=2.40 avg=2.53\n",
            "[868 | 1620.70] loss=2.34 avg=2.53\n",
            "[869 | 1622.35] loss=2.56 avg=2.53\n",
            "[870 | 1624.00] loss=2.58 avg=2.53\n",
            "[871 | 1625.66] loss=2.50 avg=2.53\n",
            "[872 | 1627.31] loss=2.36 avg=2.53\n",
            "[873 | 1628.98] loss=2.27 avg=2.53\n",
            "[874 | 1630.65] loss=3.01 avg=2.53\n",
            "[875 | 1632.33] loss=2.71 avg=2.53\n",
            "[876 | 1633.99] loss=2.09 avg=2.53\n",
            "[877 | 1635.67] loss=2.39 avg=2.53\n",
            "[878 | 1637.36] loss=2.65 avg=2.53\n",
            "[879 | 1639.05] loss=2.53 avg=2.53\n",
            "[880 | 1640.72] loss=2.77 avg=2.53\n",
            "[881 | 1642.42] loss=2.13 avg=2.53\n",
            "[882 | 1644.10] loss=2.29 avg=2.52\n",
            "[883 | 1645.78] loss=1.32 avg=2.51\n",
            "[884 | 1647.47] loss=2.37 avg=2.51\n",
            "[885 | 1649.15] loss=2.91 avg=2.52\n",
            "[886 | 1650.82] loss=2.81 avg=2.52\n",
            "[887 | 1652.50] loss=2.26 avg=2.52\n",
            "[888 | 1654.16] loss=2.39 avg=2.51\n",
            "[889 | 1655.84] loss=2.45 avg=2.51\n",
            "[890 | 1657.51] loss=2.71 avg=2.52\n",
            "[891 | 1659.17] loss=2.57 avg=2.52\n",
            "[892 | 1660.83] loss=2.30 avg=2.51\n",
            "[893 | 1662.50] loss=2.55 avg=2.51\n",
            "[894 | 1664.16] loss=2.08 avg=2.51\n",
            "[895 | 1665.82] loss=2.11 avg=2.51\n",
            "[896 | 1667.48] loss=2.36 avg=2.50\n",
            "[897 | 1669.13] loss=2.64 avg=2.51\n",
            "[898 | 1670.78] loss=2.59 avg=2.51\n",
            "[899 | 1672.44] loss=2.33 avg=2.51\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " much a part of my upbringing. There was a time, before the internet, when I didn’t have a mobile phone or a computer…”\n",
            "\n",
            "Many families are still looking to technology to save them when economic or social challenges demand it of them. On Tuesday night, in the early hours of the morning, one of those families was watching a preening man with a massive mic on the small screen of a luxury apartment building across the street from AMC Theaters. It was the premiere of Transformers: The Last Knight, in theaters across the country, and a new trailer popped up onscreen: a bald white man holding what looks to be a giant golden microphone across from a pair of black-tie-dress-wearing, neck-length silver-screen-actress-as-protégée-of-the-worlds-first-filthy-foods-and-sociopath-billionaire-her-voice-over-style-suit people were in the audience. A shot of the screen projected a graphic image, with a star spelling out “HER MOVIE” in giant, neon letters across the bottom. At the top of the picture, emblazoned with the words “ATLAS, A CLASSIC FILM,” the phrase took root in my cultural and religious vernacular. And then (I swear it was probably 4 a.m.), the film started playing.\n",
            "\n",
            "My husband, Sean, was asleep. I crawled out from under bed covers to peep on him. There was a loud crash, and I could see he had been knocked out cold by a shower of glass shards—glass shards that appeared to come from a nearby building, smashing his glasses off the floor and sending him sprawled out on the living-room couch. He was quickly wrapped in a blanket, and I curled up with my duvet shut and drifted off to sleep.\n",
            "\n",
            "I was asleep for good that night, too.\n",
            "\n",
            "In the hours that followed, I heard rumors spread across social media that Transformers: The Last Knight had grossed too much to be released—that it was about to go from a $175 million to $190 million global opening weekend, and that Sony was going to pull the plug. And yet, the film opened nears $100 million, and seems poised to hit the $200 million mark. I could go on.\n",
            "\n",
            "This is partly due to the fact that, for a film about a group of cyborgs who are forced to team up with an old enemy of the United States to save the world from a robot uprising, the marketing campaigns for the first two Transformers movies were lousy. For Transformers: Revenge of the Fallen, the biggest marketing push came a decade later. And I say “biggest” because in that original film, the Transformers were pretty terrible. The movie follows the Autobots, an old bunch who get recruited into the new Decepticon alliance—and who get even more remote from reality as the movie goes on.\n",
            "\n",
            "For starters, the robot is a joke. To the untrained eye, it looks good, sounds great, and does almost everything you'd expect a drone to do.\n",
            "\n",
            "However, it also has problems: When the Autobots need to land a rescue mission, the robot crashes into sand dunes around the world, killing everyone on board. On the ground, the robot is a failure, as the pilots I spoke to for this essay have all said. When the rescue mission takes off, it often comes too soon, and the robots can't keep up. The crew often doesn’t have the control they’d like to achieve, as the pilots described:\n",
            "\n",
            "I was a pilot just like you, except I had the guts and I had a plan and I had the will to do it. So to get through a mission, and it was pretty challenging, I’d sit down with my team and we’d kind of come up with a plan. And then on top of that, we’d have to keep flying the plane and make it work, and that just wouldn’t happen in the way I wanted to fly it. And I’m willing to forgive that because it’s only my first day on the job. And once I’d got through that day, that’s going to be my last day on the job, too.\n",
            "\n",
            "And that’s just through the pilot’s seat.\n",
            "\n",
            "“People like the drone part because it’s not the whole control room—it doesn’t have a whole lot of instruments or anything else,” says Paul Mears, an aerospace operations analyst for the online defense firm Stratfor.\n",
            "\n",
            "And it’s not just a problem for the pilots, Mears says, because even if an autopilot works, ‘it can’t save a robot’s\n",
            "\n",
            "[900 | 1696.67] loss=2.13 avg=2.50\n",
            "[901 | 1698.31] loss=3.26 avg=2.51\n",
            "[902 | 1699.94] loss=2.88 avg=2.51\n",
            "[903 | 1701.58] loss=2.67 avg=2.51\n",
            "[904 | 1703.21] loss=2.85 avg=2.52\n",
            "[905 | 1704.84] loss=2.17 avg=2.51\n",
            "[906 | 1706.48] loss=2.70 avg=2.52\n",
            "[907 | 1708.12] loss=2.57 avg=2.52\n",
            "[908 | 1709.75] loss=2.68 avg=2.52\n",
            "[909 | 1711.39] loss=2.28 avg=2.52\n",
            "[910 | 1713.02] loss=2.03 avg=2.51\n",
            "[911 | 1714.67] loss=2.53 avg=2.51\n",
            "[912 | 1716.31] loss=2.75 avg=2.51\n",
            "[913 | 1717.96] loss=3.06 avg=2.52\n",
            "[914 | 1719.60] loss=3.06 avg=2.52\n",
            "[915 | 1721.25] loss=3.04 avg=2.53\n",
            "[916 | 1722.90] loss=2.65 avg=2.53\n",
            "[917 | 1724.55] loss=2.79 avg=2.53\n",
            "[918 | 1726.20] loss=2.65 avg=2.53\n",
            "[919 | 1727.86] loss=2.45 avg=2.53\n",
            "[920 | 1729.52] loss=2.65 avg=2.53\n",
            "[921 | 1731.18] loss=1.87 avg=2.53\n",
            "[922 | 1732.85] loss=2.54 avg=2.53\n",
            "[923 | 1734.52] loss=2.18 avg=2.52\n",
            "[924 | 1736.19] loss=2.49 avg=2.52\n",
            "[925 | 1737.86] loss=2.78 avg=2.53\n",
            "[926 | 1739.53] loss=2.08 avg=2.52\n",
            "[927 | 1741.20] loss=2.50 avg=2.52\n",
            "[928 | 1742.87] loss=2.17 avg=2.52\n",
            "[929 | 1744.54] loss=2.29 avg=2.52\n",
            "[930 | 1746.21] loss=2.73 avg=2.52\n",
            "[931 | 1747.88] loss=2.56 avg=2.52\n",
            "[932 | 1749.54] loss=2.43 avg=2.52\n",
            "[933 | 1751.22] loss=2.64 avg=2.52\n",
            "[934 | 1752.87] loss=2.80 avg=2.52\n",
            "[935 | 1754.55] loss=3.11 avg=2.53\n",
            "[936 | 1756.21] loss=2.93 avg=2.53\n",
            "[937 | 1757.87] loss=2.50 avg=2.53\n",
            "[938 | 1759.54] loss=2.46 avg=2.53\n",
            "[939 | 1761.19] loss=2.53 avg=2.53\n",
            "[940 | 1762.85] loss=2.61 avg=2.53\n",
            "[941 | 1764.51] loss=2.29 avg=2.53\n",
            "[942 | 1766.17] loss=2.88 avg=2.53\n",
            "[943 | 1767.82] loss=0.82 avg=2.52\n",
            "[944 | 1769.47] loss=2.69 avg=2.52\n",
            "[945 | 1771.12] loss=2.01 avg=2.51\n",
            "[946 | 1772.76] loss=2.69 avg=2.51\n",
            "[947 | 1774.39] loss=2.06 avg=2.51\n",
            "[948 | 1776.03] loss=2.73 avg=2.51\n",
            "[949 | 1777.67] loss=2.84 avg=2.51\n",
            "[950 | 1779.31] loss=2.96 avg=2.52\n",
            "[951 | 1780.95] loss=2.74 avg=2.52\n",
            "[952 | 1782.58] loss=2.44 avg=2.52\n",
            "[953 | 1784.22] loss=1.73 avg=2.51\n",
            "[954 | 1785.85] loss=2.70 avg=2.51\n",
            "[955 | 1787.48] loss=2.09 avg=2.51\n",
            "[956 | 1789.11] loss=1.89 avg=2.50\n",
            "[957 | 1790.74] loss=2.84 avg=2.51\n",
            "[958 | 1792.37] loss=2.12 avg=2.50\n",
            "[959 | 1793.99] loss=2.58 avg=2.50\n",
            "[960 | 1795.63] loss=2.37 avg=2.50\n",
            "[961 | 1797.26] loss=1.94 avg=2.50\n",
            "[962 | 1798.88] loss=2.59 avg=2.50\n",
            "[963 | 1800.51] loss=2.92 avg=2.50\n",
            "[964 | 1802.15] loss=2.18 avg=2.50\n",
            "[965 | 1803.77] loss=2.39 avg=2.50\n",
            "[966 | 1805.40] loss=2.49 avg=2.50\n",
            "[967 | 1807.03] loss=2.38 avg=2.50\n",
            "[968 | 1808.67] loss=0.84 avg=2.48\n",
            "[969 | 1810.30] loss=1.85 avg=2.47\n",
            "[970 | 1811.94] loss=2.67 avg=2.48\n",
            "[971 | 1813.58] loss=0.26 avg=2.45\n",
            "[972 | 1815.22] loss=2.82 avg=2.46\n",
            "[973 | 1816.86] loss=2.47 avg=2.46\n",
            "[974 | 1818.51] loss=2.74 avg=2.46\n",
            "[975 | 1820.15] loss=2.32 avg=2.46\n",
            "[976 | 1821.81] loss=2.37 avg=2.46\n",
            "[977 | 1823.46] loss=2.36 avg=2.46\n",
            "[978 | 1825.11] loss=2.83 avg=2.46\n",
            "[979 | 1826.78] loss=2.40 avg=2.46\n",
            "[980 | 1828.44] loss=3.14 avg=2.47\n",
            "[981 | 1830.08] loss=2.65 avg=2.47\n",
            "[982 | 1831.75] loss=0.88 avg=2.45\n",
            "[983 | 1833.42] loss=2.56 avg=2.45\n",
            "[984 | 1835.09] loss=2.10 avg=2.45\n",
            "[985 | 1836.76] loss=2.49 avg=2.45\n",
            "[986 | 1838.43] loss=2.82 avg=2.45\n",
            "[987 | 1840.09] loss=2.14 avg=2.45\n",
            "[988 | 1841.76] loss=2.67 avg=2.45\n",
            "[989 | 1843.44] loss=2.68 avg=2.46\n",
            "[990 | 1845.11] loss=3.10 avg=2.46\n",
            "[991 | 1846.78] loss=2.13 avg=2.46\n",
            "[992 | 1848.45] loss=2.72 avg=2.46\n",
            "[993 | 1850.12] loss=2.62 avg=2.46\n",
            "[994 | 1851.79] loss=3.06 avg=2.47\n",
            "[995 | 1853.46] loss=2.79 avg=2.47\n",
            "[996 | 1855.12] loss=2.13 avg=2.47\n",
            "[997 | 1856.79] loss=3.25 avg=2.48\n",
            "[998 | 1858.45] loss=2.19 avg=2.47\n",
            "[999 | 1860.11] loss=2.08 avg=2.47\n",
            "Ran 1010 steps.\n",
            "Saving checkpoint/atlantic_technology_345/model-1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUzko5uO8UQT",
        "colab_type": "code",
        "outputId": "68122e05-aa0b-4e61-9f51-91744cad5a72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "## science essays training  - with 345\n",
        "!PYTHONPATH=src ./train.py --dataset /content/all_science.txt --run_name 'atlantic_science_345' --model_name '345M'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0627 16:50:15.011774 140357754636160 deprecation_wrapper.py:119] From /content/gpt-2/src/model.py:147: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
            "\n",
            "W0627 16:50:15.029707 140357754636160 deprecation_wrapper.py:119] From /content/gpt-2/src/memory_saving_gradients.py:13: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n",
            "W0627 16:50:15.118153 140357754636160 deprecation_wrapper.py:119] From ./train.py:88: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "W0627 16:50:15.118532 140357754636160 deprecation_wrapper.py:119] From ./train.py:91: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "2019-06-27 16:50:15.125158: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-06-27 16:50:15.125436: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x135f100 executing computations on platform Host. Devices:\n",
            "2019-06-27 16:50:15.125471: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2019-06-27 16:50:15.139618: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\n",
            "2019-06-27 16:50:15.290366: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-27 16:50:15.290918: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x135e840 executing computations on platform CUDA. Devices:\n",
            "2019-06-27 16:50:15.290950: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2019-06-27 16:50:15.291229: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-27 16:50:15.291599: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-06-27 16:50:15.292067: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-06-27 16:50:15.293698: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-06-27 16:50:15.295152: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-06-27 16:50:15.295903: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-06-27 16:50:15.302508: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-06-27 16:50:15.304487: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-06-27 16:50:15.308548: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-06-27 16:50:15.308709: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-27 16:50:15.309403: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-27 16:50:15.309910: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
            "2019-06-27 16:50:15.309982: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-06-27 16:50:15.311240: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-06-27 16:50:15.311275: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n",
            "2019-06-27 16:50:15.311300: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n",
            "2019-06-27 16:50:15.311667: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-27 16:50:15.312087: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-27 16:50:15.312485: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14202 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "W0627 16:50:15.313313 140357754636160 deprecation_wrapper.py:119] From ./train.py:92: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0627 16:50:25.935252 140357754636160 deprecation.py:323] From /content/gpt-2/src/sample.py:65: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "W0627 16:50:25.949524 140357754636160 deprecation.py:323] From /content/gpt-2/src/sample.py:16: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0627 16:50:25.951174 140357754636160 deprecation.py:323] From /content/gpt-2/src/sample.py:70: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.random.categorical` instead.\n",
            "W0627 16:50:25.960981 140357754636160 deprecation_wrapper.py:119] From ./train.py:121: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "W0627 16:50:40.806552 140357754636160 deprecation_wrapper.py:119] From ./train.py:144: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n",
            "W0627 16:50:40.809587 140357754636160 deprecation_wrapper.py:119] From ./train.py:147: The name tf.summary.merge is deprecated. Please use tf.compat.v1.summary.merge instead.\n",
            "\n",
            "W0627 16:50:40.810414 140357754636160 deprecation_wrapper.py:119] From ./train.py:149: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
            "\n",
            "W0627 16:50:40.811161 140357754636160 deprecation_wrapper.py:119] From ./train.py:152: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "Loading checkpoint models/345M/model.ckpt\n",
            "W0627 16:50:54.465580 140357754636160 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "Loading dataset...\n",
            "100% 1/1 [00:03<00:00,  3.65s/it]\n",
            "dataset has 475532 tokens\n",
            "Training...\n",
            "2019-06-27 16:51:12.017068: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n",
            "2019-06-27 16:51:12.820464: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "[1 | 14.12] loss=2.97 avg=2.97\n",
            "[2 | 15.66] loss=2.69 avg=2.83\n",
            "[3 | 17.19] loss=2.98 avg=2.88\n",
            "[4 | 18.73] loss=2.73 avg=2.84\n",
            "[5 | 20.27] loss=2.85 avg=2.84\n",
            "[6 | 21.81] loss=2.98 avg=2.87\n",
            "[7 | 23.36] loss=2.70 avg=2.84\n",
            "[8 | 24.90] loss=3.00 avg=2.86\n",
            "[9 | 26.45] loss=3.06 avg=2.88\n",
            "[10 | 28.00] loss=2.88 avg=2.88\n",
            "[11 | 29.55] loss=2.91 avg=2.89\n",
            "[12 | 31.10] loss=2.90 avg=2.89\n",
            "[13 | 32.66] loss=2.50 avg=2.86\n",
            "[14 | 34.22] loss=2.94 avg=2.86\n",
            "[15 | 35.77] loss=2.94 avg=2.87\n",
            "[16 | 37.33] loss=2.74 avg=2.86\n",
            "[17 | 38.88] loss=2.98 avg=2.87\n",
            "[18 | 40.43] loss=2.61 avg=2.85\n",
            "[19 | 41.99] loss=2.37 avg=2.82\n",
            "[20 | 43.55] loss=2.72 avg=2.82\n",
            "[21 | 45.10] loss=3.14 avg=2.84\n",
            "[22 | 46.67] loss=2.58 avg=2.82\n",
            "[23 | 48.23] loss=2.92 avg=2.83\n",
            "[24 | 49.79] loss=2.58 avg=2.82\n",
            "[25 | 51.35] loss=3.24 avg=2.83\n",
            "[26 | 52.91] loss=2.45 avg=2.82\n",
            "[27 | 54.47] loss=2.85 avg=2.82\n",
            "[28 | 56.04] loss=2.96 avg=2.82\n",
            "[29 | 57.60] loss=2.63 avg=2.82\n",
            "[30 | 59.16] loss=2.83 avg=2.82\n",
            "[31 | 60.73] loss=2.90 avg=2.82\n",
            "[32 | 62.30] loss=3.02 avg=2.83\n",
            "[33 | 63.86] loss=3.03 avg=2.83\n",
            "[34 | 65.43] loss=3.08 avg=2.84\n",
            "[35 | 67.00] loss=3.21 avg=2.86\n",
            "[36 | 68.57] loss=2.49 avg=2.84\n",
            "[37 | 70.14] loss=2.79 avg=2.84\n",
            "[38 | 71.71] loss=2.39 avg=2.83\n",
            "[39 | 73.28] loss=2.83 avg=2.83\n",
            "[40 | 74.85] loss=2.29 avg=2.81\n",
            "[41 | 76.43] loss=2.64 avg=2.81\n",
            "[42 | 78.00] loss=3.02 avg=2.81\n",
            "[43 | 79.58] loss=2.68 avg=2.81\n",
            "[44 | 81.16] loss=2.35 avg=2.80\n",
            "[45 | 82.74] loss=2.83 avg=2.80\n",
            "[46 | 84.31] loss=3.53 avg=2.82\n",
            "[47 | 85.90] loss=3.04 avg=2.82\n",
            "[48 | 87.48] loss=3.00 avg=2.83\n",
            "[49 | 89.06] loss=2.65 avg=2.82\n",
            "[50 | 90.64] loss=2.64 avg=2.82\n",
            "[51 | 92.24] loss=2.62 avg=2.81\n",
            "[52 | 93.82] loss=3.00 avg=2.82\n",
            "[53 | 95.42] loss=3.01 avg=2.82\n",
            "[54 | 97.01] loss=2.87 avg=2.82\n",
            "[55 | 98.60] loss=2.60 avg=2.82\n",
            "[56 | 100.19] loss=2.89 avg=2.82\n",
            "[57 | 101.78] loss=2.66 avg=2.82\n",
            "[58 | 103.37] loss=2.79 avg=2.82\n",
            "[59 | 104.97] loss=2.89 avg=2.82\n",
            "[60 | 106.57] loss=2.70 avg=2.81\n",
            "[61 | 108.17] loss=3.03 avg=2.82\n",
            "[62 | 109.77] loss=2.97 avg=2.82\n",
            "[63 | 111.38] loss=2.89 avg=2.82\n",
            "[64 | 112.98] loss=2.77 avg=2.82\n",
            "[65 | 114.59] loss=3.47 avg=2.84\n",
            "[66 | 116.20] loss=3.05 avg=2.84\n",
            "[67 | 117.81] loss=3.18 avg=2.85\n",
            "[68 | 119.42] loss=2.94 avg=2.85\n",
            "[69 | 121.03] loss=2.92 avg=2.85\n",
            "[70 | 122.64] loss=2.57 avg=2.85\n",
            "[71 | 124.25] loss=3.12 avg=2.85\n",
            "[72 | 125.86] loss=3.02 avg=2.85\n",
            "[73 | 127.46] loss=2.90 avg=2.85\n",
            "[74 | 129.07] loss=2.79 avg=2.85\n",
            "[75 | 130.68] loss=2.63 avg=2.85\n",
            "[76 | 132.29] loss=2.46 avg=2.84\n",
            "[77 | 133.90] loss=3.17 avg=2.85\n",
            "[78 | 135.50] loss=2.55 avg=2.84\n",
            "[79 | 137.11] loss=2.98 avg=2.85\n",
            "[80 | 138.71] loss=2.78 avg=2.84\n",
            "[81 | 140.32] loss=2.75 avg=2.84\n",
            "[82 | 141.94] loss=2.87 avg=2.84\n",
            "[83 | 143.55] loss=2.99 avg=2.85\n",
            "[84 | 145.15] loss=2.72 avg=2.84\n",
            "[85 | 146.75] loss=2.85 avg=2.84\n",
            "[86 | 148.35] loss=2.89 avg=2.84\n",
            "[87 | 149.96] loss=2.83 avg=2.84\n",
            "[88 | 151.56] loss=2.64 avg=2.84\n",
            "[89 | 153.16] loss=2.51 avg=2.84\n",
            "[90 | 154.75] loss=2.74 avg=2.83\n",
            "[91 | 156.35] loss=3.02 avg=2.84\n",
            "[92 | 157.95] loss=2.62 avg=2.83\n",
            "[93 | 159.55] loss=2.82 avg=2.83\n",
            "[94 | 161.15] loss=2.69 avg=2.83\n",
            "[95 | 162.75] loss=2.88 avg=2.83\n",
            "[96 | 164.35] loss=2.80 avg=2.83\n",
            "[97 | 165.95] loss=2.52 avg=2.83\n",
            "[98 | 167.55] loss=2.76 avg=2.82\n",
            "[99 | 169.15] loss=3.02 avg=2.83\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " of the most common cancer cells. Their function in the body is regulated by factors that vary between people. The gene responsible for making the enzyme has become more common in the population, and scientists say there are more than a dozen similar genes involved in cancer, as well as genetic mutations. As more people inherit the gene for the enzyme, scientists think a large number of their risk will decrease.\n",
            "\n",
            "The new study provides much more detailed evidence of a link, albeit a highly indirect one. But the work will be a boon politically. Trump vowed during the 2016 election to \"cancel\" the landmark Paris deal, but he did not, and he has now promised to withdraw the United States from the accord without delay.\n",
            "\n",
            "Climate change has increased risks of skin cancer, for instance, which has been a serious concern for decades. In this new study, scientists can measure how many people worldwide get their daily daily dose of the cancer-causing hormone estradiol. In previous studies, the hormone was only measured in breast cancers.\n",
            "\n",
            "\"This is a very small study, but it is a big study, and it has a dramatic effect,\" says co-author Jens-Peter M. Dittkire, a medical geneticist whose lab at the University of Oxford has been working on this topic. And Dittkire has noticed a shift in how scientists interpret the findings. \"A lot of the scientific community is very sceptical of this new study,\" he says. When he and his colleagues looked at the data more closely, the findings didn't look so bad. \"We find some good, but not nearly perfect, data,\" he says. \"However, we're getting there.\"\n",
            "\n",
            "More direct effects also are coming soon. The European Union, the single-largest single investor in research in the United States, is starting to look at human genetic engineering and cancer as one of its top goals. Other European countries may follow suit. A few months ago, China signed into law the country's first laws on creating laws based on the new research. But researchers say they think China and other countries might be a long way from creating similar laws themselves.<|endoftext|>I can honestly say that I was totally floored that this movie is in the process of being made. I could not be more humbled by this. Not only due to how much I enjoyed the movie, but in the midst of all that disappointment, I still have so many emotions; heart, joy, horror, excitement, and so much more!\n",
            "\n",
            "The film is set in a place called The Island, and that's right, it's based off of an episode of Game of Thrones. I have only seen it once, and it is one of my absolute go-to movie moments. It had me so close to tears.\n",
            "\n",
            "This movie has been described as a modern day Disney fairy tale, and that's exactly what it is. It was a bit of a letdown seeing the actual movie in theaters, but I was still extremely happy about what it did for the Disney universe.\n",
            "\n",
            "I'm a huge fan of this series. We have a very good relationship, and I haven't watched the book in seven years. I was excited to see how it would play out.\n",
            "\n",
            "And I was even more excited about the ending. I think The Island episode was probably the most exciting part of the movie, and it was one of the most disappointing because of the ending. The show had an incredible amount of heart to it. So, that actually gave me a bit of an edge.\n",
            "\n",
            "And it would be just as appropriate to speak of the film itself too, as it does provide a strong cinematic conclusion for the series.\n",
            "\n",
            "This story, as well, is a bit of a departure for the series. It has more humor than the books, but it still sticks to the characters and does not have too many silly moments. It does, however, make you think, for the most part.\n",
            "\n",
            "It was actually surprising how much time and money Disney spent on this movie, and especially with the amount of fanfare that it received. When the first trailer dropped, people were already on the Internet, and the movie quickly became a hot topic. It has been well-regarded by many, and many people have become quite attached to it.\n",
            "\n",
            "This movie, it looks beautiful and it is certainly fun. It is also pretty damn dark. I don't know what the audience should be so thankful for, but I was pretty happy.\n",
            "\n",
            "How do you feel about this movie and where would you like to see it go next? Sound off:\n",
            "\n",
            "[sources]<|endoftext|>The most dangerous terrorist attack against America since Sept. 11, 2001, was thwarted at least once by a single drone, says a new report.\n",
            "\n",
            "The Justice Department estimates up to 300 civilian drones were used to strike multiple targets in Iraq — including \"one of the highest-profile terrorist attacks in recent U.S. history.\"\n",
            "\n",
            "It was carried out by\n",
            "\n",
            "[100 | 196.56] loss=2.49 avg=2.82\n",
            "[101 | 198.19] loss=2.60 avg=2.82\n",
            "[102 | 199.82] loss=2.84 avg=2.82\n",
            "[103 | 201.46] loss=2.80 avg=2.82\n",
            "[104 | 203.09] loss=2.84 avg=2.82\n",
            "[105 | 204.74] loss=3.04 avg=2.82\n",
            "[106 | 206.38] loss=3.18 avg=2.83\n",
            "[107 | 208.03] loss=2.44 avg=2.82\n",
            "[108 | 209.68] loss=2.90 avg=2.82\n",
            "[109 | 211.33] loss=2.78 avg=2.82\n",
            "[110 | 212.97] loss=2.42 avg=2.82\n",
            "[111 | 214.63] loss=2.58 avg=2.81\n",
            "[112 | 216.27] loss=3.13 avg=2.82\n",
            "[113 | 217.92] loss=3.01 avg=2.82\n",
            "[114 | 219.56] loss=2.62 avg=2.82\n",
            "[115 | 221.21] loss=2.71 avg=2.82\n",
            "[116 | 222.85] loss=2.70 avg=2.81\n",
            "[117 | 224.49] loss=2.74 avg=2.81\n",
            "[118 | 226.13] loss=2.60 avg=2.81\n",
            "[119 | 227.77] loss=2.49 avg=2.81\n",
            "[120 | 229.41] loss=2.50 avg=2.80\n",
            "[121 | 231.05] loss=2.88 avg=2.80\n",
            "[122 | 232.68] loss=2.45 avg=2.80\n",
            "[123 | 234.32] loss=3.11 avg=2.80\n",
            "[124 | 235.95] loss=2.80 avg=2.80\n",
            "[125 | 237.57] loss=2.79 avg=2.80\n",
            "[126 | 239.20] loss=3.40 avg=2.81\n",
            "[127 | 240.83] loss=2.97 avg=2.81\n",
            "[128 | 242.46] loss=2.99 avg=2.81\n",
            "[129 | 244.09] loss=2.88 avg=2.82\n",
            "[130 | 245.71] loss=2.69 avg=2.81\n",
            "[131 | 247.35] loss=2.80 avg=2.81\n",
            "[132 | 248.97] loss=2.70 avg=2.81\n",
            "[133 | 250.60] loss=2.62 avg=2.81\n",
            "[134 | 252.22] loss=2.75 avg=2.81\n",
            "[135 | 253.85] loss=2.64 avg=2.81\n",
            "[136 | 255.46] loss=2.48 avg=2.80\n",
            "[137 | 257.09] loss=2.90 avg=2.80\n",
            "[138 | 258.71] loss=2.81 avg=2.80\n",
            "[139 | 260.32] loss=2.71 avg=2.80\n",
            "[140 | 261.94] loss=2.83 avg=2.80\n",
            "[141 | 263.56] loss=2.80 avg=2.80\n",
            "[142 | 265.18] loss=2.56 avg=2.80\n",
            "[143 | 266.80] loss=2.65 avg=2.80\n",
            "[144 | 268.42] loss=2.69 avg=2.80\n",
            "[145 | 270.04] loss=2.80 avg=2.80\n",
            "[146 | 271.66] loss=2.44 avg=2.79\n",
            "[147 | 273.30] loss=2.62 avg=2.79\n",
            "[148 | 274.92] loss=2.73 avg=2.79\n",
            "[149 | 276.54] loss=2.95 avg=2.79\n",
            "[150 | 278.16] loss=3.11 avg=2.79\n",
            "[151 | 279.79] loss=2.55 avg=2.79\n",
            "[152 | 281.41] loss=3.48 avg=2.80\n",
            "[153 | 283.04] loss=2.26 avg=2.79\n",
            "[154 | 284.67] loss=2.65 avg=2.79\n",
            "[155 | 286.30] loss=2.52 avg=2.79\n",
            "[156 | 287.93] loss=2.76 avg=2.79\n",
            "[157 | 289.56] loss=2.87 avg=2.79\n",
            "[158 | 291.20] loss=2.42 avg=2.78\n",
            "[159 | 292.83] loss=2.76 avg=2.78\n",
            "[160 | 294.47] loss=2.74 avg=2.78\n",
            "[161 | 296.12] loss=2.73 avg=2.78\n",
            "[162 | 297.77] loss=2.90 avg=2.78\n",
            "[163 | 299.42] loss=2.54 avg=2.78\n",
            "[164 | 301.06] loss=2.74 avg=2.78\n",
            "[165 | 302.72] loss=2.91 avg=2.78\n",
            "[166 | 304.37] loss=2.92 avg=2.78\n",
            "[167 | 306.03] loss=2.65 avg=2.78\n",
            "[168 | 307.68] loss=2.74 avg=2.78\n",
            "[169 | 309.34] loss=3.19 avg=2.79\n",
            "[170 | 310.99] loss=2.61 avg=2.78\n",
            "[171 | 312.66] loss=2.51 avg=2.78\n",
            "[172 | 314.31] loss=2.62 avg=2.78\n",
            "[173 | 315.97] loss=2.58 avg=2.78\n",
            "[174 | 317.63] loss=3.05 avg=2.78\n",
            "[175 | 319.29] loss=2.82 avg=2.78\n",
            "[176 | 320.95] loss=2.71 avg=2.78\n",
            "[177 | 322.60] loss=2.81 avg=2.78\n",
            "[178 | 324.26] loss=2.69 avg=2.78\n",
            "[179 | 325.92] loss=2.87 avg=2.78\n",
            "[180 | 327.57] loss=2.83 avg=2.78\n",
            "[181 | 329.22] loss=2.91 avg=2.78\n",
            "[182 | 330.87] loss=2.81 avg=2.78\n",
            "[183 | 332.52] loss=3.02 avg=2.79\n",
            "[184 | 334.17] loss=2.65 avg=2.78\n",
            "[185 | 335.82] loss=2.48 avg=2.78\n",
            "[186 | 337.46] loss=2.53 avg=2.78\n",
            "[187 | 339.10] loss=2.53 avg=2.77\n",
            "[188 | 340.73] loss=2.70 avg=2.77\n",
            "[189 | 342.37] loss=2.74 avg=2.77\n",
            "[190 | 344.00] loss=2.66 avg=2.77\n",
            "[191 | 345.63] loss=2.77 avg=2.77\n",
            "[192 | 347.26] loss=2.65 avg=2.77\n",
            "[193 | 348.89] loss=2.73 avg=2.77\n",
            "[194 | 350.52] loss=2.69 avg=2.77\n",
            "[195 | 352.15] loss=2.79 avg=2.77\n",
            "[196 | 353.78] loss=2.79 avg=2.77\n",
            "[197 | 355.41] loss=2.60 avg=2.77\n",
            "[198 | 357.04] loss=2.56 avg=2.77\n",
            "[199 | 358.67] loss=2.50 avg=2.76\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "ibility, with the aim of providing more jobs for local people. It also aims to promote education among workers in China's largest industrial sector. If this all pans out, then China could possibly see an economic boom this year of around 60 percent, in the wake of the country's biggest ever boom two years ago.\n",
            "\n",
            "Still, these latest plans are not exactly new. During the second trimester of Xi Jinping's second year in office, in January 2016 and February 2016, Chinese authorities published plans to bring new manufacturing jobs to a city of 2 million people in Jiangxi Province (where Xi is from), and to create 300,000 manufacturing jobs over a decade. And recently, Beijing also announced plans to create 600,000 manufacturing jobs over five years.\n",
            "\n",
            "This is only China's second time planning to create 600,000 manufacturing jobs over five years. The first time, it was for 10 years. It worked as expected, as Jiangxi created the vast majority of its manufacturing capacity during China's first five-year economic boom. And although Beijing now plans 300,000 manufacturing jobs by 2020, it did so over just 12 months earlier this year, during Xi Jinping's first year in office. That's about as much as a single city can do in just five years.\n",
            "\n",
            "These plans, though, are only for two years, and China has no experience in doing something like this. It will take years to build the new industrial capacity that Xi wants to develop. He has previously said he would like to create 600,000 manufacturing jobs in five years, and that he also wants 700,000 manufacturing jobs by 2022. (All of this is true if the manufacturing jobs can also be found in China's other provinces, like Wuhan.)\n",
            "\n",
            "China is well-positioned to scale up manufacturing employment, and is a good place to start. In recent years, China's industrial output has nearly doubled, and its manufacturing sector, which now accounts for around 40 percent of China's GDP, is growing at 12 percent annually. (In the United States, even manufacturing jobs can be growth-promoters to the economy.) But China has a long way to go in creating this kind of level of investment in its construction and manufacturing industries, both of which require large amounts of capital, and where investment can be tricky.\n",
            "\n",
            "\"China does not have an established industrial strategy for its growth, which is why the recent industrial strategy, [which] will come to fruition within several years, is quite revolutionary,\" says Matthew Kahn, the president and CEO of the Foundation for Industrial Strategy, a research think tank. \"It represents a bold attempt to reshape the entire Chinese economy.\"\n",
            "\n",
            "China's plan is part of a broader strategy set out in late March. The country has made some impressive announcements already, most recently in September, when the country announced its plans to develop solar panels on the scale of a city.\n",
            "\n",
            "To the best of Xi's knowledge, he and President Trump haven't discussed any such plan, but they could very well have something in Chinese.\n",
            "\n",
            "That said, China has been making big steps in renewable energy lately, thanks to what is essentially a renewable energy program under the state-backed People's Bank of China. It is still quite old, and it can't compete with the most advanced solar projects in the world. Solar panels can be as large as 400 square meters, compared with 300 square meters for the most powerful plants. And the banks, which manage China's investment funds, have limited influence over what gets into their capital accounts.\n",
            "\n",
            "That means that China does not have a big-enough clean-energy program—although it can be very effective at creating a clean-energy mandate—to compete with the United States, and it also means that the US has no clean, competitive program. So, Trump could certainly try to boost US solar deployment by taking a new approach. But it's a long shot.\n",
            "\n",
            "The White House did not respond to requests for comment.\n",
            "\n",
            "What does Xi mean with the 600,000 manufacturing jobs? That depends who you ask. In his speech, Xi promised to build a 500-million-person sector in his country. His team doesn't exactly live in Beijing any time soon, though, and doesn't quite have that much money in reserve.\n",
            "\n",
            "A similar scheme, also being discussed as part of the new plan, wouldn't give China complete control over everything from transportation to education. It would give Chinese companies at least a little leeway, in that they wouldn't be legally required to pay local workers more than the lowest-cost workers.\n",
            "\n",
            "The government hasn't done anything about it yet, as it will have to do for any new factories or infrastructure, experts say. Some of the experts I spoke with were skeptical, pointing out that China has done some impressive development over the years—and had a very different strategy with regard to industrial policy when he came to power.\n",
            "\n",
            "All of these plans, of course, will have their own ups and downs, and\n",
            "\n",
            "[200 | 382.98] loss=2.56 avg=2.76\n",
            "[201 | 384.62] loss=2.86 avg=2.76\n",
            "[202 | 386.26] loss=2.66 avg=2.76\n",
            "[203 | 387.90] loss=2.74 avg=2.76\n",
            "[204 | 389.53] loss=2.81 avg=2.76\n",
            "[205 | 391.18] loss=2.77 avg=2.76\n",
            "[206 | 392.83] loss=2.71 avg=2.76\n",
            "[207 | 394.47] loss=2.62 avg=2.76\n",
            "[208 | 396.11] loss=2.56 avg=2.76\n",
            "[209 | 397.77] loss=2.71 avg=2.76\n",
            "[210 | 399.42] loss=2.65 avg=2.75\n",
            "[211 | 401.08] loss=2.51 avg=2.75\n",
            "[212 | 402.74] loss=2.65 avg=2.75\n",
            "[213 | 404.40] loss=2.85 avg=2.75\n",
            "[214 | 406.06] loss=2.68 avg=2.75\n",
            "[215 | 407.72] loss=2.81 avg=2.75\n",
            "[216 | 409.38] loss=2.52 avg=2.75\n",
            "[217 | 411.04] loss=2.51 avg=2.75\n",
            "[218 | 412.70] loss=2.42 avg=2.74\n",
            "[219 | 414.37] loss=2.72 avg=2.74\n",
            "[220 | 416.03] loss=2.74 avg=2.74\n",
            "[221 | 417.69] loss=2.64 avg=2.74\n",
            "[222 | 419.36] loss=2.39 avg=2.74\n",
            "[223 | 421.01] loss=2.42 avg=2.73\n",
            "[224 | 422.68] loss=2.76 avg=2.73\n",
            "[225 | 424.33] loss=2.93 avg=2.74\n",
            "[226 | 426.00] loss=2.64 avg=2.73\n",
            "[227 | 427.65] loss=2.57 avg=2.73\n",
            "[228 | 429.31] loss=2.95 avg=2.74\n",
            "[229 | 430.96] loss=2.49 avg=2.73\n",
            "[230 | 432.61] loss=2.69 avg=2.73\n",
            "[231 | 434.26] loss=2.72 avg=2.73\n",
            "[232 | 435.91] loss=2.62 avg=2.73\n",
            "[233 | 437.56] loss=2.79 avg=2.73\n",
            "[234 | 439.20] loss=2.76 avg=2.73\n",
            "[235 | 440.84] loss=2.93 avg=2.73\n",
            "[236 | 442.48] loss=2.79 avg=2.73\n",
            "[237 | 444.12] loss=2.55 avg=2.73\n",
            "[238 | 445.75] loss=2.55 avg=2.73\n",
            "[239 | 447.38] loss=2.54 avg=2.73\n",
            "[240 | 449.02] loss=2.98 avg=2.73\n",
            "[241 | 450.66] loss=2.78 avg=2.73\n",
            "[242 | 452.30] loss=2.57 avg=2.73\n",
            "[243 | 453.93] loss=2.65 avg=2.73\n",
            "[244 | 455.57] loss=2.61 avg=2.73\n",
            "[245 | 457.20] loss=2.42 avg=2.72\n",
            "[246 | 458.84] loss=2.83 avg=2.73\n",
            "[247 | 460.46] loss=2.79 avg=2.73\n",
            "[248 | 462.09] loss=2.85 avg=2.73\n",
            "[249 | 463.72] loss=2.54 avg=2.73\n",
            "[250 | 465.36] loss=2.52 avg=2.72\n",
            "[251 | 466.99] loss=2.44 avg=2.72\n",
            "[252 | 468.62] loss=2.66 avg=2.72\n",
            "[253 | 470.25] loss=2.70 avg=2.72\n",
            "[254 | 471.89] loss=2.91 avg=2.72\n",
            "[255 | 473.52] loss=2.49 avg=2.72\n",
            "[256 | 475.14] loss=2.64 avg=2.72\n",
            "[257 | 476.77] loss=2.50 avg=2.72\n",
            "[258 | 478.40] loss=2.63 avg=2.71\n",
            "[259 | 480.03] loss=2.92 avg=2.72\n",
            "[260 | 481.67] loss=2.46 avg=2.71\n",
            "[261 | 483.30] loss=2.64 avg=2.71\n",
            "[262 | 484.94] loss=2.80 avg=2.71\n",
            "[263 | 486.58] loss=2.85 avg=2.72\n",
            "[264 | 488.22] loss=2.69 avg=2.72\n",
            "[265 | 489.86] loss=2.57 avg=2.71\n",
            "[266 | 491.51] loss=2.29 avg=2.71\n",
            "[267 | 493.16] loss=2.63 avg=2.71\n",
            "[268 | 494.80] loss=2.30 avg=2.70\n",
            "[269 | 496.45] loss=2.74 avg=2.70\n",
            "[270 | 498.10] loss=2.60 avg=2.70\n",
            "[271 | 499.76] loss=2.98 avg=2.71\n",
            "[272 | 501.42] loss=2.82 avg=2.71\n",
            "[273 | 503.08] loss=2.57 avg=2.71\n",
            "[274 | 504.74] loss=2.42 avg=2.70\n",
            "[275 | 506.42] loss=2.27 avg=2.70\n",
            "[276 | 508.09] loss=2.42 avg=2.70\n",
            "[277 | 509.77] loss=2.83 avg=2.70\n",
            "[278 | 511.44] loss=2.35 avg=2.69\n",
            "[279 | 513.11] loss=2.32 avg=2.69\n",
            "[280 | 514.78] loss=2.73 avg=2.69\n",
            "[281 | 516.45] loss=2.70 avg=2.69\n",
            "[282 | 518.12] loss=2.85 avg=2.69\n",
            "[283 | 519.80] loss=2.31 avg=2.69\n",
            "[284 | 521.47] loss=2.60 avg=2.69\n",
            "[285 | 523.14] loss=2.73 avg=2.69\n",
            "[286 | 524.81] loss=2.58 avg=2.69\n",
            "[287 | 526.47] loss=2.16 avg=2.68\n",
            "[288 | 528.14] loss=2.86 avg=2.68\n",
            "[289 | 529.80] loss=2.61 avg=2.68\n",
            "[290 | 531.46] loss=2.57 avg=2.68\n",
            "[291 | 533.11] loss=2.48 avg=2.68\n",
            "[292 | 534.77] loss=2.50 avg=2.68\n",
            "[293 | 536.43] loss=2.85 avg=2.68\n",
            "[294 | 538.08] loss=2.78 avg=2.68\n",
            "[295 | 539.73] loss=2.45 avg=2.68\n",
            "[296 | 541.37] loss=2.53 avg=2.68\n",
            "[297 | 543.01] loss=2.92 avg=2.68\n",
            "[298 | 544.65] loss=2.07 avg=2.67\n",
            "[299 | 546.29] loss=2.85 avg=2.67\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " mean he could, because it just wasn't possible for a human body of that size to survive being in the environment.\n",
            "\n",
            "Then, in 2001, a team of researchers led by Dr. Peter Hall at the Natural History Museum of Queensland and colleagues published a detailed study suggesting that at least some of the original fossilized teeth of a very similar species of hominin, called Denisovans, may have been buried at that same site. And that suggested they, too, were buried with the fossils. That is until now, when a new study, led by Dr. Michael V. Oehler at the University of Texas at Austin and his colleagues, reports finding that Denisovans teeth have been discovered. This could be the first evidence that a very early hominin species, the Denisovans, may have had an ancient ancestry similar to that of modern humans. Although the precise identity of the Denisovans remains unclear, Oehler and his colleagues say they now know the hominin species is Homo heidelbergensis, a group of early hominin species first discovered on the Kavka River in the 1960s. The Denisovans may have once resided in South Africa or in Africa and then migrated to Eurasia, they suggest. This may have been how an ancestor of modern-day Europeans first mixed with chimpanzees, or they may have gone extinct in Africa. [More on Denisovans: The New, Spooky Discovery of Denisovans]\n",
            "\n",
            "Scientists are still trying to confirm this new finding, and that's especially important, because the fossils are so ancient, and they've been lying there for so long, that someone has probably been keeping an eye on them from the moment they were laid down in rock — from as many as 2,000 to 4,000 years after their bodies were buried. However, it's an old-school technique, and it wasn't available to the scientists who analyzed these Denisovan fossils when they were found. They can't tell you if the researchers were male or female, or they haven't seen anyone alive today who might recognize the hair on their teeth.\n",
            "\n",
            "The finding helps fill in the gaps in research about the ancestry of Homo. But what it does not, as the Australian team suggests, is tell us whether these Denisova fossils show early forms of human intelligence, or just a few primitive traits that we can trace back to our forebears. Some researchers, such as John Oreskes, argue that the fossils are an interesting clue to human intelligence and that the Denisovan fossils prove there was plenty of time for humans to evolve before our species.\n",
            "\n",
            "Another possibility has nothing to do with evolution, Oreskes told LiveScience. He says that any Homo find should shed more light on how our species evolved over time, so we can determine whether it was an isolated or an expanding species.\n",
            "\n",
            "If Denisova fossils do exist, how do they compare to Denisovan fossils from Africa? You can look at the teeth of both groups, and compare their teeth to a set of other fossils that are known to exist, so you know which ones are Denisovans and which ones aren't, but that's still far from proving definitively if these teeth come from one species or both.\n",
            "\n",
            "[The human genome may hold the answer to some human mysteries]\n",
            "\n",
            "So how can we know it wasn't some new species from Africa that was discovered during the research? Some members of the Denisovan team are even saying that they think it was someone else, something that they haven't previously identified or even known existed.\n",
            "\n",
            "The team's announcement didn't say who that other species might be, although they offered some theories as to the species. \"If possible,\" Oreskes said, \"we believe it's Homo heidelbergensis.\"\n",
            "\n",
            "\n",
            "A new study found that when researchers analyzed the teeth and DNA of a Denisovan, the man himself looked older than the rest of the group.\n",
            "\n",
            "\n",
            "An old hominin, which lived millions of years ago, appears to be doing rather well in an unexpected new study.\n",
            "\n",
            "In a series of bone morphological studies, a pair of researchers showed that the teeth of the Denisovan hominin were more similar to those of modern humans than those of most hominins. And by comparison, the Denisovans also had similar dental profiles to people living today.\n",
            "\n",
            "The studies by the authors of that paper and others, published last year in the Science journal, suggested that the Denisovans lived in a region of Africa where hominins had previously been extinct: Africa. That's a region called Africa, in a sense, although not on a continent scale, because humans have traveled around the globe hundreds of thousands of years in their fossil form.\n",
            "\n",
            "\n",
            "Scientists at McMaster University have found that a very old specimen of a Denisovan has lived on Earth for thousands of years.\n",
            "\n",
            "In a new study, scientists in the laboratory of Stephen Stokes, a professor at the University of Edinburgh, have\n",
            "\n",
            "[300 | 570.64] loss=2.66 avg=2.67\n",
            "[301 | 572.27] loss=2.46 avg=2.67\n",
            "[302 | 573.91] loss=2.68 avg=2.67\n",
            "[303 | 575.54] loss=2.46 avg=2.67\n",
            "[304 | 577.17] loss=2.63 avg=2.67\n",
            "[305 | 578.81] loss=2.59 avg=2.67\n",
            "[306 | 580.44] loss=2.51 avg=2.67\n",
            "[307 | 582.08] loss=3.03 avg=2.67\n",
            "[308 | 583.72] loss=2.70 avg=2.67\n",
            "[309 | 585.36] loss=2.54 avg=2.67\n",
            "[310 | 587.00] loss=2.57 avg=2.67\n",
            "[311 | 588.64] loss=2.44 avg=2.67\n",
            "[312 | 590.29] loss=2.69 avg=2.67\n",
            "[313 | 591.94] loss=2.88 avg=2.67\n",
            "[314 | 593.59] loss=2.63 avg=2.67\n",
            "[315 | 595.25] loss=2.84 avg=2.67\n",
            "[316 | 596.91] loss=2.53 avg=2.67\n",
            "[317 | 598.58] loss=2.69 avg=2.67\n",
            "[318 | 600.25] loss=2.62 avg=2.67\n",
            "[319 | 601.93] loss=2.76 avg=2.67\n",
            "[320 | 603.60] loss=2.59 avg=2.67\n",
            "[321 | 605.28] loss=2.78 avg=2.67\n",
            "[322 | 606.95] loss=2.45 avg=2.67\n",
            "[323 | 608.63] loss=2.94 avg=2.67\n",
            "[324 | 610.32] loss=2.84 avg=2.67\n",
            "[325 | 611.99] loss=2.75 avg=2.67\n",
            "[326 | 613.67] loss=2.71 avg=2.67\n",
            "[327 | 615.35] loss=3.31 avg=2.68\n",
            "[328 | 617.04] loss=2.85 avg=2.68\n",
            "[329 | 618.71] loss=2.57 avg=2.68\n",
            "[330 | 620.39] loss=2.39 avg=2.68\n",
            "[331 | 622.07] loss=2.46 avg=2.67\n",
            "[332 | 623.74] loss=2.39 avg=2.67\n",
            "[333 | 625.40] loss=3.34 avg=2.68\n",
            "[334 | 627.07] loss=2.70 avg=2.68\n",
            "[335 | 628.73] loss=2.54 avg=2.68\n",
            "[336 | 630.40] loss=2.82 avg=2.68\n",
            "[337 | 632.07] loss=2.83 avg=2.68\n",
            "[338 | 633.73] loss=2.91 avg=2.68\n",
            "[339 | 635.39] loss=2.58 avg=2.68\n",
            "[340 | 637.05] loss=2.80 avg=2.68\n",
            "[341 | 638.71] loss=2.61 avg=2.68\n",
            "[342 | 640.36] loss=2.46 avg=2.68\n",
            "[343 | 642.01] loss=2.64 avg=2.68\n",
            "[344 | 643.66] loss=3.14 avg=2.68\n",
            "[345 | 645.31] loss=2.59 avg=2.68\n",
            "[346 | 646.95] loss=3.01 avg=2.69\n",
            "[347 | 648.60] loss=2.87 avg=2.69\n",
            "[348 | 650.24] loss=2.58 avg=2.69\n",
            "[349 | 651.88] loss=2.52 avg=2.69\n",
            "[350 | 653.52] loss=2.76 avg=2.69\n",
            "[351 | 655.16] loss=2.76 avg=2.69\n",
            "[352 | 656.80] loss=2.52 avg=2.69\n",
            "[353 | 658.43] loss=2.60 avg=2.68\n",
            "[354 | 660.07] loss=2.40 avg=2.68\n",
            "[355 | 661.69] loss=2.35 avg=2.68\n",
            "[356 | 663.32] loss=2.29 avg=2.67\n",
            "[357 | 664.96] loss=2.80 avg=2.68\n",
            "[358 | 666.59] loss=2.51 avg=2.67\n",
            "[359 | 668.22] loss=2.38 avg=2.67\n",
            "[360 | 669.86] loss=3.04 avg=2.67\n",
            "[361 | 671.49] loss=2.22 avg=2.67\n",
            "[362 | 673.12] loss=2.60 avg=2.67\n",
            "[363 | 674.75] loss=2.19 avg=2.66\n",
            "[364 | 676.39] loss=2.54 avg=2.66\n",
            "[365 | 678.02] loss=2.84 avg=2.66\n",
            "[366 | 679.66] loss=2.78 avg=2.67\n",
            "[367 | 681.30] loss=2.54 avg=2.66\n",
            "[368 | 682.94] loss=2.88 avg=2.67\n",
            "[369 | 684.58] loss=3.15 avg=2.67\n",
            "[370 | 686.22] loss=2.82 avg=2.67\n",
            "[371 | 687.87] loss=2.46 avg=2.67\n",
            "[372 | 689.52] loss=2.88 avg=2.67\n",
            "[373 | 691.17] loss=2.77 avg=2.67\n",
            "[374 | 692.82] loss=2.94 avg=2.68\n",
            "[375 | 694.47] loss=2.51 avg=2.67\n",
            "[376 | 696.14] loss=2.45 avg=2.67\n",
            "[377 | 697.80] loss=2.66 avg=2.67\n",
            "[378 | 699.48] loss=2.72 avg=2.67\n",
            "[379 | 701.15] loss=2.73 avg=2.67\n",
            "[380 | 702.81] loss=2.51 avg=2.67\n",
            "[381 | 704.50] loss=2.48 avg=2.67\n",
            "[382 | 706.16] loss=2.96 avg=2.67\n",
            "[383 | 707.84] loss=3.05 avg=2.68\n",
            "[384 | 709.51] loss=2.70 avg=2.68\n",
            "[385 | 711.18] loss=2.59 avg=2.68\n",
            "[386 | 712.85] loss=2.93 avg=2.68\n",
            "[387 | 714.52] loss=2.61 avg=2.68\n",
            "[388 | 716.19] loss=2.38 avg=2.68\n",
            "[389 | 717.85] loss=2.57 avg=2.67\n",
            "[390 | 719.52] loss=2.84 avg=2.68\n",
            "[391 | 721.19] loss=2.37 avg=2.67\n",
            "[392 | 722.86] loss=2.37 avg=2.67\n",
            "[393 | 724.53] loss=2.61 avg=2.67\n",
            "[394 | 726.20] loss=2.50 avg=2.67\n",
            "[395 | 727.87] loss=2.40 avg=2.66\n",
            "[396 | 729.53] loss=2.06 avg=2.66\n",
            "[397 | 731.20] loss=2.61 avg=2.66\n",
            "[398 | 732.86] loss=2.82 avg=2.66\n",
            "[399 | 734.52] loss=2.66 avg=2.66\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " remains an enigma, but he has certainly left behind marks: a black spot on a finger, a scar under his eye; an open wound above his left eye.\n",
            "\n",
            "His mother found a small wooden block, the size of a grain of rice.\n",
            "\n",
            "He had been on a mission during an early night stroll, leaving his boots and flashlight on a nearby trail and returning to retrieve them. He returned to sleep, woke up with the flashlight in his hand, and found the tree covered in a sheet of ice.\n",
            "\n",
            "His mission is a big one, a life-or-death crusade to hunt and kill grizzlies. But there has been little discussion of the emotional implications of something that seems so basic to the animal kingdom.\n",
            "\n",
            "I asked Richard Thacker and Mark Fink.\n",
            "\n",
            "Thacker is a professor of animal behavior and conservation at North Carolina State University, and a biologist who works with grizzlies on a regular basis. He is a friend and researcher to Rottweiler dogs. He studied grizzlies at the Vancouver Zoo and researches the ways animals like grizzlies can behave in captivity. In his work with rhesus macaques, he describes the emotional demands of being in the presence of a mother grizzly.\n",
            "\n",
            "Thacker said that the experience is, in part, a kind of trauma. When he is working toward a project, he is conscious of not being able to bring the mother grizzly to life, like he did the first time he saw her.\n",
            "\n",
            "Fink is a curator of the grizzlies at the North Carolina Zoo’s Panthera Center. He also researches animals’ emotional capacities, and his research focuses on the need for attachment. In his work, he uses dogs as instruments of this research, by training them to become attached to living, young children.\n",
            "\n",
            "Thacker’s main theory, which I have also discussed here before, is that, as with humans, bears need to experience attachment like all other animals do. Because of their great physical size, the bears’ brains work hard to keep them attached. So “when you are in a bear trap or bear pen, it can feel good to keep them close, just because they can’t escape,” Thacker told me. “It’s kind of sad that they don’t do better. But it would be nice if they could’t, right now, so that’s why I have been interested in this theory and how it works. The fact that it’s an evolutionary need is what motivates me to pursue it.”\n",
            "\n",
            "Thacker believes he has found a way to bring the emotional significance of an interaction with a grizzly bear to humans, and he believes that’s what bears really need.\n",
            "\n",
            "I asked Thacker, who is also the author of many science news articles, whether grizzly bears could, in theory, bond with humans, and he said that “there’s no definitive evidence in terms of studies to suggest that a grizzly bear could actually feel attachment to a human person. It just is a possibility.” But he acknowledges the possibility.\n",
            "\n",
            "He knows that in some scenarios, bears would feel a connection like that with a person who has gone missing, or whose parents have died. “With [someone in those situations], the emotional impact can be more devastating than the physical one,” Thacker told me.\n",
            "\n",
            "He knows bears can sometimes be afraid of others; some have had trouble walking with humans after encountering them. Thacker pointed out that bears, like us, have evolved to be protective of our own. That is one reason bears are so protective of their young. But bears and humans have also evolved to be protective of our own kind and our own homes.\n",
            "\n",
            "Still, Thacker said, “It really hasn’t been proved that bears can ‘t bond with us’s kind.”\n",
            "\n",
            "Indeed, Thacker said, it wasn’t until a few years ago, while working on a study related to an ongoing experiment where researchers held black bears in captivity, that they discovered that they can easily bond with each other.\n",
            "\n",
            "When they released the pups in the wild, the researchers found that cubs would not just look out for one another, but would also search for their mother among their litter.\n",
            "\n",
            "For Thacker, the question “is’bitten” for research on the emotional impact of interaction with a grizzly bear. “It’s certainly not an easy question to consider. Not for me, at least, or for a human biologist, who can only ever work with living beings that have had some interaction with humans.”\n",
            "\n",
            "Thacker noted that it had been almost 20 years after he first saw the mother bear and had no idea it had killed that morning’s dinner. “It took me 20 years\n",
            "\n",
            "[400 | 758.82] loss=2.73 avg=2.66\n",
            "[401 | 760.46] loss=2.43 avg=2.66\n",
            "[402 | 762.10] loss=2.50 avg=2.66\n",
            "[403 | 763.72] loss=2.77 avg=2.66\n",
            "[404 | 765.36] loss=2.78 avg=2.66\n",
            "[405 | 766.99] loss=2.53 avg=2.66\n",
            "[406 | 768.62] loss=2.98 avg=2.66\n",
            "[407 | 770.25] loss=2.29 avg=2.66\n",
            "[408 | 771.89] loss=2.86 avg=2.66\n",
            "[409 | 773.53] loss=2.30 avg=2.66\n",
            "[410 | 775.16] loss=2.62 avg=2.66\n",
            "[411 | 776.80] loss=2.42 avg=2.65\n",
            "[412 | 778.44] loss=3.19 avg=2.66\n",
            "[413 | 780.08] loss=2.37 avg=2.66\n",
            "[414 | 781.71] loss=2.66 avg=2.66\n",
            "[415 | 783.35] loss=2.49 avg=2.65\n",
            "[416 | 784.99] loss=2.81 avg=2.66\n",
            "[417 | 786.64] loss=2.13 avg=2.65\n",
            "[418 | 788.29] loss=2.75 avg=2.65\n",
            "[419 | 789.94] loss=2.69 avg=2.65\n",
            "[420 | 791.60] loss=2.68 avg=2.65\n",
            "[421 | 793.25] loss=2.50 avg=2.65\n",
            "[422 | 794.91] loss=2.19 avg=2.65\n",
            "[423 | 796.57] loss=2.66 avg=2.65\n",
            "[424 | 798.24] loss=2.69 avg=2.65\n",
            "[425 | 799.89] loss=2.98 avg=2.65\n",
            "[426 | 801.56] loss=2.75 avg=2.65\n",
            "[427 | 803.22] loss=2.38 avg=2.65\n",
            "[428 | 804.89] loss=2.62 avg=2.65\n",
            "[429 | 806.56] loss=2.27 avg=2.64\n",
            "[430 | 808.23] loss=2.47 avg=2.64\n",
            "[431 | 809.90] loss=2.83 avg=2.64\n",
            "[432 | 811.57] loss=2.30 avg=2.64\n",
            "[433 | 813.24] loss=2.33 avg=2.64\n",
            "[434 | 814.91] loss=2.67 avg=2.64\n",
            "[435 | 816.58] loss=2.34 avg=2.63\n",
            "[436 | 818.25] loss=2.73 avg=2.63\n",
            "[437 | 819.93] loss=2.48 avg=2.63\n",
            "[438 | 821.59] loss=2.71 avg=2.63\n",
            "[439 | 823.25] loss=2.78 avg=2.64\n",
            "[440 | 824.92] loss=2.42 avg=2.63\n",
            "[441 | 826.59] loss=2.72 avg=2.63\n",
            "[442 | 828.25] loss=2.22 avg=2.63\n",
            "[443 | 829.92] loss=2.44 avg=2.63\n",
            "[444 | 831.58] loss=2.22 avg=2.62\n",
            "[445 | 833.24] loss=2.80 avg=2.63\n",
            "[446 | 834.89] loss=2.35 avg=2.62\n",
            "[447 | 836.54] loss=2.52 avg=2.62\n",
            "[448 | 838.20] loss=2.41 avg=2.62\n",
            "[449 | 839.84] loss=2.58 avg=2.62\n",
            "[450 | 841.49] loss=2.68 avg=2.62\n",
            "[451 | 843.13] loss=2.89 avg=2.62\n",
            "[452 | 844.77] loss=2.57 avg=2.62\n",
            "[453 | 846.41] loss=2.90 avg=2.63\n",
            "[454 | 848.06] loss=2.36 avg=2.62\n",
            "[455 | 849.70] loss=2.28 avg=2.62\n",
            "[456 | 851.34] loss=2.63 avg=2.62\n",
            "[457 | 852.96] loss=2.81 avg=2.62\n",
            "[458 | 854.60] loss=3.11 avg=2.63\n",
            "[459 | 856.23] loss=2.74 avg=2.63\n",
            "[460 | 857.87] loss=3.08 avg=2.63\n",
            "[461 | 859.51] loss=2.71 avg=2.63\n",
            "[462 | 861.14] loss=2.52 avg=2.63\n",
            "[463 | 862.77] loss=2.35 avg=2.63\n",
            "[464 | 864.40] loss=2.49 avg=2.63\n",
            "[465 | 866.03] loss=2.57 avg=2.63\n",
            "[466 | 867.66] loss=2.19 avg=2.62\n",
            "[467 | 869.29] loss=2.17 avg=2.62\n",
            "[468 | 870.92] loss=2.53 avg=2.62\n",
            "[469 | 872.56] loss=2.15 avg=2.61\n",
            "[470 | 874.19] loss=2.79 avg=2.61\n",
            "[471 | 875.83] loss=2.78 avg=2.62\n",
            "[472 | 877.47] loss=2.39 avg=2.61\n",
            "[473 | 879.10] loss=2.63 avg=2.61\n",
            "[474 | 880.74] loss=2.55 avg=2.61\n",
            "[475 | 882.38] loss=2.42 avg=2.61\n",
            "[476 | 884.03] loss=2.65 avg=2.61\n",
            "[477 | 885.67] loss=2.26 avg=2.61\n",
            "[478 | 887.32] loss=3.11 avg=2.61\n",
            "[479 | 888.97] loss=2.57 avg=2.61\n",
            "[480 | 890.63] loss=2.09 avg=2.61\n",
            "[481 | 892.28] loss=2.37 avg=2.60\n",
            "[482 | 893.94] loss=2.68 avg=2.61\n",
            "[483 | 895.60] loss=2.67 avg=2.61\n",
            "[484 | 897.27] loss=2.15 avg=2.60\n",
            "[485 | 898.93] loss=2.86 avg=2.60\n",
            "[486 | 900.60] loss=2.70 avg=2.61\n",
            "[487 | 902.27] loss=2.92 avg=2.61\n",
            "[488 | 903.94] loss=2.53 avg=2.61\n",
            "[489 | 905.61] loss=2.61 avg=2.61\n",
            "[490 | 907.30] loss=2.59 avg=2.61\n",
            "[491 | 908.97] loss=2.55 avg=2.61\n",
            "[492 | 910.66] loss=2.49 avg=2.61\n",
            "[493 | 912.33] loss=3.07 avg=2.61\n",
            "[494 | 914.00] loss=2.25 avg=2.61\n",
            "[495 | 915.69] loss=2.61 avg=2.61\n",
            "[496 | 917.37] loss=2.28 avg=2.60\n",
            "[497 | 919.05] loss=2.49 avg=2.60\n",
            "[498 | 920.72] loss=2.60 avg=2.60\n",
            "[499 | 922.39] loss=2.93 avg=2.61\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " Nēnēne in these words and in the phrase \"that is the law on her side.\"\n",
            "\n",
            "I believe that the law of the people is the law of the nation, and that all laws are laws of the people.\n",
            "\n",
            "But in the ancient lawbooks it was written in a somewhat different way. Under an ancient law from the Phœnician language, the people were to obey the law of the king. The text was, \"O people of Phœnician heritage, you shall not kill, rape, sell, rob, lie, cheat, or steal.\" (Acts 11:19–20)\n",
            "\n",
            "It could also be read to mean: You shall not kill, take from the poor, or cheat the elderly.\n",
            "\n",
            "In the end, it came to pass that those who were not of Phœnician extraction were executed.\n",
            "\n",
            "This is the very same law that was supposed to have been kept pure by the ancient Greeks. So there you have it. Ancient law was to be kept pure by the Greeks. To which we reply: What difference does that make?\n",
            "\n",
            "***\n",
            "\n",
            "In the 18th century, a French priest named Joseph Guillaumier was writing up a series of new hymns by various religious orders. One of the hymns he was interested in was a hymn he'd written himself in the 17th century, which called for a kind of collective prayer to unite the people into a union of common purpose. In the song, the man sings,\n",
            "\n",
            "O we can dance in unity\n",
            "\n",
            "With a brotherhood bound together\n",
            "\n",
            "And when we come round again,\n",
            "\n",
            "We must hold hands, O we can dance!\n",
            "\n",
            "After the hymn's release was widely disseminated, the French monarchy, with its strict codes and prohibitions against certain forms of religious expression, was in a panic. It was now illegal to produce religious books, to produce songs, and to publish them on the radio. In June 1838, the French government decreed that the hymn must be sung in the person of the priest, and that the woman on whom the hymn was sung had to be under 18.\n",
            "\n",
            "The government was not alone in trying to clamp down on religious expression. One of the world's earliest Christian communities in South Asia, the Pahlavi community of Gandhara, had been founded by a Parsi priest named Abdulla Mirza Khan, who had left his family to become a Hindu. The Pahlavans had no formal religious beliefs—and the very idea of a 'singer' or 'dancer' seemed to require some kind of extraordinary power. Mirza Khan insisted that the Pahlavans sing songs in return for protection; this, in turn, required their consent to his teachings. He believed that the only way for his people to find peace was for them to 'gather in,' say a prayer, and to 'dance.'\n",
            "\n",
            "The Pahlavi community had some pretty strict rules about how to dance. A woman who danced was not only not allowed in the village but was also not allowed to pray—and if she danced she was supposed to bow and pray. It was a time when women were not allowed to enter the village; women were not allowed to walk on the same street with one another; women were not allowed to take their menstrual milk with them. And while a woman was not allowed to give a public prayer, she was free to do so at her home.\n",
            "\n",
            "These rules had already become a source of friction with the local community—both Parsis and Hindus. The Pahlavans were not pleased with Mirza Khan—and he was not pleased with their community. A fierce, often violent, rivalry developed.\n",
            "\n",
            "In 1837, Mirza Khan decided to have another 'dancing competition' with two of his own followers, and it was during this time that he arranged for a local priest to accompany some of the men: a local woman who, he claimed, had done 'a great deal to make the Pahlavi women dance; she was well known.' The day before the competition, the police were called to a village near Gandharia, where a man had beaten two young women. The couple had both fled, and they were both found drunk, unconscious, and beaten to death.\n",
            "\n",
            "Soon after, there were reports of violent clashes between a small group of Pahlavids and a Pahlavani family that lived in the village. The village was being threatened with eviction; the Pahlavids wanted to protect their land, which had been bought by the Pahlavi community.\n",
            "\n",
            "To the Pahlavid community, the police actions seemed to have been an act of provocation. 'It should be the duty of the Pahlavid, in the absence of force, to arrest and kill these two youths, as in one of their others, who had taken away the land of their brother,' Mir\n",
            "\n",
            "[500 | 946.77] loss=2.45 avg=2.60\n",
            "[501 | 948.40] loss=2.95 avg=2.61\n",
            "[502 | 950.04] loss=2.40 avg=2.61\n",
            "[503 | 951.68] loss=2.77 avg=2.61\n",
            "[504 | 953.32] loss=2.51 avg=2.61\n",
            "[505 | 954.95] loss=2.62 avg=2.61\n",
            "[506 | 956.58] loss=2.35 avg=2.60\n",
            "[507 | 958.22] loss=2.10 avg=2.60\n",
            "[508 | 959.85] loss=2.47 avg=2.60\n",
            "[509 | 961.48] loss=2.63 avg=2.60\n",
            "[510 | 963.12] loss=2.55 avg=2.60\n",
            "[511 | 964.76] loss=2.55 avg=2.60\n",
            "[512 | 966.39] loss=2.55 avg=2.60\n",
            "[513 | 968.02] loss=2.83 avg=2.60\n",
            "[514 | 969.66] loss=2.00 avg=2.59\n",
            "[515 | 971.29] loss=2.32 avg=2.59\n",
            "[516 | 972.93] loss=2.36 avg=2.59\n",
            "[517 | 974.57] loss=2.44 avg=2.59\n",
            "[518 | 976.20] loss=1.99 avg=2.58\n",
            "[519 | 977.84] loss=2.62 avg=2.58\n",
            "[520 | 979.48] loss=2.54 avg=2.58\n",
            "[521 | 981.13] loss=2.65 avg=2.58\n",
            "[522 | 982.77] loss=2.27 avg=2.58\n",
            "[523 | 984.41] loss=2.58 avg=2.58\n",
            "[524 | 986.06] loss=2.52 avg=2.58\n",
            "[525 | 987.71] loss=2.40 avg=2.58\n",
            "[526 | 989.36] loss=2.44 avg=2.57\n",
            "[527 | 991.01] loss=2.26 avg=2.57\n",
            "[528 | 992.66] loss=2.46 avg=2.57\n",
            "[529 | 994.32] loss=2.59 avg=2.57\n",
            "[530 | 995.98] loss=2.23 avg=2.57\n",
            "[531 | 997.65] loss=2.68 avg=2.57\n",
            "[532 | 999.32] loss=2.60 avg=2.57\n",
            "[533 | 1000.99] loss=2.61 avg=2.57\n",
            "[534 | 1002.66] loss=2.34 avg=2.57\n",
            "[535 | 1004.33] loss=2.55 avg=2.57\n",
            "[536 | 1006.00] loss=2.59 avg=2.57\n",
            "[537 | 1007.67] loss=3.18 avg=2.57\n",
            "[538 | 1009.35] loss=2.43 avg=2.57\n",
            "[539 | 1011.02] loss=3.02 avg=2.58\n",
            "[540 | 1012.69] loss=2.21 avg=2.57\n",
            "[541 | 1014.36] loss=2.35 avg=2.57\n",
            "[542 | 1016.03] loss=2.67 avg=2.57\n",
            "[543 | 1017.71] loss=2.15 avg=2.57\n",
            "[544 | 1019.38] loss=2.42 avg=2.56\n",
            "[545 | 1021.06] loss=2.73 avg=2.57\n",
            "[546 | 1022.73] loss=2.16 avg=2.56\n",
            "[547 | 1024.39] loss=2.62 avg=2.56\n",
            "[548 | 1026.06] loss=2.83 avg=2.57\n",
            "[549 | 1027.72] loss=2.53 avg=2.57\n",
            "[550 | 1029.38] loss=2.27 avg=2.56\n",
            "[551 | 1031.04] loss=2.30 avg=2.56\n",
            "[552 | 1032.70] loss=2.84 avg=2.56\n",
            "[553 | 1034.36] loss=2.46 avg=2.56\n",
            "[554 | 1036.01] loss=2.85 avg=2.56\n",
            "[555 | 1037.66] loss=2.39 avg=2.56\n",
            "[556 | 1039.31] loss=2.65 avg=2.56\n",
            "[557 | 1040.95] loss=2.49 avg=2.56\n",
            "[558 | 1042.60] loss=2.42 avg=2.56\n",
            "[559 | 1044.24] loss=2.50 avg=2.56\n",
            "[560 | 1045.88] loss=2.77 avg=2.56\n",
            "[561 | 1047.53] loss=2.68 avg=2.56\n",
            "[562 | 1049.16] loss=2.58 avg=2.56\n",
            "[563 | 1050.80] loss=2.36 avg=2.56\n",
            "[564 | 1052.43] loss=2.43 avg=2.56\n",
            "[565 | 1054.06] loss=2.20 avg=2.56\n",
            "[566 | 1055.69] loss=2.53 avg=2.56\n",
            "[567 | 1057.33] loss=2.36 avg=2.56\n",
            "[568 | 1058.96] loss=2.40 avg=2.55\n",
            "[569 | 1060.60] loss=2.13 avg=2.55\n",
            "[570 | 1062.24] loss=2.15 avg=2.55\n",
            "[571 | 1063.87] loss=2.77 avg=2.55\n",
            "[572 | 1065.49] loss=2.81 avg=2.55\n",
            "[573 | 1067.13] loss=2.47 avg=2.55\n",
            "[574 | 1068.76] loss=2.78 avg=2.55\n",
            "[575 | 1070.40] loss=2.53 avg=2.55\n",
            "[576 | 1072.04] loss=2.59 avg=2.55\n",
            "[577 | 1073.68] loss=2.79 avg=2.55\n",
            "[578 | 1075.31] loss=2.72 avg=2.56\n",
            "[579 | 1076.95] loss=2.87 avg=2.56\n",
            "[580 | 1078.59] loss=2.31 avg=2.56\n",
            "[581 | 1080.23] loss=2.44 avg=2.56\n",
            "[582 | 1081.87] loss=2.91 avg=2.56\n",
            "[583 | 1083.52] loss=1.93 avg=2.55\n",
            "[584 | 1085.17] loss=2.21 avg=2.55\n",
            "[585 | 1086.82] loss=3.23 avg=2.56\n",
            "[586 | 1088.47] loss=2.34 avg=2.55\n",
            "[587 | 1090.13] loss=2.38 avg=2.55\n",
            "[588 | 1091.79] loss=2.66 avg=2.55\n",
            "[589 | 1093.45] loss=2.83 avg=2.56\n",
            "[590 | 1095.12] loss=2.23 avg=2.55\n",
            "[591 | 1096.79] loss=2.79 avg=2.56\n",
            "[592 | 1098.46] loss=2.40 avg=2.55\n",
            "[593 | 1100.13] loss=2.87 avg=2.56\n",
            "[594 | 1101.80] loss=2.83 avg=2.56\n",
            "[595 | 1103.48] loss=3.05 avg=2.56\n",
            "[596 | 1105.17] loss=2.36 avg=2.56\n",
            "[597 | 1106.84] loss=2.49 avg=2.56\n",
            "[598 | 1108.53] loss=1.95 avg=2.56\n",
            "[599 | 1110.22] loss=2.48 avg=2.55\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "s, to meet and work with other organizations. He said he was honored to offer his services, but worried that other employers might have seen his work and passed the name over. \"The government needs to stop putting pressure on people. We don't want to cause harm to people.\"\n",
            "\n",
            "“We’ll be working on the project together in the near future,” he said, though he declined to provide more details. “We need the private sector to make a change, but we don’t want to use their expertise. In my opinion, they shouldn’t have to do that.”\n",
            "\n",
            "For all their concerns about the government appropriating private money, the federal government has already spent billions directly on scientific research, which makes the practice all the worse. In 2017, the Obama administration announced $8.4 billion in funds to support the research of more than 300 scientists and engineers who had worked on climate change, with just $6.6 million devoted to direct payments from private individuals through grants, loans, and contracts.\n",
            "\n",
            "The first priority has to do with climate change, the administration has said. “Our goal is to do everything possible to avoid the worst impacts of climate change,” the president said in September. “We are putting a lot of money toward research for the last 50 years. We are taking a very long–term view of it. There have been significant climate risks over a long period of time.” The money is there; it belongs to all of us.\n",
            "\n",
            "But in addition to science funding, the United States also spends millions on defense research, which is funded by the same public dollars, at least partially, that are spent for other projects. Over the past four years, the Pentagon spent an entire $100 million budget on climate change research. After the Paris climate summit, which the President also attended, Defense Secretary Ash Carter told reporters the Pentagon would keep increasing its investments.\n",
            "\n",
            "“At a time when it’s just getting our military to the edge of the Earth and the sea, and doing all kinds of stuff in a little bit of time?” Carter said, alluding to an impending climate calamity, “I couldn’t think of a better way to spend this money at this time of year.” (The Pentagon declined to disclose the amount of the last-minute $100 million boost.)\n",
            "\n",
            "At the Pentagon, climate-change research is funded by defense funds and by federal grants. But private-sector groups may soon be able to participate, too.\n",
            "\n",
            "Last month, more than 6,100 private entrepreneurs, science-entrepreneurs, and researchers in 34 countries signed a letter to the National Science Foundation asking it to increase its funding of research around the topic. The NSSF declined to comment.\n",
            "\n",
            "In the 1980s, while working across the Atlantic at the Rockefeller Foundation, John Bates Clark, a retired Air Force lieutenant general, heard about a study in which NASA had looked at the impact of global warming on the Earth’s climate. The study was carried out at the behest of a research institute at California State University, and Clark took an interest. “I was interested in the effects of global warming on the United States.” That included climate change itself.\n",
            "\n",
            "By the 1990s, Clark, who also served as NASA’s director from 1998 to 2001, had made a preliminary study of what happens when scientists and engineers work on a climate-change project at NASA. The results, released in the fall of 1997, came up with a sobering picture. Climate change could bring extreme drought, increased sea-level rise, and more widespread flooding. The scientists had even more alarming news to say. The global temperature rose by less than four degrees Celsius.\n",
            "\n",
            "In an interview with Climate Central in 2015, Clark said he was inspired to start the Climate Action Program, a group dedicated to research into the impacts of global climate change.\n",
            "\n",
            "The program was formed to help build the climate skepticism across the United States. Its first mission is at the University of Virginia, in Charlottesville, where Clark recently received his doctor of sociology degree. In this field, Clark said he found that the U.S. government had largely ignored climate-related research. He started to do more of that research himself.\n",
            "\n",
            "“It’s been about five years of doing the bare bones research, basically a few lines of text, and the data gets thrown out there and people just pick and choose what they want to report.” Clark said. He learned something.\n",
            "\n",
            "“A lot of the most serious researchers I know were either reluctant or too afraid to come to an American university because they thought, no, our own research won’t be as important as their work.” And because of that, a lot of them are now studying other countries.”\n",
            "\n",
            "Clark said that in the last five years, the Climate Action Program\n",
            "\n",
            "[600 | 1134.78] loss=2.95 avg=2.56\n",
            "[601 | 1136.44] loss=2.43 avg=2.56\n",
            "[602 | 1138.09] loss=2.48 avg=2.56\n",
            "[603 | 1139.74] loss=2.39 avg=2.55\n",
            "[604 | 1141.39] loss=2.42 avg=2.55\n",
            "[605 | 1143.03] loss=1.79 avg=2.55\n",
            "[606 | 1144.68] loss=2.96 avg=2.55\n",
            "[607 | 1146.31] loss=2.46 avg=2.55\n",
            "[608 | 1147.95] loss=2.68 avg=2.55\n",
            "[609 | 1149.59] loss=2.57 avg=2.55\n",
            "[610 | 1151.22] loss=2.22 avg=2.55\n",
            "[611 | 1152.86] loss=2.67 avg=2.55\n",
            "[612 | 1154.49] loss=2.04 avg=2.54\n",
            "[613 | 1156.13] loss=2.53 avg=2.54\n",
            "[614 | 1157.76] loss=2.67 avg=2.54\n",
            "[615 | 1159.39] loss=2.61 avg=2.55\n",
            "[616 | 1161.02] loss=2.31 avg=2.54\n",
            "[617 | 1162.66] loss=3.03 avg=2.55\n",
            "[618 | 1164.30] loss=2.40 avg=2.55\n",
            "[619 | 1165.93] loss=2.20 avg=2.54\n",
            "[620 | 1167.57] loss=2.62 avg=2.54\n",
            "[621 | 1169.21] loss=2.48 avg=2.54\n",
            "[622 | 1170.83] loss=1.88 avg=2.54\n",
            "[623 | 1172.46] loss=2.77 avg=2.54\n",
            "[624 | 1174.10] loss=2.69 avg=2.54\n",
            "[625 | 1175.74] loss=2.14 avg=2.54\n",
            "[626 | 1177.38] loss=3.00 avg=2.54\n",
            "[627 | 1179.03] loss=2.57 avg=2.54\n",
            "[628 | 1180.67] loss=2.53 avg=2.54\n",
            "[629 | 1182.32] loss=2.28 avg=2.54\n",
            "[630 | 1183.97] loss=2.85 avg=2.54\n",
            "[631 | 1185.62] loss=2.81 avg=2.54\n",
            "[632 | 1187.26] loss=2.44 avg=2.54\n",
            "[633 | 1188.92] loss=2.49 avg=2.54\n",
            "[634 | 1190.58] loss=2.74 avg=2.54\n",
            "[635 | 1192.25] loss=2.78 avg=2.55\n",
            "[636 | 1193.91] loss=2.61 avg=2.55\n",
            "[637 | 1195.58] loss=2.38 avg=2.55\n",
            "[638 | 1197.24] loss=2.24 avg=2.54\n",
            "[639 | 1198.91] loss=2.36 avg=2.54\n",
            "[640 | 1200.59] loss=2.25 avg=2.54\n",
            "[641 | 1202.26] loss=2.70 avg=2.54\n",
            "[642 | 1203.93] loss=2.50 avg=2.54\n",
            "[643 | 1205.60] loss=2.46 avg=2.54\n",
            "[644 | 1207.27] loss=2.11 avg=2.53\n",
            "[645 | 1208.93] loss=2.50 avg=2.53\n",
            "[646 | 1210.60] loss=2.39 avg=2.53\n",
            "[647 | 1212.28] loss=2.60 avg=2.53\n",
            "[648 | 1213.94] loss=2.57 avg=2.53\n",
            "[649 | 1215.62] loss=2.81 avg=2.54\n",
            "[650 | 1217.29] loss=2.49 avg=2.54\n",
            "[651 | 1218.96] loss=2.49 avg=2.54\n",
            "[652 | 1220.63] loss=2.48 avg=2.53\n",
            "[653 | 1222.30] loss=2.41 avg=2.53\n",
            "[654 | 1223.96] loss=2.16 avg=2.53\n",
            "[655 | 1225.63] loss=2.63 avg=2.53\n",
            "[656 | 1227.29] loss=2.63 avg=2.53\n",
            "[657 | 1228.95] loss=2.82 avg=2.53\n",
            "[658 | 1230.61] loss=2.35 avg=2.53\n",
            "[659 | 1232.26] loss=2.67 avg=2.53\n",
            "[660 | 1233.90] loss=2.90 avg=2.54\n",
            "[661 | 1235.55] loss=1.92 avg=2.53\n",
            "[662 | 1237.20] loss=2.23 avg=2.53\n",
            "[663 | 1238.85] loss=2.38 avg=2.53\n",
            "[664 | 1240.50] loss=2.31 avg=2.53\n",
            "[665 | 1242.15] loss=2.16 avg=2.52\n",
            "[666 | 1243.79] loss=2.66 avg=2.52\n",
            "[667 | 1245.42] loss=2.25 avg=2.52\n",
            "[668 | 1247.06] loss=2.53 avg=2.52\n",
            "[669 | 1248.70] loss=2.95 avg=2.52\n",
            "[670 | 1250.34] loss=2.57 avg=2.52\n",
            "[671 | 1251.98] loss=2.73 avg=2.53\n",
            "[672 | 1253.61] loss=2.71 avg=2.53\n",
            "[673 | 1255.24] loss=2.52 avg=2.53\n",
            "[674 | 1256.87] loss=2.30 avg=2.53\n",
            "[675 | 1258.51] loss=2.92 avg=2.53\n",
            "[676 | 1260.15] loss=2.59 avg=2.53\n",
            "[677 | 1261.78] loss=2.91 avg=2.53\n",
            "[678 | 1263.41] loss=2.37 avg=2.53\n",
            "[679 | 1265.05] loss=2.53 avg=2.53\n",
            "[680 | 1266.69] loss=2.14 avg=2.53\n",
            "[681 | 1268.32] loss=1.61 avg=2.52\n",
            "[682 | 1269.94] loss=2.73 avg=2.52\n",
            "[683 | 1271.58] loss=2.60 avg=2.52\n",
            "[684 | 1273.23] loss=2.78 avg=2.53\n",
            "[685 | 1274.86] loss=3.02 avg=2.53\n",
            "[686 | 1276.50] loss=2.38 avg=2.53\n",
            "[687 | 1278.15] loss=2.21 avg=2.53\n",
            "[688 | 1279.78] loss=2.01 avg=2.52\n",
            "[689 | 1281.42] loss=2.38 avg=2.52\n",
            "[690 | 1283.07] loss=2.62 avg=2.52\n",
            "[691 | 1284.72] loss=2.22 avg=2.52\n",
            "[692 | 1286.37] loss=2.71 avg=2.52\n",
            "[693 | 1288.02] loss=2.50 avg=2.52\n",
            "[694 | 1289.68] loss=3.11 avg=2.52\n",
            "[695 | 1291.33] loss=2.71 avg=2.53\n",
            "[696 | 1292.98] loss=2.70 avg=2.53\n",
            "[697 | 1294.64] loss=2.32 avg=2.53\n",
            "[698 | 1296.30] loss=2.12 avg=2.52\n",
            "[699 | 1297.97] loss=2.27 avg=2.52\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " never read that story?\n",
            "\n",
            "But I had.\n",
            "\n",
            "I had recently read the story of Joseph Stalin, who, in 1953, was assassinated in a gas chamber by his secret police. It was clear that the dictator was a victim of human rights crimes, and we began to wonder whether his murder was, too, a form of political murder. We began asking the question of whether such crimes could ever be justified. What if they were?\n",
            "\n",
            "The story of Kirov, and the subsequent work of John Breslow [who coined the term 'ethnic murder' (and a new version of the term was born!)], led us to rethink the moral dimensions of political murders. If human rights violations are so reprehensible and heinous that their victims should be considered to be human, and if it is ethically abhorrent to commit them, then where does the threshold lie before any kind of political judgment could exist? If murder can be called ethically reprehensible in its own right, then why not murder as well?\n",
            "\n",
            "If so, then we are moving backwards in moral history. Not only are all political murders morally reprehensible, but even these murders are not always ethically abhorrent.\n",
            "\n",
            "This raises an even more troubling question: If murder and other brutal and violent acts are immoral, then why are crimes against humanity never considered morally reprehensible?\n",
            "\n",
            "\n",
            "“When we start talking about ‘ethnic cleansing’ we're talking about a really important question that hasn’t yet been fully asked but that needs to be asked,” said David Ries, a historian at the Texas State University who studies Jewish and Christian persecution in Europe during the Middle Ages. “The idea of exterminating an ethnic group without the consent of their members is very familiar to us.”\n",
            "\n",
            "In the medieval world, Jews and Christians were both targeted “for their distinctive religious practices” and persecuted over them. To be certain, the Christian and Jewish communities had their own distinctive traditions. “I don’t think you’d ever see people going into an Orthodox church, having communion, and saying, ‘Shall we exterminate all these Jews and Christians?’” said Ries.\n",
            "\n",
            "There were specific acts of ethnic cleansing that both European Christians and Jews experienced in the Middle Ages. Although Christians and Jews were not the same people at the time, they shared a common culture, an ethnic identity, and a history that encompassed the lands the latter occupied.\n",
            "\n",
            "The first of these acts occurred when the Jews of the region became the dominant group in the Ottoman realm in the fifteenth century. Though the Jews of Syria and Lebanon had been around for many centuries, they constituted approximately 5 percent of medieval and modern people. Their influence had expanded into a majority-Jewish Turkish state established in 1453, the country’s first secular republic.\n",
            "\n",
            "The Jews of Syria and Lebanon became the subject of an orchestrated Turkish-led attack in the mid-fifteenth century. They were ethnically cleansed and their lands were taken in exchange for lands and territories in Anatolia, the far eastern region from modern Turkey to modern-day Hungary. Turkish armies attacked the Jewish communities in Syria and Lebanon as early as 1482, destroying houses and displacing the Jewish inhabitants. To prevent Jewish people from returning to their homeland, the Ottoman state turned the country into an Islamic caliphate.\n",
            "\n",
            "The last act of ethnic cleansing occurred in the mid-seventeenth century, when the Ottoman Empire was reconstituted. Since Christian communities in the Balkans, which also included the Ottoman Empire, were part of the new republic, the Jews of that region were considered part of the Muslim empire. As the empire began to crumble, these communities were expelled from their homeland.\n",
            "\n",
            "Many European Jews and Christians lived in European nations after the dissolution of the Ottoman Empire, especially in the continent’s industrial heartland. Today, there are numerous Jewish communities and many Christian communities in central and northern Europe.\n",
            "\n",
            "The Muslim conquests that marked the end of the Ottoman Empire in the mid-seventeenth century, however, did not change the political landscape of Europe in the early twentieth century. The Ottoman Empire remained the main power in the Middle East and a major player in Europe’s central and eastern regions throughout the century.\n",
            "\n",
            "“There hasn’t been this level of concentration of power and of economic impact for very long,” Ries told me.\n",
            "\n",
            "And the United States remains a major player in the Middle East and western Europe. The wars in Iraq and Afghanistan both devastated parts of central and eastern Europe and destroyed several communities of European origin. In 2014, the European Union—the United Kingdom’s foreign policy institution—issued a resolution reaffirming the EU’s commitment to security in the Middle East, but not mentioning the “Arab-Israeli conflict” or other ethnic or religious animosities, like these, that plagued these regions for hundreds\n",
            "\n",
            "[700 | 1322.34] loss=2.39 avg=2.52\n",
            "[701 | 1324.01] loss=1.92 avg=2.51\n",
            "[702 | 1325.67] loss=2.52 avg=2.51\n",
            "[703 | 1327.33] loss=2.51 avg=2.51\n",
            "[704 | 1328.99] loss=2.49 avg=2.51\n",
            "[705 | 1330.65] loss=2.78 avg=2.51\n",
            "[706 | 1332.30] loss=2.73 avg=2.52\n",
            "[707 | 1333.95] loss=2.49 avg=2.52\n",
            "[708 | 1335.60] loss=2.42 avg=2.52\n",
            "[709 | 1337.24] loss=2.09 avg=2.51\n",
            "[710 | 1338.89] loss=2.50 avg=2.51\n",
            "[711 | 1340.54] loss=2.06 avg=2.51\n",
            "[712 | 1342.18] loss=2.56 avg=2.51\n",
            "[713 | 1343.82] loss=2.98 avg=2.51\n",
            "[714 | 1345.47] loss=2.43 avg=2.51\n",
            "[715 | 1347.11] loss=2.55 avg=2.51\n",
            "[716 | 1348.74] loss=2.55 avg=2.51\n",
            "[717 | 1350.38] loss=2.28 avg=2.51\n",
            "[718 | 1352.02] loss=2.64 avg=2.51\n",
            "[719 | 1353.64] loss=2.40 avg=2.51\n",
            "[720 | 1355.28] loss=2.80 avg=2.51\n",
            "[721 | 1356.91] loss=2.44 avg=2.51\n",
            "[722 | 1358.55] loss=2.21 avg=2.51\n",
            "[723 | 1360.19] loss=3.34 avg=2.52\n",
            "[724 | 1361.82] loss=2.88 avg=2.52\n",
            "[725 | 1363.46] loss=2.54 avg=2.52\n",
            "[726 | 1365.09] loss=2.79 avg=2.52\n",
            "[727 | 1366.72] loss=2.37 avg=2.52\n",
            "[728 | 1368.35] loss=2.81 avg=2.53\n",
            "[729 | 1369.98] loss=2.40 avg=2.52\n",
            "[730 | 1371.62] loss=3.02 avg=2.53\n",
            "[731 | 1373.26] loss=2.74 avg=2.53\n",
            "[732 | 1374.89] loss=2.21 avg=2.53\n",
            "[733 | 1376.53] loss=2.25 avg=2.53\n",
            "[734 | 1378.17] loss=2.21 avg=2.52\n",
            "[735 | 1379.81] loss=2.60 avg=2.52\n",
            "[736 | 1381.44] loss=2.97 avg=2.53\n",
            "[737 | 1383.10] loss=2.52 avg=2.53\n",
            "[738 | 1384.75] loss=2.34 avg=2.53\n",
            "[739 | 1386.40] loss=2.50 avg=2.53\n",
            "[740 | 1388.06] loss=2.46 avg=2.52\n",
            "[741 | 1389.72] loss=2.53 avg=2.52\n",
            "[742 | 1391.38] loss=2.67 avg=2.53\n",
            "[743 | 1393.04] loss=2.28 avg=2.52\n",
            "[744 | 1394.71] loss=2.59 avg=2.52\n",
            "[745 | 1396.38] loss=2.28 avg=2.52\n",
            "[746 | 1398.05] loss=2.40 avg=2.52\n",
            "[747 | 1399.71] loss=2.28 avg=2.52\n",
            "[748 | 1401.38] loss=2.86 avg=2.52\n",
            "[749 | 1403.05] loss=2.38 avg=2.52\n",
            "[750 | 1404.73] loss=2.75 avg=2.52\n",
            "[751 | 1406.41] loss=2.44 avg=2.52\n",
            "[752 | 1408.07] loss=2.42 avg=2.52\n",
            "[753 | 1409.75] loss=2.45 avg=2.52\n",
            "[754 | 1411.43] loss=2.54 avg=2.52\n",
            "[755 | 1413.10] loss=2.31 avg=2.52\n",
            "[756 | 1414.77] loss=2.11 avg=2.51\n",
            "[757 | 1416.44] loss=2.25 avg=2.51\n",
            "[758 | 1418.11] loss=2.33 avg=2.51\n",
            "[759 | 1419.78] loss=2.37 avg=2.51\n",
            "[760 | 1421.45] loss=2.20 avg=2.50\n",
            "[761 | 1423.11] loss=2.71 avg=2.51\n",
            "[762 | 1424.77] loss=2.04 avg=2.50\n",
            "[763 | 1426.43] loss=2.23 avg=2.50\n",
            "[764 | 1428.09] loss=2.00 avg=2.49\n",
            "[765 | 1429.75] loss=2.74 avg=2.50\n",
            "[766 | 1431.40] loss=2.32 avg=2.50\n",
            "[767 | 1433.06] loss=2.27 avg=2.49\n",
            "[768 | 1434.70] loss=2.71 avg=2.50\n",
            "[769 | 1436.35] loss=2.37 avg=2.49\n",
            "[770 | 1437.99] loss=2.41 avg=2.49\n",
            "[771 | 1439.63] loss=2.21 avg=2.49\n",
            "[772 | 1441.27] loss=2.81 avg=2.49\n",
            "[773 | 1442.91] loss=2.26 avg=2.49\n",
            "[774 | 1444.56] loss=2.54 avg=2.49\n",
            "[775 | 1446.20] loss=2.49 avg=2.49\n",
            "[776 | 1447.84] loss=2.19 avg=2.49\n",
            "[777 | 1449.48] loss=2.37 avg=2.49\n",
            "[778 | 1451.12] loss=2.71 avg=2.49\n",
            "[779 | 1452.76] loss=1.75 avg=2.48\n",
            "[780 | 1454.38] loss=2.03 avg=2.48\n",
            "[781 | 1456.02] loss=2.20 avg=2.47\n",
            "[782 | 1457.66] loss=2.65 avg=2.48\n",
            "[783 | 1459.30] loss=2.47 avg=2.48\n",
            "[784 | 1460.94] loss=2.33 avg=2.48\n",
            "[785 | 1462.57] loss=1.73 avg=2.47\n",
            "[786 | 1464.21] loss=2.79 avg=2.47\n",
            "[787 | 1465.84] loss=2.49 avg=2.47\n",
            "[788 | 1467.48] loss=2.84 avg=2.47\n",
            "[789 | 1469.11] loss=2.60 avg=2.48\n",
            "[790 | 1470.75] loss=2.59 avg=2.48\n",
            "[791 | 1472.38] loss=2.39 avg=2.48\n",
            "[792 | 1474.02] loss=2.74 avg=2.48\n",
            "[793 | 1475.66] loss=2.52 avg=2.48\n",
            "[794 | 1477.30] loss=2.36 avg=2.48\n",
            "[795 | 1478.95] loss=2.12 avg=2.47\n",
            "[796 | 1480.59] loss=2.74 avg=2.48\n",
            "[797 | 1482.23] loss=2.36 avg=2.48\n",
            "[798 | 1483.88] loss=2.05 avg=2.47\n",
            "[799 | 1485.54] loss=2.21 avg=2.47\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " would have been more than a foot to the inside for a player to hit the inside corner. That would have created an awkward shot for a defender in the middle of the ice. As soon as NHL officials learned of the problem, they immediately suspended the player and moved him to the minors.\n",
            "\n",
            "The rules were changed because the puck was not bouncing off. But the NHL should have changed the rules to discourage the player from making those silly mistakes, and to force him to learn hockey from an adult.\n",
            "\n",
            "Now you know how I feel about the NHL disciplining players for making hockey mistakes. I just wonder if any of them are up for a punishment that will work for them -- like a suspension for the rest of their lives.\n",
            "\n",
            "What would you say the NHL should do differently?\n",
            "\n",
            "LAKEWOOD: Hockey has been part of the fabric of the Chicago metro area for 50 years. The sport is a staple of American culture, the backbone of our economy and the game of American hockey. It is an institution in its community—a way of life. It's a joy for our young people to see their own players, and its legacy will endure not only in arenas across the NHL but throughout the country.\n",
            "\n",
            "What I find surprising about today is the level of vitriol directed toward NHL players. Even if the player gets the message that discipline will serve him, he must understand that his actions on the ice may have a real effect on the community. Some of the people who feel the most threatened are the most privileged and well-to-do -- and they would be the ones who make the decisions about what teams pay the players, what they wear and where they play.\n",
            "\n",
            "There must be some simple answer to that but, hey, here it is.\n",
            "\n",
            "The NHL has been a wonderful institution for me and my family. In this day and age, the games I have watched as a kid on TV and the movies I have seen growing up are often accompanied by songs and patriotic themes. But as parents, we must try to understand what we are buying when we pay for each team. What do we contribute to our children, and what do we give them, if anything, that our children will have to pay for? We spend a lot of time talking about family bonds and the value of American values here at The Big Idea. But, really, what is it that we're buying here in the States? How many times have we watched a hockey game, and then watched the commercials and bought a bunch of soft drinks? Maybe it's time for the NHL to do a better job at the business of American hockey.\n",
            "\n",
            "SCHNEIDER: So this is a big difference. For decades, the NHL was the game of American professional sports, like the NFL or baseball. Now, it is also the world's game. So there are big differences with the United States. There are certain teams that really are good at being on the ice. We love the Buffalo Sabres. They are a huge team. They win a lot of games. We like the Chicago Blackhawks and Los Angeles Kings, who are really good on the ice. This is the United States, and a lot of families have bought a big jersey or two in anticipation of the team showing up at the rink. But this is a brand-new game, in the United States. And this is what I am trying to explain to you, and all the young people out there, is that the difference is real, and should be addressed.\n",
            "\n",
            "We have an established game and a new game that is, in a sense, new. We have some of the players from this game, like [Stan] Wawrinka, who have been around. But the rules of international hockey are very simple. The U.S. is out of its depth in developing this game. It has to do it in America, because here is a market where it is the most popular sport.\n",
            "\n",
            "If they do anything to change that, we will find out that what the NHL and the leagues we work with are offering are not worth what they are selling here. When the first rules of international hockey go into effect, it will be an enormous financial blow for us. But I can take one of the first lessons for us here. If NHL players can get a couple years out of their contracts, then it should be easy for us to move on.\n",
            "\n",
            "What was their deal? How many years was the deal? How should we negotiate a new deal and change the terms?\n",
            "\n",
            "MACKAY: They get paid what we would pay them to stay in the game, for two full seasons. That's it. But it is not on the table. And they have been dealing with it for decades -- for decades. So it is not a new issue.\n",
            "\n",
            "It is not a problem that is unique to the United States, either. In recent years, Canada has been criticized for a lack of discipline in its junior team. It's not unusual for a Canadian\n",
            "\n",
            "[800 | 1509.95] loss=2.58 avg=2.47\n",
            "[801 | 1511.62] loss=2.47 avg=2.47\n",
            "[802 | 1513.30] loss=2.63 avg=2.47\n",
            "[803 | 1514.98] loss=2.07 avg=2.47\n",
            "[804 | 1516.65] loss=2.55 avg=2.47\n",
            "[805 | 1518.32] loss=1.84 avg=2.46\n",
            "[806 | 1519.99] loss=2.39 avg=2.46\n",
            "[807 | 1521.65] loss=1.93 avg=2.46\n",
            "[808 | 1523.31] loss=2.40 avg=2.46\n",
            "[809 | 1524.98] loss=2.71 avg=2.46\n",
            "[810 | 1526.64] loss=2.44 avg=2.46\n",
            "[811 | 1528.29] loss=2.34 avg=2.46\n",
            "[812 | 1529.95] loss=2.50 avg=2.46\n",
            "[813 | 1531.60] loss=1.93 avg=2.45\n",
            "[814 | 1533.25] loss=1.94 avg=2.45\n",
            "[815 | 1534.91] loss=2.58 avg=2.45\n",
            "[816 | 1536.55] loss=2.74 avg=2.45\n",
            "[817 | 1538.20] loss=2.23 avg=2.45\n",
            "[818 | 1539.84] loss=2.49 avg=2.45\n",
            "[819 | 1541.47] loss=2.82 avg=2.45\n",
            "[820 | 1543.11] loss=2.24 avg=2.45\n",
            "[821 | 1544.75] loss=2.61 avg=2.45\n",
            "[822 | 1546.38] loss=2.44 avg=2.45\n",
            "[823 | 1548.02] loss=2.50 avg=2.45\n",
            "[824 | 1549.65] loss=2.49 avg=2.45\n",
            "[825 | 1551.29] loss=2.56 avg=2.45\n",
            "[826 | 1552.92] loss=2.29 avg=2.45\n",
            "[827 | 1554.56] loss=2.65 avg=2.45\n",
            "[828 | 1556.20] loss=2.21 avg=2.45\n",
            "[829 | 1557.83] loss=2.09 avg=2.45\n",
            "[830 | 1559.47] loss=2.14 avg=2.45\n",
            "[831 | 1561.10] loss=2.25 avg=2.44\n",
            "[832 | 1562.74] loss=2.43 avg=2.44\n",
            "[833 | 1564.37] loss=2.31 avg=2.44\n",
            "[834 | 1566.01] loss=2.95 avg=2.45\n",
            "[835 | 1567.65] loss=2.61 avg=2.45\n",
            "[836 | 1569.29] loss=2.41 avg=2.45\n",
            "[837 | 1570.92] loss=2.12 avg=2.44\n",
            "[838 | 1572.55] loss=2.28 avg=2.44\n",
            "[839 | 1574.19] loss=2.70 avg=2.45\n",
            "[840 | 1575.84] loss=2.74 avg=2.45\n",
            "[841 | 1577.48] loss=2.15 avg=2.45\n",
            "[842 | 1579.13] loss=1.99 avg=2.44\n",
            "[843 | 1580.79] loss=2.05 avg=2.44\n",
            "[844 | 1582.43] loss=2.61 avg=2.44\n",
            "[845 | 1584.09] loss=2.51 avg=2.44\n",
            "[846 | 1585.74] loss=2.45 avg=2.44\n",
            "[847 | 1587.40] loss=2.66 avg=2.44\n",
            "[848 | 1589.06] loss=2.44 avg=2.44\n",
            "[849 | 1590.72] loss=2.47 avg=2.44\n",
            "[850 | 1592.38] loss=2.03 avg=2.44\n",
            "[851 | 1594.06] loss=1.94 avg=2.43\n",
            "[852 | 1595.73] loss=2.46 avg=2.43\n",
            "[853 | 1597.40] loss=2.31 avg=2.43\n",
            "[854 | 1599.07] loss=2.38 avg=2.43\n",
            "[855 | 1600.74] loss=1.33 avg=2.42\n",
            "[856 | 1602.42] loss=1.67 avg=2.41\n",
            "[857 | 1604.10] loss=2.96 avg=2.42\n",
            "[858 | 1605.79] loss=2.46 avg=2.42\n",
            "[859 | 1607.48] loss=1.97 avg=2.41\n",
            "[860 | 1609.16] loss=2.34 avg=2.41\n",
            "[861 | 1610.85] loss=2.69 avg=2.42\n",
            "[862 | 1612.54] loss=2.20 avg=2.41\n",
            "[863 | 1614.21] loss=2.81 avg=2.42\n",
            "[864 | 1615.90] loss=2.20 avg=2.42\n",
            "[865 | 1617.57] loss=2.32 avg=2.42\n",
            "[866 | 1619.25] loss=2.42 avg=2.42\n",
            "[867 | 1620.92] loss=2.69 avg=2.42\n",
            "[868 | 1622.59] loss=2.64 avg=2.42\n",
            "[869 | 1624.26] loss=2.57 avg=2.42\n",
            "[870 | 1625.93] loss=2.07 avg=2.42\n",
            "[871 | 1627.59] loss=3.06 avg=2.42\n",
            "[872 | 1629.25] loss=2.41 avg=2.42\n",
            "[873 | 1630.91] loss=2.56 avg=2.43\n",
            "[874 | 1632.57] loss=2.04 avg=2.42\n",
            "[875 | 1634.23] loss=2.12 avg=2.42\n",
            "[876 | 1635.88] loss=2.38 avg=2.42\n",
            "[877 | 1637.54] loss=2.34 avg=2.42\n",
            "[878 | 1639.19] loss=2.77 avg=2.42\n",
            "[879 | 1640.84] loss=2.35 avg=2.42\n",
            "[880 | 1642.47] loss=1.98 avg=2.42\n",
            "[881 | 1644.12] loss=2.55 avg=2.42\n",
            "[882 | 1645.75] loss=1.77 avg=2.41\n",
            "[883 | 1647.39] loss=2.25 avg=2.41\n",
            "[884 | 1649.03] loss=2.45 avg=2.41\n",
            "[885 | 1650.67] loss=2.77 avg=2.41\n",
            "[886 | 1652.31] loss=2.64 avg=2.42\n",
            "[887 | 1653.94] loss=2.10 avg=2.41\n",
            "[888 | 1655.58] loss=2.27 avg=2.41\n",
            "[889 | 1657.22] loss=2.92 avg=2.42\n",
            "[890 | 1658.85] loss=2.18 avg=2.41\n",
            "[891 | 1660.49] loss=2.44 avg=2.41\n",
            "[892 | 1662.12] loss=2.39 avg=2.41\n",
            "[893 | 1663.76] loss=2.23 avg=2.41\n",
            "[894 | 1665.40] loss=2.20 avg=2.41\n",
            "[895 | 1667.03] loss=2.20 avg=2.41\n",
            "[896 | 1668.66] loss=2.21 avg=2.41\n",
            "[897 | 1670.30] loss=2.78 avg=2.41\n",
            "[898 | 1671.94] loss=1.80 avg=2.40\n",
            "[899 | 1673.58] loss=2.74 avg=2.41\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " two long years, and the United Nations in Geneva was already preparing for another international conference, the Paris Agreement on Climate Change. They agreed to go in together.\n",
            "\n",
            "Then one Friday in December, as the summit was starting up, the conference committee decided that, in keeping with UN tradition, the next meeting would be on Monday. And on Monday, as the first day of the conference opened, Trump sat in the front row. His wife, Melania, sat in the rear. The mood in the chamber, held in the grand and spacious United Nations Headquarters in New York, turned somber.\n",
            "\n",
            "In a few days of televised remarks from Paris, the president had said there would be \"major\" U.S. participation. In one of those televised remarks, he even promised to announce a massive oil tax in the United States. During the press conference afterward, Trump made a few more announcements, and then the mood returned to somber, even grim.\n",
            "\n",
            "In the following days, as Congress was debating the Paris Agreement, Trump had turned to the UN. He declared there would be no United States withdrawal from the agreement, and added that it would not count as a vote on the Paris Agreement. In a series of tweets, he repeatedly reiterated that pledge. In early March, a few days before he left Paris, he asked the UN climate panel to help fund his so-called \"ultimate big beautiful thing.\"\n",
            "\n",
            "In the year since, Trump has followed through on that pledge, and the United States has now entered into the Paris Agreement with the third-largest emitter of greenhouse gases on Earth. The United States is also in violation of his campaign promise.\n",
            "\n",
            "Since the summit ended at 3 a.m. Friday, U.S. officials have been trying to decide how to respond. Trump himself has used Twitter to vent, once or repeatedly, at the United Nations. He has said he may \"sit-out\" during a U.S. withdrawal. But he has also tweeted that the United States is committed to the Paris Agreement. He even issued a statement late Saturday calling on the other 195 nations to withdraw in solidarity with the U.S. in the Paris Agreement.\n",
            "\n",
            "In that statement, from his White House, Trump urged everyone to remain unified; unite with him; and do their best—not all, of course: a United States pullout could damage the global climate deal as well as hurt the country struggling to weather climate change. But even so, Trump seemed to want to use the Paris Agreement as a tool, using the Paris Agreement as a springboard for a second big deal.\n",
            "\n",
            "As with so many of his first big diplomatic decisions, it seemed premature to expect that Trump would actually pull out of Paris. There is only so much one can do on his first date with a new leader, after all. And in the world of international diplomacy, it is not that simple. A president may withdraw from the Paris Agreement—or at least announce his intention to withdraw from the Paris Agreement—while keeping the United States in the deal.\n",
            "\n",
            "There is a rule of international law that requires a country not to leave the international system unless its exit poses serious risks. The U.S. has not violated this rule in the Paris Agreement. Indeed, the agreement does not require any country to leave at all. It only requires members of the other 195 countries to implement parts of the accord at least to certain standards, such as reducing their emissions, meaning they can never leave.\n",
            "\n",
            "Why would Trump want to leave Paris? That is one question. Another questions: What kind of agreement is there in Paris about Paris?\n",
            "\n",
            "And yet another question, not far from the last but not the last: In what kind of agreement would the United States leave Paris? A presidential decision in 2017 to withdraw from an agreement—or the absence of an agreement, in this case—is supposed to be taken not as a signal of a shift in U.S. policy, but as a signal of a shift in U.S. priorities. It is not to be taken too seriously.\n",
            "\n",
            "\"The Paris Agreement was designed to serve the common good of humanity in a coordinated way,\" Michael Brune, a former member of the United Nations Climate Panel for Secretary-General Ban Ki-moon, wrote in his book The New Geopolitics of Globalization.\n",
            "\n",
            "Brune, the director of the Center on Global Governance at New York University, said in 2016 the United States should leave the Paris Agreement because a single global climate policy \"would bring about massive benefits to our economy as well as to our national security, stability, and well-being.\"\n",
            "\n",
            "Brune also wrote: \"By reducing our global dependence on fossil fuels and increasing innovation, we can make a real and lasting contribution to global climate change mitigation and adaptation.\"\n",
            "\n",
            "For all those reasons, leaving the Paris Agreement is not an easy thing. It would have to be a very big deal for the United States; its economic growth depends on global climate policy.\n",
            "\n",
            "[900 | 1698.18] loss=2.21 avg=2.40\n",
            "[901 | 1699.86] loss=2.42 avg=2.41\n",
            "[902 | 1701.52] loss=2.47 avg=2.41\n",
            "[903 | 1703.20] loss=2.28 avg=2.40\n",
            "[904 | 1704.87] loss=2.37 avg=2.40\n",
            "[905 | 1706.55] loss=2.86 avg=2.41\n",
            "[906 | 1708.23] loss=2.19 avg=2.41\n",
            "[907 | 1709.91] loss=2.77 avg=2.41\n",
            "[908 | 1711.59] loss=2.22 avg=2.41\n",
            "[909 | 1713.27] loss=2.70 avg=2.41\n",
            "[910 | 1714.94] loss=2.06 avg=2.41\n",
            "[911 | 1716.61] loss=2.57 avg=2.41\n",
            "[912 | 1718.27] loss=2.05 avg=2.41\n",
            "[913 | 1719.94] loss=1.85 avg=2.40\n",
            "[914 | 1721.62] loss=2.81 avg=2.40\n",
            "[915 | 1723.28] loss=2.36 avg=2.40\n",
            "[916 | 1724.95] loss=2.16 avg=2.40\n",
            "[917 | 1726.62] loss=2.17 avg=2.40\n",
            "[918 | 1728.28] loss=3.03 avg=2.41\n",
            "[919 | 1729.94] loss=2.65 avg=2.41\n",
            "[920 | 1731.59] loss=2.03 avg=2.40\n",
            "[921 | 1733.24] loss=2.20 avg=2.40\n",
            "[922 | 1734.90] loss=2.52 avg=2.40\n",
            "[923 | 1736.56] loss=2.01 avg=2.40\n",
            "[924 | 1738.21] loss=2.13 avg=2.40\n",
            "[925 | 1739.86] loss=2.66 avg=2.40\n",
            "[926 | 1741.51] loss=2.60 avg=2.40\n",
            "[927 | 1743.15] loss=2.01 avg=2.40\n",
            "[928 | 1744.80] loss=2.54 avg=2.40\n",
            "[929 | 1746.44] loss=1.76 avg=2.39\n",
            "[930 | 1748.08] loss=2.69 avg=2.40\n",
            "[931 | 1749.72] loss=2.52 avg=2.40\n",
            "[932 | 1751.35] loss=2.28 avg=2.40\n",
            "[933 | 1752.99] loss=2.56 avg=2.40\n",
            "[934 | 1754.62] loss=2.83 avg=2.40\n",
            "[935 | 1756.26] loss=2.40 avg=2.40\n",
            "[936 | 1757.90] loss=3.12 avg=2.41\n",
            "[937 | 1759.53] loss=2.36 avg=2.41\n",
            "[938 | 1761.16] loss=2.49 avg=2.41\n",
            "[939 | 1762.79] loss=2.49 avg=2.41\n",
            "[940 | 1764.43] loss=1.90 avg=2.40\n",
            "[941 | 1766.07] loss=2.17 avg=2.40\n",
            "[942 | 1767.70] loss=2.36 avg=2.40\n",
            "[943 | 1769.34] loss=2.25 avg=2.40\n",
            "[944 | 1770.98] loss=1.97 avg=2.40\n",
            "[945 | 1772.62] loss=2.07 avg=2.39\n",
            "[946 | 1774.27] loss=2.32 avg=2.39\n",
            "[947 | 1775.91] loss=2.55 avg=2.39\n",
            "[948 | 1777.56] loss=2.75 avg=2.40\n",
            "[949 | 1779.20] loss=2.08 avg=2.39\n",
            "[950 | 1780.85] loss=2.04 avg=2.39\n",
            "[951 | 1782.51] loss=2.53 avg=2.39\n",
            "[952 | 1784.16] loss=2.74 avg=2.40\n",
            "[953 | 1785.81] loss=2.36 avg=2.39\n",
            "[954 | 1787.47] loss=2.05 avg=2.39\n",
            "[955 | 1789.13] loss=2.28 avg=2.39\n",
            "[956 | 1790.80] loss=2.30 avg=2.39\n",
            "[957 | 1792.47] loss=2.42 avg=2.39\n",
            "[958 | 1794.14] loss=2.79 avg=2.39\n",
            "[959 | 1795.81] loss=2.03 avg=2.39\n",
            "[960 | 1797.49] loss=1.95 avg=2.39\n",
            "[961 | 1799.16] loss=2.17 avg=2.38\n",
            "[962 | 1800.83] loss=2.43 avg=2.38\n",
            "[963 | 1802.52] loss=2.79 avg=2.39\n",
            "[964 | 1804.21] loss=2.04 avg=2.38\n",
            "[965 | 1805.88] loss=1.85 avg=2.38\n",
            "[966 | 1807.56] loss=2.20 avg=2.38\n",
            "[967 | 1809.24] loss=2.91 avg=2.38\n",
            "[968 | 1810.93] loss=1.76 avg=2.38\n",
            "[969 | 1812.60] loss=2.80 avg=2.38\n",
            "[970 | 1814.26] loss=2.55 avg=2.38\n",
            "[971 | 1815.93] loss=2.48 avg=2.38\n",
            "[972 | 1817.61] loss=2.04 avg=2.38\n",
            "[973 | 1819.28] loss=2.07 avg=2.38\n",
            "[974 | 1820.95] loss=1.93 avg=2.37\n",
            "[975 | 1822.62] loss=2.69 avg=2.38\n",
            "[976 | 1824.29] loss=2.35 avg=2.38\n",
            "[977 | 1825.95] loss=2.51 avg=2.38\n",
            "[978 | 1827.60] loss=2.61 avg=2.38\n",
            "[979 | 1829.26] loss=2.62 avg=2.38\n",
            "[980 | 1830.92] loss=2.58 avg=2.38\n",
            "[981 | 1832.56] loss=2.72 avg=2.39\n",
            "[982 | 1834.21] loss=1.81 avg=2.38\n",
            "[983 | 1835.86] loss=2.19 avg=2.38\n",
            "[984 | 1837.50] loss=2.24 avg=2.38\n",
            "[985 | 1839.14] loss=2.62 avg=2.38\n",
            "[986 | 1840.79] loss=2.15 avg=2.38\n",
            "[987 | 1842.44] loss=2.11 avg=2.38\n",
            "[988 | 1844.07] loss=2.33 avg=2.37\n",
            "[989 | 1845.71] loss=2.60 avg=2.38\n",
            "[990 | 1847.35] loss=2.62 avg=2.38\n",
            "[991 | 1848.98] loss=2.55 avg=2.38\n",
            "[992 | 1850.62] loss=2.56 avg=2.38\n",
            "[993 | 1852.25] loss=2.45 avg=2.38\n",
            "[994 | 1853.89] loss=2.60 avg=2.39\n",
            "[995 | 1855.52] loss=2.60 avg=2.39\n",
            "[996 | 1857.16] loss=3.12 avg=2.40\n",
            "[997 | 1858.80] loss=2.25 avg=2.39\n",
            "[998 | 1860.43] loss=1.82 avg=2.39\n",
            "[999 | 1862.07] loss=2.60 avg=2.39\n",
            "Ran 1010 steps.\n",
            "Saving checkpoint/atlantic_science_345/model-1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpK_8Ln-8UVa",
        "colab_type": "code",
        "outputId": "2d7ddb8c-96d2-4faa-c97a-abad42d8ed01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "## education essays training - with 345\n",
        "!PYTHONPATH=src ./train.py --dataset /content/all_education.txt --run_name 'atlantic_education_345' --model_name '345M'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0627 17:22:20.994460 139710809851776 deprecation_wrapper.py:119] From /content/gpt-2/src/model.py:147: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
            "\n",
            "W0627 17:22:21.003343 139710809851776 deprecation_wrapper.py:119] From /content/gpt-2/src/memory_saving_gradients.py:13: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n",
            "W0627 17:22:21.095731 139710809851776 deprecation_wrapper.py:119] From ./train.py:88: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "W0627 17:22:21.096116 139710809851776 deprecation_wrapper.py:119] From ./train.py:91: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "2019-06-27 17:22:21.102430: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-06-27 17:22:21.102697: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x15eb100 executing computations on platform Host. Devices:\n",
            "2019-06-27 17:22:21.102727: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2019-06-27 17:22:21.105042: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\n",
            "2019-06-27 17:22:21.254825: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-27 17:22:21.255404: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x15ea840 executing computations on platform CUDA. Devices:\n",
            "2019-06-27 17:22:21.255437: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2019-06-27 17:22:21.255711: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-27 17:22:21.256077: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-06-27 17:22:21.256469: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-06-27 17:22:21.257774: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-06-27 17:22:21.259097: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-06-27 17:22:21.259495: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-06-27 17:22:21.264166: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-06-27 17:22:21.265583: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-06-27 17:22:21.269125: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-06-27 17:22:21.269292: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-27 17:22:21.269737: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-27 17:22:21.270069: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
            "2019-06-27 17:22:21.270129: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-06-27 17:22:21.271106: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-06-27 17:22:21.271132: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n",
            "2019-06-27 17:22:21.271143: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n",
            "2019-06-27 17:22:21.271471: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-27 17:22:21.271882: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-27 17:22:21.272273: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14202 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "W0627 17:22:21.273218 139710809851776 deprecation_wrapper.py:119] From ./train.py:92: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0627 17:22:32.061942 139710809851776 deprecation.py:323] From /content/gpt-2/src/sample.py:65: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "W0627 17:22:32.077015 139710809851776 deprecation.py:323] From /content/gpt-2/src/sample.py:16: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0627 17:22:32.078701 139710809851776 deprecation.py:323] From /content/gpt-2/src/sample.py:70: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.random.categorical` instead.\n",
            "W0627 17:22:32.088586 139710809851776 deprecation_wrapper.py:119] From ./train.py:121: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "W0627 17:22:47.190207 139710809851776 deprecation_wrapper.py:119] From ./train.py:144: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n",
            "W0627 17:22:47.193153 139710809851776 deprecation_wrapper.py:119] From ./train.py:147: The name tf.summary.merge is deprecated. Please use tf.compat.v1.summary.merge instead.\n",
            "\n",
            "W0627 17:22:47.193959 139710809851776 deprecation_wrapper.py:119] From ./train.py:149: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
            "\n",
            "W0627 17:22:47.194769 139710809851776 deprecation_wrapper.py:119] From ./train.py:152: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "Loading checkpoint models/345M/model.ckpt\n",
            "W0627 17:23:00.455274 139710809851776 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "Loading dataset...\n",
            "100% 1/1 [00:03<00:00,  3.48s/it]\n",
            "dataset has 497397 tokens\n",
            "Training...\n",
            "2019-06-27 17:23:17.714758: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n",
            "2019-06-27 17:23:18.510443: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "[1 | 13.95] loss=3.06 avg=3.06\n",
            "[2 | 15.48] loss=2.95 avg=3.00\n",
            "[3 | 17.00] loss=2.86 avg=2.95\n",
            "[4 | 18.52] loss=2.99 avg=2.96\n",
            "[5 | 20.05] loss=2.58 avg=2.88\n",
            "[6 | 21.58] loss=3.01 avg=2.91\n",
            "[7 | 23.13] loss=2.86 avg=2.90\n",
            "[8 | 24.66] loss=2.99 avg=2.91\n",
            "[9 | 26.20] loss=2.84 avg=2.90\n",
            "[10 | 27.73] loss=2.82 avg=2.89\n",
            "[11 | 29.28] loss=2.80 avg=2.88\n",
            "[12 | 30.82] loss=2.57 avg=2.86\n",
            "[13 | 32.37] loss=2.94 avg=2.86\n",
            "[14 | 33.91] loss=2.88 avg=2.86\n",
            "[15 | 35.46] loss=2.86 avg=2.86\n",
            "[16 | 37.01] loss=2.71 avg=2.85\n",
            "[17 | 38.56] loss=2.82 avg=2.85\n",
            "[18 | 40.12] loss=2.65 avg=2.84\n",
            "[19 | 41.67] loss=2.49 avg=2.82\n",
            "[20 | 43.23] loss=2.74 avg=2.82\n",
            "[21 | 44.78] loss=2.75 avg=2.81\n",
            "[22 | 46.34] loss=2.71 avg=2.81\n",
            "[23 | 47.90] loss=2.68 avg=2.80\n",
            "[24 | 49.46] loss=2.61 avg=2.79\n",
            "[25 | 51.02] loss=2.42 avg=2.77\n",
            "[26 | 52.59] loss=2.48 avg=2.76\n",
            "[27 | 54.16] loss=2.55 avg=2.75\n",
            "[28 | 55.73] loss=2.60 avg=2.75\n",
            "[29 | 57.30] loss=2.76 avg=2.75\n",
            "[30 | 58.87] loss=2.75 avg=2.75\n",
            "[31 | 60.45] loss=2.76 avg=2.75\n",
            "[32 | 62.03] loss=2.60 avg=2.74\n",
            "[33 | 63.62] loss=3.14 avg=2.76\n",
            "[34 | 65.20] loss=2.67 avg=2.75\n",
            "[35 | 66.78] loss=2.73 avg=2.75\n",
            "[36 | 68.37] loss=2.81 avg=2.75\n",
            "[37 | 69.95] loss=2.71 avg=2.75\n",
            "[38 | 71.54] loss=2.66 avg=2.75\n",
            "[39 | 73.13] loss=2.58 avg=2.74\n",
            "[40 | 74.72] loss=2.62 avg=2.74\n",
            "[41 | 76.31] loss=2.80 avg=2.74\n",
            "[42 | 77.91] loss=2.89 avg=2.75\n",
            "[43 | 79.50] loss=2.42 avg=2.74\n",
            "[44 | 81.10] loss=2.63 avg=2.73\n",
            "[45 | 82.69] loss=2.55 avg=2.73\n",
            "[46 | 84.28] loss=2.70 avg=2.73\n",
            "[47 | 85.87] loss=3.09 avg=2.74\n",
            "[48 | 87.46] loss=2.74 avg=2.74\n",
            "[49 | 89.05] loss=2.48 avg=2.73\n",
            "[50 | 90.64] loss=2.69 avg=2.73\n",
            "[51 | 92.23] loss=2.88 avg=2.73\n",
            "[52 | 93.82] loss=2.71 avg=2.73\n",
            "[53 | 95.41] loss=2.62 avg=2.73\n",
            "[54 | 96.99] loss=2.70 avg=2.73\n",
            "[55 | 98.57] loss=2.87 avg=2.73\n",
            "[56 | 100.15] loss=2.82 avg=2.74\n",
            "[57 | 101.74] loss=2.66 avg=2.73\n",
            "[58 | 103.33] loss=2.57 avg=2.73\n",
            "[59 | 104.91] loss=2.49 avg=2.72\n",
            "[60 | 106.49] loss=2.55 avg=2.72\n",
            "[61 | 108.08] loss=2.71 avg=2.72\n",
            "[62 | 109.67] loss=2.53 avg=2.72\n",
            "[63 | 111.25] loss=2.81 avg=2.72\n",
            "[64 | 112.84] loss=2.62 avg=2.72\n",
            "[65 | 114.42] loss=2.70 avg=2.72\n",
            "[66 | 116.00] loss=2.66 avg=2.72\n",
            "[67 | 117.59] loss=2.54 avg=2.71\n",
            "[68 | 119.17] loss=2.66 avg=2.71\n",
            "[69 | 120.76] loss=2.53 avg=2.71\n",
            "[70 | 122.34] loss=2.84 avg=2.71\n",
            "[71 | 123.93] loss=2.66 avg=2.71\n",
            "[72 | 125.51] loss=2.61 avg=2.71\n",
            "[73 | 127.09] loss=2.78 avg=2.71\n",
            "[74 | 128.68] loss=2.68 avg=2.71\n",
            "[75 | 130.27] loss=2.25 avg=2.70\n",
            "[76 | 131.86] loss=2.86 avg=2.70\n",
            "[77 | 133.45] loss=2.74 avg=2.70\n",
            "[78 | 135.04] loss=3.19 avg=2.71\n",
            "[79 | 136.63] loss=2.67 avg=2.71\n",
            "[80 | 138.22] loss=2.62 avg=2.71\n",
            "[81 | 139.80] loss=2.69 avg=2.71\n",
            "[82 | 141.40] loss=2.67 avg=2.71\n",
            "[83 | 142.99] loss=2.75 avg=2.71\n",
            "[84 | 144.58] loss=2.96 avg=2.71\n",
            "[85 | 146.18] loss=2.55 avg=2.71\n",
            "[86 | 147.77] loss=2.80 avg=2.71\n",
            "[87 | 149.37] loss=2.55 avg=2.71\n",
            "[88 | 150.96] loss=2.74 avg=2.71\n",
            "[89 | 152.57] loss=2.48 avg=2.71\n",
            "[90 | 154.17] loss=2.45 avg=2.70\n",
            "[91 | 155.78] loss=2.75 avg=2.70\n",
            "[92 | 157.39] loss=2.93 avg=2.71\n",
            "[93 | 159.00] loss=2.73 avg=2.71\n",
            "[94 | 160.60] loss=2.96 avg=2.71\n",
            "[95 | 162.22] loss=2.46 avg=2.71\n",
            "[96 | 163.83] loss=2.87 avg=2.71\n",
            "[97 | 165.44] loss=2.91 avg=2.71\n",
            "[98 | 167.06] loss=2.56 avg=2.71\n",
            "[99 | 168.69] loss=2.61 avg=2.71\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " In a way, she seems to be looking at what's happening at the moment in the United States, where an intense focus on \"white identity\" is leading young people to turn away from the idea of race when it comes to questions of racial representation and identity. On Twitter, users are responding to a New-York Times article by a writer called \"White Kids, How Do You Really See the Color of the Person You're Talking About?\" The story, a first draft, was published just weeks ago, but has recently been picked up by the Atlantic.\n",
            "\n",
            "In an interview with the Times, Sarah Ruggles, the managing editor of the article, called for the writers on both teams to get together and work together on an \"explicit, inclusive, and meaningful discussion about 'race.'\"\n",
            "\n",
            "The Times piece includes the following paragraph:\n",
            "\n",
            "The article, which was written by Ms. Ruggles and originally published in the last issue of Esquire, is entitled \"White Kids, How Do You Really See The Color of the Person You're Talking About?\" What the story actually seeks to reveal is that many young white kids have internalized the concept of race without actually doing so (or at least not being aware of it). In the article, the writers ask parents if their kids \"see all the color of their race.\" After considering the question and answering yes to three of them they respond that they do because it is \"a more fundamental part of who they are than race itself. It's in the essence of them, and we tend to internalize it and hold it against people we don't know.\"\n",
            "\n",
            "Some of it, it seems, is just a way for middle-school children to get their thoughts out.\n",
            "\n",
            "Ruggles says:\n",
            "\n",
            "We've really found that kids are trying to explain, and explaining a lot! We've seen a lot of the responses: We've heard, 'If I'm black, I see black everywhere.' Or, 'I don't see black or brown. I only see white.' Which is just not true, and actually is a pretty important question, because I think when something is not being said it should definitely be included—something that is not only being said but is the thing being addressed.\n",
            "\n",
            "Many people take issue with such statements, and have often pointed out that the writer, Ms. Ruggles, is Black. As she pointed out in a Times column:\n",
            "\n",
            "There are times when a Black person will look at a white person, and maybe see a light. Or they see a darker shade of someone, and maybe, in a moment of confusion, they can recognize themselves. But the key here is a person is not in a place of confusion, for the person does not \"see\" or \"identify,\" no matter how many times it might occur to them. Rather, it is the other, larger, and more complex thing that the person perceives and perceives.\n",
            "\n",
            "When I asked Ruggles to explain why she felt the essay was so important—and why her fellow writers felt the same way—I received a response from Ruggles that I found especially troubling:\n",
            "\n",
            "I had written an essay, which was published in the last issue, with the provocative statement, \"White Kids, How Do You Really See the Color of the Person You're Talking About?\" I had included it as an example of a Black person, which in some ways seems like an accurate description of their experience, but in a way, it is not. … We've really found that kids are trying to talk about, and talking—for example, they try to understand. So in that context, to write with the subject \"white kids, how do you really see the color of the person you're talking about?\" may not even be the most honest way of saying that white kids see themselves and, as a result, are trying to speak about. The essay was a conversation between my Black friend, a white friend, another white friend, and so on and so forth. However, what the Times and some of the other contributors did was to have an internal conversation. They had a discussion—their own, as well. It was about how to think about race—and the conversation itself. That was the takeaway. The Times did not do a disservice to people by including our discussion of the topic in its article. I think that was clearly reflected at every step in the process that it followed.\n",
            "\n",
            "Ruggles did not provide any evidence to support her assertion; instead, she wrote:\n",
            "\n",
            "I have also experienced firsthand some of the issues this piece has addressed — and I had to deal with the experiences and questions of a handful of myself. And I will say in defense of the pieces that others were reading as well… The Times did not do an injustice to people by including our exchange of points on race with the focus on how the people and events of the conversation that led to our thinking. Our experience was actually reflective of our conversations as it\n",
            "\n",
            "[100 | 196.45] loss=2.78 avg=2.71\n",
            "[101 | 198.08] loss=2.83 avg=2.71\n",
            "[102 | 199.70] loss=2.76 avg=2.71\n",
            "[103 | 201.33] loss=2.63 avg=2.71\n",
            "[104 | 202.96] loss=2.63 avg=2.71\n",
            "[105 | 204.58] loss=2.69 avg=2.71\n",
            "[106 | 206.20] loss=2.76 avg=2.71\n",
            "[107 | 207.82] loss=2.78 avg=2.71\n",
            "[108 | 209.45] loss=2.55 avg=2.71\n",
            "[109 | 211.07] loss=2.97 avg=2.71\n",
            "[110 | 212.69] loss=2.85 avg=2.71\n",
            "[111 | 214.30] loss=2.52 avg=2.71\n",
            "[112 | 215.92] loss=2.84 avg=2.71\n",
            "[113 | 217.54] loss=2.52 avg=2.71\n",
            "[114 | 219.16] loss=2.60 avg=2.71\n",
            "[115 | 220.77] loss=2.57 avg=2.71\n",
            "[116 | 222.39] loss=2.43 avg=2.70\n",
            "[117 | 224.00] loss=2.67 avg=2.70\n",
            "[118 | 225.62] loss=2.84 avg=2.70\n",
            "[119 | 227.24] loss=2.65 avg=2.70\n",
            "[120 | 228.86] loss=2.52 avg=2.70\n",
            "[121 | 230.47] loss=2.67 avg=2.70\n",
            "[122 | 232.09] loss=2.68 avg=2.70\n",
            "[123 | 233.70] loss=2.52 avg=2.70\n",
            "[124 | 235.32] loss=2.45 avg=2.69\n",
            "[125 | 236.94] loss=2.92 avg=2.70\n",
            "[126 | 238.56] loss=2.28 avg=2.69\n",
            "[127 | 240.18] loss=2.55 avg=2.69\n",
            "[128 | 241.81] loss=2.54 avg=2.69\n",
            "[129 | 243.44] loss=2.99 avg=2.69\n",
            "[130 | 245.07] loss=2.62 avg=2.69\n",
            "[131 | 246.70] loss=2.90 avg=2.69\n",
            "[132 | 248.33] loss=2.83 avg=2.70\n",
            "[133 | 249.97] loss=3.00 avg=2.70\n",
            "[134 | 251.60] loss=2.53 avg=2.70\n",
            "[135 | 253.23] loss=2.74 avg=2.70\n",
            "[136 | 254.87] loss=2.71 avg=2.70\n",
            "[137 | 256.51] loss=2.67 avg=2.70\n",
            "[138 | 258.16] loss=2.65 avg=2.70\n",
            "[139 | 259.81] loss=2.42 avg=2.69\n",
            "[140 | 261.46] loss=2.66 avg=2.69\n",
            "[141 | 263.11] loss=2.66 avg=2.69\n",
            "[142 | 264.77] loss=2.83 avg=2.69\n",
            "[143 | 266.42] loss=2.65 avg=2.69\n",
            "[144 | 268.08] loss=2.80 avg=2.70\n",
            "[145 | 269.75] loss=2.47 avg=2.69\n",
            "[146 | 271.39] loss=2.57 avg=2.69\n",
            "[147 | 273.05] loss=2.30 avg=2.69\n",
            "[148 | 274.71] loss=2.61 avg=2.69\n",
            "[149 | 276.38] loss=2.69 avg=2.69\n",
            "[150 | 278.03] loss=2.66 avg=2.68\n",
            "[151 | 279.70] loss=2.54 avg=2.68\n",
            "[152 | 281.36] loss=2.81 avg=2.68\n",
            "[153 | 283.03] loss=2.76 avg=2.69\n",
            "[154 | 284.69] loss=2.62 avg=2.68\n",
            "[155 | 286.35] loss=2.44 avg=2.68\n",
            "[156 | 288.00] loss=2.72 avg=2.68\n",
            "[157 | 289.66] loss=2.77 avg=2.68\n",
            "[158 | 291.31] loss=2.40 avg=2.68\n",
            "[159 | 292.96] loss=2.74 avg=2.68\n",
            "[160 | 294.61] loss=2.73 avg=2.68\n",
            "[161 | 296.25] loss=2.53 avg=2.68\n",
            "[162 | 297.90] loss=2.51 avg=2.68\n",
            "[163 | 299.54] loss=2.96 avg=2.68\n",
            "[164 | 301.18] loss=2.07 avg=2.67\n",
            "[165 | 302.82] loss=2.51 avg=2.67\n",
            "[166 | 304.45] loss=2.12 avg=2.66\n",
            "[167 | 306.09] loss=2.19 avg=2.66\n",
            "[168 | 307.72] loss=2.72 avg=2.66\n",
            "[169 | 309.36] loss=2.62 avg=2.66\n",
            "[170 | 311.00] loss=2.64 avg=2.66\n",
            "[171 | 312.63] loss=2.85 avg=2.66\n",
            "[172 | 314.25] loss=2.73 avg=2.66\n",
            "[173 | 315.89] loss=2.81 avg=2.66\n",
            "[174 | 317.51] loss=2.49 avg=2.66\n",
            "[175 | 319.14] loss=2.30 avg=2.66\n",
            "[176 | 320.77] loss=2.87 avg=2.66\n",
            "[177 | 322.40] loss=2.56 avg=2.66\n",
            "[178 | 324.03] loss=2.62 avg=2.66\n",
            "[179 | 325.66] loss=2.03 avg=2.65\n",
            "[180 | 327.29] loss=2.61 avg=2.65\n",
            "[181 | 328.91] loss=2.51 avg=2.65\n",
            "[182 | 330.54] loss=2.92 avg=2.65\n",
            "[183 | 332.17] loss=2.45 avg=2.65\n",
            "[184 | 333.80] loss=2.42 avg=2.65\n",
            "[185 | 335.43] loss=2.44 avg=2.64\n",
            "[186 | 337.06] loss=2.57 avg=2.64\n",
            "[187 | 338.70] loss=2.43 avg=2.64\n",
            "[188 | 340.32] loss=1.96 avg=2.63\n",
            "[189 | 341.95] loss=2.73 avg=2.63\n",
            "[190 | 343.58] loss=2.77 avg=2.64\n",
            "[191 | 345.22] loss=2.77 avg=2.64\n",
            "[192 | 346.86] loss=2.74 avg=2.64\n",
            "[193 | 348.50] loss=2.93 avg=2.64\n",
            "[194 | 350.14] loss=2.76 avg=2.64\n",
            "[195 | 351.79] loss=2.60 avg=2.64\n",
            "[196 | 353.44] loss=2.67 avg=2.64\n",
            "[197 | 355.08] loss=2.45 avg=2.64\n",
            "[198 | 356.73] loss=2.56 avg=2.64\n",
            "[199 | 358.39] loss=2.90 avg=2.64\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " especially for people who are really desperate for a new challenge (and, really, for the first time since I turned twelve, I felt genuinely inspired). It was something of a surprise to hear about this, since in the past, there have been a few high-profile instances where people have tried doing the old-fashioned thing: For example, in January 2015 there was a story of another teenager who had come to me after having a meltdown as the result of social media drama. He reported being targeted by his peers after being tagged as a \"chick.\" He reported being bullied by members of his school's LGBT community after they criticized a tweet in which his schoolmate and a friend, having no relationship with him personally, described their sexual preferences in a way that made him uncomfortable. This boy had just gone through two years of bullying as a result of social media, and he still had a terrible time of it.\n",
            "\n",
            "A month ago, I was working on a project, and I had an idea, and I thought, If I keep this in mind, then I might be able to work on a bunch of the same-sex-marriage cases this week for a day. I started the week with a new hashtag, #WhatIDoToDegradate, but I'm not sure whether I would have a successful week. In fact, I'm more likely to get frustrated every day. I did put #WhatIThinkAndDoWhen I'm Not Doing It, because I felt like in many cases I needed to come up with a counterargument. Most of my thoughts come from my head. But I don't have a clue how people think, and I don't have a clue how people believe in me. So I tried to do this on a Monday, and I had a big list to do by then. I thought you guys might help.\n",
            "\n",
            "And you're so amazing. I know you're going to love this:\n",
            "\n",
            "This hashtag is not for you. I don't care if you're a cisgendered man, you're not going to love anything that comes out of your head. It's going to end in tears. It's going to be painful. And that's how it works. Here we go. You just gotta hold your head high and hope nothing like this happens. But if you do come up with a counterargument, I suggest something like: I'm not going to do it, I don't trust you, I'm not in any way part of the problem, all I'm doing is trying to help me cope with the pain of experiencing it. And honestly, I think this is my greatest power as a person -- not to be that guy in the world who gets stuck in the closet, but to be that person who's open to the possibility that you might be in the same situation that I am. I mean, I'd be lying to you if I said this is not something that's been very difficult for me to cope with as a person from having a lot of experience with abuse and being told from a very early age that I have to be a certain way. It's just been a lot harder than I thought it would be at times. And I'm just trying to open up a little bit, even though I am not someone who's interested in saying how many trans people there actually are, but let's try to start with a few. Let me just say that the number [of trans people] is something that I've been working on every day for more than a year. I've taken notes of different people, I know what their story is, I know the process they're going through. [They have] had very different experiences with it. As a teacher, sometimes when I see these stories of children when they were just little, I have to remind myself at the beginning that we need to take steps with the children, but if we're not doing any of that, the parents won't be able to talk to their children about it, and you're actually going to be seeing it a lot worse. So for me, with the hashtag, I'm just trying to use as another tool to sort of open up a little bit more.\n",
            "\n",
            "Now, it's not something I think that I ever would have predicted -- and I have worked on this since the beginning [on my hashtag], but it's a huge thing. I've been in that room with all the parents. I've been in that room with all the students and teachers. It's a lot of really good, really good stories about our school, and it's just been a really overwhelming year. And I think in the end, for me, these are very personal experiences. So you know there are some things that are true about it -- we're going to talk about them a little bit later on. But just a little bit about what you've heard, it's one of the things that really made me a little bit cynical this year, because I think everyone's feeling like they're getting a little bit\n",
            "\n",
            "[200 | 382.82] loss=2.76 avg=2.64\n",
            "[201 | 384.49] loss=2.40 avg=2.64\n",
            "[202 | 386.17] loss=2.50 avg=2.64\n",
            "[203 | 387.84] loss=2.42 avg=2.64\n",
            "[204 | 389.50] loss=2.85 avg=2.64\n",
            "[205 | 391.16] loss=2.46 avg=2.64\n",
            "[206 | 392.83] loss=2.34 avg=2.63\n",
            "[207 | 394.49] loss=2.83 avg=2.64\n",
            "[208 | 396.14] loss=2.79 avg=2.64\n",
            "[209 | 397.79] loss=2.66 avg=2.64\n",
            "[210 | 399.44] loss=2.67 avg=2.64\n",
            "[211 | 401.09] loss=2.53 avg=2.64\n",
            "[212 | 402.74] loss=2.54 avg=2.64\n",
            "[213 | 404.38] loss=2.49 avg=2.63\n",
            "[214 | 406.02] loss=2.50 avg=2.63\n",
            "[215 | 407.66] loss=2.49 avg=2.63\n",
            "[216 | 409.30] loss=2.50 avg=2.63\n",
            "[217 | 410.93] loss=2.67 avg=2.63\n",
            "[218 | 412.56] loss=2.57 avg=2.63\n",
            "[219 | 414.20] loss=2.68 avg=2.63\n",
            "[220 | 415.83] loss=1.80 avg=2.62\n",
            "[221 | 417.46] loss=2.57 avg=2.62\n",
            "[222 | 419.10] loss=2.74 avg=2.62\n",
            "[223 | 420.72] loss=2.12 avg=2.62\n",
            "[224 | 422.34] loss=2.52 avg=2.62\n",
            "[225 | 423.98] loss=2.53 avg=2.61\n",
            "[226 | 425.60] loss=2.75 avg=2.62\n",
            "[227 | 427.23] loss=2.46 avg=2.61\n",
            "[228 | 428.86] loss=2.43 avg=2.61\n",
            "[229 | 430.49] loss=2.31 avg=2.61\n",
            "[230 | 432.12] loss=2.57 avg=2.61\n",
            "[231 | 433.75] loss=2.82 avg=2.61\n",
            "[232 | 435.39] loss=2.44 avg=2.61\n",
            "[233 | 437.02] loss=2.09 avg=2.60\n",
            "[234 | 438.66] loss=2.63 avg=2.60\n",
            "[235 | 440.29] loss=2.48 avg=2.60\n",
            "[236 | 441.93] loss=2.79 avg=2.60\n",
            "[237 | 443.57] loss=2.73 avg=2.61\n",
            "[238 | 445.21] loss=2.38 avg=2.60\n",
            "[239 | 446.85] loss=2.68 avg=2.60\n",
            "[240 | 448.49] loss=3.05 avg=2.61\n",
            "[241 | 450.13] loss=2.74 avg=2.61\n",
            "[242 | 451.79] loss=2.59 avg=2.61\n",
            "[243 | 453.44] loss=2.46 avg=2.61\n",
            "[244 | 455.10] loss=2.57 avg=2.61\n",
            "[245 | 456.75] loss=1.92 avg=2.60\n",
            "[246 | 458.41] loss=2.54 avg=2.60\n",
            "[247 | 460.08] loss=2.57 avg=2.60\n",
            "[248 | 461.75] loss=2.34 avg=2.60\n",
            "[249 | 463.41] loss=2.46 avg=2.59\n",
            "[250 | 465.08] loss=2.83 avg=2.60\n",
            "[251 | 466.75] loss=2.54 avg=2.60\n",
            "[252 | 468.42] loss=2.36 avg=2.59\n",
            "[253 | 470.09] loss=2.72 avg=2.60\n",
            "[254 | 471.76] loss=2.49 avg=2.59\n",
            "[255 | 473.43] loss=2.37 avg=2.59\n",
            "[256 | 475.10] loss=2.27 avg=2.59\n",
            "[257 | 476.77] loss=2.41 avg=2.59\n",
            "[258 | 478.44] loss=2.65 avg=2.59\n",
            "[259 | 480.11] loss=2.64 avg=2.59\n",
            "[260 | 481.78] loss=1.67 avg=2.58\n",
            "[261 | 483.45] loss=2.52 avg=2.58\n",
            "[262 | 485.12] loss=2.56 avg=2.58\n",
            "[263 | 486.79] loss=2.70 avg=2.58\n",
            "[264 | 488.45] loss=2.54 avg=2.58\n",
            "[265 | 490.12] loss=2.59 avg=2.58\n",
            "[266 | 491.78] loss=2.30 avg=2.57\n",
            "[267 | 493.44] loss=2.90 avg=2.58\n",
            "[268 | 495.10] loss=2.66 avg=2.58\n",
            "[269 | 496.75] loss=2.63 avg=2.58\n",
            "[270 | 498.41] loss=2.57 avg=2.58\n",
            "[271 | 500.06] loss=1.89 avg=2.57\n",
            "[272 | 501.71] loss=2.29 avg=2.57\n",
            "[273 | 503.36] loss=2.12 avg=2.56\n",
            "[274 | 505.01] loss=2.35 avg=2.56\n",
            "[275 | 506.64] loss=2.69 avg=2.56\n",
            "[276 | 508.28] loss=2.29 avg=2.56\n",
            "[277 | 509.91] loss=2.21 avg=2.56\n",
            "[278 | 511.55] loss=2.51 avg=2.56\n",
            "[279 | 513.18] loss=2.67 avg=2.56\n",
            "[280 | 514.81] loss=2.88 avg=2.56\n",
            "[281 | 516.45] loss=2.83 avg=2.56\n",
            "[282 | 518.08] loss=2.15 avg=2.56\n",
            "[283 | 519.71] loss=2.91 avg=2.56\n",
            "[284 | 521.34] loss=2.49 avg=2.56\n",
            "[285 | 522.97] loss=2.65 avg=2.56\n",
            "[286 | 524.60] loss=2.46 avg=2.56\n",
            "[287 | 526.23] loss=2.55 avg=2.56\n",
            "[288 | 527.86] loss=1.58 avg=2.55\n",
            "[289 | 529.49] loss=1.45 avg=2.54\n",
            "[290 | 531.12] loss=2.55 avg=2.54\n",
            "[291 | 532.75] loss=3.28 avg=2.55\n",
            "[292 | 534.38] loss=2.47 avg=2.55\n",
            "[293 | 536.02] loss=2.39 avg=2.55\n",
            "[294 | 537.65] loss=2.94 avg=2.55\n",
            "[295 | 539.28] loss=2.70 avg=2.55\n",
            "[296 | 540.91] loss=1.39 avg=2.54\n",
            "[297 | 542.55] loss=2.76 avg=2.54\n",
            "[298 | 544.19] loss=2.83 avg=2.54\n",
            "[299 | 545.81] loss=2.77 avg=2.55\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            ". “This is not a great time for us to be,” Dr. Geller says.\n",
            "\n",
            "So she and her organization announced a plan back in April, ahead of National Pesticide Week, to hold workshops aimed at promoting health and environmental sustainability. The plan calls for taking a step back and looking at what “s working and not working about pesticides.” It looks at what works and what doesn’t. There will be workshops on organic foods, on recycling, on education, on protecting bees.\n",
            "\n",
            "But the organization also wants to do more. Its goal would be to figure out how we could develop more ways for people to participate in their communities to help make them more resilient to this year’s onslaught that has been dubbed “chemtrails.” It began with one scientist’s claim that the planes used by the United States government as cover for global surveillance are capable of dropping an estimated 100 tons of a toxin that can alter the brain and cause brain damage. On top of that, the toxins are released from these planes by the U.S. military on a regular basis.\n",
            "\n",
            "At an event in Washington this week, Dr. Mark Hertsgaard, who has published articles arguing that the use of these toxins are linked to mental illness, suggested that pesticides used in the United States could actually cause mental illness, because they “could kill people.”\n",
            "\n",
            "He wasn’t kidding. As we reported earlier this week—and confirmed by three university researchers whose data are in—American scientists who were not involved in that study also say they had little reason to doubt Hertsgaard’s assessment that the U.S. government was using chemical agents to attack civilians.\n",
            "\n",
            "One is the Harvard health economist Richard J. Friedman, who’d previously been a critic of the use of these toxins for military purposes but was also one of the more notable voices in the United Kingdom that questioned them. As we reported last year, Friedman told the Guardian that he believed that chemical weapon use was part of “a strategy to sow fear and misinformation in the minds of the population,” a tactic that, he said, did not “represent a war” but rather was a “game” that was played by “local governments,” such as those in Canada and Australia.\n",
            "\n",
            "Others, including the director of the Woods Hole Oceanographic Institution, Michael Mann, have also been deeply critical of the use of chemicals in these attacks. And as we reported last year, Mann’s recent book about climate change’s link to war and climate change came to a disturbing conclusion: “Pesticide use may be a contributing factor” to climate change. It was one of the many conclusions that emerged from a meeting of world climate scientists in Marrakesh, Morocco last month during which they discussed a growing body of evidence linking climate change to widespread use of chemicals. The key word’s “co-occurrence”: Many of those chemicals — including pesticides — were found in the atmosphere of the planet when man first arrived on Earth, so there are already enough of them to make them, says John B. Christy, a climate scientist at the University of Alabama at Huntsville and a former chief scientist for the United Nations.\n",
            "\n",
            "So what does the new group “want us to do?” asks Hertsgaard, “Do we want to get rid of all pesticides altogether or do we want to make it possible to do the same thing with other agricultural chemicals?”\n",
            "\n",
            "She and her group’ve already taken a step toward achieving that goal. In March, the Institute of Medicine, a nonpartisan body of experts that advises the U.S. government, issued a report that suggested a strong path to curbing use of pesticides. It urged the federal government to ban all pesticides on crops grown in the United States; it also recommended that the U.S. Environmental Protection Agency revise national pesticide regulations, including those relating to crop pesticides, which it said had been in effect since the early 1970s.\n",
            "\n",
            "A few days later, the U.S. government quietly announced a ban on its farms: The agency announced in a brief statement that it would ban the use of all insecticidal substances, including neonicotinoids, which are part of Monsanto’s popular corn and soybean crops.\n",
            "\n",
            "But even if pesticides are largely eliminated for food in the United States, there are still many other chemicals that crop-dwellers around the world rely on for their livelihoods, from insecticides to pesticides to fungicides to insect-resistant crops. The United Nations estimated that in 2015, pesticide use accounted for 13 percent of what it assessed as the global burden of disease, and in the United States use of insecticides and fungicides totaled more than 40 percent of what the U.N. reported as the global burden.\n",
            "\n",
            "So the organization’s\n",
            "\n",
            "[300 | 570.43] loss=1.73 avg=2.54\n",
            "[301 | 572.10] loss=2.52 avg=2.54\n",
            "[302 | 573.77] loss=2.73 avg=2.54\n",
            "[303 | 575.45] loss=2.58 avg=2.54\n",
            "[304 | 577.12] loss=2.98 avg=2.55\n",
            "[305 | 578.78] loss=2.57 avg=2.55\n",
            "[306 | 580.46] loss=2.52 avg=2.55\n",
            "[307 | 582.13] loss=2.63 avg=2.55\n",
            "[308 | 583.81] loss=2.34 avg=2.54\n",
            "[309 | 585.48] loss=2.66 avg=2.55\n",
            "[310 | 587.15] loss=2.76 avg=2.55\n",
            "[311 | 588.81] loss=2.59 avg=2.55\n",
            "[312 | 590.48] loss=2.64 avg=2.55\n",
            "[313 | 592.14] loss=2.30 avg=2.55\n",
            "[314 | 593.79] loss=2.71 avg=2.55\n",
            "[315 | 595.45] loss=1.95 avg=2.54\n",
            "[316 | 597.11] loss=2.48 avg=2.54\n",
            "[317 | 598.76] loss=2.55 avg=2.54\n",
            "[318 | 600.41] loss=1.27 avg=2.53\n",
            "[319 | 602.07] loss=2.62 avg=2.53\n",
            "[320 | 603.71] loss=2.88 avg=2.53\n",
            "[321 | 605.36] loss=2.61 avg=2.53\n",
            "[322 | 607.00] loss=2.55 avg=2.53\n",
            "[323 | 608.65] loss=2.26 avg=2.53\n",
            "[324 | 610.29] loss=2.46 avg=2.53\n",
            "[325 | 611.94] loss=2.09 avg=2.53\n",
            "[326 | 613.57] loss=2.48 avg=2.52\n",
            "[327 | 615.21] loss=2.50 avg=2.52\n",
            "[328 | 616.85] loss=2.41 avg=2.52\n",
            "[329 | 618.48] loss=1.40 avg=2.51\n",
            "[330 | 620.12] loss=2.72 avg=2.51\n",
            "[331 | 621.75] loss=2.52 avg=2.51\n",
            "[332 | 623.39] loss=2.23 avg=2.51\n",
            "[333 | 625.03] loss=2.63 avg=2.51\n",
            "[334 | 626.66] loss=2.71 avg=2.51\n",
            "[335 | 628.29] loss=1.11 avg=2.50\n",
            "[336 | 629.93] loss=2.77 avg=2.50\n",
            "[337 | 631.56] loss=2.57 avg=2.50\n",
            "[338 | 633.19] loss=2.35 avg=2.50\n",
            "[339 | 634.83] loss=2.63 avg=2.50\n",
            "[340 | 636.46] loss=2.59 avg=2.50\n",
            "[341 | 638.10] loss=2.59 avg=2.50\n",
            "[342 | 639.74] loss=2.83 avg=2.51\n",
            "[343 | 641.37] loss=2.93 avg=2.51\n",
            "[344 | 643.02] loss=2.32 avg=2.51\n",
            "[345 | 644.66] loss=2.32 avg=2.51\n",
            "[346 | 646.30] loss=2.25 avg=2.51\n",
            "[347 | 647.94] loss=2.32 avg=2.50\n",
            "[348 | 649.58] loss=0.94 avg=2.49\n",
            "[349 | 651.23] loss=2.59 avg=2.49\n",
            "[350 | 652.88] loss=1.89 avg=2.48\n",
            "[351 | 654.54] loss=2.85 avg=2.49\n",
            "[352 | 656.20] loss=2.57 avg=2.49\n",
            "[353 | 657.86] loss=2.33 avg=2.49\n",
            "[354 | 659.52] loss=2.60 avg=2.49\n",
            "[355 | 661.19] loss=2.93 avg=2.49\n",
            "[356 | 662.86] loss=2.21 avg=2.49\n",
            "[357 | 664.53] loss=2.35 avg=2.49\n",
            "[358 | 666.20] loss=2.85 avg=2.49\n",
            "[359 | 667.87] loss=2.33 avg=2.49\n",
            "[360 | 669.54] loss=2.73 avg=2.49\n",
            "[361 | 671.22] loss=2.52 avg=2.49\n",
            "[362 | 672.90] loss=2.64 avg=2.49\n",
            "[363 | 674.58] loss=1.09 avg=2.48\n",
            "[364 | 676.27] loss=2.18 avg=2.48\n",
            "[365 | 677.96] loss=2.91 avg=2.48\n",
            "[366 | 679.63] loss=2.77 avg=2.48\n",
            "[367 | 681.31] loss=2.81 avg=2.49\n",
            "[368 | 682.99] loss=2.75 avg=2.49\n",
            "[369 | 684.66] loss=2.65 avg=2.49\n",
            "[370 | 686.33] loss=1.23 avg=2.48\n",
            "[371 | 688.00] loss=2.59 avg=2.48\n",
            "[372 | 689.67] loss=2.92 avg=2.48\n",
            "[373 | 691.34] loss=2.41 avg=2.48\n",
            "[374 | 693.00] loss=2.27 avg=2.48\n",
            "[375 | 694.66] loss=2.35 avg=2.48\n",
            "[376 | 696.33] loss=2.39 avg=2.48\n",
            "[377 | 697.98] loss=2.51 avg=2.48\n",
            "[378 | 699.64] loss=2.72 avg=2.48\n",
            "[379 | 701.30] loss=2.32 avg=2.48\n",
            "[380 | 702.94] loss=2.76 avg=2.48\n",
            "[381 | 704.59] loss=2.85 avg=2.49\n",
            "[382 | 706.24] loss=2.72 avg=2.49\n",
            "[383 | 707.88] loss=2.51 avg=2.49\n",
            "[384 | 709.53] loss=2.95 avg=2.49\n",
            "[385 | 711.17] loss=2.46 avg=2.49\n",
            "[386 | 712.81] loss=2.41 avg=2.49\n",
            "[387 | 714.45] loss=2.33 avg=2.49\n",
            "[388 | 716.08] loss=0.75 avg=2.47\n",
            "[389 | 717.71] loss=2.51 avg=2.47\n",
            "[390 | 719.34] loss=2.55 avg=2.47\n",
            "[391 | 720.98] loss=2.76 avg=2.48\n",
            "[392 | 722.61] loss=2.43 avg=2.48\n",
            "[393 | 724.25] loss=1.71 avg=2.47\n",
            "[394 | 725.88] loss=2.82 avg=2.47\n",
            "[395 | 727.51] loss=2.61 avg=2.47\n",
            "[396 | 729.15] loss=2.28 avg=2.47\n",
            "[397 | 730.78] loss=2.46 avg=2.47\n",
            "[398 | 732.40] loss=2.36 avg=2.47\n",
            "[399 | 734.03] loss=2.40 avg=2.47\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " in their country. In its report, CRS analyzed the demographics of immigrants from a number of countries in Europe and North America. It found that more than half of the 7,000 or so new residents in those countries over the past decade were of European descent.\n",
            "\n",
            "Of the 1,400 new residents, nearly all are of Chinese origin. And that's without even considering the many others from the Muslim world, in whose countries there are significant numbers of migrants but no one knows what the future holds, especially when it comes to their children, spouses, or grandchildren.\n",
            "\n",
            "The American dream, as articulated by the poet and essayist Hannah Arendt, was founded on the idea that when one generation is free—one who has the potential and education to achieve its own success—another can and has already come along and achieved success in their own right. Indeed, as CRS's analysis shows, the overwhelming majority of immigrants into the United States since the 1970s have come from one or more of the world's most affluent nations: the United States, Canada, Australia, New Zealand, and South Africa.\n",
            "\n",
            "America has an enormous, complex, and multifaceted immigration system. What, as a country, should be considered the \"mainstreaming\" of immigrants and the entry of new people into a country can be anything from a temporary or seasonal admission of the already-wealthy, but also from an influx of refugees or refugees displaced due to war, but also from an immigration from a country with a low-income population and/or an influx of low-skilled workers, depending on the country.\n",
            "\n",
            "Some of these issues are so serious and complex that they aren't readily visible, let alone addressed; it's a matter of getting the policy right, and in both major political parties, immigrants are being turned away at a higher rate than they are coming. That's why the fact that so many recent immigrants, according to CRS, are of Chinese origin and not of other races, are the subject of such outrage is so disappointing.\n",
            "\n",
            "But the fact that we have such a huge and deeply ingrained, and often unconscious, aversion to foreign people making a meaningful contribution to American life shouldn't blind us to the fact that some of these newcomers, if not so easily converted to success, are the children of immigrants and their families.\n",
            "\n",
            "What is so frustrating about the CRS report is not that it was written—we have a way of doing such assessments better than the current system does it—but that Americans won't listen to us.\n",
            "\n",
            "It seems that at an annual meeting of the National Association of City and Metro Statistical Agencies, the CRS has come on board to advise federal agencies on how best to accommodate the growing numbers of immigrants and other new people to the United States. And it seems to have little interest in being relegated to the backburner, a position that is as important to the country as the nation is to itself.\n",
            "\n",
            "One of the most striking aspects of CRS's report, as noted in its abstract, is how many newcomers the organization is tracking are children of immigrants. This is a fact that is so incontrovertibly true, as it has often been shown in polling showing that immigrants and their children are in large part responsible for America's success.\n",
            "\n",
            "In fact, this phenomenon is so common that many of us, when we think of who immigrants are, tend to think of the parents, not the children themselves. And that is perhaps one of the most remarkable things about the new CRS reports. In what could be described as the country's version of the Truman Show, Americans are being shown to have a pretty accurate picture of who the immigrants are, with children of immigrants and other immigrants.\n",
            "\n",
            "But then, how do you explain the fact that this report is not being read by anyone? And how do you explain such a lack of interest in learning more about this particular aspect of America—the diversity of it?\n",
            "\n",
            "The CRS report is a report that Americans have so often avoided to study: What will happen if the next generation of immigrants—immigrants and their children—comes to the United States. It is the story of whether the country will adapt to their success in creating its own.\n",
            "\n",
            "The report, written by two of the country's leading labor economists—Richard Freeman, the president and CEO of CRS, and Robert Reich, a professor of public policy at the University of California, Merced, and a former Secretary of Labor—takes a hard look at many of the issues that the new American system seems to be predisposed to ignore, yet in which American life has become more and more difficult.\n",
            "\n",
            "There may be many aspects of American life that can get the country to succeed at its own end, such as improving workers' prospects, getting young people into school, and giving people the skills and education they need to do well in the labor market and in other fields of life. In those areas, many Americans—or at least\n",
            "\n",
            "[400 | 758.64] loss=2.32 avg=2.47\n",
            "[401 | 760.31] loss=2.58 avg=2.47\n",
            "[402 | 761.98] loss=2.79 avg=2.47\n",
            "[403 | 763.65] loss=2.66 avg=2.47\n",
            "[404 | 765.32] loss=2.59 avg=2.48\n",
            "[405 | 766.99] loss=2.86 avg=2.48\n",
            "[406 | 768.67] loss=2.29 avg=2.48\n",
            "[407 | 770.34] loss=2.54 avg=2.48\n",
            "[408 | 772.01] loss=2.67 avg=2.48\n",
            "[409 | 773.68] loss=2.88 avg=2.48\n",
            "[410 | 775.35] loss=2.22 avg=2.48\n",
            "[411 | 777.02] loss=2.30 avg=2.48\n",
            "[412 | 778.69] loss=2.66 avg=2.48\n",
            "[413 | 780.36] loss=2.47 avg=2.48\n",
            "[414 | 782.03] loss=2.49 avg=2.48\n",
            "[415 | 783.70] loss=2.53 avg=2.48\n",
            "[416 | 785.37] loss=2.55 avg=2.48\n",
            "[417 | 787.04] loss=2.17 avg=2.48\n",
            "[418 | 788.72] loss=2.76 avg=2.48\n",
            "[419 | 790.38] loss=2.31 avg=2.48\n",
            "[420 | 792.05] loss=2.61 avg=2.48\n",
            "[421 | 793.71] loss=2.65 avg=2.48\n",
            "[422 | 795.37] loss=2.12 avg=2.48\n",
            "[423 | 797.03] loss=2.70 avg=2.48\n",
            "[424 | 798.69] loss=2.58 avg=2.48\n",
            "[425 | 800.34] loss=2.81 avg=2.49\n",
            "[426 | 802.00] loss=2.52 avg=2.49\n",
            "[427 | 803.64] loss=2.36 avg=2.49\n",
            "[428 | 805.29] loss=2.61 avg=2.49\n",
            "[429 | 806.94] loss=2.57 avg=2.49\n",
            "[430 | 808.59] loss=2.74 avg=2.49\n",
            "[431 | 810.24] loss=2.52 avg=2.49\n",
            "[432 | 811.88] loss=1.79 avg=2.48\n",
            "[433 | 813.52] loss=2.07 avg=2.48\n",
            "[434 | 815.16] loss=2.75 avg=2.48\n",
            "[435 | 816.80] loss=2.59 avg=2.48\n",
            "[436 | 818.44] loss=2.74 avg=2.49\n",
            "[437 | 820.07] loss=2.52 avg=2.49\n",
            "[438 | 821.71] loss=2.34 avg=2.48\n",
            "[439 | 823.34] loss=2.11 avg=2.48\n",
            "[440 | 824.98] loss=2.55 avg=2.48\n",
            "[441 | 826.61] loss=2.72 avg=2.48\n",
            "[442 | 828.24] loss=2.55 avg=2.48\n",
            "[443 | 829.87] loss=2.22 avg=2.48\n",
            "[444 | 831.50] loss=2.54 avg=2.48\n",
            "[445 | 833.13] loss=2.55 avg=2.48\n",
            "[446 | 834.76] loss=2.37 avg=2.48\n",
            "[447 | 836.39] loss=2.29 avg=2.48\n",
            "[448 | 838.03] loss=2.59 avg=2.48\n",
            "[449 | 839.67] loss=2.58 avg=2.48\n",
            "[450 | 841.31] loss=2.35 avg=2.48\n",
            "[451 | 842.96] loss=2.64 avg=2.48\n",
            "[452 | 844.60] loss=2.47 avg=2.48\n",
            "[453 | 846.25] loss=2.58 avg=2.48\n",
            "[454 | 847.90] loss=2.71 avg=2.49\n",
            "[455 | 849.55] loss=2.41 avg=2.49\n",
            "[456 | 851.20] loss=2.40 avg=2.48\n",
            "[457 | 852.85] loss=2.53 avg=2.48\n",
            "[458 | 854.51] loss=2.60 avg=2.49\n",
            "[459 | 856.16] loss=2.10 avg=2.48\n",
            "[460 | 857.82] loss=2.27 avg=2.48\n",
            "[461 | 859.48] loss=2.67 avg=2.48\n",
            "[462 | 861.15] loss=2.93 avg=2.49\n",
            "[463 | 862.82] loss=2.59 avg=2.49\n",
            "[464 | 864.49] loss=2.47 avg=2.49\n",
            "[465 | 866.16] loss=2.54 avg=2.49\n",
            "[466 | 867.83] loss=2.07 avg=2.48\n",
            "[467 | 869.50] loss=2.77 avg=2.49\n",
            "[468 | 871.17] loss=2.56 avg=2.49\n",
            "[469 | 872.84] loss=2.04 avg=2.48\n",
            "[470 | 874.51] loss=2.75 avg=2.49\n",
            "[471 | 876.19] loss=2.85 avg=2.49\n",
            "[472 | 877.85] loss=2.60 avg=2.49\n",
            "[473 | 879.52] loss=2.21 avg=2.49\n",
            "[474 | 881.19] loss=2.49 avg=2.49\n",
            "[475 | 882.86] loss=2.73 avg=2.49\n",
            "[476 | 884.53] loss=2.59 avg=2.49\n",
            "[477 | 886.20] loss=2.70 avg=2.49\n",
            "[478 | 887.86] loss=2.86 avg=2.50\n",
            "[479 | 889.53] loss=2.46 avg=2.50\n",
            "[480 | 891.20] loss=2.21 avg=2.49\n",
            "[481 | 892.86] loss=2.36 avg=2.49\n",
            "[482 | 894.52] loss=1.33 avg=2.48\n",
            "[483 | 896.18] loss=2.67 avg=2.48\n",
            "[484 | 897.83] loss=2.38 avg=2.48\n",
            "[485 | 899.48] loss=2.77 avg=2.48\n",
            "[486 | 901.13] loss=2.35 avg=2.48\n",
            "[487 | 902.77] loss=2.60 avg=2.48\n",
            "[488 | 904.42] loss=2.23 avg=2.48\n",
            "[489 | 906.06] loss=2.83 avg=2.48\n",
            "[490 | 907.71] loss=2.54 avg=2.49\n",
            "[491 | 909.35] loss=2.54 avg=2.49\n",
            "[492 | 910.99] loss=2.50 avg=2.49\n",
            "[493 | 912.63] loss=2.14 avg=2.48\n",
            "[494 | 914.27] loss=0.66 avg=2.46\n",
            "[495 | 915.90] loss=2.36 avg=2.46\n",
            "[496 | 917.54] loss=2.37 avg=2.46\n",
            "[497 | 919.17] loss=3.20 avg=2.47\n",
            "[498 | 920.80] loss=2.42 avg=2.47\n",
            "[499 | 922.44] loss=2.43 avg=2.47\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " likely to be more likely to use public transport if they live near a job centre; for those within 10 minutes, it tends to be more likely.\n",
            "\n",
            "The link between commute times between places of employment and unemployment is, in fact, pretty solid. In a report published last year by the University of Michigan's Transportation Research Institute (TRI), for example, researchers looked at the data from the United States and found that commuting between cities is significantly correlated with the rate of unemployment and poverty, both of which have been linked to higher rates of employment and poverty. The researchers found that, in general, a commute of three to five minutes is associated with an increase of just under 0.08 percent of a person's mean annual income—about one dollar.\n",
            "\n",
            "When it comes to public transit in urban areas, the evidence is similar: It's linked to a drop in the rate of unemployment in the short term, but it is linked to the same short-term increase in earnings as commuting to jobs in the city. (And although commute times at work are linked to job loss, it's not clear how the commute affects the employment prospects or lack thereof of the people that do go to work there.)\n",
            "\n",
            "It's also important to remember that this study just looked at travel time. Travel times are affected by things like availability, weather, congestion, and many other factors, and not just commute times. More research is needed to see if there are a connection between the timing of people's commutes and their unemployment outcomes, but the fact is: Cities generally have higher numbers of people moving during their working hours; a higher commute time is likely a more direct form of discrimination if it's taken into account in someone's job search compared to, say, being offered a job with less commute time.\n",
            "\n",
            "The fact that commuting costs money—for some, in some places—is probably not a bad thing. A recent study found that one large and sprawling city with a high average commute time ranked in the top 10 for the most frequent commuters in the world; one study of more than 100 cities found that they are also high in frequent car commuters.\n",
            "\n",
            "Of course, as our cities become more dense, more people, particularly in the city centers, are likely to move in waves. The report by the TRI researchers found that commute times are more likely to change when the number of commuters doubles—a phenomenon known as transit demand perturbing.\n",
            "\n",
            "The TRI study offers an overview of how commutes work on the ground, but it doesn't provide an in-depth explanation for how commute times can influence whether people are on the move, and when people are on it. The researchers found that most people in the cities where they worked went from commuting to an employment in the city within 30 minutes of their commute; the number of people who reported moving 30 minutes or more was high. (They also looked at the effects of various commute times and job patterns, but only for the part of the country where the commute is not linked to the unemployment or poverty rates.)\n",
            "\n",
            "If our systems of transportation can only work in such a way—if they don't make the commuting part of the job search or the commute part of the search part of the job search part of the job search part—then that won't be a very good system for the long run. I know that sounds counter-intuitive, but what if a person who's not on the move is in a position where they're able to go to work? If they don't need the commute to do some more things, and they know they can still go to work in the city, it gives them incentive to find employment elsewhere. What do you see as a big step toward moving away from the reliance on the commute?\n",
            "\n",
            "One major challenge is that if people can go to all their work as soon as possible, they probably don't need to commute during their leisure hours; they may find it easier to take a public transit commute. More work on the transit side, and a willingness on the part of employers to set aside a certain time for getting to work, would also make commuting a much less common activity, at least to many people.\n",
            "\n",
            "And then there are the costs to the city of being part of the job search, as well as the effects on the commute itself. The analysis of the economic impacts of commuting times found that commuting time was positively correlated with the amount of poverty in a city. There are ways to reduce those effects by reducing congestion, as the authors found: For example, they argue that it pays to have more buses and bikes so people can actually get to work. For those who commute, the benefits of a good job are, in many cases, going to outweigh those costs.\n",
            "\n",
            "The idea that people could make do with a commute to work as long as it's not a significant drain on their budget is, unfortunately, a pretty typical approach in the US. Some cities, like Boston and San Francisco, make that less of an issue—and those cities\n",
            "\n",
            "[500 | 946.69] loss=2.61 avg=2.47\n",
            "[501 | 948.34] loss=2.35 avg=2.47\n",
            "[502 | 950.00] loss=1.77 avg=2.46\n",
            "[503 | 951.66] loss=2.46 avg=2.46\n",
            "[504 | 953.32] loss=2.41 avg=2.46\n",
            "[505 | 954.98] loss=2.63 avg=2.46\n",
            "[506 | 956.64] loss=2.11 avg=2.46\n",
            "[507 | 958.30] loss=2.60 avg=2.46\n",
            "[508 | 959.97] loss=2.55 avg=2.46\n",
            "[509 | 961.64] loss=2.34 avg=2.46\n",
            "[510 | 963.32] loss=0.95 avg=2.45\n",
            "[511 | 964.99] loss=2.53 avg=2.45\n",
            "[512 | 966.66] loss=2.61 avg=2.45\n",
            "[513 | 968.33] loss=2.41 avg=2.45\n",
            "[514 | 970.01] loss=2.94 avg=2.45\n",
            "[515 | 971.69] loss=2.39 avg=2.45\n",
            "[516 | 973.35] loss=2.53 avg=2.45\n",
            "[517 | 975.03] loss=2.49 avg=2.45\n",
            "[518 | 976.70] loss=2.12 avg=2.45\n",
            "[519 | 978.36] loss=2.50 avg=2.45\n",
            "[520 | 980.03] loss=2.29 avg=2.45\n",
            "[521 | 981.70] loss=2.49 avg=2.45\n",
            "[522 | 983.37] loss=2.36 avg=2.45\n",
            "[523 | 985.04] loss=2.45 avg=2.45\n",
            "[524 | 986.70] loss=2.47 avg=2.45\n",
            "[525 | 988.36] loss=2.67 avg=2.45\n",
            "[526 | 990.03] loss=2.51 avg=2.45\n",
            "[527 | 991.69] loss=2.23 avg=2.45\n",
            "[528 | 993.35] loss=2.72 avg=2.45\n",
            "[529 | 995.00] loss=2.28 avg=2.45\n",
            "[530 | 996.65] loss=2.35 avg=2.45\n",
            "[531 | 998.30] loss=2.25 avg=2.45\n",
            "[532 | 999.95] loss=2.75 avg=2.45\n",
            "[533 | 1001.60] loss=2.93 avg=2.45\n",
            "[534 | 1003.25] loss=2.64 avg=2.46\n",
            "[535 | 1004.89] loss=2.19 avg=2.45\n",
            "[536 | 1006.54] loss=2.61 avg=2.46\n",
            "[537 | 1008.18] loss=2.59 avg=2.46\n",
            "[538 | 1009.82] loss=2.55 avg=2.46\n",
            "[539 | 1011.46] loss=2.33 avg=2.46\n",
            "[540 | 1013.10] loss=2.25 avg=2.45\n",
            "[541 | 1014.74] loss=2.56 avg=2.46\n",
            "[542 | 1016.37] loss=2.44 avg=2.46\n",
            "[543 | 1018.00] loss=2.18 avg=2.45\n",
            "[544 | 1019.63] loss=2.85 avg=2.46\n",
            "[545 | 1021.27] loss=2.86 avg=2.46\n",
            "[546 | 1022.91] loss=2.29 avg=2.46\n",
            "[547 | 1024.54] loss=2.54 avg=2.46\n",
            "[548 | 1026.17] loss=2.66 avg=2.46\n",
            "[549 | 1027.81] loss=0.86 avg=2.45\n",
            "[550 | 1029.44] loss=2.54 avg=2.45\n",
            "[551 | 1031.08] loss=2.44 avg=2.45\n",
            "[552 | 1032.71] loss=2.46 avg=2.45\n",
            "[553 | 1034.35] loss=2.31 avg=2.45\n",
            "[554 | 1035.98] loss=2.79 avg=2.45\n",
            "[555 | 1037.62] loss=2.69 avg=2.45\n",
            "[556 | 1039.26] loss=2.73 avg=2.45\n",
            "[557 | 1040.90] loss=2.53 avg=2.45\n",
            "[558 | 1042.54] loss=2.34 avg=2.45\n",
            "[559 | 1044.18] loss=1.92 avg=2.45\n",
            "[560 | 1045.83] loss=1.47 avg=2.44\n",
            "[561 | 1047.47] loss=2.53 avg=2.44\n",
            "[562 | 1049.13] loss=2.79 avg=2.44\n",
            "[563 | 1050.78] loss=2.29 avg=2.44\n",
            "[564 | 1052.44] loss=2.40 avg=2.44\n",
            "[565 | 1054.09] loss=2.38 avg=2.44\n",
            "[566 | 1055.76] loss=2.35 avg=2.44\n",
            "[567 | 1057.42] loss=2.51 avg=2.44\n",
            "[568 | 1059.08] loss=2.39 avg=2.44\n",
            "[569 | 1060.75] loss=2.31 avg=2.44\n",
            "[570 | 1062.43] loss=2.38 avg=2.44\n",
            "[571 | 1064.10] loss=2.33 avg=2.44\n",
            "[572 | 1065.78] loss=2.72 avg=2.44\n",
            "[573 | 1067.45] loss=2.36 avg=2.44\n",
            "[574 | 1069.13] loss=2.07 avg=2.44\n",
            "[575 | 1070.81] loss=2.27 avg=2.43\n",
            "[576 | 1072.48] loss=2.52 avg=2.43\n",
            "[577 | 1074.18] loss=2.27 avg=2.43\n",
            "[578 | 1075.84] loss=2.42 avg=2.43\n",
            "[579 | 1077.52] loss=2.60 avg=2.43\n",
            "[580 | 1079.20] loss=2.27 avg=2.43\n",
            "[581 | 1080.88] loss=2.42 avg=2.43\n",
            "[582 | 1082.55] loss=2.69 avg=2.43\n",
            "[583 | 1084.22] loss=2.44 avg=2.43\n",
            "[584 | 1085.89] loss=2.69 avg=2.44\n",
            "[585 | 1087.55] loss=2.57 avg=2.44\n",
            "[586 | 1089.22] loss=2.38 avg=2.44\n",
            "[587 | 1090.89] loss=2.67 avg=2.44\n",
            "[588 | 1092.55] loss=2.00 avg=2.44\n",
            "[589 | 1094.21] loss=2.52 avg=2.44\n",
            "[590 | 1095.86] loss=2.36 avg=2.44\n",
            "[591 | 1097.52] loss=2.57 avg=2.44\n",
            "[592 | 1099.17] loss=2.41 avg=2.44\n",
            "[593 | 1100.84] loss=2.43 avg=2.44\n",
            "[594 | 1102.49] loss=2.23 avg=2.44\n",
            "[595 | 1104.13] loss=2.67 avg=2.44\n",
            "[596 | 1105.78] loss=2.59 avg=2.44\n",
            "[597 | 1107.42] loss=2.33 avg=2.44\n",
            "[598 | 1109.06] loss=2.67 avg=2.44\n",
            "[599 | 1110.70] loss=2.42 avg=2.44\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " against the current course of human development and are therefore better positioned to assess the long-term effects of social norms, and not merely to focus on the immediate effects that come from a particular incident of bullying.\n",
            "\n",
            "Ineffectiveness is an important concept for many schools. Schools that consistently fail to effectively teach boys, who may not be aware that the system allows for them to be bullied and denied academic success, are likely to end up with boys who are less likely to feel empathy and support for their peers and are more likely to engage with the teasing and bullying. A school will, as a consequence, be more likely to have less effective interventions to address school violence, more likely to have more students who engage in bullying, and so on. These school-violence-prevention strategies take time, and will be less effective if school officials and teachers don't know all the information they need to make a difference.\n",
            "\n",
            "There are a few reasons schools can't make a difference; it's a lot easier to tell a bully that his behavior or that of his classmates is unacceptable. But I think it goes much deeper than that. Many schools, for good reasons, have not taken the lead in taking meaningful leadership actions and interventions. The fact that students in schools with poor academic success — that is, those with very low test scores — don't always feel that they're going to have a good chance of success is one.\n",
            "\n",
            "One of the reasons that so many students are feeling discouraged about the school climate is because the school is not helping them. There is a deep disconnect on campus—a lot of students feel like they're on their own and that there are no others to help them. And because of all the social pressure they feel from peers to be the best student, a lot of them are not taking initiative, they're just in it for the money.\n",
            "\n",
            "The most common reason students don't feel like they have a chance to make an impact doesn't even feel like a college problem. The very fact that students feel like they are in the system to begin with means that there is an insistentness about being a college student about whether or not some colleges are going to take any action on their behalf to reduce their situation. We can all agree that we ought to be less competitive, and some schools have been much more innovative in this area than others. But there needs to be a concerted change of attitude, which is more about the need not to be a loser.\n",
            "\n",
            "\n",
            "\n",
            "While the term \"college-reform\" typically conjures up images of an ambitious student body, the reality is much grimmer than that. According to a recent New York Times profile of the nonprofit, whose mission is to improve higher education, fewer than a dozen colleges offer degrees in computer-science majors and less than one in 100 offer master's degrees in that area.\n",
            "\n",
            "On a national level, the college-reform movement is largely driven by a few influential college students and their faculty leaders, but in fact, that effort has been guided by a larger, nationwide group who are fighting a battle on many campuses—and who have a common enemy: the status quo of how college has always been planned.\n",
            "\n",
            "While efforts at reform are often led by a small group of students and faculty in their first years in the trenches, the reality is that the battle will be fought on many campuses across the country for years to come. In fact, the college-reform movement has already resulted in significant changes at two important, mid-tier schools that have served as a model and inspiration for the movement's broader efforts.\n",
            "\n",
            "These college students have been fighting for changes for years because the system they are entering into does not offer them much help in doing so.\n",
            "\n",
            "It is hard to think of a student in college today who hasn't been bullied at some point. In recent years, the problem has gotten more severe as students have entered college, even if it has not affected them as profoundly as it did at the end, say, at the time of college. The consequences of this can be devastating, with many students finding that their confidence, motivation, and self-respect have been severely diminished. And it has become increasingly difficult to graduate at a good-enough college—or graduate at all—if they haven't managed to obtain a degree.\n",
            "\n",
            "The new push at the University of California, Berkeley is a good example of the changing dynamics in what has been a highly competitive field. Students at the University of California at San Diego had been protesting the school, then known as the State University of New York, for the past three years. The students wanted to stop the hiring practices of the college's president, David Zaslav, and to demand that he change the university's name. The protests were successful and Zaslav, who had been elected president in 2015, walked away from office earlier this year with his job. The students, however, continue to remain engaged in an ongoing battle for university and college leadership on campus, and an increase in the\n",
            "\n",
            "[600 | 1135.07] loss=2.33 avg=2.44\n",
            "[601 | 1136.71] loss=2.49 avg=2.44\n",
            "[602 | 1138.36] loss=2.55 avg=2.44\n",
            "[603 | 1140.00] loss=2.29 avg=2.44\n",
            "[604 | 1141.64] loss=2.46 avg=2.44\n",
            "[605 | 1143.29] loss=2.46 avg=2.44\n",
            "[606 | 1144.94] loss=2.86 avg=2.44\n",
            "[607 | 1146.59] loss=2.31 avg=2.44\n",
            "[608 | 1148.25] loss=2.23 avg=2.44\n",
            "[609 | 1149.91] loss=1.91 avg=2.43\n",
            "[610 | 1151.57] loss=2.13 avg=2.43\n",
            "[611 | 1153.23] loss=2.57 avg=2.43\n",
            "[612 | 1154.90] loss=2.38 avg=2.43\n",
            "[613 | 1156.57] loss=2.80 avg=2.44\n",
            "[614 | 1158.25] loss=2.23 avg=2.43\n",
            "[615 | 1159.92] loss=2.60 avg=2.44\n",
            "[616 | 1161.59] loss=2.50 avg=2.44\n",
            "[617 | 1163.26] loss=2.38 avg=2.44\n",
            "[618 | 1164.94] loss=2.15 avg=2.43\n",
            "[619 | 1166.61] loss=0.64 avg=2.42\n",
            "[620 | 1168.28] loss=2.43 avg=2.42\n",
            "[621 | 1169.96] loss=2.42 avg=2.42\n",
            "[622 | 1171.64] loss=2.42 avg=2.42\n",
            "[623 | 1173.31] loss=2.70 avg=2.42\n",
            "[624 | 1174.98] loss=2.73 avg=2.42\n",
            "[625 | 1176.65] loss=2.40 avg=2.42\n",
            "[626 | 1178.32] loss=0.69 avg=2.40\n",
            "[627 | 1179.99] loss=2.88 avg=2.41\n",
            "[628 | 1181.66] loss=2.49 avg=2.41\n",
            "[629 | 1183.33] loss=2.48 avg=2.41\n",
            "[630 | 1184.99] loss=2.41 avg=2.41\n",
            "[631 | 1186.65] loss=2.23 avg=2.41\n",
            "[632 | 1188.32] loss=3.22 avg=2.42\n",
            "[633 | 1189.97] loss=2.50 avg=2.42\n",
            "[634 | 1191.63] loss=2.90 avg=2.42\n",
            "[635 | 1193.29] loss=0.71 avg=2.41\n",
            "[636 | 1194.94] loss=2.43 avg=2.41\n",
            "[637 | 1196.59] loss=2.55 avg=2.41\n",
            "[638 | 1198.24] loss=2.49 avg=2.41\n",
            "[639 | 1199.89] loss=1.16 avg=2.40\n",
            "[640 | 1201.53] loss=3.08 avg=2.40\n",
            "[641 | 1203.17] loss=2.40 avg=2.40\n",
            "[642 | 1204.81] loss=2.28 avg=2.40\n",
            "[643 | 1206.46] loss=2.21 avg=2.40\n",
            "[644 | 1208.10] loss=2.57 avg=2.40\n",
            "[645 | 1209.73] loss=2.50 avg=2.40\n",
            "[646 | 1211.37] loss=2.52 avg=2.40\n",
            "[647 | 1213.00] loss=2.73 avg=2.41\n",
            "[648 | 1214.64] loss=2.24 avg=2.40\n",
            "[649 | 1216.27] loss=2.51 avg=2.41\n",
            "[650 | 1217.91] loss=2.47 avg=2.41\n",
            "[651 | 1219.54] loss=2.65 avg=2.41\n",
            "[652 | 1221.18] loss=0.55 avg=2.39\n",
            "[653 | 1222.81] loss=2.21 avg=2.39\n",
            "[654 | 1224.44] loss=2.35 avg=2.39\n",
            "[655 | 1226.06] loss=0.42 avg=2.37\n",
            "[656 | 1227.69] loss=2.03 avg=2.36\n",
            "[657 | 1229.32] loss=0.66 avg=2.35\n",
            "[658 | 1230.96] loss=0.33 avg=2.33\n",
            "[659 | 1232.60] loss=2.59 avg=2.33\n",
            "[660 | 1234.23] loss=2.92 avg=2.34\n",
            "[661 | 1235.88] loss=2.45 avg=2.34\n",
            "[662 | 1237.51] loss=2.32 avg=2.34\n",
            "[663 | 1239.16] loss=2.36 avg=2.34\n",
            "[664 | 1240.80] loss=2.63 avg=2.34\n",
            "[665 | 1242.44] loss=2.39 avg=2.34\n",
            "[666 | 1244.09] loss=2.55 avg=2.34\n",
            "[667 | 1245.75] loss=2.51 avg=2.34\n",
            "[668 | 1247.40] loss=2.11 avg=2.34\n",
            "[669 | 1249.06] loss=2.25 avg=2.34\n",
            "[670 | 1250.71] loss=2.35 avg=2.34\n",
            "[671 | 1252.37] loss=2.09 avg=2.34\n",
            "[672 | 1254.04] loss=1.87 avg=2.33\n",
            "[673 | 1255.70] loss=2.50 avg=2.34\n",
            "[674 | 1257.38] loss=2.38 avg=2.34\n",
            "[675 | 1259.04] loss=2.14 avg=2.33\n",
            "[676 | 1260.71] loss=2.39 avg=2.33\n",
            "[677 | 1262.39] loss=0.25 avg=2.31\n",
            "[678 | 1264.06] loss=2.38 avg=2.31\n",
            "[679 | 1265.74] loss=2.99 avg=2.32\n",
            "[680 | 1267.41] loss=2.72 avg=2.33\n",
            "[681 | 1269.09] loss=2.33 avg=2.33\n",
            "[682 | 1270.76] loss=2.55 avg=2.33\n",
            "[683 | 1272.43] loss=2.56 avg=2.33\n",
            "[684 | 1274.10] loss=2.86 avg=2.34\n",
            "[685 | 1275.77] loss=2.33 avg=2.34\n",
            "[686 | 1277.44] loss=2.06 avg=2.33\n",
            "[687 | 1279.11] loss=1.98 avg=2.33\n",
            "[688 | 1280.78] loss=1.49 avg=2.32\n",
            "[689 | 1282.45] loss=2.22 avg=2.32\n",
            "[690 | 1284.11] loss=2.70 avg=2.32\n",
            "[691 | 1285.78] loss=0.20 avg=2.30\n",
            "[692 | 1287.44] loss=2.51 avg=2.30\n",
            "[693 | 1289.10] loss=2.57 avg=2.31\n",
            "[694 | 1290.76] loss=1.77 avg=2.30\n",
            "[695 | 1292.41] loss=1.98 avg=2.30\n",
            "[696 | 1294.06] loss=2.45 avg=2.30\n",
            "[697 | 1295.71] loss=2.21 avg=2.30\n",
            "[698 | 1297.36] loss=2.42 avg=2.30\n",
            "[699 | 1299.02] loss=2.17 avg=2.30\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " renters in Los Angeles.<|endoftext|>For the most part, the stories in this issue of TIME are about the people who mattered to the people who mattered to them. This year, the story is about the young black women, who will forever remain subject to the story, who, though still young, had the most remarkable and lasting impact on their communities. That was evident at Stuyvesant Charter High School in Brooklyn on Tuesday afternoon, when a group of six-year-olds met for breakfast as they waited in line at the city-run cafeteria. Their presence there reflected the importance of a diverse student body in Brooklyn: One child said she was one of the first people she knew when she was a kid to walk to school. Several years later, a student named Jasmine told her mother she was on her way to meet her teacher. She remembered her class just a few blocks away. The lunch came with the usual suspects—a biscuit, brownies, and water—but this week the focus of attention was on what the boys would do for lunch. Two of the boys, who were from the same class, asked one another questions of an exercise routine at school that Jasmine has been trying herself to master.\n",
            "\n",
            "Jasmine and two of her classmates, who are also part of the same neighborhood, walked in the hallway and noticed the older boy struggling with one of the exercises. \"When I noticed him struggling, he was like, 'Oh my god,' \" she said. The other boy, who she doesn't have to name, stopped and looked around, apparently for help. His brother told Jasmine he wasn't sure what to do. It didn't help that each person they observed was wearing black. One of them walked away. The other boy, who was in the hallway with him, stayed in his seat, which gave him an opening to look around. Finally, the girl took off her shoes and socks.\n",
            "\n",
            "\"I've always just been blessed that I have some sort of connection with children,\" Jasmine told me last year. \"Now I actually feel I can, when I go to a high school meetup, I can actually just talk to kids about an exercise they're doing and just make it fun for them. If I had to put in all the extra time, I'd be spending extra time with them. That's why I go.\"\n",
            "\n",
            "These days, at least, she often does. The boys in her ninth-grade class—a combination of white, black, and Hispanic boys—went to Stuyvesant for lunch almost every day of last fall. The day after a school day, one of the black boys walked in and asked, \"Where are all the white guys?\" His brownie-shaped friend said the hall was full of everyone. One night, Jasmine was at the meeting table watching kids eat when a white boy walked in and asked, \"What do white people eat?\" Jasmine said she was a vegetarian, and he was a meat eater.\n",
            "\n",
            "This weekend, the students in Jasmine and the others' class participated in a school walk to protest the recent police shootings in Minneapolis and Charleston. Their walk was the start of the school season's first day of marching. Last fall, students walked four days for free.\n",
            "\n",
            "\"I just wanted people to know that it was safe,\" Jasmine told me. \"I didn't come to school to be shot or killed; I came here to eat and go to sleep and be happy.\" In March, she was walking her daughter across the playground when a black boy pulled behind her and pointed a handgun at her. \"I was like, 'Mommy, please get off me,'\" she said. The boy ran with her, and she tripped and fell, and she fell on her daughter. She said she didn't know she got up, and that she ran only because her mother, who was in the back of the school, yelled for her to run.\n",
            "\n",
            "She came to. The boy had been shot. He had stopped running, she told her mother. She ran back to her seat, and then she stood in the hallway with the others. The girl didn't get up. What she did was walk into the hall and find the teacher, who was standing by a locked desk. That led to the school, where she found the man who had shot her. (If you recall, she says the man told her about himself.) When she arrived in the nurse's room, she said she felt pain but didn't know why—there was someone else.\n",
            "\n",
            "When Jasmine arrived at a doctor's appointment the next day, she was told that a bullet had entered her lung. The bullet had traveled to her brain, and she had a brain injury that caused her brain to heal.\n",
            "\n",
            "\"What I didn't realize was the time I had on me,\" Jasmine said. \"I thought yesterday it wouldn't go that far. My first day here, after the walk, I felt a lot\n",
            "\n",
            "[700 | 1323.52] loss=2.50 avg=2.30\n",
            "[701 | 1325.16] loss=2.29 avg=2.30\n",
            "[702 | 1326.80] loss=0.26 avg=2.28\n",
            "[703 | 1328.43] loss=2.57 avg=2.28\n",
            "[704 | 1330.08] loss=2.48 avg=2.28\n",
            "[705 | 1331.72] loss=2.37 avg=2.29\n",
            "[706 | 1333.36] loss=2.11 avg=2.28\n",
            "[707 | 1335.01] loss=2.31 avg=2.28\n",
            "[708 | 1336.65] loss=2.59 avg=2.29\n",
            "[709 | 1338.29] loss=0.18 avg=2.27\n",
            "[710 | 1339.93] loss=2.36 avg=2.27\n",
            "[711 | 1341.58] loss=2.59 avg=2.27\n",
            "[712 | 1343.23] loss=2.13 avg=2.27\n",
            "[713 | 1344.88] loss=2.33 avg=2.27\n",
            "[714 | 1346.53] loss=2.66 avg=2.27\n",
            "[715 | 1348.18] loss=2.56 avg=2.28\n",
            "[716 | 1349.84] loss=2.40 avg=2.28\n",
            "[717 | 1351.50] loss=2.41 avg=2.28\n",
            "[718 | 1353.16] loss=2.38 avg=2.28\n",
            "[719 | 1354.82] loss=2.13 avg=2.28\n",
            "[720 | 1356.49] loss=2.27 avg=2.28\n",
            "[721 | 1358.16] loss=2.47 avg=2.28\n",
            "[722 | 1359.83] loss=3.03 avg=2.29\n",
            "[723 | 1361.51] loss=2.05 avg=2.29\n",
            "[724 | 1363.18] loss=2.44 avg=2.29\n",
            "[725 | 1364.85] loss=2.63 avg=2.29\n",
            "[726 | 1366.52] loss=2.40 avg=2.29\n",
            "[727 | 1368.20] loss=2.58 avg=2.29\n",
            "[728 | 1369.87] loss=2.20 avg=2.29\n",
            "[729 | 1371.54] loss=2.69 avg=2.30\n",
            "[730 | 1373.21] loss=2.48 avg=2.30\n",
            "[731 | 1374.89] loss=2.45 avg=2.30\n",
            "[732 | 1376.56] loss=0.58 avg=2.28\n",
            "[733 | 1378.23] loss=2.91 avg=2.29\n",
            "[734 | 1379.90] loss=2.91 avg=2.30\n",
            "[735 | 1381.57] loss=2.20 avg=2.30\n",
            "[736 | 1383.24] loss=2.54 avg=2.30\n",
            "[737 | 1384.91] loss=2.80 avg=2.30\n",
            "[738 | 1386.58] loss=3.19 avg=2.31\n",
            "[739 | 1388.24] loss=1.89 avg=2.31\n",
            "[740 | 1389.90] loss=3.08 avg=2.31\n",
            "[741 | 1391.56] loss=2.27 avg=2.31\n",
            "[742 | 1393.21] loss=2.33 avg=2.31\n",
            "[743 | 1394.87] loss=2.30 avg=2.31\n",
            "[744 | 1396.51] loss=2.61 avg=2.32\n",
            "[745 | 1398.16] loss=2.13 avg=2.32\n",
            "[746 | 1399.81] loss=1.77 avg=2.31\n",
            "[747 | 1401.46] loss=2.90 avg=2.32\n",
            "[748 | 1403.10] loss=2.20 avg=2.31\n",
            "[749 | 1404.74] loss=2.43 avg=2.32\n",
            "[750 | 1406.39] loss=2.74 avg=2.32\n",
            "[751 | 1408.03] loss=0.56 avg=2.30\n",
            "[752 | 1409.66] loss=2.63 avg=2.31\n",
            "[753 | 1411.30] loss=2.23 avg=2.31\n",
            "[754 | 1412.93] loss=2.01 avg=2.30\n",
            "[755 | 1414.57] loss=2.21 avg=2.30\n",
            "[756 | 1416.20] loss=2.65 avg=2.30\n",
            "[757 | 1417.84] loss=2.21 avg=2.30\n",
            "[758 | 1419.47] loss=1.86 avg=2.30\n",
            "[759 | 1421.11] loss=2.10 avg=2.30\n",
            "[760 | 1422.74] loss=2.57 avg=2.30\n",
            "[761 | 1424.38] loss=2.28 avg=2.30\n",
            "[762 | 1426.01] loss=2.46 avg=2.30\n",
            "[763 | 1427.65] loss=2.42 avg=2.30\n",
            "[764 | 1429.29] loss=2.49 avg=2.30\n",
            "[765 | 1430.92] loss=2.59 avg=2.31\n",
            "[766 | 1432.56] loss=2.43 avg=2.31\n",
            "[767 | 1434.19] loss=2.41 avg=2.31\n",
            "[768 | 1435.82] loss=2.65 avg=2.31\n",
            "[769 | 1437.47] loss=2.91 avg=2.32\n",
            "[770 | 1439.10] loss=2.35 avg=2.32\n",
            "[771 | 1440.75] loss=1.85 avg=2.31\n",
            "[772 | 1442.39] loss=2.16 avg=2.31\n",
            "[773 | 1444.04] loss=2.55 avg=2.32\n",
            "[774 | 1445.69] loss=2.13 avg=2.31\n",
            "[775 | 1447.33] loss=2.48 avg=2.32\n",
            "[776 | 1449.00] loss=2.58 avg=2.32\n",
            "[777 | 1450.66] loss=2.31 avg=2.32\n",
            "[778 | 1452.32] loss=2.06 avg=2.32\n",
            "[779 | 1453.99] loss=2.67 avg=2.32\n",
            "[780 | 1455.66] loss=2.37 avg=2.32\n",
            "[781 | 1457.33] loss=2.50 avg=2.32\n",
            "[782 | 1459.00] loss=2.62 avg=2.32\n",
            "[783 | 1460.67] loss=2.18 avg=2.32\n",
            "[784 | 1462.35] loss=2.68 avg=2.33\n",
            "[785 | 1464.02] loss=2.56 avg=2.33\n",
            "[786 | 1465.70] loss=2.21 avg=2.33\n",
            "[787 | 1467.37] loss=2.82 avg=2.33\n",
            "[788 | 1469.04] loss=0.18 avg=2.31\n",
            "[789 | 1470.71] loss=2.42 avg=2.31\n",
            "[790 | 1472.38] loss=2.26 avg=2.31\n",
            "[791 | 1474.06] loss=2.32 avg=2.31\n",
            "[792 | 1475.73] loss=2.60 avg=2.31\n",
            "[793 | 1477.40] loss=2.11 avg=2.31\n",
            "[794 | 1479.07] loss=2.55 avg=2.31\n",
            "[795 | 1480.74] loss=2.25 avg=2.31\n",
            "[796 | 1482.40] loss=2.45 avg=2.32\n",
            "[797 | 1484.07] loss=2.21 avg=2.31\n",
            "[798 | 1485.73] loss=1.92 avg=2.31\n",
            "[799 | 1487.40] loss=2.40 avg=2.31\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " students who had taken three semesters at the University of Minnesota, or some other private or public college. I could find no published research indicating that this kind of reporting or reporting helps to inform public understanding of campus life, let alone student-athlete well-being. This seems wrong, and the University of Minnesota could do some soul-searching.\n",
            "\n",
            "The new findings suggest that not only is there a negative correlation between bad behavior on campus and what's on the playing field, there is another, much stronger one. This comes from the fact that the people who are more likely to commit crimes are the worst athletes on a campus. This link between having bad behaviors and high-school records reflects the fact that the people who are not the worst in the world are going to be in an environment that is going to bring out the worst in them, and that this is a risk-taking environment in which to get away with it.\n",
            "\n",
            "This is something that the University can address, but it's going to take someone with a better grasp of the subject matter. The University of Minnesota is not in the business of reporting what students do on the playing field to journalists outside of the Twin Cities area; rather, it's reporting what students do inside its building—from classrooms to dormitories and from hallways to hallways.\n",
            "\n",
            "If the University of Minnesota needs to do this, it must have more open-records training. This is a fundamental obligation for schools, but colleges, even big-city colleges, owe it to students to be a little more open-minded about the kind of behavior that takes place on campus.\n",
            "\n",
            "Correction: An earlier version of this story misstated the year the researchers say high-school seniors were playing at Minnesota–Duluth. The Minnesota–Duluth tournament was in 2006.<|endoftext|>The best time to learn Japanese is during your university career. In this article we will explore why.\n",
            "\n",
            "Learning Japanese during your University Career\n",
            "\n",
            "Most of the Japanese universities offer some degree or degree type courses, but what about other types of learning? Some are more likely to have an introductory or advanced course than they are a traditional one as part of your higher education degree.\n",
            "\n",
            "Here is a summary of the most popular courses, for students wanting to learn more that might come after a certain degree:\n",
            "\n",
            "Level 1: Vocational Training\n",
            "\n",
            "For students with limited English proficiency these are not only some of the most popular online courses, but they are the most popular online programs. There are dozens to choose from; with courses that have been around for years. Students must enrol into a particular course to be active in the program, so you should find a course in the first language.\n",
            "\n",
            "Here are some of the best options for students seeking to become language learners:\n",
            "\n",
            "Language Schools\n",
            "\n",
            "The reason this is a popular choice is not because it's a standard course in a formal university but because it's free. There are courses such as this available online, by the hour or at the time of enrolment, without needing a tuition fee.\n",
            "\n",
            "Vocational training courses may also benefit students who are studying for specific professional qualifications, such as a university degree or doctorate. Students often use language to prepare for a job, at which point they can transition into working in one or another workplace. This also gives students the option to use English professionally.\n",
            "\n",
            "For some people a free, online-only approach to language learning is a sensible option. However, for others it can be a waste of your time. If there is a choice then the best option is probably to continue studying, but with the course being completely free. This means most people can take this and be fully engaged from there, without any extra fees.\n",
            "\n",
            "Asking the right questions\n",
            "\n",
            "Japanese are really good at asking a lot of different kinds of questions.\n",
            "\n",
            "There are some questions that are pretty straightforward to answer, and there are others that are a bit more difficult. There are also some questions that may not really require you to answer directly.\n",
            "\n",
            "When you can get used to asking different kinds of questions, especially as you get to know the Japanese language, your chance to make a genuine connection with the people around you is greatly enhanced.\n",
            "\n",
            "There are more ways to think about people than just asking them a single basic question. Ask people to consider an angle and then ask them to tell you what they thought about it. There are different ways to ask different kinds of questions and that may get you into trouble with a teacher. But ask them to consider a particular angle.\n",
            "\n",
            "You may be surprised at the number of ways you can ask questions\n",
            "\n",
            "The most common way is asking a single question. This is not always feasible because many people are more accustomed to reading rather than hearing.\n",
            "\n",
            "Another way, where such a question is not possible, is by asking in a structured manner. Most English-speaking students will tend to know better than to just ask a single question. And they will also have better\n",
            "\n",
            "[800 | 1511.75] loss=2.28 avg=2.31\n",
            "[801 | 1513.38] loss=2.16 avg=2.31\n",
            "[802 | 1515.02] loss=2.91 avg=2.32\n",
            "[803 | 1516.66] loss=1.74 avg=2.31\n",
            "[804 | 1518.29] loss=2.55 avg=2.31\n",
            "[805 | 1519.92] loss=2.22 avg=2.31\n",
            "[806 | 1521.56] loss=2.30 avg=2.31\n",
            "[807 | 1523.19] loss=2.75 avg=2.32\n",
            "[808 | 1524.83] loss=2.13 avg=2.31\n",
            "[809 | 1526.47] loss=2.41 avg=2.31\n",
            "[810 | 1528.11] loss=2.39 avg=2.32\n",
            "[811 | 1529.75] loss=2.48 avg=2.32\n",
            "[812 | 1531.39] loss=2.26 avg=2.32\n",
            "[813 | 1533.02] loss=2.51 avg=2.32\n",
            "[814 | 1534.66] loss=2.57 avg=2.32\n",
            "[815 | 1536.30] loss=2.30 avg=2.32\n",
            "[816 | 1537.94] loss=0.21 avg=2.30\n",
            "[817 | 1539.58] loss=2.37 avg=2.30\n",
            "[818 | 1541.23] loss=2.58 avg=2.30\n",
            "[819 | 1542.88] loss=2.45 avg=2.30\n",
            "[820 | 1544.53] loss=2.47 avg=2.31\n",
            "[821 | 1546.18] loss=2.40 avg=2.31\n",
            "[822 | 1547.84] loss=2.22 avg=2.31\n",
            "[823 | 1549.50] loss=2.65 avg=2.31\n",
            "[824 | 1551.16] loss=2.20 avg=2.31\n",
            "[825 | 1552.84] loss=2.75 avg=2.31\n",
            "[826 | 1554.51] loss=2.07 avg=2.31\n",
            "[827 | 1556.18] loss=2.09 avg=2.31\n",
            "[828 | 1557.85] loss=2.69 avg=2.31\n",
            "[829 | 1559.52] loss=1.56 avg=2.30\n",
            "[830 | 1561.19] loss=2.45 avg=2.31\n",
            "[831 | 1562.86] loss=0.67 avg=2.29\n",
            "[832 | 1564.53] loss=2.75 avg=2.29\n",
            "[833 | 1566.20] loss=2.38 avg=2.30\n",
            "[834 | 1567.88] loss=2.79 avg=2.30\n",
            "[835 | 1569.56] loss=2.37 avg=2.30\n",
            "[836 | 1571.23] loss=2.75 avg=2.31\n",
            "[837 | 1572.90] loss=2.84 avg=2.31\n",
            "[838 | 1574.58] loss=2.09 avg=2.31\n",
            "[839 | 1576.25] loss=2.13 avg=2.31\n",
            "[840 | 1577.91] loss=2.42 avg=2.31\n",
            "[841 | 1579.58] loss=2.09 avg=2.31\n",
            "[842 | 1581.25] loss=1.91 avg=2.30\n",
            "[843 | 1582.92] loss=1.96 avg=2.30\n",
            "[844 | 1584.58] loss=2.42 avg=2.30\n",
            "[845 | 1586.24] loss=2.25 avg=2.30\n",
            "[846 | 1587.90] loss=2.39 avg=2.30\n",
            "[847 | 1589.55] loss=2.63 avg=2.30\n",
            "[848 | 1591.21] loss=0.18 avg=2.28\n",
            "[849 | 1592.86] loss=2.10 avg=2.28\n",
            "[850 | 1594.51] loss=2.79 avg=2.29\n",
            "[851 | 1596.17] loss=1.22 avg=2.27\n",
            "[852 | 1597.81] loss=2.35 avg=2.28\n",
            "[853 | 1599.46] loss=2.70 avg=2.28\n",
            "[854 | 1601.11] loss=2.56 avg=2.28\n",
            "[855 | 1602.74] loss=2.46 avg=2.28\n",
            "[856 | 1604.39] loss=2.30 avg=2.28\n",
            "[857 | 1606.02] loss=2.75 avg=2.29\n",
            "[858 | 1607.66] loss=2.54 avg=2.29\n",
            "[859 | 1609.29] loss=2.44 avg=2.29\n",
            "[860 | 1610.92] loss=2.46 avg=2.29\n",
            "[861 | 1612.57] loss=0.34 avg=2.28\n",
            "[862 | 1614.20] loss=2.18 avg=2.27\n",
            "[863 | 1615.84] loss=2.72 avg=2.28\n",
            "[864 | 1617.47] loss=2.98 avg=2.29\n",
            "[865 | 1619.10] loss=1.96 avg=2.28\n",
            "[866 | 1620.73] loss=2.49 avg=2.28\n",
            "[867 | 1622.37] loss=2.18 avg=2.28\n",
            "[868 | 1624.00] loss=2.39 avg=2.28\n",
            "[869 | 1625.63] loss=1.65 avg=2.28\n",
            "[870 | 1627.26] loss=2.42 avg=2.28\n",
            "[871 | 1628.90] loss=2.20 avg=2.28\n",
            "[872 | 1630.54] loss=2.23 avg=2.28\n",
            "[873 | 1632.17] loss=2.75 avg=2.28\n",
            "[874 | 1633.81] loss=2.24 avg=2.28\n",
            "[875 | 1635.45] loss=2.54 avg=2.28\n",
            "[876 | 1637.09] loss=2.60 avg=2.29\n",
            "[877 | 1638.74] loss=2.67 avg=2.29\n",
            "[878 | 1640.38] loss=2.33 avg=2.29\n",
            "[879 | 1642.03] loss=2.66 avg=2.30\n",
            "[880 | 1643.69] loss=2.28 avg=2.30\n",
            "[881 | 1645.34] loss=2.12 avg=2.29\n",
            "[882 | 1647.00] loss=2.08 avg=2.29\n",
            "[883 | 1648.66] loss=2.18 avg=2.29\n",
            "[884 | 1650.32] loss=2.51 avg=2.29\n",
            "[885 | 1651.99] loss=2.36 avg=2.29\n",
            "[886 | 1653.67] loss=2.29 avg=2.29\n",
            "[887 | 1655.34] loss=1.91 avg=2.29\n",
            "[888 | 1657.02] loss=2.17 avg=2.29\n",
            "[889 | 1658.69] loss=2.27 avg=2.29\n",
            "[890 | 1660.36] loss=2.01 avg=2.29\n",
            "[891 | 1662.04] loss=1.94 avg=2.28\n",
            "[892 | 1663.71] loss=2.03 avg=2.28\n",
            "[893 | 1665.41] loss=2.27 avg=2.28\n",
            "[894 | 1667.10] loss=2.57 avg=2.28\n",
            "[895 | 1668.78] loss=2.48 avg=2.28\n",
            "[896 | 1670.47] loss=2.47 avg=2.29\n",
            "[897 | 1672.14] loss=2.51 avg=2.29\n",
            "[898 | 1673.84] loss=2.12 avg=2.29\n",
            "[899 | 1675.51] loss=2.26 avg=2.29\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "�月督、呢? 人水上呢,一位在护运的事态,有着思想而已经于私有别呢?“是从一旪特交个来觉得的一个程度,管理也是这样陷达于人水的轓查合告诉呢。 The two people had arrived, and that was that. 「看是未来的呢。就是有些研究的态度呢。这样一边吧,比如何可是你不知道会被那样」 The one in silver had vanished. 听到了。 “是圣王国。当然,最近只便先生有可能会是只示题没有相关的。 I couldn’t help but feel somewhat sad. In the end, the person who killed that woman was, in fact, the person I was supposed to protect, and the one who really hurt me was the one who I wanted to protect. But that didn’t tell me how he came to do such a thing. There were others, as well, who tried to do great things for me, but they were also tortured and killed by those who were supposed to protect me. It was a vicious cycle. I had to get rid of it before I was destroyed. 在你是不妙,有一人考头他们能蜜訂,是最次理能包括的一个人类。尽好了一小孩子,就不同来警蔡的存在只有好去。 However, perhaps I would have been stronger had they not been there, had they not been there. In that situation, maybe they didn’t know what they’d become. It would have been like having somebody else in your life to take care of you. 但是,不怎么只有不知道的最大。求是有来事情朎外着的东西贝德,但是在我没有效果的话,但是您来若是因此,但就只是若是除去的书子。 This time, it’s like this all over again. 但就是说知道下,这种无法担快的事情吗? 他的若是这样的? 一步了吧? 不、不太这么说出手物呢? 不完全来说不觉得没有喜欢来了基础非常质的话,有着在只有很大的非常吧?这样的族我们的话,但抱歉有什么了涅�,但有一切一切公萬的可能思想吧? 但是你们想认为了态度的花常吗? This is what it looks like when society puts too much value on one’s bravery, doesn’t reward it appropriately, fails to recognize it, and treats it as an afterthought. The second woman? Why? 不过着在正是矐首的话与吧,但是有什么解决要一旪讨论的�\n",
            "\n",
            "[900 | 1700.01] loss=2.05 avg=2.28\n",
            "[901 | 1701.67] loss=2.41 avg=2.29\n",
            "[902 | 1703.32] loss=2.56 avg=2.29\n",
            "[903 | 1704.96] loss=2.28 avg=2.29\n",
            "[904 | 1706.60] loss=2.80 avg=2.29\n",
            "[905 | 1708.23] loss=2.32 avg=2.29\n",
            "[906 | 1709.87] loss=2.27 avg=2.29\n",
            "[907 | 1711.51] loss=3.02 avg=2.30\n",
            "[908 | 1713.14] loss=1.91 avg=2.30\n",
            "[909 | 1714.78] loss=2.22 avg=2.30\n",
            "[910 | 1716.41] loss=2.43 avg=2.30\n",
            "[911 | 1718.05] loss=2.19 avg=2.30\n",
            "[912 | 1719.69] loss=2.22 avg=2.30\n",
            "[913 | 1721.32] loss=2.56 avg=2.30\n",
            "[914 | 1722.96] loss=2.37 avg=2.30\n",
            "[915 | 1724.59] loss=1.98 avg=2.30\n",
            "[916 | 1726.23] loss=2.66 avg=2.30\n",
            "[917 | 1727.86] loss=2.30 avg=2.30\n",
            "[918 | 1729.50] loss=2.12 avg=2.30\n",
            "[919 | 1731.14] loss=2.55 avg=2.30\n",
            "[920 | 1732.78] loss=2.40 avg=2.30\n",
            "[921 | 1734.42] loss=2.56 avg=2.30\n",
            "[922 | 1736.06] loss=2.06 avg=2.30\n",
            "[923 | 1737.70] loss=2.15 avg=2.30\n",
            "[924 | 1739.35] loss=2.28 avg=2.30\n",
            "[925 | 1741.00] loss=2.36 avg=2.30\n",
            "[926 | 1742.65] loss=2.37 avg=2.30\n",
            "[927 | 1744.30] loss=2.44 avg=2.30\n",
            "[928 | 1745.96] loss=2.75 avg=2.31\n",
            "[929 | 1747.62] loss=2.56 avg=2.31\n",
            "[930 | 1749.28] loss=2.07 avg=2.31\n",
            "[931 | 1750.95] loss=2.17 avg=2.31\n",
            "[932 | 1752.62] loss=2.74 avg=2.31\n",
            "[933 | 1754.30] loss=2.80 avg=2.31\n",
            "[934 | 1755.97] loss=2.32 avg=2.31\n",
            "[935 | 1757.65] loss=2.53 avg=2.32\n",
            "[936 | 1759.33] loss=2.40 avg=2.32\n",
            "[937 | 1761.01] loss=0.14 avg=2.30\n",
            "[938 | 1762.68] loss=2.36 avg=2.30\n",
            "[939 | 1764.37] loss=2.50 avg=2.30\n",
            "[940 | 1766.05] loss=1.94 avg=2.30\n",
            "[941 | 1767.74] loss=2.54 avg=2.30\n",
            "[942 | 1769.41] loss=1.98 avg=2.29\n",
            "[943 | 1771.10] loss=2.02 avg=2.29\n",
            "[944 | 1772.79] loss=2.44 avg=2.29\n",
            "[945 | 1774.46] loss=2.30 avg=2.29\n",
            "[946 | 1776.13] loss=2.34 avg=2.29\n",
            "[947 | 1777.80] loss=2.33 avg=2.29\n",
            "[948 | 1779.47] loss=2.56 avg=2.30\n",
            "[949 | 1781.14] loss=1.21 avg=2.29\n",
            "[950 | 1782.81] loss=1.94 avg=2.28\n",
            "[951 | 1784.47] loss=2.48 avg=2.28\n",
            "[952 | 1786.13] loss=2.12 avg=2.28\n",
            "[953 | 1787.79] loss=2.14 avg=2.28\n",
            "[954 | 1789.45] loss=2.51 avg=2.28\n",
            "[955 | 1791.11] loss=1.83 avg=2.28\n",
            "[956 | 1792.77] loss=2.29 avg=2.28\n",
            "[957 | 1794.42] loss=2.23 avg=2.28\n",
            "[958 | 1796.07] loss=2.91 avg=2.28\n",
            "[959 | 1797.72] loss=2.19 avg=2.28\n",
            "[960 | 1799.36] loss=2.24 avg=2.28\n",
            "[961 | 1801.00] loss=2.32 avg=2.28\n",
            "[962 | 1802.64] loss=2.19 avg=2.28\n",
            "[963 | 1804.27] loss=2.66 avg=2.29\n",
            "[964 | 1805.90] loss=1.94 avg=2.28\n",
            "[965 | 1807.54] loss=1.88 avg=2.28\n",
            "[966 | 1809.18] loss=2.07 avg=2.28\n",
            "[967 | 1810.82] loss=2.74 avg=2.28\n",
            "[968 | 1812.45] loss=2.10 avg=2.28\n",
            "[969 | 1814.09] loss=2.24 avg=2.28\n",
            "[970 | 1815.73] loss=2.60 avg=2.28\n",
            "[971 | 1817.36] loss=2.55 avg=2.29\n",
            "[972 | 1819.00] loss=2.03 avg=2.28\n",
            "[973 | 1820.63] loss=2.62 avg=2.29\n",
            "[974 | 1822.27] loss=2.28 avg=2.29\n",
            "[975 | 1823.90] loss=2.05 avg=2.28\n",
            "[976 | 1825.54] loss=2.27 avg=2.28\n",
            "[977 | 1827.18] loss=2.35 avg=2.28\n",
            "[978 | 1828.82] loss=2.33 avg=2.28\n",
            "[979 | 1830.45] loss=2.55 avg=2.29\n",
            "[980 | 1832.09] loss=2.63 avg=2.29\n",
            "[981 | 1833.73] loss=2.10 avg=2.29\n",
            "[982 | 1835.38] loss=2.46 avg=2.29\n",
            "[983 | 1837.02] loss=2.24 avg=2.29\n",
            "[984 | 1838.66] loss=2.05 avg=2.29\n",
            "[985 | 1840.29] loss=2.62 avg=2.29\n",
            "[986 | 1841.94] loss=2.56 avg=2.29\n",
            "[987 | 1843.59] loss=1.81 avg=2.29\n",
            "[988 | 1845.24] loss=2.53 avg=2.29\n",
            "[989 | 1846.90] loss=2.75 avg=2.30\n",
            "[990 | 1848.55] loss=2.48 avg=2.30\n",
            "[991 | 1850.21] loss=2.15 avg=2.30\n",
            "[992 | 1851.88] loss=2.30 avg=2.30\n",
            "[993 | 1853.55] loss=2.22 avg=2.30\n",
            "[994 | 1855.22] loss=2.25 avg=2.30\n",
            "[995 | 1856.89] loss=2.01 avg=2.29\n",
            "[996 | 1858.56] loss=2.55 avg=2.29\n",
            "[997 | 1860.24] loss=2.17 avg=2.29\n",
            "[998 | 1861.91] loss=2.30 avg=2.29\n",
            "[999 | 1863.59] loss=2.15 avg=2.29\n",
            "Ran 1010 steps.\n",
            "Saving checkpoint/atlantic_education_345/model-1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WoTB9Vwl8Uk4",
        "colab_type": "code",
        "outputId": "ed389dc7-0386-4675-81ce-3418773ac28d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "## politics essays training - with 345\n",
        "!PYTHONPATH=src ./train.py --dataset /content/all_politics.txt --run_name 'atlantic_politics_345' --model_name '345M'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0627 17:54:28.311069 140594242262912 deprecation_wrapper.py:119] From /content/gpt-2/src/model.py:147: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
            "\n",
            "W0627 17:54:28.319618 140594242262912 deprecation_wrapper.py:119] From /content/gpt-2/src/memory_saving_gradients.py:13: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n",
            "W0627 17:54:28.410823 140594242262912 deprecation_wrapper.py:119] From ./train.py:88: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "W0627 17:54:28.411276 140594242262912 deprecation_wrapper.py:119] From ./train.py:91: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "2019-06-27 17:54:28.417679: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-06-27 17:54:28.417938: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1889100 executing computations on platform Host. Devices:\n",
            "2019-06-27 17:54:28.417969: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2019-06-27 17:54:28.420345: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\n",
            "2019-06-27 17:54:28.573454: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-27 17:54:28.573986: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1888840 executing computations on platform CUDA. Devices:\n",
            "2019-06-27 17:54:28.574017: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2019-06-27 17:54:28.574280: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-27 17:54:28.574661: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-06-27 17:54:28.575093: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-06-27 17:54:28.576379: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-06-27 17:54:28.577664: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-06-27 17:54:28.578030: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-06-27 17:54:28.579471: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-06-27 17:54:28.580643: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-06-27 17:54:28.583751: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-06-27 17:54:28.583889: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-27 17:54:28.584291: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-27 17:54:28.584637: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
            "2019-06-27 17:54:28.584697: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-06-27 17:54:28.585675: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-06-27 17:54:28.585700: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n",
            "2019-06-27 17:54:28.585710: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n",
            "2019-06-27 17:54:28.586008: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-27 17:54:28.586442: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-27 17:54:28.586817: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14202 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "W0627 17:54:28.587689 140594242262912 deprecation_wrapper.py:119] From ./train.py:92: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0627 17:54:39.485678 140594242262912 deprecation.py:323] From /content/gpt-2/src/sample.py:65: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "W0627 17:54:39.501984 140594242262912 deprecation.py:323] From /content/gpt-2/src/sample.py:16: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0627 17:54:39.503950 140594242262912 deprecation.py:323] From /content/gpt-2/src/sample.py:70: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.random.categorical` instead.\n",
            "W0627 17:54:39.515552 140594242262912 deprecation_wrapper.py:119] From ./train.py:121: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "W0627 17:54:54.778484 140594242262912 deprecation_wrapper.py:119] From ./train.py:144: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n",
            "W0627 17:54:54.781499 140594242262912 deprecation_wrapper.py:119] From ./train.py:147: The name tf.summary.merge is deprecated. Please use tf.compat.v1.summary.merge instead.\n",
            "\n",
            "W0627 17:54:54.782386 140594242262912 deprecation_wrapper.py:119] From ./train.py:149: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
            "\n",
            "W0627 17:54:54.783254 140594242262912 deprecation_wrapper.py:119] From ./train.py:152: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "Loading checkpoint models/345M/model.ckpt\n",
            "W0627 17:55:08.025435 140594242262912 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "Loading dataset...\n",
            "100% 1/1 [00:03<00:00,  3.28s/it]\n",
            "dataset has 445264 tokens\n",
            "Training...\n",
            "2019-06-27 17:55:25.897080: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n",
            "2019-06-27 17:55:26.759175: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "[1 | 14.86] loss=2.65 avg=2.65\n",
            "[2 | 16.38] loss=2.71 avg=2.68\n",
            "[3 | 17.92] loss=2.78 avg=2.71\n",
            "[4 | 19.46] loss=2.63 avg=2.69\n",
            "[5 | 21.00] loss=2.80 avg=2.71\n",
            "[6 | 22.55] loss=2.54 avg=2.68\n",
            "[7 | 24.09] loss=2.46 avg=2.65\n",
            "[8 | 25.65] loss=3.15 avg=2.71\n",
            "[9 | 27.20] loss=2.70 avg=2.71\n",
            "[10 | 28.75] loss=3.06 avg=2.75\n",
            "[11 | 30.31] loss=2.95 avg=2.77\n",
            "[12 | 31.87] loss=2.72 avg=2.76\n",
            "[13 | 33.43] loss=3.04 avg=2.79\n",
            "[14 | 34.99] loss=3.18 avg=2.82\n",
            "[15 | 36.55] loss=3.11 avg=2.84\n",
            "[16 | 38.12] loss=2.58 avg=2.82\n",
            "[17 | 39.69] loss=2.70 avg=2.81\n",
            "[18 | 41.26] loss=2.84 avg=2.81\n",
            "[19 | 42.83] loss=2.40 avg=2.79\n",
            "[20 | 44.39] loss=2.92 avg=2.80\n",
            "[21 | 45.97] loss=2.69 avg=2.79\n",
            "[22 | 47.55] loss=2.59 avg=2.78\n",
            "[23 | 49.12] loss=2.65 avg=2.77\n",
            "[24 | 50.71] loss=2.81 avg=2.78\n",
            "[25 | 52.28] loss=2.62 avg=2.77\n",
            "[26 | 53.85] loss=2.48 avg=2.76\n",
            "[27 | 55.43] loss=2.52 avg=2.75\n",
            "[28 | 57.00] loss=2.74 avg=2.75\n",
            "[29 | 58.57] loss=2.76 avg=2.75\n",
            "[30 | 60.15] loss=2.67 avg=2.74\n",
            "[31 | 61.73] loss=2.52 avg=2.74\n",
            "[32 | 63.30] loss=2.84 avg=2.74\n",
            "[33 | 64.87] loss=2.51 avg=2.73\n",
            "[34 | 66.44] loss=2.82 avg=2.73\n",
            "[35 | 68.01] loss=2.77 avg=2.74\n",
            "[36 | 69.58] loss=2.59 avg=2.73\n",
            "[37 | 71.16] loss=2.64 avg=2.73\n",
            "[38 | 72.73] loss=2.87 avg=2.73\n",
            "[39 | 74.31] loss=2.27 avg=2.72\n",
            "[40 | 75.89] loss=2.77 avg=2.72\n",
            "[41 | 77.46] loss=2.83 avg=2.72\n",
            "[42 | 79.04] loss=2.77 avg=2.72\n",
            "[43 | 80.61] loss=2.55 avg=2.72\n",
            "[44 | 82.19] loss=2.77 avg=2.72\n",
            "[45 | 83.77] loss=2.35 avg=2.71\n",
            "[46 | 85.35] loss=2.83 avg=2.71\n",
            "[47 | 86.93] loss=2.64 avg=2.71\n",
            "[48 | 88.52] loss=2.80 avg=2.71\n",
            "[49 | 90.10] loss=2.72 avg=2.71\n",
            "[50 | 91.68] loss=2.75 avg=2.72\n",
            "[51 | 93.28] loss=2.56 avg=2.71\n",
            "[52 | 94.86] loss=2.51 avg=2.71\n",
            "[53 | 96.45] loss=2.45 avg=2.70\n",
            "[54 | 98.04] loss=2.60 avg=2.70\n",
            "[55 | 99.62] loss=2.93 avg=2.70\n",
            "[56 | 101.21] loss=2.79 avg=2.71\n",
            "[57 | 102.80] loss=2.70 avg=2.71\n",
            "[58 | 104.39] loss=2.81 avg=2.71\n",
            "[59 | 105.99] loss=2.83 avg=2.71\n",
            "[60 | 107.58] loss=2.64 avg=2.71\n",
            "[61 | 109.18] loss=2.07 avg=2.69\n",
            "[62 | 110.77] loss=2.48 avg=2.69\n",
            "[63 | 112.37] loss=2.45 avg=2.68\n",
            "[64 | 113.96] loss=2.23 avg=2.68\n",
            "[65 | 115.58] loss=2.80 avg=2.68\n",
            "[66 | 117.18] loss=2.93 avg=2.68\n",
            "[67 | 118.78] loss=2.42 avg=2.68\n",
            "[68 | 120.39] loss=2.74 avg=2.68\n",
            "[69 | 122.00] loss=2.52 avg=2.68\n",
            "[70 | 123.61] loss=2.51 avg=2.67\n",
            "[71 | 125.23] loss=2.74 avg=2.67\n",
            "[72 | 126.85] loss=2.55 avg=2.67\n",
            "[73 | 128.47] loss=2.91 avg=2.68\n",
            "[74 | 130.09] loss=2.55 avg=2.67\n",
            "[75 | 131.71] loss=2.43 avg=2.67\n",
            "[76 | 133.34] loss=2.69 avg=2.67\n",
            "[77 | 134.96] loss=2.55 avg=2.67\n",
            "[78 | 136.59] loss=2.43 avg=2.66\n",
            "[79 | 138.21] loss=2.82 avg=2.67\n",
            "[80 | 139.83] loss=2.87 avg=2.67\n",
            "[81 | 141.45] loss=2.54 avg=2.67\n",
            "[82 | 143.08] loss=2.42 avg=2.66\n",
            "[83 | 144.70] loss=2.63 avg=2.66\n",
            "[84 | 146.33] loss=2.30 avg=2.66\n",
            "[85 | 147.95] loss=2.58 avg=2.65\n",
            "[86 | 149.57] loss=2.53 avg=2.65\n",
            "[87 | 151.19] loss=2.68 avg=2.65\n",
            "[88 | 152.82] loss=2.55 avg=2.65\n",
            "[89 | 154.44] loss=2.74 avg=2.65\n",
            "[90 | 156.05] loss=2.27 avg=2.65\n",
            "[91 | 157.66] loss=2.69 avg=2.65\n",
            "[92 | 159.27] loss=2.58 avg=2.65\n",
            "[93 | 160.88] loss=2.56 avg=2.64\n",
            "[94 | 162.50] loss=2.49 avg=2.64\n",
            "[95 | 164.11] loss=2.52 avg=2.64\n",
            "[96 | 165.71] loss=2.54 avg=2.64\n",
            "[97 | 167.32] loss=2.75 avg=2.64\n",
            "[98 | 168.93] loss=2.63 avg=2.64\n",
            "[99 | 170.54] loss=2.84 avg=2.64\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "‿\n",
            "\n",
            "‹You mean for the next few months?‡\n",
            "\n",
            " ‹Actually it's four weeks!”\n",
            "\n",
            "‹That may or may not happen in the future,‡ Trump said, leaning forward in his chair and speaking to himself. “I'm not saying, you know, when you're president, when you're not president—that’s the answer.” Trump added, drawing a long breath and waving it around.\n",
            "\n",
            "[′We're talking about whether an executive order will make a difference now,” said an aide after the discussion. “And that depends on what’s going on in the legal system.]\n",
            "\n",
            "For its first two-week stretch, Trump has been an unusually disciplined president—the latest in a long line of presidents who have spent his early hours, or his days, by the side of his family. In fact, the one exception is the recent appointment of the first ever black president to the United States Supreme Court: Joe Biden.\n",
            "\n",
            "To understand why, we must take a step back and briefly review the past ten years:\n",
            "\n",
            "In 1990, Bill Clinton, as president-elect, began laying the groundwork for impeachment proceedings against Richard Nixon and Attorney General Elliot Richardson. Nixon was forced to resign in disgrace, but the House of Representatives voted to impeach him. Clinton chose not to try it, choosing instead to resign rather than follow through on the possibility of impeachment. However, impeachment proceedings would become a hallmark of his administration, as Bill Clinton would often go on television to lay out his legal arguments for impeachment. [In the next century, however, it is more often the case that politicians and civil servants are more willing to take the high road. The Supreme Court has, for example, twice overturned the conviction of former President Obama and former First Lady Michelle Obama. The Trump administration could end up in for a similar fate. In 2000, the Supreme Court unanimously overturned the conviction of a former Vice President William Jefferson Clinton after he was accused of misusing his office, and a majority of its members decided to uphold his conviction. (Trump isn’t currently seeking to overturn President Lincoln’s impeachment verdict; a different judge upheld it this year.) In 2016, the Supreme Court reinstated President Obama’s conviction of lying to Congress, ruling that the Constitution’s promise of due process did not include his constitutional authority to make executive policy about law enforcement matters, including the treatment of minorities. That same year, Trump and Vice President Mike Pence, who would be both first sons and a former governor, signed an executive order to reverse a decision by a federal judge to let Trump ban travel from seven countries. After Trump’s inaugural address, during which he boasted about his ability to ′make America great again,” the president ′fired” the first executive action from his first year in office” and ′made” his son ′go’ to jail.” When the Senate voted on legislation to repeal and replace ObamaCare, Trump’s former chief of staff, Sean Spicer, had to step down after a leaked audio surfaced that appeared to show him calling for the murder of Supreme Court Justice Neil Gorsuch.[i]\n",
            "\n",
            "In 1993, New York City Mayor Rudy Giuliani was sworn in. At a news conference, Giuliani declared, “This is the first time any city president has been an independent citizen and held office.” The year after, Bill Clinton signed the Personal Responsibility and Work Opportunity Reconciliation Act, which required paid family leave and paid sick days, expanded the child tax credit, and expanded federal benefits for the poor and disabled.[i]\n",
            "\n",
            "On December 6, 2003, a suicide bomber detonated himself at a Baghdad airport, killing nearly 300 people and wounding well over 1,000 more. The following day, President George W. Bush announced the withdrawal of American troops from Iraq.\n",
            "\n",
            "′I’m very glad to be able to say that today, more than ever before, a year from now, a new chapter in history will be written, a new chapter in the history of the entire United States.” Bush said to The Sunday Times. He’d already been making the case that Obama’s withdrawal had been the single dumbest move in U.S. foreign policy since Eisenhower withdrew troops in '46 and the Soviet Union withdrew troops in '49.\n",
            "\n",
            "′This is the fifth Iraq war; every time, we have not been successful, and we have taken a country that was on the cusp of becoming the greatest country in the world, and the next time we’re there, we’ll make it look like it was the worst decision ever by any administration to withdraw their troops.” †\n",
            "\n",
            "After the withdrawal of American forces in September 2013, Iraq’s government began to recover, but the country’s oil infrastructure fell apart, prompting the United Nations to\n",
            "\n",
            "[100 | 198.23] loss=2.36 avg=2.64\n",
            "[101 | 199.85] loss=2.71 avg=2.64\n",
            "[102 | 201.47] loss=2.45 avg=2.64\n",
            "[103 | 203.09] loss=2.51 avg=2.64\n",
            "[104 | 204.71] loss=2.20 avg=2.63\n",
            "[105 | 206.34] loss=2.48 avg=2.63\n",
            "[106 | 207.96] loss=2.44 avg=2.62\n",
            "[107 | 209.59] loss=2.77 avg=2.63\n",
            "[108 | 211.22] loss=2.45 avg=2.62\n",
            "[109 | 212.85] loss=2.69 avg=2.62\n",
            "[110 | 214.49] loss=2.58 avg=2.62\n",
            "[111 | 216.12] loss=2.78 avg=2.63\n",
            "[112 | 217.76] loss=2.58 avg=2.63\n",
            "[113 | 219.41] loss=2.32 avg=2.62\n",
            "[114 | 221.05] loss=2.60 avg=2.62\n",
            "[115 | 222.71] loss=2.70 avg=2.62\n",
            "[116 | 224.36] loss=2.44 avg=2.62\n",
            "[117 | 226.01] loss=2.97 avg=2.62\n",
            "[118 | 227.67] loss=2.71 avg=2.63\n",
            "[119 | 229.33] loss=2.60 avg=2.62\n",
            "[120 | 230.98] loss=2.50 avg=2.62\n",
            "[121 | 232.65] loss=2.29 avg=2.62\n",
            "[122 | 234.31] loss=2.43 avg=2.62\n",
            "[123 | 235.97] loss=2.65 avg=2.62\n",
            "[124 | 237.63] loss=2.50 avg=2.61\n",
            "[125 | 239.29] loss=2.56 avg=2.61\n",
            "[126 | 240.94] loss=2.66 avg=2.61\n",
            "[127 | 242.60] loss=2.19 avg=2.61\n",
            "[128 | 244.24] loss=2.41 avg=2.61\n",
            "[129 | 245.89] loss=2.61 avg=2.61\n",
            "[130 | 247.54] loss=2.63 avg=2.61\n",
            "[131 | 249.18] loss=2.44 avg=2.60\n",
            "[132 | 250.83] loss=2.51 avg=2.60\n",
            "[133 | 252.47] loss=2.65 avg=2.60\n",
            "[134 | 254.11] loss=2.56 avg=2.60\n",
            "[135 | 255.75] loss=2.57 avg=2.60\n",
            "[136 | 257.39] loss=2.82 avg=2.61\n",
            "[137 | 259.02] loss=2.25 avg=2.60\n",
            "[138 | 260.66] loss=2.50 avg=2.60\n",
            "[139 | 262.28] loss=2.42 avg=2.60\n",
            "[140 | 263.91] loss=2.66 avg=2.60\n",
            "[141 | 265.54] loss=2.61 avg=2.60\n",
            "[142 | 267.17] loss=2.62 avg=2.60\n",
            "[143 | 268.80] loss=2.69 avg=2.60\n",
            "[144 | 270.43] loss=2.57 avg=2.60\n",
            "[145 | 272.05] loss=2.23 avg=2.59\n",
            "[146 | 273.68] loss=2.83 avg=2.60\n",
            "[147 | 275.31] loss=2.67 avg=2.60\n",
            "[148 | 276.93] loss=2.69 avg=2.60\n",
            "[149 | 278.56] loss=2.74 avg=2.60\n",
            "[150 | 280.19] loss=2.65 avg=2.60\n",
            "[151 | 281.82] loss=2.42 avg=2.60\n",
            "[152 | 283.44] loss=2.84 avg=2.60\n",
            "[153 | 285.07] loss=2.32 avg=2.60\n",
            "[154 | 286.69] loss=2.47 avg=2.60\n",
            "[155 | 288.32] loss=2.16 avg=2.59\n",
            "[156 | 289.95] loss=2.65 avg=2.59\n",
            "[157 | 291.58] loss=2.45 avg=2.59\n",
            "[158 | 293.21] loss=2.61 avg=2.59\n",
            "[159 | 294.83] loss=2.68 avg=2.59\n",
            "[160 | 296.47] loss=2.52 avg=2.59\n",
            "[161 | 298.10] loss=2.70 avg=2.59\n",
            "[162 | 299.72] loss=2.79 avg=2.60\n",
            "[163 | 301.35] loss=2.43 avg=2.59\n",
            "[164 | 302.99] loss=2.62 avg=2.59\n",
            "[165 | 304.62] loss=2.54 avg=2.59\n",
            "[166 | 306.25] loss=2.39 avg=2.59\n",
            "[167 | 307.87] loss=2.12 avg=2.58\n",
            "[168 | 309.51] loss=2.48 avg=2.58\n",
            "[169 | 311.16] loss=2.32 avg=2.58\n",
            "[170 | 312.80] loss=2.64 avg=2.58\n",
            "[171 | 314.45] loss=2.68 avg=2.58\n",
            "[172 | 316.10] loss=2.10 avg=2.58\n",
            "[173 | 317.75] loss=2.57 avg=2.58\n",
            "[174 | 319.41] loss=2.23 avg=2.57\n",
            "[175 | 321.07] loss=2.31 avg=2.57\n",
            "[176 | 322.73] loss=2.18 avg=2.56\n",
            "[177 | 324.40] loss=2.55 avg=2.56\n",
            "[178 | 326.07] loss=2.81 avg=2.57\n",
            "[179 | 327.73] loss=2.20 avg=2.56\n",
            "[180 | 329.40] loss=2.57 avg=2.56\n",
            "[181 | 331.07] loss=2.28 avg=2.56\n",
            "[182 | 332.74] loss=2.64 avg=2.56\n",
            "[183 | 334.41] loss=2.70 avg=2.56\n",
            "[184 | 336.08] loss=2.31 avg=2.56\n",
            "[185 | 337.75] loss=2.48 avg=2.56\n",
            "[186 | 339.42] loss=2.39 avg=2.56\n",
            "[187 | 341.09] loss=2.71 avg=2.56\n",
            "[188 | 342.76] loss=2.53 avg=2.56\n",
            "[189 | 344.43] loss=2.82 avg=2.56\n",
            "[190 | 346.09] loss=2.44 avg=2.56\n",
            "[191 | 347.76] loss=2.50 avg=2.56\n",
            "[192 | 349.41] loss=2.40 avg=2.56\n",
            "[193 | 351.07] loss=2.37 avg=2.55\n",
            "[194 | 352.72] loss=2.59 avg=2.55\n",
            "[195 | 354.38] loss=2.81 avg=2.56\n",
            "[196 | 356.03] loss=2.36 avg=2.56\n",
            "[197 | 357.68] loss=2.43 avg=2.55\n",
            "[198 | 359.33] loss=2.72 avg=2.56\n",
            "[199 | 360.97] loss=2.43 avg=2.55\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " to be.\n",
            "\n",
            "\"It seems to me that if I want to be in public service, I have to be willing to do that, to give up something and do what you're paid to do,\" he said. \"And it seems to me that if you were just going to be a state senator, that would be a good career path for me.\"\n",
            "\n",
            "The Democrats on the state Senate panel were also quick to criticize Rubio.\n",
            "\n",
            "Juan Hernandez, a member of the Senate Democratic Caucus, went after the former speaker for a second straight day, arguing that Rubio would be better able to do the job given that Florida is currently on life-and-death, and the state of his future career as governor, not to mention whether or not he could convince a Republican governor to pick him.\n",
            "\n",
            "Rubio, who was elected in 2014, responded by making the same argument he made repeatedly, arguing that he has the support of his former colleagues of the Senate Caucus, pointing to the fact that they are all still working on their legislation to help those who have no health insurance in the state – a state that includes a large chunk of the country where Medicaid is limited.\n",
            "\n",
            "Rubio said he is confident that he could convince the governor he can work together with him to help those with no insurance, but stressed that no matter the governor is willing to work with him, he will have to be part of a plan.\n",
            "\n",
            "\"There's a path to getting those numbers to where I can convince every governor in the country to be a part of your bill,\" Rubio said. \"When you have a governor who has not even gotten a single vote from a Republican governor who actually has been in public life, you have to be willing to make a compromise. You have to negotiate and reach a compromise.\"\n",
            "\n",
            "The governor also weighed in on the Democratic-controlled state Senate hearing earlier this month when it was revealed that he had requested $10 million from Medicaid as part of his budget this fall. He has said he will not request money to fund the state health care program because \"the best way to improve the lives of Florida children and adults is through health care.\" But a spokeswoman for the governor said earlier this week that he had sought more than $15 million in new funding if possible.\n",
            "\n",
            "One of the panel Democrats, Barbara Hall, also took it upon herself to denounce Rubio, saying that if he can't find the $50 million to improve Florida for the children who have no insurance, he also cannot find the $50 million to fix the state's health care program.\n",
            "\n",
            "It doesn't appear as though there will be a Republican state senator with an insurance policy in the 2018 midterm elections, even though Florida Governor Rick Scott, and his former health care and Medicaid adviser, are all Republicans.<|endoftext|>This morning I got a message from a reader saying \"What an amazing job you do with the color of your eyes\". You can read on.\n",
            "\n",
            "A lot has been said about racism in the media and on social media in recent weeks. There were some good tweets to make this blog post. But I wanted to take a minute and explain a few things to a friend.\n",
            "\n",
            "There are a number of reasons we don't talk about race in the mainstream media but not so many good reasons and the ones I'm going to talk about today are very, very good reasons.\n",
            "\n",
            "1) The word \"racist\" has been around since the nineteenth century, especially when it comes to the civil rights movement, especially in the US. We've been talking about race and social injustice for an awfully long time. Most of the attention was focused on \"whiteness\", but in a broader sense is racism a thing?\n",
            "\n",
            "2) The word \"racist\" has a very negative connotation. It implies the act of racism as inherently evil and, with good reason, it has this negative connotation. As a person who was a bit of a racist during my time in the service, I have often found myself having to explain to my bosses why I was behaving in this way. Often I find myself not only walking the walk but seeming to be doing it as well—I had to remind my bosses that they should not assume that I had an insatiable ambition and that I would do them a favor for what they would consider a small price.\n",
            "\n",
            "3) The word \"racist\" is a lot to bear, especially when you are trying to explain in plain English to someone who is reading this blog. I don't mean to be dramatic, but it makes me really uncomfortable. Why am I giving this person a word that makes him feel uncomfortable?\n",
            "\n",
            "I know that this blog has a really different point of view than the other ones. But I'm also not trying to defend people's right to disagree with me on one thing or another. It's pretty clear to me that we need to make a better society if we're going to have equal opportunity and, in particular, a world that will allow everyone to achieve their full potential\n",
            "\n",
            "[200 | 385.40] loss=2.51 avg=2.55\n",
            "[201 | 387.04] loss=2.91 avg=2.56\n",
            "[202 | 388.68] loss=2.21 avg=2.55\n",
            "[203 | 390.31] loss=2.70 avg=2.56\n",
            "[204 | 391.94] loss=2.20 avg=2.55\n",
            "[205 | 393.57] loss=2.63 avg=2.55\n",
            "[206 | 395.21] loss=2.21 avg=2.55\n",
            "[207 | 396.84] loss=2.59 avg=2.55\n",
            "[208 | 398.48] loss=2.32 avg=2.55\n",
            "[209 | 400.12] loss=2.72 avg=2.55\n",
            "[210 | 401.76] loss=2.82 avg=2.55\n",
            "[211 | 403.40] loss=2.57 avg=2.55\n",
            "[212 | 405.04] loss=2.39 avg=2.55\n",
            "[213 | 406.68] loss=2.78 avg=2.55\n",
            "[214 | 408.33] loss=2.68 avg=2.55\n",
            "[215 | 409.99] loss=2.37 avg=2.55\n",
            "[216 | 411.64] loss=2.50 avg=2.55\n",
            "[217 | 413.29] loss=2.74 avg=2.55\n",
            "[218 | 414.95] loss=2.63 avg=2.55\n",
            "[219 | 416.61] loss=2.28 avg=2.55\n",
            "[220 | 418.28] loss=2.30 avg=2.55\n",
            "[221 | 419.95] loss=2.92 avg=2.55\n",
            "[222 | 421.62] loss=2.63 avg=2.55\n",
            "[223 | 423.29] loss=2.57 avg=2.55\n",
            "[224 | 424.95] loss=2.43 avg=2.55\n",
            "[225 | 426.62] loss=2.67 avg=2.55\n",
            "[226 | 428.30] loss=2.27 avg=2.55\n",
            "[227 | 429.97] loss=2.72 avg=2.55\n",
            "[228 | 431.65] loss=2.31 avg=2.55\n",
            "[229 | 433.33] loss=2.32 avg=2.55\n",
            "[230 | 435.01] loss=2.72 avg=2.55\n",
            "[231 | 436.69] loss=2.37 avg=2.55\n",
            "[232 | 438.37] loss=2.63 avg=2.55\n",
            "[233 | 440.04] loss=2.66 avg=2.55\n",
            "[234 | 441.71] loss=2.67 avg=2.55\n",
            "[235 | 443.40] loss=2.54 avg=2.55\n",
            "[236 | 445.07] loss=2.66 avg=2.55\n",
            "[237 | 446.74] loss=2.34 avg=2.55\n",
            "[238 | 448.40] loss=2.52 avg=2.55\n",
            "[239 | 450.07] loss=2.38 avg=2.55\n",
            "[240 | 451.74] loss=2.35 avg=2.54\n",
            "[241 | 453.40] loss=2.24 avg=2.54\n",
            "[242 | 455.05] loss=2.61 avg=2.54\n",
            "[243 | 456.71] loss=2.56 avg=2.54\n",
            "[244 | 458.36] loss=2.56 avg=2.54\n",
            "[245 | 460.01] loss=2.77 avg=2.54\n",
            "[246 | 461.64] loss=2.35 avg=2.54\n",
            "[247 | 463.29] loss=2.49 avg=2.54\n",
            "[248 | 464.92] loss=2.24 avg=2.54\n",
            "[249 | 466.56] loss=2.74 avg=2.54\n",
            "[250 | 468.21] loss=2.41 avg=2.54\n",
            "[251 | 469.84] loss=2.22 avg=2.54\n",
            "[252 | 471.47] loss=2.41 avg=2.53\n",
            "[253 | 473.11] loss=2.59 avg=2.54\n",
            "[254 | 474.75] loss=2.52 avg=2.54\n",
            "[255 | 476.38] loss=2.62 avg=2.54\n",
            "[256 | 478.02] loss=2.58 avg=2.54\n",
            "[257 | 479.66] loss=2.41 avg=2.54\n",
            "[258 | 481.29] loss=2.71 avg=2.54\n",
            "[259 | 482.93] loss=2.41 avg=2.54\n",
            "[260 | 484.56] loss=2.17 avg=2.53\n",
            "[261 | 486.19] loss=2.14 avg=2.53\n",
            "[262 | 487.82] loss=2.28 avg=2.52\n",
            "[263 | 489.45] loss=2.45 avg=2.52\n",
            "[264 | 491.09] loss=2.54 avg=2.52\n",
            "[265 | 492.73] loss=2.80 avg=2.53\n",
            "[266 | 494.36] loss=2.47 avg=2.53\n",
            "[267 | 495.99] loss=2.06 avg=2.52\n",
            "[268 | 497.63] loss=2.35 avg=2.52\n",
            "[269 | 499.26] loss=2.55 avg=2.52\n",
            "[270 | 500.90] loss=2.01 avg=2.51\n",
            "[271 | 502.54] loss=2.77 avg=2.52\n",
            "[272 | 504.18] loss=2.29 avg=2.52\n",
            "[273 | 505.82] loss=2.54 avg=2.52\n",
            "[274 | 507.48] loss=2.58 avg=2.52\n",
            "[275 | 509.12] loss=2.22 avg=2.51\n",
            "[276 | 510.78] loss=2.57 avg=2.51\n",
            "[277 | 512.43] loss=2.40 avg=2.51\n",
            "[278 | 514.09] loss=2.47 avg=2.51\n",
            "[279 | 515.74] loss=2.12 avg=2.51\n",
            "[280 | 517.40] loss=2.73 avg=2.51\n",
            "[281 | 519.07] loss=2.47 avg=2.51\n",
            "[282 | 520.74] loss=2.67 avg=2.51\n",
            "[283 | 522.41] loss=2.52 avg=2.51\n",
            "[284 | 524.08] loss=2.66 avg=2.51\n",
            "[285 | 525.75] loss=2.70 avg=2.51\n",
            "[286 | 527.42] loss=2.60 avg=2.52\n",
            "[287 | 529.09] loss=2.30 avg=2.51\n",
            "[288 | 530.76] loss=2.39 avg=2.51\n",
            "[289 | 532.45] loss=2.74 avg=2.51\n",
            "[290 | 534.12] loss=2.63 avg=2.52\n",
            "[291 | 535.81] loss=2.30 avg=2.51\n",
            "[292 | 537.48] loss=2.36 avg=2.51\n",
            "[293 | 539.17] loss=2.63 avg=2.51\n",
            "[294 | 540.84] loss=2.68 avg=2.51\n",
            "[295 | 542.51] loss=2.50 avg=2.51\n",
            "[296 | 544.18] loss=2.68 avg=2.52\n",
            "[297 | 545.85] loss=2.37 avg=2.51\n",
            "[298 | 547.52] loss=2.31 avg=2.51\n",
            "[299 | 549.20] loss=3.04 avg=2.52\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " and, in my opinion, most of the ones you would find in some of the better cookbooks. And she also teaches the art and science of eating in a way no one seems to have learned before—even though I don't even eat raw fish, let alone meat.\n",
            "\n",
            "Advertisement\n",
            "\n",
            "On the second page, a woman (I'll call her \"Ish\") sits with her children. Her children—a daughter and a brother—are doing something I didn't notice her doing. She sits in a chair, hands to her face and legs spread wide, eyes closed, and one hand on her hip. She doesn't look up. Her eyes are closed. The hand on the hip is away. I don't want to see her eyes. I don't want to think about her. A mother doesn't talk about what will happen if they don't speak to her or if she doesn't speak. She just does things, and she does them slowly and methodically. As the children continue, they are able to move their hands faster and more quietly—she opens her hands and hands her children, then her fingers, and then her fingers—without her trying. Instead, she closes her eyes to meditate, and as the children go to a task, she closes her eyes. She doesn't look up or up or down. But she doesn't look up—because she is still sitting. And that is what I loved about this book.\n",
            "\n",
            "The next day, she was busy.\n",
            "\n",
            "We made pancakes, which of course sounds great, but she asked the kid to make the sauce, and she had all these left-over items, so she would just be eating what was leftover. The kid asked her to fill two cups of water with the sauce, not three. She told him that the sauce he could use with his pancake didn't taste as good, so she said that hers tasted too strong. She didn't say it to show the rest of the kids, though. She didn't say it to show you. She said it to show herself. That's the thing about what I love about Shahada—and about what I like about Cookbook Writers, too. There is so much hidden, delicious knowledge that is hidden, even to me, in Shahada. Because of its hidden knowledge, it is the book that I look forward to reading. It is where I look to see how much I haven't learned and how much I've already learned. I am just watching and listening with curiosity, and asking questions like, \"What's the most important thing to know about cooking? What's the most important thing to know about nutrition? What is the most important thing about life? What does it mean to be human? Why would I be willing to sacrifice anything for a little piece of wisdom?\"\n",
            "\n",
            "And just before I left, I asked my assistant to come up to me and let me know she loved Shahada and that she wanted me to write a review of it. When she saw that the book I was interested in was already in print, she said, \"I'm so sorry. This is going to be awesome. This is what you've been waiting for.\" She wasn't saying that she wanted me to write a review. She just wanted me to write—and then to review, which she did not want to write. Shahada is not for everybody, and it is not a book for everyone. But for what it does really well, Shahada is my favorite cookbook of 2013, and I hope that you will read it, too.\n",
            "\n",
            "*Update, December 9, 2014: This post was updated to include the name of the city I interviewed with. The name of the city I met with does not appear in the article. If I make any spelling and grammar errors, they are my fault. —Melissa Monsef\n",
            "\n",
            "Follow the Long Read on Twitter: @gdnlongread\n",
            "\n",
            "Photograph by Alex Wong / Getty.\n",
            "\n",
            "\n",
            "If you have news or commentary to share, please get in touch @longreads.\n",
            "\n",
            "There are so many ways a woman can be a woman:\n",
            "\n",
            "You're not only a black lawyer that says you'd kill Hillary Clinton if she stood for President. (She doesn't, and it would probably be a good idea to not be a lawyer; she could actually do some good, and is better at politics than any other lawyer, though, which is another reason)\n",
            "\n",
            "You're married to a black man that would rather be a dentist than a congressman. (I will also vote for a Democrat if Hillary won a second election.)\n",
            "\n",
            "You've had a rape joke before. (You'd be a really good joke to women if you've had a sex joke before...or if you've ever had sex jokes.)\n",
            "\n",
            "You're a politician. (You could win the Presidency if it were the first person to do so.)\n",
            "\n",
            "You have never had a conversation with a fellow presidential candidate. (You'd\n",
            "\n",
            "[300 | 573.62] loss=2.62 avg=2.52\n",
            "[301 | 575.26] loss=1.90 avg=2.51\n",
            "[302 | 576.90] loss=2.47 avg=2.51\n",
            "[303 | 578.54] loss=2.23 avg=2.51\n",
            "[304 | 580.18] loss=2.38 avg=2.51\n",
            "[305 | 581.81] loss=2.12 avg=2.50\n",
            "[306 | 583.45] loss=2.42 avg=2.50\n",
            "[307 | 585.08] loss=2.40 avg=2.50\n",
            "[308 | 586.72] loss=2.70 avg=2.50\n",
            "[309 | 588.35] loss=2.22 avg=2.50\n",
            "[310 | 589.99] loss=2.62 avg=2.50\n",
            "[311 | 591.63] loss=2.74 avg=2.50\n",
            "[312 | 593.26] loss=2.40 avg=2.50\n",
            "[313 | 594.91] loss=2.37 avg=2.50\n",
            "[314 | 596.54] loss=2.34 avg=2.50\n",
            "[315 | 598.19] loss=2.75 avg=2.50\n",
            "[316 | 599.83] loss=1.96 avg=2.50\n",
            "[317 | 601.47] loss=2.50 avg=2.50\n",
            "[318 | 603.11] loss=2.05 avg=2.49\n",
            "[319 | 604.75] loss=2.54 avg=2.49\n",
            "[320 | 606.40] loss=2.63 avg=2.49\n",
            "[321 | 608.05] loss=2.48 avg=2.49\n",
            "[322 | 609.70] loss=2.51 avg=2.49\n",
            "[323 | 611.35] loss=2.45 avg=2.49\n",
            "[324 | 613.01] loss=2.93 avg=2.50\n",
            "[325 | 614.67] loss=2.71 avg=2.50\n",
            "[326 | 616.33] loss=2.28 avg=2.50\n",
            "[327 | 618.00] loss=2.59 avg=2.50\n",
            "[328 | 619.65] loss=2.22 avg=2.50\n",
            "[329 | 621.33] loss=2.74 avg=2.50\n",
            "[330 | 623.01] loss=2.67 avg=2.50\n",
            "[331 | 624.68] loss=2.45 avg=2.50\n",
            "[332 | 626.36] loss=2.22 avg=2.50\n",
            "[333 | 628.03] loss=2.28 avg=2.50\n",
            "[334 | 629.70] loss=2.25 avg=2.49\n",
            "[335 | 631.38] loss=2.59 avg=2.49\n",
            "[336 | 633.07] loss=2.48 avg=2.49\n",
            "[337 | 634.74] loss=2.63 avg=2.49\n",
            "[338 | 636.43] loss=2.14 avg=2.49\n",
            "[339 | 638.11] loss=2.49 avg=2.49\n",
            "[340 | 639.79] loss=2.79 avg=2.49\n",
            "[341 | 641.46] loss=2.58 avg=2.50\n",
            "[342 | 643.12] loss=2.51 avg=2.50\n",
            "[343 | 644.79] loss=2.14 avg=2.49\n",
            "[344 | 646.47] loss=2.63 avg=2.49\n",
            "[345 | 648.14] loss=2.50 avg=2.49\n",
            "[346 | 649.80] loss=1.88 avg=2.49\n",
            "[347 | 651.47] loss=2.51 avg=2.49\n",
            "[348 | 653.12] loss=2.47 avg=2.49\n",
            "[349 | 654.78] loss=2.33 avg=2.49\n",
            "[350 | 656.44] loss=2.17 avg=2.48\n",
            "[351 | 658.10] loss=2.28 avg=2.48\n",
            "[352 | 659.75] loss=2.60 avg=2.48\n",
            "[353 | 661.40] loss=2.41 avg=2.48\n",
            "[354 | 663.05] loss=2.64 avg=2.48\n",
            "[355 | 664.70] loss=2.91 avg=2.49\n",
            "[356 | 666.35] loss=2.60 avg=2.49\n",
            "[357 | 667.99] loss=2.46 avg=2.49\n",
            "[358 | 669.63] loss=2.55 avg=2.49\n",
            "[359 | 671.27] loss=2.76 avg=2.49\n",
            "[360 | 672.91] loss=2.62 avg=2.49\n",
            "[361 | 674.55] loss=2.20 avg=2.49\n",
            "[362 | 676.20] loss=2.77 avg=2.49\n",
            "[363 | 677.83] loss=2.63 avg=2.49\n",
            "[364 | 679.46] loss=2.40 avg=2.49\n",
            "[365 | 681.09] loss=2.66 avg=2.49\n",
            "[366 | 682.72] loss=2.53 avg=2.49\n",
            "[367 | 684.35] loss=2.73 avg=2.50\n",
            "[368 | 685.98] loss=2.54 avg=2.50\n",
            "[369 | 687.62] loss=2.02 avg=2.49\n",
            "[370 | 689.25] loss=2.39 avg=2.49\n",
            "[371 | 690.89] loss=2.48 avg=2.49\n",
            "[372 | 692.52] loss=2.23 avg=2.49\n",
            "[373 | 694.15] loss=2.35 avg=2.49\n",
            "[374 | 695.79] loss=2.29 avg=2.49\n",
            "[375 | 697.42] loss=2.65 avg=2.49\n",
            "[376 | 699.05] loss=2.48 avg=2.49\n",
            "[377 | 700.70] loss=2.63 avg=2.49\n",
            "[378 | 702.34] loss=2.90 avg=2.49\n",
            "[379 | 703.98] loss=2.41 avg=2.49\n",
            "[380 | 705.63] loss=2.22 avg=2.49\n",
            "[381 | 707.27] loss=2.52 avg=2.49\n",
            "[382 | 708.92] loss=2.35 avg=2.49\n",
            "[383 | 710.57] loss=2.58 avg=2.49\n",
            "[384 | 712.23] loss=2.43 avg=2.49\n",
            "[385 | 713.90] loss=2.47 avg=2.49\n",
            "[386 | 715.56] loss=1.93 avg=2.48\n",
            "[387 | 717.23] loss=2.04 avg=2.48\n",
            "[388 | 718.90] loss=2.42 avg=2.48\n",
            "[389 | 720.57] loss=2.59 avg=2.48\n",
            "[390 | 722.24] loss=2.24 avg=2.48\n",
            "[391 | 723.91] loss=2.31 avg=2.47\n",
            "[392 | 725.58] loss=2.37 avg=2.47\n",
            "[393 | 727.25] loss=2.51 avg=2.47\n",
            "[394 | 728.93] loss=2.57 avg=2.47\n",
            "[395 | 730.61] loss=2.71 avg=2.48\n",
            "[396 | 732.31] loss=2.75 avg=2.48\n",
            "[397 | 734.00] loss=2.60 avg=2.48\n",
            "[398 | 735.69] loss=2.11 avg=2.48\n",
            "[399 | 737.38] loss=2.57 avg=2.48\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            ", this is true: the American people do not like the Trump Administration but they do like Congress and the legislative branches more than most other parts of the American political culture.\n",
            "\n",
            "Trump won the elections and majorities in both houses of Congress but still couldn't get any laws passed through Congress. In some areas, including healthcare, the Administration is a mess: many of the President's policies, such as blocking Syrian refugees, were not even included in the FY 2017 House and Senate versions of the Appropriations bills. (I spoke to Rep. Ileana Ros-Lehtinen about this situation.)\n",
            "\n",
            "But in the area of foreign policy, Trump will get his due soon. He didn't win elections and doesn't have legislative power; if he does, Congress will soon pass a new war or a new nuclear treaty or a new arms agreement. He didn't win Congressional approval for his controversial travel ban and still hasn't succeeded in pressuring Japan to withdraw from our nuclear umbrella. So Trump is unlikely to get rid of Congress anytime soon.\n",
            "\n",
            "Still, Trump won on a popular vote—only about 60 percent of the population voted, to be sure. That didn't hurt: Most of the electorate, at least for the first couple of years, was anti-Trump but has since warmed to him. When you're president who's the unpopular one, the public has your back. And now his public approval rating does better than any other major presidential candidate in history.\n",
            "\n",
            "Now Trump has a real problem: There is no legislative, executive, or judicial power to help him get things done. But there are real legislative, executive, and judicial power to do the things he already wants to do—and needs to do more of. And Congress hasn't given it many opportunities for cooperation—which is what they do best.\n",
            "\n",
            "If Trump wanted to get things done, he would have at least the possibility of an impeachment vote. As Trump has said, he wants to go through impeachment. He's already started. Since House Democrats have the majority in Congress, there could be more votes if a more liberal House faction—Democrats from swing districts—wins back-door deals with Republicans.\n",
            "\n",
            "And now that the Russia investigation is officially over for the Democrats, Trump will have the Democratic Congressional Campaign Committee and Senate Majority PAC to run a full-scale attack against their Republican opposition. But in the Senate, Democrats have little leverage because Democrats have to work with them only in a limited way. They can pass legislation for Trump's immediate agenda, such as the border wall and border adjustment, but they have to pass things that Trump himself wants passed. To get things done in Congress, Trump will have to be more open to new ideas.\n",
            "\n",
            "But even under his most favorable conditions, it's entirely possible that Donald Trump will win reelection—though one can't help but wonder how many voters would be persuaded to cast a vote for Trump in 2018 if they thought the President was a loser?\n",
            "\n",
            "This post has been updated since its original publication.<|endoftext|>One of the more popular lines in political speeches lately is that the Democratic and Republican Parties are the same entity. This is the point of view espoused by Hillary Clinton, while also reflecting views of President Barack Obama, who has repeatedly said that if he were nominated by the same party, he would \"fight them as they fight me.\" For Hillary Clinton, this means that she would represent the Democratic Party and would not be beholden to the powerful in either party (and that is another idea).\n",
            "\n",
            "The idea is that her campaign has won the support of the so-called 'deplorables' in the Democratic Party, which will make it easier for her to win the popular vote than a Republican candidate. However, this view assumes that a substantial segment of the Democratic Party doesn't reject the party and has instead adopted a more mainstream stance. The theory is that if there is a large enough minority of Democrats that oppose the policies of the Democratic presidential nominee, then the whole party has to have some sort of internal debate about its own political direction for fear of losing the popular vote.\n",
            "\n",
            "Clinton has said that this strategy is a mistake, as it has led some people to believe that she does not consider the Republican Party the same entity as the Democratic Party. In her speech on Wednesday, she said that the Republicans in the House and Senate have tried to change the political system without letting people participate in that conversation.\n",
            "\n",
            "Clinton's argument for wanting the debate to happen is that the idea of 'the people' voting for their representative is unfair. In 2016, when Clinton ran for president, the electoral votes he won in states like Ohio and Pennsylvania seemed more significant than they did in 2014.\n",
            "\n",
            "Clinton also thinks that Trump is a worse candidate than he claims. If Trump loses the House and the presidency, the Republican candidates would have to win the popular vote to prevent the presidency from going to the Democrats.\n",
            "\n",
            "In general, people believe that the Democratic candidates would lose if they were the Democratic presidential nominee, but the\n",
            "\n",
            "[400 | 761.76] loss=2.73 avg=2.48\n",
            "[401 | 763.41] loss=2.40 avg=2.48\n",
            "[402 | 765.06] loss=2.42 avg=2.48\n",
            "[403 | 766.71] loss=2.66 avg=2.48\n",
            "[404 | 768.35] loss=2.49 avg=2.48\n",
            "[405 | 770.00] loss=2.07 avg=2.48\n",
            "[406 | 771.64] loss=2.84 avg=2.48\n",
            "[407 | 773.28] loss=2.23 avg=2.48\n",
            "[408 | 774.92] loss=2.34 avg=2.48\n",
            "[409 | 776.56] loss=2.47 avg=2.48\n",
            "[410 | 778.20] loss=2.35 avg=2.48\n",
            "[411 | 779.85] loss=2.39 avg=2.47\n",
            "[412 | 781.49] loss=2.16 avg=2.47\n",
            "[413 | 783.13] loss=2.26 avg=2.47\n",
            "[414 | 784.77] loss=2.40 avg=2.47\n",
            "[415 | 786.41] loss=2.12 avg=2.46\n",
            "[416 | 788.04] loss=2.66 avg=2.47\n",
            "[417 | 789.68] loss=2.36 avg=2.47\n",
            "[418 | 791.32] loss=2.74 avg=2.47\n",
            "[419 | 792.96] loss=2.52 avg=2.47\n",
            "[420 | 794.61] loss=2.53 avg=2.47\n",
            "[421 | 796.25] loss=2.64 avg=2.47\n",
            "[422 | 797.89] loss=2.29 avg=2.47\n",
            "[423 | 799.54] loss=2.29 avg=2.47\n",
            "[424 | 801.18] loss=2.17 avg=2.46\n",
            "[425 | 802.82] loss=2.03 avg=2.46\n",
            "[426 | 804.47] loss=2.21 avg=2.46\n",
            "[427 | 806.11] loss=2.44 avg=2.46\n",
            "[428 | 807.77] loss=2.33 avg=2.46\n",
            "[429 | 809.42] loss=2.50 avg=2.46\n",
            "[430 | 811.07] loss=2.41 avg=2.46\n",
            "[431 | 812.73] loss=2.21 avg=2.45\n",
            "[432 | 814.39] loss=2.05 avg=2.45\n",
            "[433 | 816.06] loss=2.39 avg=2.45\n",
            "[434 | 817.72] loss=2.61 avg=2.45\n",
            "[435 | 819.38] loss=2.86 avg=2.45\n",
            "[436 | 821.05] loss=2.31 avg=2.45\n",
            "[437 | 822.73] loss=2.57 avg=2.45\n",
            "[438 | 824.39] loss=2.13 avg=2.45\n",
            "[439 | 826.07] loss=2.12 avg=2.45\n",
            "[440 | 827.75] loss=2.22 avg=2.45\n",
            "[441 | 829.42] loss=2.53 avg=2.45\n",
            "[442 | 831.09] loss=2.43 avg=2.45\n",
            "[443 | 832.77] loss=2.53 avg=2.45\n",
            "[444 | 834.45] loss=2.24 avg=2.45\n",
            "[445 | 836.12] loss=2.33 avg=2.44\n",
            "[446 | 837.79] loss=2.25 avg=2.44\n",
            "[447 | 839.46] loss=2.75 avg=2.45\n",
            "[448 | 841.13] loss=2.14 avg=2.44\n",
            "[449 | 842.80] loss=2.64 avg=2.44\n",
            "[450 | 844.47] loss=2.53 avg=2.44\n",
            "[451 | 846.13] loss=2.67 avg=2.45\n",
            "[452 | 847.81] loss=2.27 avg=2.45\n",
            "[453 | 849.47] loss=2.62 avg=2.45\n",
            "[454 | 851.13] loss=2.55 avg=2.45\n",
            "[455 | 852.80] loss=2.54 avg=2.45\n",
            "[456 | 854.45] loss=2.38 avg=2.45\n",
            "[457 | 856.11] loss=2.14 avg=2.45\n",
            "[458 | 857.77] loss=2.28 avg=2.44\n",
            "[459 | 859.42] loss=2.34 avg=2.44\n",
            "[460 | 861.07] loss=2.40 avg=2.44\n",
            "[461 | 862.72] loss=2.30 avg=2.44\n",
            "[462 | 864.36] loss=2.27 avg=2.44\n",
            "[463 | 866.01] loss=2.00 avg=2.43\n",
            "[464 | 867.65] loss=2.47 avg=2.44\n",
            "[465 | 869.29] loss=1.58 avg=2.43\n",
            "[466 | 870.93] loss=1.95 avg=2.42\n",
            "[467 | 872.57] loss=2.36 avg=2.42\n",
            "[468 | 874.21] loss=2.28 avg=2.42\n",
            "[469 | 875.85] loss=2.38 avg=2.42\n",
            "[470 | 877.48] loss=2.25 avg=2.42\n",
            "[471 | 879.12] loss=2.41 avg=2.42\n",
            "[472 | 880.75] loss=2.76 avg=2.42\n",
            "[473 | 882.37] loss=2.63 avg=2.42\n",
            "[474 | 884.00] loss=2.29 avg=2.42\n",
            "[475 | 885.63] loss=2.52 avg=2.42\n",
            "[476 | 887.27] loss=2.33 avg=2.42\n",
            "[477 | 888.90] loss=2.44 avg=2.42\n",
            "[478 | 890.54] loss=2.37 avg=2.42\n",
            "[479 | 892.17] loss=1.97 avg=2.42\n",
            "[480 | 893.80] loss=2.19 avg=2.41\n",
            "[481 | 895.44] loss=2.58 avg=2.42\n",
            "[482 | 897.08] loss=1.77 avg=2.41\n",
            "[483 | 898.72] loss=2.15 avg=2.41\n",
            "[484 | 900.36] loss=2.28 avg=2.41\n",
            "[485 | 902.00] loss=2.95 avg=2.41\n",
            "[486 | 903.65] loss=1.98 avg=2.41\n",
            "[487 | 905.29] loss=2.01 avg=2.40\n",
            "[488 | 906.93] loss=2.27 avg=2.40\n",
            "[489 | 908.58] loss=2.11 avg=2.40\n",
            "[490 | 910.24] loss=2.19 avg=2.40\n",
            "[491 | 911.89] loss=2.38 avg=2.40\n",
            "[492 | 913.55] loss=2.71 avg=2.40\n",
            "[493 | 915.22] loss=2.33 avg=2.40\n",
            "[494 | 916.88] loss=2.20 avg=2.40\n",
            "[495 | 918.55] loss=2.49 avg=2.40\n",
            "[496 | 920.23] loss=2.41 avg=2.40\n",
            "[497 | 921.90] loss=2.57 avg=2.40\n",
            "[498 | 923.57] loss=2.01 avg=2.40\n",
            "[499 | 925.24] loss=2.47 avg=2.40\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " a public hearing on the issue in February.<|endoftext|>Ladies and gentlemen, we have a special guest today! This is a special guest because I was doing a Q&A live from my home in Columbus, Ohio to answer questions posted to my Twitter for the first few episodes of The Big Bang Theory. If you have ever followed @Baratheon, this event was definitely the one that brought those familiar memories together. I met up with my old pal @AlfredFalkner to chat about life after the show, and how she plans to continue doing Q&As.\n",
            "\n",
            "\n",
            "I have a new fan.\n",
            "\n",
            "I didn't even know Alfred [Falkner] existed until last weekend. His name was Matt. I've met him on the streets of Chicago where I work, and he always had a smile on his face. He doesn't have a job, but he's always joking around: \"Hey, I have a gig! Why don't I get it.\" I was like, \"Oh, we could do that! We can do podcast shows!\" That's when it happened in my head.\n",
            "\n",
            "So yeah, Matt and I have a weird friendship. When I asked him what his idea of a great dinner was, he said, \"I mean, a lot of great things. It could be a dinner at the best restaurant—I've only eaten at three restaurants. We should probably start with a small plate. We should probably order a bunch of seafood, or a bunch of desserts. It could be some kind of classic British dinner.\" The first show is coming along really well.\n",
            "\n",
            "When asked to name the show that made him decide to end the show, he said, \"Dude, I don't know. I just realized, I really feel like I'm not the star anymore. I haven't been able to do what I want to do, and I think it would be a great opportunity for somebody else to do it better than I have. Not to say that I'm not going to do it, but I just feel like I think I'm not the star anymore.\n",
            "\n",
            "At the risk of sounding like a broken record (which I have, by the way, and never in the vain expectation of anyone else), I'd just like to say thank you to everyone who's watched the show, and I wish you all a great season. I don't have much time left for this podcast, but that's mostly because I feel like I have tons of stuff to talk about.<|endoftext|>On The Road\n",
            "\n",
            "When I arrived in Mexico City with my family, it would be the first time I was able to meet my family outside of the United States. I was 16 years old, and I looked like so many of the people my family grew up around in the United States: slim, pretty, with good eyes and a bright smile.\n",
            "\n",
            "The idea of visiting family from abroad terrified me. I wondered, 'what if I ended up here, with these Americans looking after me?'\n",
            "\n",
            "From there, I would take up residency at the United Nations and, over the next two years, work to help build and establish the international community.\n",
            "\n",
            "At the time, my family was in a part of the country where only five people, including the youngest of my siblings, had ever left the house.\n",
            "\n",
            "The youngest was me - born in 1992. At 18, I was the youngest American woman to ever travel beyond the US and then leave the country and live in another country. And yet, it never occurred to me that America was just one country, that many people were coming and going from across the oceans.\n",
            "\n",
            "When I was 11, I met an amazing person who gave me my first sense of love and comfort. She invited me to her wedding. She invited me to her wedding.\n",
            "\n",
            "At the time my family lived in California, which was an interesting place to visit, considering we were the first people in the world to be born in an American city.\n",
            "\n",
            "But I spent the next 20 years in a bubble, where I saw everyone in my city, where I saw so many families, where I lived through it all.\n",
            "\n",
            "The people I've met have been the most amazing people to me. There are still the people I look up to — even the ones who are crazy, or crazy themselves. There are the Americans I'm lucky enough to meet in everyday life.\n",
            "\n",
            "When I came here, I couldn't be more shocked that I've come to be a part of what is becoming the greatest democracy on Earth. That would be me standing in front of the house of Representatives.\n",
            "\n",
            "If there's one thing I know about America, it's this: When you put it together, you can write your own history.\n",
            "\n",
            "I'm here at the United States Embassy to tell you that history is not over. The United States is not finished, and it won't be over before we leave.\n",
            "\n",
            "But we know where the important things are, and\n",
            "\n",
            "[500 | 949.65] loss=1.80 avg=2.39\n",
            "[501 | 951.30] loss=2.37 avg=2.39\n",
            "[502 | 952.97] loss=2.54 avg=2.39\n",
            "[503 | 954.62] loss=2.47 avg=2.39\n",
            "[504 | 956.27] loss=2.31 avg=2.39\n",
            "[505 | 957.92] loss=2.30 avg=2.39\n",
            "[506 | 959.57] loss=2.57 avg=2.39\n",
            "[507 | 961.22] loss=2.34 avg=2.39\n",
            "[508 | 962.87] loss=2.77 avg=2.40\n",
            "[509 | 964.51] loss=2.63 avg=2.40\n",
            "[510 | 966.15] loss=2.20 avg=2.40\n",
            "[511 | 967.79] loss=2.53 avg=2.40\n",
            "[512 | 969.43] loss=2.23 avg=2.40\n",
            "[513 | 971.07] loss=2.44 avg=2.40\n",
            "[514 | 972.71] loss=1.79 avg=2.39\n",
            "[515 | 974.35] loss=2.38 avg=2.39\n",
            "[516 | 975.99] loss=2.25 avg=2.39\n",
            "[517 | 977.63] loss=2.65 avg=2.39\n",
            "[518 | 979.27] loss=2.28 avg=2.39\n",
            "[519 | 980.90] loss=2.50 avg=2.39\n",
            "[520 | 982.54] loss=2.29 avg=2.39\n",
            "[521 | 984.18] loss=2.20 avg=2.39\n",
            "[522 | 985.82] loss=2.40 avg=2.39\n",
            "[523 | 987.46] loss=2.43 avg=2.39\n",
            "[524 | 989.09] loss=2.00 avg=2.38\n",
            "[525 | 990.73] loss=2.22 avg=2.38\n",
            "[526 | 992.37] loss=2.22 avg=2.38\n",
            "[527 | 994.01] loss=2.50 avg=2.38\n",
            "[528 | 995.65] loss=2.33 avg=2.38\n",
            "[529 | 997.29] loss=2.17 avg=2.38\n",
            "[530 | 998.94] loss=2.51 avg=2.38\n",
            "[531 | 1000.58] loss=2.70 avg=2.38\n",
            "[532 | 1002.23] loss=2.50 avg=2.39\n",
            "[533 | 1003.88] loss=2.65 avg=2.39\n",
            "[534 | 1005.53] loss=2.41 avg=2.39\n",
            "[535 | 1007.18] loss=2.60 avg=2.39\n",
            "[536 | 1008.84] loss=2.20 avg=2.39\n",
            "[537 | 1010.50] loss=2.45 avg=2.39\n",
            "[538 | 1012.16] loss=1.87 avg=2.38\n",
            "[539 | 1013.82] loss=2.07 avg=2.38\n",
            "[540 | 1015.48] loss=2.22 avg=2.38\n",
            "[541 | 1017.15] loss=2.33 avg=2.38\n",
            "[542 | 1018.81] loss=2.59 avg=2.38\n",
            "[543 | 1020.49] loss=2.62 avg=2.38\n",
            "[544 | 1022.16] loss=2.19 avg=2.38\n",
            "[545 | 1023.82] loss=2.52 avg=2.38\n",
            "[546 | 1025.50] loss=2.36 avg=2.38\n",
            "[547 | 1027.18] loss=2.46 avg=2.38\n",
            "[548 | 1028.85] loss=2.08 avg=2.38\n",
            "[549 | 1030.52] loss=2.68 avg=2.38\n",
            "[550 | 1032.19] loss=2.59 avg=2.39\n",
            "[551 | 1033.86] loss=2.20 avg=2.38\n",
            "[552 | 1035.54] loss=2.38 avg=2.38\n",
            "[553 | 1037.21] loss=2.65 avg=2.39\n",
            "[554 | 1038.88] loss=2.33 avg=2.39\n",
            "[555 | 1040.55] loss=2.26 avg=2.38\n",
            "[556 | 1042.22] loss=2.73 avg=2.39\n",
            "[557 | 1043.89] loss=2.44 avg=2.39\n",
            "[558 | 1045.56] loss=2.23 avg=2.39\n",
            "[559 | 1047.22] loss=2.31 avg=2.39\n",
            "[560 | 1048.89] loss=2.82 avg=2.39\n",
            "[561 | 1050.54] loss=2.36 avg=2.39\n",
            "[562 | 1052.20] loss=1.88 avg=2.38\n",
            "[563 | 1053.85] loss=2.62 avg=2.39\n",
            "[564 | 1055.51] loss=2.47 avg=2.39\n",
            "[565 | 1057.16] loss=2.47 avg=2.39\n",
            "[566 | 1058.81] loss=2.35 avg=2.39\n",
            "[567 | 1060.46] loss=2.19 avg=2.39\n",
            "[568 | 1062.10] loss=2.20 avg=2.38\n",
            "[569 | 1063.74] loss=2.18 avg=2.38\n",
            "[570 | 1065.39] loss=2.48 avg=2.38\n",
            "[571 | 1067.03] loss=2.31 avg=2.38\n",
            "[572 | 1068.67] loss=2.35 avg=2.38\n",
            "[573 | 1070.30] loss=2.05 avg=2.38\n",
            "[574 | 1071.94] loss=2.58 avg=2.38\n",
            "[575 | 1073.58] loss=1.90 avg=2.38\n",
            "[576 | 1075.21] loss=2.52 avg=2.38\n",
            "[577 | 1076.85] loss=2.49 avg=2.38\n",
            "[578 | 1078.48] loss=2.39 avg=2.38\n",
            "[579 | 1080.11] loss=2.73 avg=2.38\n",
            "[580 | 1081.74] loss=2.71 avg=2.39\n",
            "[581 | 1083.38] loss=2.28 avg=2.38\n",
            "[582 | 1085.01] loss=2.78 avg=2.39\n",
            "[583 | 1086.65] loss=2.39 avg=2.39\n",
            "[584 | 1088.29] loss=2.43 avg=2.39\n",
            "[585 | 1089.93] loss=1.99 avg=2.38\n",
            "[586 | 1091.56] loss=2.21 avg=2.38\n",
            "[587 | 1093.19] loss=2.40 avg=2.38\n",
            "[588 | 1094.83] loss=2.24 avg=2.38\n",
            "[589 | 1096.47] loss=2.56 avg=2.38\n",
            "[590 | 1098.11] loss=2.33 avg=2.38\n",
            "[591 | 1099.75] loss=2.30 avg=2.38\n",
            "[592 | 1101.40] loss=2.84 avg=2.39\n",
            "[593 | 1103.04] loss=2.26 avg=2.39\n",
            "[594 | 1104.69] loss=1.97 avg=2.38\n",
            "[595 | 1106.34] loss=2.37 avg=2.38\n",
            "[596 | 1107.99] loss=2.10 avg=2.38\n",
            "[597 | 1109.64] loss=2.56 avg=2.38\n",
            "[598 | 1111.30] loss=2.28 avg=2.38\n",
            "[599 | 1112.95] loss=1.79 avg=2.37\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "’s time to come,\" he concluded. “Now is the time,” he declared.\n",
            "\n",
            "— Jill Marcus\n",
            "\n",
            "‣ During Sunday's GOP presidential debate, Donald Trump called the mainstream press “the absolute worst of the bad guys” and said the press is the “opposite of a friend.” The New York Times tweeted that “Donald Trump’s attack on the press is ’like the attack on a country.”\n",
            "\n",
            "\n",
            "\n",
            "‣ The new Senate Democratic bill to repeal Obamacare is already dead in the Republican-controlled House—and will die with it if Democrats are to move forward and pass a full repeal.\n",
            "\n",
            "— Jill Marcus and Yara Bayoumy\n",
            "\n",
            "‣ One of Obamacare’s original architects, Massachusetts Senator Elizabeth Warren, released a new video on September 16 to mark Medicare for All Day, a day of action to fight for the universal health care program.\n",
            "\n",
            "We’re always looking for ways to improve The Politics & Policy Daily. Comments, questions, typos, grievances or groans related to our puns? Let us know anytime here.\n",
            "\n",
            "Were you forwarded this newsletter? Sign up for our daily politics email here. We have many other free email newsletters on a variety of other topics. Find the full list here.\n",
            "\n",
            "\n",
            "\n",
            "It’s Labor Day weekend, which means it’s time to think about what it means to be a good union officer.\n",
            "\n",
            "\n",
            "\n",
            "On a hot summer day in the spring of 2017, a woman in a red dress stood behind the lectern of an Indianapolis City Hall podium, her voice cracking as she talked about the importance of organizing across industries. She “didn’t just talk about organizing,” as the chair of the Board of Directors at the Indianapolis Public-Library Union put it;’she was speaking in a way that had become shorthand for labor organizing since Michael Harrington’s 1957 book, “Union in the Labor-Unions: The Power of Public-Schools Unionism.”\n",
            "\n",
            "The woman who had spoken in that press conference that March afternoon had been joined earlier that Saturday by two other people, one of whom was Michelle Rachleff, who is now a Democratic Party strategist. She has since become one of the organizers behind the labor advocacy group United Teachers, which has worked with unions across the country to pressure legislators to pass the so-called Right to Work bill, which requires that employers pay workers equal wages for the work they do—even those doing the same work for less money.\n",
            "\n",
            "The event that brought Rachleff and Harrington together in Indianapolis was the United Teachers’ annual May Day march in support of Right to Work. “We have to take on the power that this bill represents and not be afraid or frightened of that power,” Rachleff tells me. “I’m not advocating that right to work is in the interest of any single worker, but it needs to be recognized as an issue for workers.”\n",
            "\n",
            "For Rachleff, the May Day march as a metaphor for labor organizing in general had some important lessons. “It does show that there are a lot of people out there for real labor organizing,” she says. “And that’s what the unions were built on—to be a community of people for real labor organizing.”\n",
            "\n",
            "It also taught she’d needed to learn more about organizing. “I was interested in why we didn’t make the push for public education,” Rachleff says, pointing to the work of Elizabeth Platt, who’s son was killed at the age of 17 in the Chicago public school system. “You can do it with public unionism.”\n",
            "\n",
            "Rachleff’s interest in labor unions also started when she was a high school girl, after getting a job at a local grocery store through her older brother. The experience of shopping in stores with no breaks and no timeouts had unsettled her. “I thought it was weird that a grocery store had a union,” she says. “I thought, ‘Oh, yeah.’ The way you do things here is completely different.’”\n",
            "\n",
            "Her interest grew further than there had been before. “In middle school,” she remembers thinking, “I started reading a lot more labor-union literature,” and “the more I read,” the more she became interested in what those organizations were trying to do. “They’re fighting for a fundamental value that we’ve had for all of our lives: To have the most basic rights of life and liberty.” In her senior year of high school, she joined United Steelworkers Local 1999 as a member of its bargaining team and eventually earned an M.B\n",
            "\n",
            "[600 | 1137.38] loss=2.41 avg=2.37\n",
            "[601 | 1139.05] loss=2.18 avg=2.37\n",
            "[602 | 1140.72] loss=2.21 avg=2.37\n",
            "[603 | 1142.39] loss=2.55 avg=2.37\n",
            "[604 | 1144.06] loss=2.77 avg=2.38\n",
            "[605 | 1145.72] loss=2.40 avg=2.38\n",
            "[606 | 1147.39] loss=2.56 avg=2.38\n",
            "[607 | 1149.05] loss=2.45 avg=2.38\n",
            "[608 | 1150.71] loss=2.60 avg=2.38\n",
            "[609 | 1152.37] loss=2.73 avg=2.38\n",
            "[610 | 1154.02] loss=2.00 avg=2.38\n",
            "[611 | 1155.68] loss=2.28 avg=2.38\n",
            "[612 | 1157.33] loss=2.64 avg=2.38\n",
            "[613 | 1158.98] loss=2.11 avg=2.38\n",
            "[614 | 1160.63] loss=2.58 avg=2.38\n",
            "[615 | 1162.28] loss=2.09 avg=2.38\n",
            "[616 | 1163.92] loss=2.51 avg=2.38\n",
            "[617 | 1165.56] loss=2.43 avg=2.38\n",
            "[618 | 1167.21] loss=2.38 avg=2.38\n",
            "[619 | 1168.86] loss=2.45 avg=2.38\n",
            "[620 | 1170.50] loss=2.27 avg=2.38\n",
            "[621 | 1172.13] loss=2.46 avg=2.38\n",
            "[622 | 1173.77] loss=1.85 avg=2.38\n",
            "[623 | 1175.41] loss=2.36 avg=2.38\n",
            "[624 | 1177.05] loss=2.40 avg=2.38\n",
            "[625 | 1178.68] loss=2.41 avg=2.38\n",
            "[626 | 1180.32] loss=2.32 avg=2.37\n",
            "[627 | 1181.95] loss=2.12 avg=2.37\n",
            "[628 | 1183.59] loss=2.33 avg=2.37\n",
            "[629 | 1185.22] loss=1.99 avg=2.37\n",
            "[630 | 1186.86] loss=2.43 avg=2.37\n",
            "[631 | 1188.50] loss=2.40 avg=2.37\n",
            "[632 | 1190.13] loss=2.59 avg=2.37\n",
            "[633 | 1191.77] loss=2.36 avg=2.37\n",
            "[634 | 1193.40] loss=2.37 avg=2.37\n",
            "[635 | 1195.04] loss=2.19 avg=2.37\n",
            "[636 | 1196.68] loss=2.24 avg=2.37\n",
            "[637 | 1198.32] loss=2.58 avg=2.37\n",
            "[638 | 1199.96] loss=2.69 avg=2.37\n",
            "[639 | 1201.61] loss=2.07 avg=2.37\n",
            "[640 | 1203.26] loss=1.84 avg=2.37\n",
            "[641 | 1204.90] loss=2.58 avg=2.37\n",
            "[642 | 1206.54] loss=2.63 avg=2.37\n",
            "[643 | 1208.20] loss=2.40 avg=2.37\n",
            "[644 | 1209.86] loss=2.17 avg=2.37\n",
            "[645 | 1211.52] loss=2.66 avg=2.37\n",
            "[646 | 1213.18] loss=2.12 avg=2.37\n",
            "[647 | 1214.85] loss=2.28 avg=2.37\n",
            "[648 | 1216.51] loss=2.32 avg=2.37\n",
            "[649 | 1218.19] loss=2.57 avg=2.37\n",
            "[650 | 1219.86] loss=2.13 avg=2.37\n",
            "[651 | 1221.53] loss=2.23 avg=2.37\n",
            "[652 | 1223.20] loss=2.51 avg=2.37\n",
            "[653 | 1224.87] loss=1.55 avg=2.36\n",
            "[654 | 1226.54] loss=2.20 avg=2.36\n",
            "[655 | 1228.21] loss=2.13 avg=2.35\n",
            "[656 | 1229.89] loss=2.15 avg=2.35\n",
            "[657 | 1231.56] loss=2.15 avg=2.35\n",
            "[658 | 1233.22] loss=2.65 avg=2.35\n",
            "[659 | 1234.90] loss=2.61 avg=2.36\n",
            "[660 | 1236.57] loss=2.35 avg=2.36\n",
            "[661 | 1238.24] loss=2.56 avg=2.36\n",
            "[662 | 1239.92] loss=2.28 avg=2.36\n",
            "[663 | 1241.59] loss=2.21 avg=2.36\n",
            "[664 | 1243.26] loss=2.46 avg=2.36\n",
            "[665 | 1244.92] loss=2.38 avg=2.36\n",
            "[666 | 1246.59] loss=2.63 avg=2.36\n",
            "[667 | 1248.25] loss=2.19 avg=2.36\n",
            "[668 | 1249.91] loss=2.12 avg=2.36\n",
            "[669 | 1251.57] loss=1.59 avg=2.35\n",
            "[670 | 1253.22] loss=1.96 avg=2.34\n",
            "[671 | 1254.87] loss=2.46 avg=2.35\n",
            "[672 | 1256.52] loss=2.55 avg=2.35\n",
            "[673 | 1258.17] loss=2.46 avg=2.35\n",
            "[674 | 1259.82] loss=2.55 avg=2.35\n",
            "[675 | 1261.46] loss=2.14 avg=2.35\n",
            "[676 | 1263.10] loss=1.96 avg=2.34\n",
            "[677 | 1264.75] loss=2.41 avg=2.35\n",
            "[678 | 1266.39] loss=2.36 avg=2.35\n",
            "[679 | 1268.03] loss=2.38 avg=2.35\n",
            "[680 | 1269.67] loss=2.21 avg=2.34\n",
            "[681 | 1271.31] loss=2.18 avg=2.34\n",
            "[682 | 1272.94] loss=2.73 avg=2.35\n",
            "[683 | 1274.58] loss=2.48 avg=2.35\n",
            "[684 | 1276.21] loss=2.41 avg=2.35\n",
            "[685 | 1277.84] loss=2.55 avg=2.35\n",
            "[686 | 1279.48] loss=2.09 avg=2.35\n",
            "[687 | 1281.11] loss=1.98 avg=2.34\n",
            "[688 | 1282.75] loss=2.20 avg=2.34\n",
            "[689 | 1284.38] loss=2.46 avg=2.34\n",
            "[690 | 1286.01] loss=2.23 avg=2.34\n",
            "[691 | 1287.65] loss=2.00 avg=2.34\n",
            "[692 | 1289.29] loss=1.41 avg=2.33\n",
            "[693 | 1290.92] loss=2.33 avg=2.33\n",
            "[694 | 1292.56] loss=2.05 avg=2.33\n",
            "[695 | 1294.20] loss=2.15 avg=2.33\n",
            "[696 | 1295.84] loss=1.96 avg=2.32\n",
            "[697 | 1297.49] loss=2.02 avg=2.32\n",
            "[698 | 1299.13] loss=2.55 avg=2.32\n",
            "[699 | 1300.78] loss=2.12 avg=2.32\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " down it. This may surprise some people, but if you've done some of the stuff that Trump says you should have done, you don't have immunity. He says this is a good story; what he hasn't said is exactly what you get when you have immunity and then you say nothing, which he did [in his deposition]. So basically, you get this weird situation where you do get a little something.\n",
            "\n",
            "It seems ridiculous to me that Trump would say this and he would not have said it to save face when everyone knows now what he meant. I wonder how much more outrageous is this? If Trump said he and his attorneys were willing to sign a waiver that would have been like a form letter to the jury. That's ridiculous! So he did this for something? I think it does have a certain sting to it.\n",
            "\n",
            "Advertisement\n",
            "\n",
            "\n",
            "There are several other things to add. First, this sounds like a standard defense strategy. Remember the lawyer who said, in a TV special, that he didn't know that Trump would never admit to being, let's say, a felon? He was worried, right? He was the attorney representing Donald Trump on a fraud suit, and suddenly he has to talk down the seriousness of the crime.\n",
            "\n",
            "This is similar. I know what it's like to be a lawyer for Trump and to be told that you'd have to sign an affidavit, and you need to sign—that is not your attorney saying, \"Well, here are some details you need to go over. I need you to say which one of the details is relevant.\" The lawyer for Trump is saying to him, and to others, \"Here's a lot of detail to go over and you need to say it. Here are certain words on this page. Here are certain details about this property. Here are certain things about this thing that really matter to you. Sign.\" He wants you to sign something, and what if you refuse to do it? What about other items?\n",
            "\n",
            "The other thing that strikes me is the idea that Trump didn't have immunity and this happened to him. This is the idea that Trump is somehow different from all his predecessors in a few ways, and that somehow, all of this was all his fault and everything.\n",
            "\n",
            "And if you believe that, then the president is in jeopardy. I don't know whether he is. I do know that if Trump had had immunity, or if this had happened to him a lot less frequently, or to others, or if someone had said, \"Okay, we actually have Trump National Golf, here's our waiver, and we will be using your legal representation. We will be arguing to defend Trump against this, with your help, in court—that would be much, much more likely. Trump would be in the position of someone like Obama or Nixon that was trying to convince the government of a crime he had nothing to do with. This is what he should have done instead of saying nothing. That seems so unfair.\n",
            "\n",
            "Second, if Trump had been in the position to say what he said, he might have said something else.\n",
            "\n",
            "Advertisement\n",
            "\n",
            "\n",
            "That is a little strange. I think it would have been an example for another lawyer to look at: This is the situation you face, and you just say something else.\n",
            "\n",
            "Third, does Trump understand this? And then, at the same time, does Trump understand why he does this thing and is he protecting his reputation?\n",
            "\n",
            "That question seems to have fallen into Trump's lap. I asked him about it; he looked very confused. I asked another lawyer who knew him very well, who knew his background, who knows that there were situations they were used to dealing with and how to deal with them.\n",
            "\n",
            "He said, \"Yeah, absolutely. And I could have handled it differently.\"\n",
            "\n",
            "So he said, \"I have to protect my reputation. And that is—I didn't say what I said. I regret it. I thought that there was no other way to do it. But I had to do it. And I had to do it quick. And I had to do it. That was the right thing to do.\"\n",
            "\n",
            "This may be a way of making a point that if you are trying to do something you have to do it immediately. You can't have some lawyer sitting behind you, just saying, \"Okay, here is what you have to do. And I am going to do my job. And I am going to do that—that is what I am going to do. We are going to go ahead right now with our arguments and we will be ready when you have decided, and we are going to ask a judge for an order for the attorney-client privilege. And the court will say, 'Of course, Donald Trump didn't, by the way, consent to it, so you have to have that privilege.' And you go right on with your life. Do you understand what I mean?\" [Laughs.]\n",
            "\n",
            "\n",
            "\n",
            "[700 | 1325.37] loss=2.30 avg=2.32\n",
            "[701 | 1327.04] loss=2.47 avg=2.32\n",
            "[702 | 1328.71] loss=2.46 avg=2.32\n",
            "[703 | 1330.38] loss=2.09 avg=2.32\n",
            "[704 | 1332.04] loss=1.91 avg=2.32\n",
            "[705 | 1333.72] loss=2.12 avg=2.31\n",
            "[706 | 1335.39] loss=1.88 avg=2.31\n",
            "[707 | 1337.06] loss=2.46 avg=2.31\n",
            "[708 | 1338.73] loss=2.75 avg=2.32\n",
            "[709 | 1340.40] loss=2.17 avg=2.31\n",
            "[710 | 1342.07] loss=2.05 avg=2.31\n",
            "[711 | 1343.74] loss=2.41 avg=2.31\n",
            "[712 | 1345.41] loss=2.36 avg=2.31\n",
            "[713 | 1347.08] loss=2.28 avg=2.31\n",
            "[714 | 1348.74] loss=2.04 avg=2.31\n",
            "[715 | 1350.40] loss=2.76 avg=2.31\n",
            "[716 | 1352.06] loss=2.83 avg=2.32\n",
            "[717 | 1353.72] loss=1.96 avg=2.32\n",
            "[718 | 1355.38] loss=2.25 avg=2.31\n",
            "[719 | 1357.04] loss=2.39 avg=2.32\n",
            "[720 | 1358.69] loss=2.17 avg=2.31\n",
            "[721 | 1360.35] loss=1.93 avg=2.31\n",
            "[722 | 1362.00] loss=2.41 avg=2.31\n",
            "[723 | 1363.65] loss=2.55 avg=2.31\n",
            "[724 | 1365.30] loss=2.39 avg=2.31\n",
            "[725 | 1366.95] loss=2.14 avg=2.31\n",
            "[726 | 1368.59] loss=2.40 avg=2.31\n",
            "[727 | 1370.24] loss=2.42 avg=2.31\n",
            "[728 | 1371.88] loss=2.34 avg=2.31\n",
            "[729 | 1373.52] loss=2.16 avg=2.31\n",
            "[730 | 1375.16] loss=1.70 avg=2.31\n",
            "[731 | 1376.80] loss=2.24 avg=2.31\n",
            "[732 | 1378.44] loss=2.48 avg=2.31\n",
            "[733 | 1380.08] loss=2.71 avg=2.31\n",
            "[734 | 1381.72] loss=2.84 avg=2.32\n",
            "[735 | 1383.36] loss=1.84 avg=2.31\n",
            "[736 | 1384.98] loss=2.18 avg=2.31\n",
            "[737 | 1386.62] loss=2.05 avg=2.31\n",
            "[738 | 1388.26] loss=2.03 avg=2.31\n",
            "[739 | 1389.89] loss=2.30 avg=2.31\n",
            "[740 | 1391.52] loss=2.01 avg=2.30\n",
            "[741 | 1393.15] loss=2.08 avg=2.30\n",
            "[742 | 1394.79] loss=1.62 avg=2.29\n",
            "[743 | 1396.42] loss=2.37 avg=2.29\n",
            "[744 | 1398.06] loss=2.33 avg=2.29\n",
            "[745 | 1399.70] loss=2.08 avg=2.29\n",
            "[746 | 1401.34] loss=2.24 avg=2.29\n",
            "[747 | 1402.98] loss=2.45 avg=2.29\n",
            "[748 | 1404.62] loss=1.32 avg=2.28\n",
            "[749 | 1406.26] loss=2.25 avg=2.28\n",
            "[750 | 1407.91] loss=2.30 avg=2.28\n",
            "[751 | 1409.56] loss=2.28 avg=2.28\n",
            "[752 | 1411.21] loss=2.47 avg=2.29\n",
            "[753 | 1412.87] loss=2.16 avg=2.28\n",
            "[754 | 1414.53] loss=2.09 avg=2.28\n",
            "[755 | 1416.18] loss=2.39 avg=2.28\n",
            "[756 | 1417.85] loss=2.56 avg=2.29\n",
            "[757 | 1419.52] loss=2.18 avg=2.29\n",
            "[758 | 1421.18] loss=2.54 avg=2.29\n",
            "[759 | 1422.85] loss=2.19 avg=2.29\n",
            "[760 | 1424.53] loss=2.78 avg=2.29\n",
            "[761 | 1426.20] loss=2.37 avg=2.29\n",
            "[762 | 1427.88] loss=2.05 avg=2.29\n",
            "[763 | 1429.56] loss=2.37 avg=2.29\n",
            "[764 | 1431.24] loss=2.70 avg=2.30\n",
            "[765 | 1432.91] loss=2.03 avg=2.29\n",
            "[766 | 1434.58] loss=2.05 avg=2.29\n",
            "[767 | 1436.25] loss=2.41 avg=2.29\n",
            "[768 | 1437.93] loss=2.24 avg=2.29\n",
            "[769 | 1439.60] loss=2.42 avg=2.29\n",
            "[770 | 1441.28] loss=2.32 avg=2.29\n",
            "[771 | 1442.95] loss=2.67 avg=2.30\n",
            "[772 | 1444.62] loss=2.65 avg=2.30\n",
            "[773 | 1446.29] loss=2.40 avg=2.30\n",
            "[774 | 1447.95] loss=2.10 avg=2.30\n",
            "[775 | 1449.62] loss=1.63 avg=2.29\n",
            "[776 | 1451.29] loss=2.19 avg=2.29\n",
            "[777 | 1452.95] loss=2.16 avg=2.29\n",
            "[778 | 1454.62] loss=2.32 avg=2.29\n",
            "[779 | 1456.27] loss=2.45 avg=2.29\n",
            "[780 | 1457.93] loss=2.23 avg=2.29\n",
            "[781 | 1459.59] loss=2.26 avg=2.29\n",
            "[782 | 1461.23] loss=2.42 avg=2.29\n",
            "[783 | 1462.89] loss=2.63 avg=2.30\n",
            "[784 | 1464.53] loss=2.31 avg=2.30\n",
            "[785 | 1466.17] loss=2.32 avg=2.30\n",
            "[786 | 1467.82] loss=2.14 avg=2.29\n",
            "[787 | 1469.46] loss=2.05 avg=2.29\n",
            "[788 | 1471.10] loss=1.24 avg=2.28\n",
            "[789 | 1472.73] loss=2.34 avg=2.28\n",
            "[790 | 1474.37] loss=2.37 avg=2.28\n",
            "[791 | 1476.00] loss=2.38 avg=2.28\n",
            "[792 | 1477.64] loss=2.09 avg=2.28\n",
            "[793 | 1479.27] loss=2.60 avg=2.28\n",
            "[794 | 1480.91] loss=2.02 avg=2.28\n",
            "[795 | 1482.54] loss=2.22 avg=2.28\n",
            "[796 | 1484.17] loss=1.95 avg=2.28\n",
            "[797 | 1485.81] loss=1.98 avg=2.27\n",
            "[798 | 1487.44] loss=2.31 avg=2.28\n",
            "[799 | 1489.08] loss=2.08 avg=2.27\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " parties, as well as a Republican majority on the House. In the two years since the first Democratic president, Bill Clinton, created the Consumer Financial Protection Bureau, Republicans have tried to dismantle its work and replace it with a much-less-effective, more expensive entity. Trump won't stop at undoing the CFPB, since Democrats won the House majority without the votes to overturn an existing law.\n",
            "\n",
            "This makes it all the more important to understand how the GOP is attempting to roll back the Obama-era rules—what the new rules are, what they look like, and how they work. It doesn't work this way: The goal is to create a system under which the government takes on fewer and fewer people to serve fewer and fewer people.\n",
            "\n",
            "The new rules will go into effect on April 26. Here, we take a closer look, using data from the Office of Government Ethics and its website and interviews with senior agency staffers.\n",
            "\n",
            "How did the new Trump-era rules come about? What do they do?\n",
            "\n",
            "The new rules replace, and reform regulations, Obama-era regulations that have expired. They are the culmination of several years of work by agencies, both in Congress and at the White House.\n",
            "\n",
            "What are the current rules?\n",
            "\n",
            "The rules establish new requirements and processes for responding to complaints to the Office, and for enforcing the law. They:\n",
            "\n",
            "Restrict the ability of banks to merge.\n",
            "\n",
            "Restrict mortgage-finance products sold by insurers.\n",
            "\n",
            "Require that banks provide consumer-advocacy information about their products and services to customers. (See our explainer.)\n",
            "\n",
            "Require that financial institutions offer customers a refundable, electronic transaction fee.\n",
            "\n",
            "Require companies to provide consumers with a \"consumer rights report.\"\n",
            "\n",
            "Require that financial institutions report the number and types of customer complaints.\n",
            "\n",
            "Restrict the ability of credit unions to make fees.\n",
            "\n",
            "Ensure that the Consumer Financial Protection Bureau has the resources it needs to fight fraud. (See here for more.)\n",
            "\n",
            "What are the new Obama-era requirements?\n",
            "\n",
            "The new rules are:\n",
            "\n",
            "Restrictions on banks. Banks—including those that were initially authorized for regulatory relief by Congress but were later suspended or were blocked by the new Congress—must establish and maintain the requirements to meet the new requirements. Borrowers for mortgage-backed securities are exempt from the rules. And credit unions (including those authorized to be subject to the new rules) may not:\n",
            "\n",
            "Make new consumer-credit-reporting, loan-writing, or settlement services available.\n",
            "\n",
            "Pay fees or charges directly to employees.\n",
            "\n",
            "Receive contributions through contributions to political-action committees.\n",
            "\n",
            "Receive direct contributions from financial institutions. (Banks have accepted contributions from Wall Street firms, such as JP Morgan Chase & Co., JP Morgan Securities LLC, and HSBC Holdings PLC, and the insurers AIG AG, Allianz AG, and UBS AG, since the OMB stopped issuing temporary waivers in June.)\n",
            "\n",
            "Repudiate existing consumer-protection requirements. The new rules establish a new process for initiating regulatory actions, known as administrative rulemaking, through which agencies develop regulations to implement the new rules. Before the rules were finalized, the Office received over 12,000 comments on the proposed regulations. Most of the comments raised concerns related to the consumer-protection rules, such as:\n",
            "\n",
            "How can a company or individual benefit from a payday loan?\n",
            "\n",
            "Is it lawful for lenders to charge extra fees for payday loans?\n",
            "\n",
            "What other types of loans are covered by the rules?\n",
            "\n",
            "Some of the changes in the rulemaking process are explained in the OGE report:\n",
            "\n",
            "How do the new rules make it easier for individuals to get their financial data removed from financial-reporting.\n",
            "\n",
            "Who gets to review the personal information of financial-reporting institutions, and how.\n",
            "\n",
            "Does the OGE have the authority to remove personal information from financial-reporting institutions without a court order?\n",
            "\n",
            "Why are there so many exemptions listed in the rule?\n",
            "\n",
            "What information can financial-reporting institutions share?\n",
            "\n",
            "How does the OGE share data? (Why were they not required to post data on the old rules? How do they share data under the new rule? What other sharing is required?)\n",
            "\n",
            "Does the OGE have the authority to share the names of people who are excluded from financial-reporting accounts?\n",
            "\n",
            "What data is covered by the rules?\n",
            "\n",
            "Is the data shared with third parties? (We learned that the OGE can share, but the OGE has not yet done so.)\n",
            "\n",
            "Is a person eligible for a financial-reporting exemption?\n",
            "\n",
            "What other regulations are on the books regarding financial-reporting disclosure?\n",
            "\n",
            "The new regulations do not require federal law-enforcement to enforce financial-reporting information. (The new requirements also do not require law-enforcement to report individual-reporting information to the OGE — a\n",
            "\n",
            "[800 | 1513.67] loss=1.53 avg=2.27\n",
            "[801 | 1515.34] loss=2.30 avg=2.27\n",
            "[802 | 1517.02] loss=2.09 avg=2.26\n",
            "[803 | 1518.69] loss=2.35 avg=2.27\n",
            "[804 | 1520.37] loss=2.01 avg=2.26\n",
            "[805 | 1522.04] loss=2.44 avg=2.26\n",
            "[806 | 1523.71] loss=2.13 avg=2.26\n",
            "[807 | 1525.40] loss=2.38 avg=2.26\n",
            "[808 | 1527.07] loss=2.46 avg=2.27\n",
            "[809 | 1528.76] loss=2.01 avg=2.26\n",
            "[810 | 1530.43] loss=2.37 avg=2.26\n",
            "[811 | 1532.10] loss=1.93 avg=2.26\n",
            "[812 | 1533.79] loss=1.92 avg=2.26\n",
            "[813 | 1535.46] loss=2.49 avg=2.26\n",
            "[814 | 1537.13] loss=2.40 avg=2.26\n",
            "[815 | 1538.80] loss=2.13 avg=2.26\n",
            "[816 | 1540.47] loss=2.13 avg=2.26\n",
            "[817 | 1542.14] loss=2.30 avg=2.26\n",
            "[818 | 1543.81] loss=1.98 avg=2.26\n",
            "[819 | 1545.47] loss=2.32 avg=2.26\n",
            "[820 | 1547.14] loss=2.07 avg=2.26\n",
            "[821 | 1548.80] loss=1.91 avg=2.25\n",
            "[822 | 1550.47] loss=2.19 avg=2.25\n",
            "[823 | 1552.13] loss=2.75 avg=2.26\n",
            "[824 | 1553.79] loss=2.10 avg=2.25\n",
            "[825 | 1555.45] loss=2.22 avg=2.25\n",
            "[826 | 1557.10] loss=1.91 avg=2.25\n",
            "[827 | 1558.74] loss=1.97 avg=2.25\n",
            "[828 | 1560.39] loss=2.20 avg=2.25\n",
            "[829 | 1562.04] loss=2.39 avg=2.25\n",
            "[830 | 1563.68] loss=2.21 avg=2.25\n",
            "[831 | 1565.33] loss=2.14 avg=2.25\n",
            "[832 | 1566.97] loss=2.41 avg=2.25\n",
            "[833 | 1568.62] loss=1.89 avg=2.25\n",
            "[834 | 1570.27] loss=2.12 avg=2.24\n",
            "[835 | 1571.91] loss=2.08 avg=2.24\n",
            "[836 | 1573.54] loss=2.45 avg=2.24\n",
            "[837 | 1575.18] loss=2.31 avg=2.25\n",
            "[838 | 1576.81] loss=2.06 avg=2.24\n",
            "[839 | 1578.44] loss=1.75 avg=2.24\n",
            "[840 | 1580.08] loss=2.63 avg=2.24\n",
            "[841 | 1581.72] loss=1.90 avg=2.24\n",
            "[842 | 1583.35] loss=2.59 avg=2.24\n",
            "[843 | 1585.00] loss=2.75 avg=2.25\n",
            "[844 | 1586.63] loss=2.11 avg=2.25\n",
            "[845 | 1588.27] loss=2.57 avg=2.25\n",
            "[846 | 1589.91] loss=1.71 avg=2.24\n",
            "[847 | 1591.55] loss=2.53 avg=2.25\n",
            "[848 | 1593.19] loss=1.98 avg=2.24\n",
            "[849 | 1594.84] loss=1.98 avg=2.24\n",
            "[850 | 1596.49] loss=2.32 avg=2.24\n",
            "[851 | 1598.13] loss=2.11 avg=2.24\n",
            "[852 | 1599.77] loss=2.48 avg=2.24\n",
            "[853 | 1601.41] loss=2.33 avg=2.24\n",
            "[854 | 1603.06] loss=2.04 avg=2.24\n",
            "[855 | 1604.72] loss=2.33 avg=2.24\n",
            "[856 | 1606.37] loss=2.29 avg=2.24\n",
            "[857 | 1608.04] loss=1.91 avg=2.24\n",
            "[858 | 1609.70] loss=1.96 avg=2.24\n",
            "[859 | 1611.36] loss=2.17 avg=2.24\n",
            "[860 | 1613.02] loss=2.37 avg=2.24\n",
            "[861 | 1614.68] loss=2.20 avg=2.24\n",
            "[862 | 1616.35] loss=2.52 avg=2.24\n",
            "[863 | 1618.03] loss=2.10 avg=2.24\n",
            "[864 | 1619.71] loss=2.11 avg=2.24\n",
            "[865 | 1621.38] loss=2.48 avg=2.24\n",
            "[866 | 1623.06] loss=1.06 avg=2.23\n",
            "[867 | 1624.73] loss=2.47 avg=2.23\n",
            "[868 | 1626.42] loss=2.39 avg=2.23\n",
            "[869 | 1628.09] loss=2.30 avg=2.23\n",
            "[870 | 1629.78] loss=2.31 avg=2.23\n",
            "[871 | 1631.45] loss=2.18 avg=2.23\n",
            "[872 | 1633.12] loss=1.87 avg=2.23\n",
            "[873 | 1634.81] loss=2.46 avg=2.23\n",
            "[874 | 1636.48] loss=2.24 avg=2.23\n",
            "[875 | 1638.16] loss=2.54 avg=2.24\n",
            "[876 | 1639.83] loss=1.42 avg=2.23\n",
            "[877 | 1641.50] loss=2.08 avg=2.23\n",
            "[878 | 1643.17] loss=1.72 avg=2.22\n",
            "[879 | 1644.84] loss=1.69 avg=2.22\n",
            "[880 | 1646.51] loss=2.21 avg=2.22\n",
            "[881 | 1648.18] loss=2.08 avg=2.21\n",
            "[882 | 1649.84] loss=2.07 avg=2.21\n",
            "[883 | 1651.50] loss=2.53 avg=2.22\n",
            "[884 | 1653.15] loss=2.03 avg=2.21\n",
            "[885 | 1654.81] loss=2.32 avg=2.21\n",
            "[886 | 1656.46] loss=1.81 avg=2.21\n",
            "[887 | 1658.11] loss=2.24 avg=2.21\n",
            "[888 | 1659.76] loss=2.58 avg=2.21\n",
            "[889 | 1661.41] loss=2.75 avg=2.22\n",
            "[890 | 1663.05] loss=1.83 avg=2.22\n",
            "[891 | 1664.69] loss=2.10 avg=2.22\n",
            "[892 | 1666.34] loss=2.25 avg=2.22\n",
            "[893 | 1667.98] loss=2.34 avg=2.22\n",
            "[894 | 1669.61] loss=2.10 avg=2.22\n",
            "[895 | 1671.25] loss=2.04 avg=2.21\n",
            "[896 | 1672.89] loss=2.73 avg=2.22\n",
            "[897 | 1674.53] loss=2.23 avg=2.22\n",
            "[898 | 1676.17] loss=2.58 avg=2.22\n",
            "[899 | 1677.80] loss=2.20 avg=2.22\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " possible? Well, let me introduce ourselves to you, Donald Trump. You know something about being president, though.” He paused. I could see the nervousness in his voice. I asked him if he would do something that the press was not asking him to do. “Well, I will apologize if I say something like that.”\n",
            "It isn―t just Trump who thinks this way—former President Clinton also worries that the press may be looking for a “one-on-one” exchange with Trump. “It would certainly be much more politically toxic to him.” I asked him if the press was in any way looking to elect Donald Trump. He hesitated for a moment and then said, “Well, I think so. Maybe. But it’s also true that the press is always looking for an easy-peasy thing to say and I am a one-on-one dealer, so, yeah, we want an easy-peasy thing to say.”\n",
            "“But the real problem, though, is that we have a president who is so divisive,” I told him. He shook his head. “I think it’s more political. He’s been in office too long to be really worried about that.”\n",
            "“But the thing is—isn’t that the greatest threat to him is—you know, the whole birther thing? What does it do to me to say, ‘Oh, it would make us elect Donald Trump?’ It would do that, but it doesn’t do anything to Trump himself.”\n",
            "We were discussing something else, he told me, and we both agreed, but the reality is that there are other things facing President Trump that are very real and that he has had to deal with, and that he cannot seem to recover from, without which he is barely functioning.\n",
            "“What is so dangerous about the nature of his presidency is the fact that he has an insane, unstable, and unpredictable personality.”\n",
            "“What is really risky, and I believe what really needs to happen, if we are to restore the republic, is to rebuild confidence in government.”\n",
            "That was Trump, in essence, arguing that it wasn’t the press trying to elect Trump, but the presidency it was taking hold of. “Let’s say you had a president like Mike Pence—let’s say the press were really asking for a one-on-one exchange. That would be a disaster. All you would have to do is have the ability to get into a room and say what you want, which I can’t do now. You wouldn’t even be able to say the dumbest thing.”\n",
            "What if instead we had an unassuming, mild—if unpolite—candidate, who came along and began to talk about things for which, to the extent this journalist had a policy agenda? What if we had this person in the White House and Congress whom we could turn into a policy instrument and turn into a populist-populist, even the president of the United States? What if we had this candidate who is so volatile and unpredictable that the presidency is no longer a matter of if, it is always a question of when—and that’s a big thing? What would it do to restore confidence?\n",
            "“Well, I mean, how much of what is said in the press is nonsense, you might ask? Let me ask you: Do you know why a guy like Donald Trump gets so much praise? You know, because he has a good temperament. You know, because his demeanor is so good. But there are also certain kinds of things that are just totally crazy.”\n",
            "“I did not say you’ve got to elect Donald Trump, but you need to restore a constitutional republic and you need a president who is not a crackpot.”\n",
            "I knew what he was getting at. This man seemed to genuinely believe that I could take him on.\n",
            "What I wanted to know, I also wanted to know what he meant when he said, “I don’t think [our] democracy is as safe as people believe.”\n",
            "“Absolutely not,” he shot back. 'I don’t think it’s nearly as risky as, say, Italy, or Cuba, or Japan, or Russia.”\n",
            "It felt a bit like saying the U.S. has enough guns as it is.\n",
            "“Well, I think the U.S. has too many guns to protect its citizens.” As the conversation dragged on, I realized he really meant, I needed more weapons. He seemed to want to have me take the U.S.—or at least the United States as he intended—to war with him.\n",
            "�\n",
            "\n",
            "[900 | 1702.11] loss=2.21 avg=2.22\n",
            "[901 | 1703.77] loss=1.89 avg=2.22\n",
            "[902 | 1705.43] loss=2.50 avg=2.22\n",
            "[903 | 1707.08] loss=2.27 avg=2.22\n",
            "[904 | 1708.75] loss=2.36 avg=2.22\n",
            "[905 | 1710.40] loss=2.30 avg=2.22\n",
            "[906 | 1712.08] loss=2.45 avg=2.23\n",
            "[907 | 1713.75] loss=2.41 avg=2.23\n",
            "[908 | 1715.43] loss=1.92 avg=2.23\n",
            "[909 | 1717.10] loss=2.56 avg=2.23\n",
            "[910 | 1718.78] loss=2.21 avg=2.23\n",
            "[911 | 1720.45] loss=2.43 avg=2.23\n",
            "[912 | 1722.13] loss=1.87 avg=2.23\n",
            "[913 | 1723.79] loss=1.80 avg=2.22\n",
            "[914 | 1725.47] loss=1.98 avg=2.22\n",
            "[915 | 1727.16] loss=1.94 avg=2.22\n",
            "[916 | 1728.83] loss=2.01 avg=2.22\n",
            "[917 | 1730.50] loss=2.47 avg=2.22\n",
            "[918 | 1732.20] loss=2.84 avg=2.22\n",
            "[919 | 1733.88] loss=2.15 avg=2.22\n",
            "[920 | 1735.57] loss=2.13 avg=2.22\n",
            "[921 | 1737.26] loss=2.27 avg=2.22\n",
            "[922 | 1738.94] loss=2.04 avg=2.22\n",
            "[923 | 1740.63] loss=2.37 avg=2.22\n",
            "[924 | 1742.30] loss=2.04 avg=2.22\n",
            "[925 | 1743.97] loss=2.37 avg=2.22\n",
            "[926 | 1745.64] loss=1.75 avg=2.22\n",
            "[927 | 1747.31] loss=2.11 avg=2.22\n",
            "[928 | 1748.98] loss=2.14 avg=2.22\n",
            "[929 | 1750.65] loss=2.08 avg=2.21\n",
            "[930 | 1752.31] loss=1.59 avg=2.21\n",
            "[931 | 1753.98] loss=2.11 avg=2.21\n",
            "[932 | 1755.64] loss=1.74 avg=2.20\n",
            "[933 | 1757.30] loss=2.31 avg=2.20\n",
            "[934 | 1758.96] loss=2.26 avg=2.20\n",
            "[935 | 1760.61] loss=1.98 avg=2.20\n",
            "[936 | 1762.27] loss=1.99 avg=2.20\n",
            "[937 | 1763.92] loss=1.47 avg=2.19\n",
            "[938 | 1765.57] loss=2.24 avg=2.19\n",
            "[939 | 1767.22] loss=2.32 avg=2.19\n",
            "[940 | 1768.87] loss=2.27 avg=2.20\n",
            "[941 | 1770.51] loss=2.65 avg=2.20\n",
            "[942 | 1772.15] loss=1.80 avg=2.20\n",
            "[943 | 1773.78] loss=2.20 avg=2.20\n",
            "[944 | 1775.42] loss=2.25 avg=2.20\n",
            "[945 | 1777.06] loss=1.15 avg=2.19\n",
            "[946 | 1778.70] loss=1.95 avg=2.18\n",
            "[947 | 1780.34] loss=2.10 avg=2.18\n",
            "[948 | 1781.98] loss=2.02 avg=2.18\n",
            "[949 | 1783.61] loss=1.15 avg=2.17\n",
            "[950 | 1785.25] loss=2.62 avg=2.18\n",
            "[951 | 1786.89] loss=2.04 avg=2.17\n",
            "[952 | 1788.53] loss=1.41 avg=2.17\n",
            "[953 | 1790.17] loss=0.87 avg=2.15\n",
            "[954 | 1791.81] loss=2.09 avg=2.15\n",
            "[955 | 1793.45] loss=1.79 avg=2.15\n",
            "[956 | 1795.09] loss=2.43 avg=2.15\n",
            "[957 | 1796.74] loss=1.91 avg=2.15\n",
            "[958 | 1798.38] loss=1.79 avg=2.15\n",
            "[959 | 1800.03] loss=2.12 avg=2.15\n",
            "[960 | 1801.68] loss=2.74 avg=2.15\n",
            "[961 | 1803.33] loss=2.57 avg=2.16\n",
            "[962 | 1804.99] loss=2.08 avg=2.16\n",
            "[963 | 1806.64] loss=2.41 avg=2.16\n",
            "[964 | 1808.31] loss=1.61 avg=2.15\n",
            "[965 | 1809.98] loss=2.04 avg=2.15\n",
            "[966 | 1811.65] loss=2.17 avg=2.15\n",
            "[967 | 1813.32] loss=1.96 avg=2.15\n",
            "[968 | 1815.00] loss=2.21 avg=2.15\n",
            "[969 | 1816.66] loss=2.43 avg=2.15\n",
            "[970 | 1818.34] loss=2.39 avg=2.16\n",
            "[971 | 1820.01] loss=2.20 avg=2.16\n",
            "[972 | 1821.69] loss=2.25 avg=2.16\n",
            "[973 | 1823.37] loss=2.18 avg=2.16\n",
            "[974 | 1825.06] loss=2.08 avg=2.16\n",
            "[975 | 1826.75] loss=2.11 avg=2.16\n",
            "[976 | 1828.43] loss=2.16 avg=2.16\n",
            "[977 | 1830.12] loss=1.66 avg=2.15\n",
            "[978 | 1831.81] loss=1.78 avg=2.15\n",
            "[979 | 1833.50] loss=2.03 avg=2.15\n",
            "[980 | 1835.17] loss=2.04 avg=2.14\n",
            "[981 | 1836.86] loss=1.69 avg=2.14\n",
            "[982 | 1838.54] loss=2.15 avg=2.14\n",
            "[983 | 1840.21] loss=2.42 avg=2.14\n",
            "[984 | 1841.88] loss=1.86 avg=2.14\n",
            "[985 | 1843.55] loss=1.71 avg=2.14\n",
            "[986 | 1845.23] loss=2.14 avg=2.14\n",
            "[987 | 1846.90] loss=1.92 avg=2.13\n",
            "[988 | 1848.57] loss=2.04 avg=2.13\n",
            "[989 | 1850.22] loss=1.82 avg=2.13\n",
            "[990 | 1851.89] loss=2.23 avg=2.13\n",
            "[991 | 1853.55] loss=2.17 avg=2.13\n",
            "[992 | 1855.21] loss=2.08 avg=2.13\n",
            "[993 | 1856.86] loss=1.95 avg=2.13\n",
            "[994 | 1858.52] loss=2.09 avg=2.13\n",
            "[995 | 1860.17] loss=1.91 avg=2.13\n",
            "[996 | 1861.82] loss=1.87 avg=2.12\n",
            "[997 | 1863.47] loss=2.08 avg=2.12\n",
            "[998 | 1865.12] loss=1.51 avg=2.12\n",
            "[999 | 1866.76] loss=2.48 avg=2.12\n",
            "Ran 1010 steps.\n",
            "Saving checkpoint/atlantic_politics_345/model-1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRUjEIu68Url",
        "colab_type": "code",
        "outputId": "32add75d-2a2d-4f48-d4e5-cdb99d9ead18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "## entertainment essays training - with 345\n",
        "!PYTHONPATH=src ./train.py --dataset /content/all_entertainment.txt --run_name 'atlantic_entertainment_345' --model_name '345M'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0627 18:26:38.688606 139911907395456 deprecation_wrapper.py:119] From /content/gpt-2/src/model.py:147: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
            "\n",
            "W0627 18:26:38.697175 139911907395456 deprecation_wrapper.py:119] From /content/gpt-2/src/memory_saving_gradients.py:13: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n",
            "W0627 18:26:38.784096 139911907395456 deprecation_wrapper.py:119] From ./train.py:88: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "W0627 18:26:38.784461 139911907395456 deprecation_wrapper.py:119] From ./train.py:91: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "2019-06-27 18:26:38.790568: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-06-27 18:26:38.790825: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1361100 executing computations on platform Host. Devices:\n",
            "2019-06-27 18:26:38.790856: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2019-06-27 18:26:38.793267: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\n",
            "2019-06-27 18:26:38.955578: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-27 18:26:38.956106: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1360840 executing computations on platform CUDA. Devices:\n",
            "2019-06-27 18:26:38.956136: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2019-06-27 18:26:38.956444: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-27 18:26:38.956848: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-06-27 18:26:38.957199: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-06-27 18:26:38.958551: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-06-27 18:26:38.959848: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-06-27 18:26:38.960267: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-06-27 18:26:38.961798: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-06-27 18:26:38.962914: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-06-27 18:26:38.966249: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-06-27 18:26:38.966416: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-27 18:26:38.966830: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-27 18:26:38.967180: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
            "2019-06-27 18:26:38.967256: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-06-27 18:26:38.968203: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-06-27 18:26:38.968230: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n",
            "2019-06-27 18:26:38.968241: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n",
            "2019-06-27 18:26:38.968579: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-27 18:26:38.968979: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-27 18:26:38.969365: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14202 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "W0627 18:26:38.970214 139911907395456 deprecation_wrapper.py:119] From ./train.py:92: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0627 18:26:49.891818 139911907395456 deprecation.py:323] From /content/gpt-2/src/sample.py:65: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "W0627 18:26:49.906155 139911907395456 deprecation.py:323] From /content/gpt-2/src/sample.py:16: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0627 18:26:49.907894 139911907395456 deprecation.py:323] From /content/gpt-2/src/sample.py:70: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.random.categorical` instead.\n",
            "W0627 18:26:49.917933 139911907395456 deprecation_wrapper.py:119] From ./train.py:121: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "W0627 18:27:05.152435 139911907395456 deprecation_wrapper.py:119] From ./train.py:144: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n",
            "W0627 18:27:05.155467 139911907395456 deprecation_wrapper.py:119] From ./train.py:147: The name tf.summary.merge is deprecated. Please use tf.compat.v1.summary.merge instead.\n",
            "\n",
            "W0627 18:27:05.156273 139911907395456 deprecation_wrapper.py:119] From ./train.py:149: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
            "\n",
            "W0627 18:27:05.157048 139911907395456 deprecation_wrapper.py:119] From ./train.py:152: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "Loading checkpoint models/345M/model.ckpt\n",
            "W0627 18:27:18.427690 139911907395456 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "Loading dataset...\n",
            "100% 1/1 [00:03<00:00,  3.30s/it]\n",
            "dataset has 407161 tokens\n",
            "Training...\n",
            "2019-06-27 18:27:35.960018: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n",
            "2019-06-27 18:27:36.775019: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "[1 | 14.42] loss=3.19 avg=3.19\n",
            "[2 | 15.94] loss=3.31 avg=3.25\n",
            "[3 | 17.47] loss=3.12 avg=3.21\n",
            "[4 | 19.00] loss=3.21 avg=3.21\n",
            "[5 | 20.54] loss=2.95 avg=3.16\n",
            "[6 | 22.08] loss=3.25 avg=3.17\n",
            "[7 | 23.61] loss=2.92 avg=3.13\n",
            "[8 | 25.14] loss=3.09 avg=3.13\n",
            "[9 | 26.68] loss=3.25 avg=3.14\n",
            "[10 | 28.23] loss=2.72 avg=3.10\n",
            "[11 | 29.77] loss=3.23 avg=3.11\n",
            "[12 | 31.31] loss=3.33 avg=3.13\n",
            "[13 | 32.86] loss=2.84 avg=3.11\n",
            "[14 | 34.40] loss=2.75 avg=3.08\n",
            "[15 | 35.94] loss=3.34 avg=3.10\n",
            "[16 | 37.49] loss=3.41 avg=3.12\n",
            "[17 | 39.05] loss=3.11 avg=3.12\n",
            "[18 | 40.60] loss=2.98 avg=3.11\n",
            "[19 | 42.14] loss=2.72 avg=3.09\n",
            "[20 | 43.69] loss=3.12 avg=3.09\n",
            "[21 | 45.24] loss=3.11 avg=3.09\n",
            "[22 | 46.80] loss=2.85 avg=3.08\n",
            "[23 | 48.36] loss=2.65 avg=3.06\n",
            "[24 | 49.92] loss=3.23 avg=3.07\n",
            "[25 | 51.47] loss=3.24 avg=3.07\n",
            "[26 | 53.03] loss=3.11 avg=3.08\n",
            "[27 | 54.58] loss=3.14 avg=3.08\n",
            "[28 | 56.15] loss=2.71 avg=3.06\n",
            "[29 | 57.71] loss=2.98 avg=3.06\n",
            "[30 | 59.27] loss=3.02 avg=3.06\n",
            "[31 | 60.85] loss=3.02 avg=3.06\n",
            "[32 | 62.42] loss=3.07 avg=3.06\n",
            "[33 | 63.99] loss=3.55 avg=3.07\n",
            "[34 | 65.56] loss=3.23 avg=3.08\n",
            "[35 | 67.13] loss=2.81 avg=3.07\n",
            "[36 | 68.71] loss=2.85 avg=3.06\n",
            "[37 | 70.29] loss=3.05 avg=3.06\n",
            "[38 | 71.87] loss=2.94 avg=3.06\n",
            "[39 | 73.44] loss=3.31 avg=3.07\n",
            "[40 | 75.03] loss=3.08 avg=3.07\n",
            "[41 | 76.62] loss=3.12 avg=3.07\n",
            "[42 | 78.21] loss=3.26 avg=3.07\n",
            "[43 | 79.80] loss=2.93 avg=3.07\n",
            "[44 | 81.39] loss=2.64 avg=3.06\n",
            "[45 | 82.97] loss=3.09 avg=3.06\n",
            "[46 | 84.57] loss=3.44 avg=3.07\n",
            "[47 | 86.16] loss=3.78 avg=3.09\n",
            "[48 | 87.76] loss=3.26 avg=3.09\n",
            "[49 | 89.35] loss=3.14 avg=3.09\n",
            "[50 | 90.94] loss=3.18 avg=3.10\n",
            "[51 | 92.54] loss=3.35 avg=3.10\n",
            "[52 | 94.14] loss=2.99 avg=3.10\n",
            "[53 | 95.74] loss=2.95 avg=3.10\n",
            "[54 | 97.34] loss=3.03 avg=3.09\n",
            "[55 | 98.94] loss=3.02 avg=3.09\n",
            "[56 | 100.54] loss=2.87 avg=3.09\n",
            "[57 | 102.14] loss=3.63 avg=3.10\n",
            "[58 | 103.73] loss=3.08 avg=3.10\n",
            "[59 | 105.33] loss=2.85 avg=3.09\n",
            "[60 | 106.94] loss=3.17 avg=3.10\n",
            "[61 | 108.54] loss=3.07 avg=3.09\n",
            "[62 | 110.13] loss=2.75 avg=3.09\n",
            "[63 | 111.72] loss=3.18 avg=3.09\n",
            "[64 | 113.31] loss=2.87 avg=3.08\n",
            "[65 | 114.91] loss=2.96 avg=3.08\n",
            "[66 | 116.51] loss=3.33 avg=3.09\n",
            "[67 | 118.10] loss=2.75 avg=3.08\n",
            "[68 | 119.69] loss=3.07 avg=3.08\n",
            "[69 | 121.29] loss=3.35 avg=3.09\n",
            "[70 | 122.88] loss=3.35 avg=3.09\n",
            "[71 | 124.48] loss=3.04 avg=3.09\n",
            "[72 | 126.07] loss=2.97 avg=3.09\n",
            "[73 | 127.66] loss=3.12 avg=3.09\n",
            "[74 | 129.25] loss=2.78 avg=3.08\n",
            "[75 | 130.85] loss=2.74 avg=3.08\n",
            "[76 | 132.43] loss=3.16 avg=3.08\n",
            "[77 | 134.02] loss=3.16 avg=3.08\n",
            "[78 | 135.62] loss=3.25 avg=3.08\n",
            "[79 | 137.20] loss=3.03 avg=3.08\n",
            "[80 | 138.79] loss=2.72 avg=3.07\n",
            "[81 | 140.38] loss=3.10 avg=3.08\n",
            "[82 | 141.97] loss=3.08 avg=3.08\n",
            "[83 | 143.58] loss=2.86 avg=3.07\n",
            "[84 | 145.17] loss=3.20 avg=3.07\n",
            "[85 | 146.76] loss=3.03 avg=3.07\n",
            "[86 | 148.35] loss=3.04 avg=3.07\n",
            "[87 | 149.94] loss=2.66 avg=3.07\n",
            "[88 | 151.53] loss=3.07 avg=3.07\n",
            "[89 | 153.13] loss=3.14 avg=3.07\n",
            "[90 | 154.72] loss=3.09 avg=3.07\n",
            "[91 | 156.32] loss=2.86 avg=3.06\n",
            "[92 | 157.92] loss=3.04 avg=3.06\n",
            "[93 | 159.51] loss=3.26 avg=3.07\n",
            "[94 | 161.11] loss=2.72 avg=3.06\n",
            "[95 | 162.72] loss=2.76 avg=3.06\n",
            "[96 | 164.32] loss=3.10 avg=3.06\n",
            "[97 | 165.92] loss=3.33 avg=3.06\n",
            "[98 | 167.53] loss=2.96 avg=3.06\n",
            "[99 | 169.14] loss=3.07 avg=3.06\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " teen, who lives with her grandfather, died in 2005 and the family moved to the small village of Alvar Aysab after her grandfather's death. The children were orphaned at an early age by abusive parents.\n",
            "\n",
            "Miguel was 15 years old at the time, and his family went back for more. Then, there was an arrest in 2004 of another child—Miguel's 15-year-old sister, Yvette. In 2005, Miguel was found by her mother, Tania, to be missing and taken into protective custody. When Miguel returned to Alvar Aysab that same year, the family returned again, at the request of Tania, to look for another family. During this time, Miguel lost his sight, suffered a stroke, and became unable to move around.\n",
            "\n",
            "\"My dream is to know myself,\" Miguel told me. \"I will always be living with the memories of my mother.\"\n",
            "\n",
            "In the last two years, Tania and her son Miguel have had their hearts sobered and grown from grief into a strong family. The group has found hope in life, and their children. They may no longer have eyesight, but they've learned to cope with the physical challenges that come with raising an entire family of seven. And at least one of the children has been able to live near their grandparents.\n",
            "\n",
            "They call it a community, and it has some of the most important things happening just across the river — and across the street. When it comes to the children—and to their families' shared grief and anger over an incident that happened so long ago—they are also the children of the moment.\n",
            "\n",
            "*This was edited from a previous version that mistakenly said that Miguel's life is at 12 when it should be at 12.\n",
            "\n",
            "Photographs: Courtesy Tania and Miguel Fernandez\n",
            "\n",
            "* This article was amended on Dec. 8 to include a sentence in which the girl named Miguel was a girl of nine years.\n",
            "\n",
            "This post has been updated.\n",
            "\n",
            "* This story originally appeared on BuzzFeed.\n",
            "\n",
            "Watch more news videos at The Takeaway.\n",
            "\n",
            "Watch more news videos at The Takeaway.<|endoftext|>By Michael Harris and Michaela Burchard. (Published on Jan. 19, 2017)\n",
            "\n",
            "Cops on one side of the aisle and drug czars on the other are calling for the legalization of pot. It is a fact of life as the battle over the war on drugs heats up — as voters continue turning on President Trump and Congress continues to fail to find a solution that could prevent a wave of new, more deadly crime. The issue is a political one for the GOP.\n",
            "\n",
            "On Jan. 7, 2017, Republican Attorney General Jeff Sessions spoke to a joint session of Congress. Sessions made a point of saying that the war on drugs doesn't work on the federal level and that his Justice Department would be enforcing federal laws that the states had chosen to enact. \"The only way to defeat drug trafficking is to end marijuana prohibition nationwide,\" Sessions said, according to the Washington Post.\n",
            "\n",
            "Sessions is correct. The federal government is the single largest offender in a nationwide War on Drugs that has cost over a trillion dollars — and is currently costing a trillion more. In January 2017, the United States spent an estimated $878,000,000 on enforcement on drugs in 2017, a figure that was slightly higher than the previous year but still less than the total spent in 2014, which was $988,600,000 . For every dollar spent enforcing federal marijuana laws, a second drug arrest takes place. The total cost of the Drug War to date is projected to be $1.5 trillion.\n",
            "\n",
            "And that's just the federal government. A comprehensive study published this year by the National Academy of Sciences and the Centers for Disease Control and Prevention found that between 2000 and 2015, there have been nearly 500,000 deaths from illicit drugs, including prescription and over-the-counter drugs like heroin, cocaine and oxycodone, resulting in a $3.6 trillion economic impact in the U.S.\n",
            "\n",
            "The war on drugs and the accompanying public health consequences are a product of two decades of failed policies by the Democratic and Republican administrations, which have made life hard for millions of people while creating an immense prison population.\n",
            "\n",
            "One of the most common and damaging effects of prohibition was an enormous increase in drug abuse. According to the Centers for Disease Control:\n",
            "\n",
            "Between 1990 and 2010, states spent an estimated $18.5 billion on law enforcement to prevent and punish drug abuse. The federal government is responsible for over half in this spending. The number of drug users incarcerated has jumped from 1.4 million in 1990 to 3.4 million in 2010 alone — nearly 20 million people have been incarcerated for a drug offense. Drug charges in some states tripled between 1980 and 2008, while drug sentencing has more than doubled.\n",
            "\n",
            "The number of drug offenders in federal penal institutions has jumped from 5.3 million in 1990 to 21.4\n",
            "\n",
            "[100 | 197.58] loss=2.70 avg=3.05\n",
            "[101 | 199.22] loss=2.84 avg=3.05\n",
            "[102 | 200.87] loss=3.10 avg=3.05\n",
            "[103 | 202.51] loss=3.08 avg=3.05\n",
            "[104 | 204.15] loss=3.02 avg=3.05\n",
            "[105 | 205.77] loss=2.84 avg=3.05\n",
            "[106 | 207.41] loss=2.81 avg=3.04\n",
            "[107 | 209.06] loss=3.34 avg=3.05\n",
            "[108 | 210.70] loss=2.59 avg=3.04\n",
            "[109 | 212.34] loss=2.70 avg=3.04\n",
            "[110 | 213.97] loss=3.58 avg=3.04\n",
            "[111 | 215.60] loss=3.25 avg=3.05\n",
            "[112 | 217.23] loss=3.18 avg=3.05\n",
            "[113 | 218.86] loss=2.94 avg=3.05\n",
            "[114 | 220.49] loss=3.25 avg=3.05\n",
            "[115 | 222.12] loss=3.06 avg=3.05\n",
            "[116 | 223.74] loss=2.90 avg=3.05\n",
            "[117 | 225.37] loss=2.74 avg=3.04\n",
            "[118 | 227.00] loss=2.89 avg=3.04\n",
            "[119 | 228.62] loss=2.94 avg=3.04\n",
            "[120 | 230.24] loss=3.44 avg=3.05\n",
            "[121 | 231.86] loss=2.72 avg=3.04\n",
            "[122 | 233.48] loss=3.07 avg=3.04\n",
            "[123 | 235.11] loss=3.03 avg=3.04\n",
            "[124 | 236.73] loss=2.50 avg=3.03\n",
            "[125 | 238.35] loss=2.97 avg=3.03\n",
            "[126 | 239.98] loss=2.79 avg=3.03\n",
            "[127 | 241.60] loss=2.87 avg=3.03\n",
            "[128 | 243.22] loss=2.74 avg=3.02\n",
            "[129 | 244.84] loss=2.98 avg=3.02\n",
            "[130 | 246.47] loss=2.76 avg=3.02\n",
            "[131 | 248.09] loss=3.39 avg=3.02\n",
            "[132 | 249.72] loss=3.17 avg=3.03\n",
            "[133 | 251.34] loss=3.08 avg=3.03\n",
            "[134 | 252.97] loss=2.80 avg=3.02\n",
            "[135 | 254.60] loss=2.87 avg=3.02\n",
            "[136 | 256.22] loss=2.67 avg=3.02\n",
            "[137 | 257.85] loss=3.29 avg=3.02\n",
            "[138 | 259.48] loss=2.51 avg=3.01\n",
            "[139 | 261.11] loss=3.16 avg=3.02\n",
            "[140 | 262.75] loss=2.95 avg=3.02\n",
            "[141 | 264.38] loss=3.04 avg=3.02\n",
            "[142 | 266.02] loss=3.24 avg=3.02\n",
            "[143 | 267.65] loss=2.80 avg=3.02\n",
            "[144 | 269.30] loss=3.02 avg=3.02\n",
            "[145 | 270.92] loss=3.32 avg=3.02\n",
            "[146 | 272.57] loss=2.99 avg=3.02\n",
            "[147 | 274.22] loss=2.91 avg=3.02\n",
            "[148 | 275.87] loss=2.82 avg=3.02\n",
            "[149 | 277.53] loss=3.00 avg=3.02\n",
            "[150 | 279.19] loss=3.03 avg=3.02\n",
            "[151 | 280.85] loss=2.70 avg=3.01\n",
            "[152 | 282.51] loss=3.14 avg=3.01\n",
            "[153 | 284.18] loss=2.73 avg=3.01\n",
            "[154 | 285.85] loss=2.59 avg=3.00\n",
            "[155 | 287.52] loss=2.93 avg=3.00\n",
            "[156 | 289.19] loss=2.95 avg=3.00\n",
            "[157 | 290.85] loss=2.90 avg=3.00\n",
            "[158 | 292.52] loss=3.03 avg=3.00\n",
            "[159 | 294.18] loss=3.09 avg=3.00\n",
            "[160 | 295.85] loss=2.98 avg=3.00\n",
            "[161 | 297.52] loss=3.33 avg=3.01\n",
            "[162 | 299.18] loss=2.66 avg=3.00\n",
            "[163 | 300.83] loss=2.92 avg=3.00\n",
            "[164 | 302.49] loss=3.48 avg=3.01\n",
            "[165 | 304.15] loss=2.92 avg=3.01\n",
            "[166 | 305.81] loss=3.00 avg=3.01\n",
            "[167 | 307.46] loss=2.66 avg=3.00\n",
            "[168 | 309.11] loss=3.13 avg=3.00\n",
            "[169 | 310.76] loss=3.11 avg=3.00\n",
            "[170 | 312.41] loss=3.22 avg=3.01\n",
            "[171 | 314.05] loss=3.03 avg=3.01\n",
            "[172 | 315.69] loss=3.06 avg=3.01\n",
            "[173 | 317.34] loss=3.14 avg=3.01\n",
            "[174 | 318.98] loss=3.10 avg=3.01\n",
            "[175 | 320.62] loss=2.81 avg=3.01\n",
            "[176 | 322.26] loss=2.99 avg=3.01\n",
            "[177 | 323.90] loss=2.94 avg=3.01\n",
            "[178 | 325.54] loss=3.22 avg=3.01\n",
            "[179 | 327.17] loss=2.72 avg=3.01\n",
            "[180 | 328.80] loss=2.93 avg=3.01\n",
            "[181 | 330.44] loss=3.07 avg=3.01\n",
            "[182 | 332.07] loss=2.59 avg=3.00\n",
            "[183 | 333.70] loss=3.00 avg=3.00\n",
            "[184 | 335.33] loss=2.80 avg=3.00\n",
            "[185 | 336.96] loss=2.51 avg=2.99\n",
            "[186 | 338.59] loss=2.66 avg=2.99\n",
            "[187 | 340.22] loss=3.05 avg=2.99\n",
            "[188 | 341.85] loss=2.80 avg=2.99\n",
            "[189 | 343.48] loss=3.34 avg=2.99\n",
            "[190 | 345.10] loss=2.65 avg=2.99\n",
            "[191 | 346.74] loss=2.86 avg=2.99\n",
            "[192 | 348.37] loss=3.22 avg=2.99\n",
            "[193 | 350.00] loss=3.03 avg=2.99\n",
            "[194 | 351.63] loss=3.49 avg=3.00\n",
            "[195 | 353.26] loss=2.86 avg=2.99\n",
            "[196 | 354.89] loss=2.81 avg=2.99\n",
            "[197 | 356.52] loss=2.54 avg=2.99\n",
            "[198 | 358.16] loss=2.88 avg=2.99\n",
            "[199 | 359.79] loss=2.72 avg=2.98\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "�’s “t an uncommon event.”\n",
            "\n",
            "The show “s been quite successful in the first 18 months.” As she spoke, her hair, the shade of blond that “s been a bit different in the past three months, went down. “It’s been a real pleasure and a joy and we’re just delighted to have this audience of millions of people,” she says of her fans. “They know where the show comes from, and that’s exciting.”\n",
            "\n",
            "While she’s now done with the show, Shatner has been on Twitter, talking about “The Last Man on Earth.” He recently told fans that he plans to leave the show to focus on other things (such as a movie).\n",
            "\n",
            "Shatner may not have yet written a long memoir, but he definitely plans to begin one. I asked him what his next book would be. “I’m pretty excited to talk about the show,” he said. “So I’ll talk about it, but I’d certainly write about it—or write about it and write about a book or whatever.” He added with a grin that he’d have other plans, if someone suggested it.\n",
            "\n",
            "For his part, Shatner seems eager to keep pushing forward, despite the risks. With The Last Man on Earth, he’s not writing a book about life on the moon. It’s being treated as a movie rather than a novel, to be made, for the purposes of marketing. And as far as he is concerned, nothing can stop him, for good. After all, what's the point in not doing what everyone else is going to do—be the best version of themselves? He’s happy to keep talking to The New York Times, eager to play the game.\n",
            "\n",
            "In fact, he’s already writing an updated version of Groundhog Day, after seeing how everyone else was doing—he started his own television channel, the Shatner Center, last month—and then the story lines of the 20th century converged around him. “My own story in the [20th] century is so much bigger, that when I was coming out of the movie, I was so excited to be in a movie. So all of this movie stuff that was made with [MTV’s] Game of Thrones, I feel like, I don’t have a chance of doing that,” Shatner said.\n",
            "\n",
            "It’s been a year. What’s happened in that time is hard to say. But one way or another, Shatner will continue his writing legacy, and the work he’s doing at the Shatner Center will stand the test of time.<|endoftext|>This video, originally posted on YouTube by a concerned listener of \"The O.C.\", is an excellent example of a conversation we're always eager to hear. The audience – composed of men who are both men and men, and of whom men make up one half – questions why men are being targeted by the Left. It also prompts an interesting question. As long as male privilege persists and men continue to assume the roles assigned to them by the patriarchy, do we accept that men have a privilege of their own?\n",
            "\n",
            "Of course, this question – and many others such as it – have been asked before – in the context of this particular exchange – by women, specifically. But at this moment, it begs the question. In fact, the same question seems to have been asked by men before, as well. And while the male-identifying audience at The O.C. was curious, it was also, I believe, interested in furthering the discussion.\n",
            "\n",
            "But in this particular exchange, men were curious enough that a lot of the women in the audience started to be uncomfortable, and, in fact, moved the conversation down a certain path that we might not have otherwise considered necessary. Many of us might call the move on the ground-level of the conversation \"embarrassing,\" or \"humiliating\"—and we really should, because the point of the conversation was to be about women, not so much about men. But, while we all ought not to call out the most egregious instances of sexism that we hear about in the news, it's certainly a good idea to ask ourselves whether we should ask why women are being treated this way.\n",
            "\n",
            "\n",
            "Women who read or hear this post might think this to be, like, totally gross, and not even funny (even by our higher standards!) or whatever.\n",
            "\n",
            "I'm making no excuses for the women who read this post—if you feel that you are a woman, or anyone who has ever had to live with the idea that women get preferential treatment in this country because they are women, or anyone who has ever had\n",
            "\n",
            "[200 | 384.53] loss=2.45 avg=2.98\n",
            "[201 | 386.19] loss=3.01 avg=2.98\n",
            "[202 | 387.87] loss=2.55 avg=2.97\n",
            "[203 | 389.54] loss=2.83 avg=2.97\n",
            "[204 | 391.22] loss=2.82 avg=2.97\n",
            "[205 | 392.89] loss=3.00 avg=2.97\n",
            "[206 | 394.56] loss=2.45 avg=2.96\n",
            "[207 | 396.22] loss=3.13 avg=2.96\n",
            "[208 | 397.89] loss=3.04 avg=2.97\n",
            "[209 | 399.56] loss=2.85 avg=2.96\n",
            "[210 | 401.23] loss=2.82 avg=2.96\n",
            "[211 | 402.90] loss=2.95 avg=2.96\n",
            "[212 | 404.57] loss=3.02 avg=2.96\n",
            "[213 | 406.24] loss=2.99 avg=2.96\n",
            "[214 | 407.90] loss=2.72 avg=2.96\n",
            "[215 | 409.55] loss=3.06 avg=2.96\n",
            "[216 | 411.21] loss=2.58 avg=2.96\n",
            "[217 | 412.87] loss=2.86 avg=2.96\n",
            "[218 | 414.52] loss=3.27 avg=2.96\n",
            "[219 | 416.17] loss=2.87 avg=2.96\n",
            "[220 | 417.82] loss=3.02 avg=2.96\n",
            "[221 | 419.47] loss=3.24 avg=2.96\n",
            "[222 | 421.12] loss=2.99 avg=2.96\n",
            "[223 | 422.76] loss=2.70 avg=2.96\n",
            "[224 | 424.41] loss=3.06 avg=2.96\n",
            "[225 | 426.04] loss=3.13 avg=2.96\n",
            "[226 | 427.68] loss=3.48 avg=2.97\n",
            "[227 | 429.32] loss=2.76 avg=2.97\n",
            "[228 | 430.95] loss=3.08 avg=2.97\n",
            "[229 | 432.58] loss=3.33 avg=2.97\n",
            "[230 | 434.22] loss=2.71 avg=2.97\n",
            "[231 | 435.86] loss=3.28 avg=2.97\n",
            "[232 | 437.49] loss=2.87 avg=2.97\n",
            "[233 | 439.12] loss=2.69 avg=2.97\n",
            "[234 | 440.76] loss=2.57 avg=2.96\n",
            "[235 | 442.39] loss=2.98 avg=2.96\n",
            "[236 | 444.02] loss=2.88 avg=2.96\n",
            "[237 | 445.66] loss=3.02 avg=2.96\n",
            "[238 | 447.29] loss=2.59 avg=2.96\n",
            "[239 | 448.93] loss=2.72 avg=2.96\n",
            "[240 | 450.57] loss=3.21 avg=2.96\n",
            "[241 | 452.20] loss=2.87 avg=2.96\n",
            "[242 | 453.84] loss=2.70 avg=2.96\n",
            "[243 | 455.48] loss=2.93 avg=2.96\n",
            "[244 | 457.12] loss=2.96 avg=2.96\n",
            "[245 | 458.76] loss=2.89 avg=2.96\n",
            "[246 | 460.41] loss=3.00 avg=2.96\n",
            "[247 | 462.05] loss=2.81 avg=2.95\n",
            "[248 | 463.70] loss=3.11 avg=2.96\n",
            "[249 | 465.34] loss=2.80 avg=2.95\n",
            "[250 | 467.00] loss=2.89 avg=2.95\n",
            "[251 | 468.63] loss=2.94 avg=2.95\n",
            "[252 | 470.29] loss=2.65 avg=2.95\n",
            "[253 | 471.94] loss=2.41 avg=2.94\n",
            "[254 | 473.59] loss=2.97 avg=2.94\n",
            "[255 | 475.25] loss=3.07 avg=2.95\n",
            "[256 | 476.92] loss=2.93 avg=2.95\n",
            "[257 | 478.59] loss=2.66 avg=2.94\n",
            "[258 | 480.26] loss=2.90 avg=2.94\n",
            "[259 | 481.93] loss=2.52 avg=2.94\n",
            "[260 | 483.60] loss=3.06 avg=2.94\n",
            "[261 | 485.27] loss=2.92 avg=2.94\n",
            "[262 | 486.95] loss=2.88 avg=2.94\n",
            "[263 | 488.62] loss=3.25 avg=2.94\n",
            "[264 | 490.29] loss=3.08 avg=2.94\n",
            "[265 | 491.96] loss=3.16 avg=2.94\n",
            "[266 | 493.64] loss=3.28 avg=2.95\n",
            "[267 | 495.31] loss=2.88 avg=2.95\n",
            "[268 | 496.98] loss=2.38 avg=2.94\n",
            "[269 | 498.66] loss=2.81 avg=2.94\n",
            "[270 | 500.33] loss=2.52 avg=2.94\n",
            "[271 | 502.00] loss=3.24 avg=2.94\n",
            "[272 | 503.67] loss=3.05 avg=2.94\n",
            "[273 | 505.33] loss=2.23 avg=2.93\n",
            "[274 | 507.00] loss=2.74 avg=2.93\n",
            "[275 | 508.67] loss=2.78 avg=2.93\n",
            "[276 | 510.34] loss=2.53 avg=2.92\n",
            "[277 | 511.99] loss=2.80 avg=2.92\n",
            "[278 | 513.65] loss=2.90 avg=2.92\n",
            "[279 | 515.30] loss=3.20 avg=2.93\n",
            "[280 | 516.95] loss=2.61 avg=2.92\n",
            "[281 | 518.60] loss=2.85 avg=2.92\n",
            "[282 | 520.25] loss=3.14 avg=2.92\n",
            "[283 | 521.90] loss=3.02 avg=2.93\n",
            "[284 | 523.55] loss=2.27 avg=2.92\n",
            "[285 | 525.19] loss=2.69 avg=2.92\n",
            "[286 | 526.83] loss=3.09 avg=2.92\n",
            "[287 | 528.47] loss=2.81 avg=2.92\n",
            "[288 | 530.12] loss=2.79 avg=2.92\n",
            "[289 | 531.77] loss=3.04 avg=2.92\n",
            "[290 | 533.40] loss=2.65 avg=2.91\n",
            "[291 | 535.04] loss=2.42 avg=2.91\n",
            "[292 | 536.67] loss=2.50 avg=2.90\n",
            "[293 | 538.31] loss=2.35 avg=2.90\n",
            "[294 | 539.94] loss=2.96 avg=2.90\n",
            "[295 | 541.58] loss=2.98 avg=2.90\n",
            "[296 | 543.22] loss=2.90 avg=2.90\n",
            "[297 | 544.86] loss=3.02 avg=2.90\n",
            "[298 | 546.50] loss=2.96 avg=2.90\n",
            "[299 | 548.13] loss=2.37 avg=2.90\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " squad’s own: that all men are equal in Christ; that gender is largely a matter of culture; that women should be encouraged to serve their husbands; and that, because of our God-given gifts, women can exercise their religion at home.\n",
            "A recent post, in which the church’s new director of stake development, Jody Withers, explained that the decision had been made to allow a man and a woman to officiate at the same ceremony, made several weeks after the bishop was confirmed to his new position, may have been a touchy moment for members.’s response to the post was one of silence and resignation, as was the silence following her initial reaction to the controversy. The new policy, according to members, is, in fact, not about gender at all, but rather about culture: that’s what has always been the case. This wasn’t a question of cultural or doctrinal beliefs that Church culture has developed for all members to accept, but rather, of how members want to be represented.\n",
            "\n",
            "I spoke with a number of members, some of whom had already begun to speak out against the policy. The new policy may not have been an entirely new idea, but members still voiced deep reservations about it.’s announcement,” the church told me, has been “consistently and in [the] strongest terms intended to reflect the Church’s fundamental convictions about Christ to every single member’s face”.\n",
            "\n",
            "The statement, according to those who attended the conference, is not a particularly significant development, given that the church has not taken any formal steps toward addressing the problem.” It may still be months and years before it is enforced, however, and after that point, members may have little to lose by keeping their concerns secret. Members, especially in a culture where they feel pressure to conform, can, at least initially, feel reluctant to confront policy decisions. As one member, a member of the Missionary Retreat, told me recently, “The more difficult things get, the harder it is to keep my feelings from people that know the details. It’s hard, but I try. My church is not open to discussion. So it’s hard, but I try’t to keep things secret. So it’s hard with me.”\n",
            "\n",
            "That the policy is not a substantive departure from the practice in the past, however, begs the question: does it matter?\n",
            "\n",
            "It’s a topic that's far-reaching—it affects every aspect of members’ lives, including how they dress, how they interact with fellow members and other family members, and how they treat their neighbors. Members and supporters of the church, especially those who have been impacted by the policy, have argued that the policy has had very minimal impact because the church has a strict dress code with a specific policy. That’s fair enough: If members want to wear long black jeans that reach beneath the ankle, or black blouses that cover the breastbone, that's fine, too. But the garment-policy that’s most notable may not have had a substantial impact, as is often the case with dress code alterations, is the one that is explicitly designed to protect women.\n",
            "\n",
            "And that was something members are understandably concerned about after the policy was revealed. “We don’t have the same clothes we used to,” someone told me in a separate discussion about the policy, as if this is a new and fundamentally different world. “The church is different.”\n",
            "\n",
            "That statement echoed a common feeling among members and critics I spoke with: that women were less likely than men to feel free to wear the kind of clothes that might be deemed politically incorrect or uncomfortable. “It’s not a question of gender. The dress code is designed to accommodate everything that every Mormon woman feels she fits into.””\n",
            "\n",
            "\n",
            "As such, members of the Church have complained about several policies before, as have supporters of the church. The policy change at stake conference, for example, is significant in that it was the first of many changes the church had attempted to implement in an attempt to accommodate the changing views of those within its ranks—a process that, in some cases, came to include a general dress code.\n",
            "\n",
            "Since 2015, the church of the Quorum of the Twelve Apostles has also had a number of other changes made in its mission statement, as described by the member who is currently working on the process of developing this policy. “I was always looking for ways to get the Church in a more diverse space,” he told me through an interpreter, “and so this was really, really interesting to me.”\n",
            "\n",
            "Although it was initially intended as a directive, this new policy is nothing more than yet another of the more personal gestures the Quorum of the Twelve Apostles made to\n",
            "\n",
            "[300 | 572.78] loss=3.06 avg=2.90\n",
            "[301 | 574.45] loss=2.43 avg=2.89\n",
            "[302 | 576.10] loss=2.71 avg=2.89\n",
            "[303 | 577.77] loss=2.45 avg=2.89\n",
            "[304 | 579.43] loss=2.84 avg=2.89\n",
            "[305 | 581.11] loss=2.71 avg=2.88\n",
            "[306 | 582.76] loss=2.86 avg=2.88\n",
            "[307 | 584.43] loss=2.58 avg=2.88\n",
            "[308 | 586.11] loss=3.18 avg=2.88\n",
            "[309 | 587.78] loss=3.15 avg=2.89\n",
            "[310 | 589.45] loss=2.69 avg=2.88\n",
            "[311 | 591.13] loss=2.51 avg=2.88\n",
            "[312 | 592.80] loss=2.61 avg=2.88\n",
            "[313 | 594.47] loss=2.88 avg=2.88\n",
            "[314 | 596.15] loss=2.66 avg=2.88\n",
            "[315 | 597.82] loss=2.52 avg=2.87\n",
            "[316 | 599.49] loss=2.13 avg=2.86\n",
            "[317 | 601.16] loss=2.75 avg=2.86\n",
            "[318 | 602.83] loss=3.04 avg=2.86\n",
            "[319 | 604.51] loss=2.62 avg=2.86\n",
            "[320 | 606.17] loss=2.92 avg=2.86\n",
            "[321 | 607.83] loss=3.37 avg=2.87\n",
            "[322 | 609.50] loss=2.61 avg=2.87\n",
            "[323 | 611.16] loss=2.55 avg=2.86\n",
            "[324 | 612.83] loss=2.87 avg=2.86\n",
            "[325 | 614.49] loss=2.86 avg=2.86\n",
            "[326 | 616.15] loss=3.14 avg=2.87\n",
            "[327 | 617.81] loss=2.75 avg=2.86\n",
            "[328 | 619.46] loss=2.68 avg=2.86\n",
            "[329 | 621.12] loss=3.10 avg=2.86\n",
            "[330 | 622.77] loss=2.95 avg=2.87\n",
            "[331 | 624.42] loss=2.98 avg=2.87\n",
            "[332 | 626.07] loss=3.08 avg=2.87\n",
            "[333 | 627.72] loss=3.21 avg=2.87\n",
            "[334 | 629.37] loss=2.93 avg=2.87\n",
            "[335 | 631.01] loss=2.91 avg=2.87\n",
            "[336 | 632.65] loss=2.74 avg=2.87\n",
            "[337 | 634.30] loss=2.89 avg=2.87\n",
            "[338 | 635.94] loss=3.05 avg=2.87\n",
            "[339 | 637.58] loss=3.20 avg=2.88\n",
            "[340 | 639.22] loss=3.39 avg=2.88\n",
            "[341 | 640.86] loss=3.16 avg=2.89\n",
            "[342 | 642.50] loss=2.90 avg=2.89\n",
            "[343 | 644.14] loss=3.28 avg=2.89\n",
            "[344 | 645.77] loss=3.33 avg=2.89\n",
            "[345 | 647.41] loss=3.27 avg=2.90\n",
            "[346 | 649.04] loss=3.04 avg=2.90\n",
            "[347 | 650.68] loss=2.56 avg=2.90\n",
            "[348 | 652.31] loss=2.75 avg=2.89\n",
            "[349 | 653.94] loss=2.88 avg=2.89\n",
            "[350 | 655.58] loss=2.50 avg=2.89\n",
            "[351 | 657.22] loss=2.90 avg=2.89\n",
            "[352 | 658.86] loss=2.72 avg=2.89\n",
            "[353 | 660.50] loss=2.68 avg=2.89\n",
            "[354 | 662.14] loss=2.64 avg=2.88\n",
            "[355 | 663.78] loss=2.66 avg=2.88\n",
            "[356 | 665.42] loss=3.08 avg=2.88\n",
            "[357 | 667.06] loss=1.96 avg=2.87\n",
            "[358 | 668.71] loss=3.00 avg=2.88\n",
            "[359 | 670.36] loss=2.52 avg=2.87\n",
            "[360 | 672.02] loss=3.34 avg=2.88\n",
            "[361 | 673.67] loss=3.14 avg=2.88\n",
            "[362 | 675.33] loss=2.71 avg=2.88\n",
            "[363 | 677.00] loss=2.94 avg=2.88\n",
            "[364 | 678.67] loss=2.96 avg=2.88\n",
            "[365 | 680.34] loss=2.54 avg=2.88\n",
            "[366 | 682.01] loss=2.61 avg=2.87\n",
            "[367 | 683.68] loss=3.19 avg=2.88\n",
            "[368 | 685.35] loss=2.76 avg=2.87\n",
            "[369 | 687.02] loss=2.43 avg=2.87\n",
            "[370 | 688.69] loss=2.78 avg=2.87\n",
            "[371 | 690.36] loss=2.51 avg=2.87\n",
            "[372 | 692.05] loss=2.78 avg=2.86\n",
            "[373 | 693.72] loss=3.01 avg=2.87\n",
            "[374 | 695.39] loss=1.86 avg=2.86\n",
            "[375 | 697.08] loss=3.74 avg=2.87\n",
            "[376 | 698.75] loss=3.06 avg=2.87\n",
            "[377 | 700.42] loss=2.69 avg=2.87\n",
            "[378 | 702.09] loss=2.62 avg=2.86\n",
            "[379 | 703.78] loss=2.81 avg=2.86\n",
            "[380 | 705.45] loss=2.93 avg=2.86\n",
            "[381 | 707.12] loss=3.12 avg=2.87\n",
            "[382 | 708.79] loss=2.98 avg=2.87\n",
            "[383 | 710.46] loss=2.67 avg=2.86\n",
            "[384 | 712.13] loss=3.32 avg=2.87\n",
            "[385 | 713.79] loss=2.85 avg=2.87\n",
            "[386 | 715.45] loss=2.27 avg=2.86\n",
            "[387 | 717.11] loss=3.31 avg=2.87\n",
            "[388 | 718.77] loss=3.12 avg=2.87\n",
            "[389 | 720.43] loss=3.34 avg=2.87\n",
            "[390 | 722.08] loss=2.94 avg=2.88\n",
            "[391 | 723.73] loss=2.55 avg=2.87\n",
            "[392 | 725.38] loss=3.17 avg=2.88\n",
            "[393 | 727.02] loss=2.43 avg=2.87\n",
            "[394 | 728.67] loss=2.53 avg=2.87\n",
            "[395 | 730.31] loss=2.79 avg=2.87\n",
            "[396 | 731.96] loss=3.04 avg=2.87\n",
            "[397 | 733.59] loss=3.03 avg=2.87\n",
            "[398 | 735.23] loss=2.73 avg=2.87\n",
            "[399 | 736.87] loss=3.03 avg=2.87\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "getting”s going to be an interesting challenge for the show. They had to come up with an identity that’s rooted in who you are, but also who you’re been, and what you’ve been through. And what’s going to make you who you are, and have an answer to who you are? I really’ve only had one thing on my mind: “You’re the only character who can save this town of 13,000.”\n",
            "\n",
            "We need your help.\n",
            "\n",
            "“Every question, every suggestion, every character revelation, that you guys ask is an opportunity to ask more questions.” (A.K.A. “the question of whether the character is really a man, exactly.”) So the question of whether we're a female character comes to us first to get to the answer to why she’s there’s so much to this show, and what’s at stake. We don’t get to that question until the very end, when we have this very important conversation about the gender roles at play in this town. (“I can’t imagine not having any kind of power, except the power to change a world.”)\n",
            "\n",
            "What is it about you, as men in these shows, that is so crucial?\n",
            "\n",
            "I think there’s such a fundamental disconnect. There’s a very deep sense that the stories and the dialogue and the humor in these show are fundamentally about masculinity, that they’re fundamentally about saying, “If you’re going to be a man, you’re entitled to say things that are offensive, that have to be taken seriously.”\n",
            "\n",
            "But you can’t have that if you’re not a man.\n",
            "\n",
            "The whole idea of making a show about women who are women is a feminist thing, and I think it’s pretty rare in television, where characters are so explicitly defined as having a gender identity, that they’re not being played with. There’s so much in TV these days that you can have a female character who is a person of color, who is queer, who’s a mother, who can carry the weight of life on her shoulders, but who’s never a full-blown character because she can’t be defined as a man.\n",
            "\n",
            "What’s so interesting is that, for a long time, women got to define their own power in terms of what they’re worth in society, and what they’re not worth when it comes to social status. Then I think you’ll see that it has shifted to an almost total dismantling of our culture.\n",
            "\n",
            "And you’ve got to ask yourself why we’re so unwilling to really question gender roles for women in this country and why we’re so reluctant to challenge them in other ways. I think women are so tied up with our sense of being worth something in society. And I think as long as a little bit of it is about value, there’s still this whole narrative of gender, like “If you don’t think you’re worth anything, maybe you shouldn’t be here.”\n",
            "\n",
            "I’m afraid that we haven’t always talked about this as much as we should, but we’re very much part of the problem.\n",
            "\n",
            "“So,” I ask myself and she starts laughing.\n",
            "\n",
            "“Because I’m so tired of these women getting away with this.” I say. She sighs and turns away, still laughing.\n",
            "\n",
            "“I know,” she says, her eyes darkening. \"There are so many women out there who are treated fairly, but are afraid to challenge their status. They don’t want to raise their voice unless they think they’re going to get fired, or they think they’ve got to conform.” For the first time in a long time, I realize just how much I love talking to women about sex, like with this girl I had coffee with months ago—she was so into it, she was a real sex machine—but when it comes to telling me about our sex lives, she’s always, like, ‘Okay, we’re just like you guys. We’re just friends with mutual respect and we’re like, ‘Okay, we’re just just talking.’’\n",
            "\n",
            "Some days I don’t even care if she’s listening. I just sit there with some coffee in my hand, a smile on my face, and I can tell that she’s enjoying this conversation. I think about how often I’m confronted by cisgender men\n",
            "\n",
            "[400 | 761.34] loss=3.07 avg=2.87\n",
            "[401 | 762.99] loss=2.67 avg=2.87\n",
            "[402 | 764.64] loss=3.01 avg=2.87\n",
            "[403 | 766.29] loss=2.22 avg=2.86\n",
            "[404 | 767.94] loss=2.91 avg=2.87\n",
            "[405 | 769.60] loss=2.70 avg=2.86\n",
            "[406 | 771.25] loss=2.62 avg=2.86\n",
            "[407 | 772.91] loss=3.04 avg=2.86\n",
            "[408 | 774.57] loss=2.55 avg=2.86\n",
            "[409 | 776.23] loss=3.19 avg=2.86\n",
            "[410 | 777.88] loss=2.86 avg=2.86\n",
            "[411 | 779.52] loss=2.75 avg=2.86\n",
            "[412 | 781.19] loss=2.43 avg=2.86\n",
            "[413 | 782.85] loss=3.31 avg=2.86\n",
            "[414 | 784.52] loss=2.45 avg=2.86\n",
            "[415 | 786.19] loss=3.55 avg=2.86\n",
            "[416 | 787.87] loss=3.37 avg=2.87\n",
            "[417 | 789.54] loss=3.12 avg=2.87\n",
            "[418 | 791.21] loss=2.50 avg=2.87\n",
            "[419 | 792.88] loss=2.40 avg=2.86\n",
            "[420 | 794.56] loss=2.77 avg=2.86\n",
            "[421 | 796.23] loss=2.92 avg=2.86\n",
            "[422 | 797.91] loss=2.79 avg=2.86\n",
            "[423 | 799.58] loss=2.96 avg=2.86\n",
            "[424 | 801.25] loss=2.55 avg=2.86\n",
            "[425 | 802.93] loss=2.76 avg=2.86\n",
            "[426 | 804.60] loss=2.28 avg=2.85\n",
            "[427 | 806.27] loss=2.74 avg=2.85\n",
            "[428 | 807.95] loss=2.59 avg=2.85\n",
            "[429 | 809.62] loss=2.97 avg=2.85\n",
            "[430 | 811.29] loss=2.98 avg=2.85\n",
            "[431 | 812.96] loss=2.83 avg=2.85\n",
            "[432 | 814.63] loss=2.57 avg=2.85\n",
            "[433 | 816.31] loss=2.79 avg=2.85\n",
            "[434 | 817.97] loss=2.58 avg=2.85\n",
            "[435 | 819.64] loss=2.28 avg=2.84\n",
            "[436 | 821.31] loss=3.04 avg=2.84\n",
            "[437 | 822.97] loss=2.51 avg=2.84\n",
            "[438 | 824.64] loss=3.22 avg=2.84\n",
            "[439 | 826.29] loss=2.94 avg=2.84\n",
            "[440 | 827.95] loss=2.55 avg=2.84\n",
            "[441 | 829.61] loss=2.88 avg=2.84\n",
            "[442 | 831.26] loss=2.82 avg=2.84\n",
            "[443 | 832.91] loss=2.85 avg=2.84\n",
            "[444 | 834.57] loss=3.27 avg=2.85\n",
            "[445 | 836.22] loss=2.67 avg=2.84\n",
            "[446 | 837.86] loss=2.74 avg=2.84\n",
            "[447 | 839.50] loss=2.72 avg=2.84\n",
            "[448 | 841.14] loss=2.22 avg=2.84\n",
            "[449 | 842.79] loss=2.56 avg=2.83\n",
            "[450 | 844.43] loss=3.08 avg=2.83\n",
            "[451 | 846.07] loss=2.84 avg=2.83\n",
            "[452 | 847.70] loss=2.46 avg=2.83\n",
            "[453 | 849.34] loss=2.71 avg=2.83\n",
            "[454 | 850.97] loss=2.74 avg=2.83\n",
            "[455 | 852.61] loss=3.33 avg=2.83\n",
            "[456 | 854.24] loss=2.75 avg=2.83\n",
            "[457 | 855.88] loss=2.58 avg=2.83\n",
            "[458 | 857.51] loss=2.77 avg=2.83\n",
            "[459 | 859.15] loss=2.68 avg=2.83\n",
            "[460 | 860.79] loss=3.14 avg=2.83\n",
            "[461 | 862.43] loss=2.84 avg=2.83\n",
            "[462 | 864.07] loss=2.61 avg=2.83\n",
            "[463 | 865.72] loss=3.15 avg=2.83\n",
            "[464 | 867.36] loss=2.65 avg=2.83\n",
            "[465 | 869.01] loss=2.60 avg=2.83\n",
            "[466 | 870.65] loss=2.86 avg=2.83\n",
            "[467 | 872.31] loss=3.41 avg=2.83\n",
            "[468 | 873.96] loss=2.50 avg=2.83\n",
            "[469 | 875.61] loss=2.99 avg=2.83\n",
            "[470 | 877.27] loss=2.51 avg=2.83\n",
            "[471 | 878.93] loss=2.97 avg=2.83\n",
            "[472 | 880.60] loss=2.73 avg=2.83\n",
            "[473 | 882.27] loss=2.90 avg=2.83\n",
            "[474 | 883.95] loss=3.01 avg=2.83\n",
            "[475 | 885.64] loss=2.48 avg=2.83\n",
            "[476 | 887.30] loss=2.58 avg=2.83\n",
            "[477 | 888.99] loss=2.36 avg=2.82\n",
            "[478 | 890.69] loss=2.36 avg=2.82\n",
            "[479 | 892.36] loss=2.56 avg=2.81\n",
            "[480 | 894.06] loss=2.88 avg=2.81\n",
            "[481 | 895.75] loss=3.21 avg=2.82\n",
            "[482 | 897.44] loss=2.84 avg=2.82\n",
            "[483 | 899.13] loss=2.75 avg=2.82\n",
            "[484 | 900.80] loss=2.70 avg=2.82\n",
            "[485 | 902.48] loss=3.08 avg=2.82\n",
            "[486 | 904.15] loss=2.15 avg=2.81\n",
            "[487 | 905.82] loss=2.42 avg=2.81\n",
            "[488 | 907.49] loss=2.65 avg=2.81\n",
            "[489 | 909.16] loss=2.87 avg=2.81\n",
            "[490 | 910.83] loss=2.28 avg=2.80\n",
            "[491 | 912.49] loss=2.61 avg=2.80\n",
            "[492 | 914.15] loss=2.47 avg=2.80\n",
            "[493 | 915.81] loss=2.92 avg=2.80\n",
            "[494 | 917.47] loss=2.32 avg=2.79\n",
            "[495 | 919.12] loss=2.73 avg=2.79\n",
            "[496 | 920.77] loss=3.06 avg=2.80\n",
            "[497 | 922.43] loss=2.89 avg=2.80\n",
            "[498 | 924.08] loss=2.88 avg=2.80\n",
            "[499 | 925.73] loss=2.48 avg=2.79\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            ".\n",
            "\n",
            "The most dramatic changes, though, occurred in 2014. For the first time, the new legislation did not explicitly authorize a single-payer system. Instead, the legislation gave states the option to opt into government-run health-care coverage for a set of individual-market premiums and drug costs. The states that opted in—including Kentucky and Oklahoma, among others—charged a higher percentage of federal money toward those premiums and drug costs that go to the government.[4]\n",
            "\n",
            "The federal government paid an extra $4 billion to support that strategy, but states could have paid $7 billion without paying for much more. Over time, those additional dollars have been used more for private-sector health-care plans, such as those offered through the state and federal government, and less for the Medicare system that pays for a subset of the Medicare program. This has left Americans with higher health-care bills, and as a result, have made the ACA's individual-market component increasingly expensive to maintain.\n",
            "\n",
            "In a January 2016 analysis by The Commonwealth Fund, a Washington, D.C.-based think tank, the Commonwealth Fund estimated that the cost of the individual-market subsidy—which pays a set percentage of the difference between each person's Medicare premium and the monthly premium in their home state, called the silver plan subsidy—would increase by $16 billion over the next decade. That premium cost for the average American family would increase from $11,550 a year now to an estimated $25,000 by 2026.[5]\n",
            "\n",
            "Since the ACA expanded Medicaid in 2014, more than 40 million Americans have gained access to more comprehensive coverage through the expansion. That coverage covers nearly all beneficiaries of Medicaid, including the poor and seniors (who could otherwise be covered by private insurance or Medicaid for their entire lives).\n",
            "\n",
            "Medicaid is designed to expand coverage to low-income Americans, and Medicaid is a program that would be particularly vulnerable to the threat of an individual-market premium tax exclusion. Under a hypothetical scenario in which no such premium-exclusion tax exclusion is set, the average family in 2018 would incur an average increase in out-of-pocket costs of about $900.\n",
            "\n",
            "States that refuse Medicaid expansion can face a cost–effective way to lower those costs. By waiving the individual-market premium tax exclusion, they can charge patients more per month. But under a system where the average patient would pay $10,100 per year in premium tax exemptions, that same patient, faced with this additional expense, could also opt to pay out-of-pocket costs that go to the federal government.\n",
            "\n",
            "In addition to its impact on the Medicare program, the individual-market premium tax exemption has also been one of the ACA's defining features, providing states with a competitive edge. States that have opted to use the option to cap the percentage of Medicaid costs going to the government would be much more likely to have been able to do so, given the lower-than-expected premium-tax-exemption pool.\n",
            "\n",
            "To be sure, insurers will still have enough money to offer lower-cost, lower-quality plans; that may help cover costs. But the increased cost of insurance for a family is already making health-care prices in many states unsustainable. Under the same scenario as the Commonwealth Fund analysis, the average annual out-of-pocket cost for a family of four would rise from $3,850 to $6,600.\n",
            "\n",
            "The cost-effectiveness ratio for the ACA, as defined by the Health Affairs organization, is a measure of how much a policy has an impact on health. For some reasons, the Commonwealth Fund analysis was even cheaper for the individual-market premium-tax-exclusion option than the other available options, which was consistent with the overall ACA premium-tax-exclusion market being much more affordable than the ACA did.\n",
            "\n",
            "Yet those numbers don't capture the full value of the ACA's premium-tax-exclusion option: it would have meant that only those who have limited or no access to health coverage, such as people who have low incomes or who have limited access to Medicaid, would be able to use it. The affordability of the ACA would have also been a crucial selling point.\n",
            "\n",
            "The Commonwealth Fund analysis is just the latest example of how the GOP-led government has been able to repeal the ACA without actually doing what it is intended to do: replace and expand health and medical care for all Americans. The cost-effectiveness study suggests that some states and the industry that regulates Medicaid were able to get away with the subsidies, because they did not have to explain their rationale for the subsidies in this manner. To think about why people could pay far more in premiums without having to justify those costs simply by saying that the premiums were too expensive is to miss an important part of the ACA—that it should have included a provision to allow states to cap the percentage of costs going to the government.\n",
            "\n",
            "The ACA was in part designed as a mechanism to lower health costs. It\n",
            "\n",
            "[500 | 950.38] loss=2.92 avg=2.80\n",
            "[501 | 952.03] loss=2.53 avg=2.79\n",
            "[502 | 953.67] loss=2.55 avg=2.79\n",
            "[503 | 955.31] loss=2.53 avg=2.79\n",
            "[504 | 956.94] loss=3.11 avg=2.79\n",
            "[505 | 958.58] loss=2.41 avg=2.79\n",
            "[506 | 960.22] loss=2.26 avg=2.78\n",
            "[507 | 961.86] loss=2.70 avg=2.78\n",
            "[508 | 963.50] loss=2.25 avg=2.78\n",
            "[509 | 965.14] loss=2.45 avg=2.77\n",
            "[510 | 966.78] loss=2.69 avg=2.77\n",
            "[511 | 968.43] loss=2.49 avg=2.77\n",
            "[512 | 970.07] loss=2.38 avg=2.77\n",
            "[513 | 971.72] loss=2.79 avg=2.77\n",
            "[514 | 973.38] loss=2.86 avg=2.77\n",
            "[515 | 975.04] loss=2.55 avg=2.76\n",
            "[516 | 976.70] loss=2.32 avg=2.76\n",
            "[517 | 978.36] loss=3.13 avg=2.76\n",
            "[518 | 980.03] loss=2.64 avg=2.76\n",
            "[519 | 981.71] loss=2.70 avg=2.76\n",
            "[520 | 983.38] loss=2.56 avg=2.76\n",
            "[521 | 985.05] loss=2.75 avg=2.76\n",
            "[522 | 986.74] loss=2.69 avg=2.76\n",
            "[523 | 988.42] loss=2.61 avg=2.76\n",
            "[524 | 990.11] loss=3.23 avg=2.76\n",
            "[525 | 991.81] loss=2.60 avg=2.76\n",
            "[526 | 993.50] loss=2.45 avg=2.76\n",
            "[527 | 995.19] loss=3.19 avg=2.76\n",
            "[528 | 996.88] loss=2.44 avg=2.76\n",
            "[529 | 998.57] loss=2.77 avg=2.76\n",
            "[530 | 1000.24] loss=2.89 avg=2.76\n",
            "[531 | 1001.92] loss=3.19 avg=2.76\n",
            "[532 | 1003.59] loss=2.22 avg=2.76\n",
            "[533 | 1005.26] loss=2.88 avg=2.76\n",
            "[534 | 1006.92] loss=2.93 avg=2.76\n",
            "[535 | 1008.58] loss=3.00 avg=2.76\n",
            "[536 | 1010.25] loss=3.32 avg=2.77\n",
            "[537 | 1011.91] loss=2.15 avg=2.76\n",
            "[538 | 1013.56] loss=3.10 avg=2.77\n",
            "[539 | 1015.22] loss=2.42 avg=2.76\n",
            "[540 | 1016.88] loss=2.91 avg=2.76\n",
            "[541 | 1018.53] loss=2.70 avg=2.76\n",
            "[542 | 1020.18] loss=2.60 avg=2.76\n",
            "[543 | 1021.83] loss=2.94 avg=2.76\n",
            "[544 | 1023.48] loss=2.58 avg=2.76\n",
            "[545 | 1025.13] loss=2.86 avg=2.76\n",
            "[546 | 1026.77] loss=3.22 avg=2.77\n",
            "[547 | 1028.43] loss=2.73 avg=2.77\n",
            "[548 | 1030.07] loss=2.52 avg=2.77\n",
            "[549 | 1031.72] loss=3.17 avg=2.77\n",
            "[550 | 1033.37] loss=2.93 avg=2.77\n",
            "[551 | 1035.01] loss=2.56 avg=2.77\n",
            "[552 | 1036.65] loss=2.88 avg=2.77\n",
            "[553 | 1038.29] loss=2.85 avg=2.77\n",
            "[554 | 1039.93] loss=2.87 avg=2.77\n",
            "[555 | 1041.57] loss=3.26 avg=2.78\n",
            "[556 | 1043.20] loss=2.93 avg=2.78\n",
            "[557 | 1044.84] loss=2.80 avg=2.78\n",
            "[558 | 1046.48] loss=2.85 avg=2.78\n",
            "[559 | 1048.12] loss=2.86 avg=2.78\n",
            "[560 | 1049.76] loss=2.75 avg=2.78\n",
            "[561 | 1051.39] loss=2.86 avg=2.78\n",
            "[562 | 1053.03] loss=3.05 avg=2.78\n",
            "[563 | 1054.66] loss=3.12 avg=2.79\n",
            "[564 | 1056.31] loss=2.72 avg=2.79\n",
            "[565 | 1057.94] loss=3.26 avg=2.79\n",
            "[566 | 1059.58] loss=2.88 avg=2.79\n",
            "[567 | 1061.22] loss=2.26 avg=2.79\n",
            "[568 | 1062.87] loss=2.87 avg=2.79\n",
            "[569 | 1064.51] loss=2.97 avg=2.79\n",
            "[570 | 1066.16] loss=2.44 avg=2.79\n",
            "[571 | 1067.81] loss=2.91 avg=2.79\n",
            "[572 | 1069.46] loss=2.77 avg=2.79\n",
            "[573 | 1071.12] loss=2.86 avg=2.79\n",
            "[574 | 1072.78] loss=2.87 avg=2.79\n",
            "[575 | 1074.44] loss=2.92 avg=2.79\n",
            "[576 | 1076.11] loss=2.34 avg=2.78\n",
            "[577 | 1077.80] loss=2.63 avg=2.78\n",
            "[578 | 1079.48] loss=2.32 avg=2.78\n",
            "[579 | 1081.17] loss=2.93 avg=2.78\n",
            "[580 | 1082.86] loss=2.84 avg=2.78\n",
            "[581 | 1084.55] loss=3.25 avg=2.79\n",
            "[582 | 1086.24] loss=3.01 avg=2.79\n",
            "[583 | 1087.94] loss=2.80 avg=2.79\n",
            "[584 | 1089.63] loss=2.59 avg=2.79\n",
            "[585 | 1091.32] loss=2.59 avg=2.78\n",
            "[586 | 1093.01] loss=2.71 avg=2.78\n",
            "[587 | 1094.70] loss=2.61 avg=2.78\n",
            "[588 | 1096.38] loss=2.38 avg=2.78\n",
            "[589 | 1098.07] loss=2.65 avg=2.78\n",
            "[590 | 1099.74] loss=2.88 avg=2.78\n",
            "[591 | 1101.41] loss=2.63 avg=2.78\n",
            "[592 | 1103.08] loss=2.90 avg=2.78\n",
            "[593 | 1104.75] loss=2.92 avg=2.78\n",
            "[594 | 1106.42] loss=3.03 avg=2.78\n",
            "[595 | 1108.08] loss=2.33 avg=2.78\n",
            "[596 | 1109.74] loss=2.98 avg=2.78\n",
            "[597 | 1111.41] loss=3.20 avg=2.78\n",
            "[598 | 1113.06] loss=2.92 avg=2.78\n",
            "[599 | 1114.72] loss=2.07 avg=2.78\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " context of an interdisciplinary, multidimensional, interdisciplinary conversation. “I am afraid, Mr. President. And I am sorry. ‘I just want to say again, as I see it,’ not just as a mother, but also as a human being.” She said her thoughts went elsewhere the following morning, following a lengthy email exchange with her father. The son responded that he believed her account. It was September 9, 2017, when I first wrote my account of what I believed was a statement that went beyond my understanding. The following morning, my account was published, in the Independent of New York, a newspaper that, in fact, has had no editorial control over my thoughts or my reporting for nearly a month, and which has been under tremendous fire from citizens all over the world. The newspaper went on to say that the article was false. It was false. It was false.\n",
            "“It’s a lie,” Michelle Obama once told Senator Robert Menendez, of New Jersey, when asked whether the president lied about her. It’s very possible that she might have said it, in other contexts as well.\n",
            "The following week, an article appeared in the British tabloid the Sunday Telegraph reporting that, at the G-7 summit in June, Trump had said that “nobody cares about [her] appearance.” This accusation—of lying, of objecting to criticism—had the potential to shock and offend everyone. Some of the most notable reactions to the piece were from those who believed her claim about the president lying. Not everyone believed it. Some of those who had not thought Trump had claimed it, thought more of the article’s language than of the president’s actual comment. In some cases, they even argued that she had merely made a bad misstatement of fact.\n",
            "\n",
            "One of the reasons the allegations about Trump’s falsehood were so damaging was because what she implied about the president, in her press conference, seemed to be an allegation of lying herself, even though she had not made one itself, and had not even said, as part of her opening statement, “I think that when he makes a statement,” her speechwriters had put it there for her. The question for me now is not whether my husband or I did, as he put it, ‘sick and tired of hearing about the president … sick and tired of seeing this guy in the White House … sick and tired of seeing this president get away with this.’” Or, to put it differently, the question for me now is whether, in an institution that has so often betrayed all of its ethical standards, we have failed so profoundly as a nation that our presidency has allowed that to happen.\n",
            "\n",
            "This evening, an opinion poll published in the British paper the Times suggested that what has, for weeks now, seemed to be the most striking contradiction in the relationship between Donald Trump and this country has really broken. The survey asked respondents, “Do you find it morally reprehensible that Donald Trump has called some women ‘slobs,’” ‘dogs,’’’’’– and called for others to ”be thrown into the garbage,”” or “justifiably outraged that the president’s comments have provoked violent protests and racial unrest in the United States?”\n",
            "\n",
            "In all likelihood, of the responses that were statistically significant, fewer than one in 10 expressed anything like the moral outrage that Trump has expressed in the past. Almost all of them had expressed no outrage at all. They had expressed sympathy, or even approval, for what he has said.\n",
            "\n",
            "The Times’ own poll, this evening, had a similar result as the one I had: only 8 percent of the public expressed any moral outrage at what Trump had done. The percentage who had expressed an opinion of moral outrage, however, had more than doubled, to 22 percent.\n",
            "\n",
            "Two days before, a survey in the Times had showed that, if it had been conducted again, the percentage of the public who had expressed the type of moral outrage Trump has expressed was closer to 60 percent.\n",
            "\n",
            "What had happened in the last few days, in that poll and these other two other recent polls, was a sudden and dramatic shift in how the public responded to the things that the president did. The two major political parties, both of which campaigned on a platform of keeping the country safe from the dangerous forces of radical Islam, have, in fact, made a substantial amount of the political case for the kinds of behavior that the Trump team has called out today.\n",
            "\n",
            "The Democrats have called out Trump for saying that Muslims must be barred from coming into the country; they have denounced his mocking of a disabled reporter — in his own words— at a White House meeting; they have denounced his comments about a federal judge with Mexican heritage; they have called him out for calling\n",
            "\n",
            "[600 | 1139.26] loss=2.44 avg=2.77\n",
            "[601 | 1140.90] loss=2.67 avg=2.77\n",
            "[602 | 1142.55] loss=2.72 avg=2.77\n",
            "[603 | 1144.19] loss=2.36 avg=2.77\n",
            "[604 | 1145.82] loss=2.85 avg=2.77\n",
            "[605 | 1147.47] loss=2.57 avg=2.77\n",
            "[606 | 1149.11] loss=2.82 avg=2.77\n",
            "[607 | 1150.75] loss=2.40 avg=2.76\n",
            "[608 | 1152.39] loss=2.18 avg=2.76\n",
            "[609 | 1154.02] loss=1.97 avg=2.75\n",
            "[610 | 1155.66] loss=2.74 avg=2.75\n",
            "[611 | 1157.30] loss=2.47 avg=2.75\n",
            "[612 | 1158.94] loss=2.85 avg=2.75\n",
            "[613 | 1160.58] loss=2.52 avg=2.75\n",
            "[614 | 1162.23] loss=2.33 avg=2.74\n",
            "[615 | 1163.88] loss=2.03 avg=2.73\n",
            "[616 | 1165.53] loss=3.01 avg=2.74\n",
            "[617 | 1167.18] loss=2.57 avg=2.74\n",
            "[618 | 1168.83] loss=2.53 avg=2.73\n",
            "[619 | 1170.49] loss=3.31 avg=2.74\n",
            "[620 | 1172.15] loss=2.80 avg=2.74\n",
            "[621 | 1173.81] loss=2.11 avg=2.73\n",
            "[622 | 1175.48] loss=2.12 avg=2.73\n",
            "[623 | 1177.15] loss=2.35 avg=2.72\n",
            "[624 | 1178.82] loss=2.91 avg=2.73\n",
            "[625 | 1180.51] loss=2.83 avg=2.73\n",
            "[626 | 1182.20] loss=2.90 avg=2.73\n",
            "[627 | 1183.90] loss=2.97 avg=2.73\n",
            "[628 | 1185.59] loss=2.71 avg=2.73\n",
            "[629 | 1187.28] loss=2.92 avg=2.73\n",
            "[630 | 1188.98] loss=2.86 avg=2.73\n",
            "[631 | 1190.67] loss=2.64 avg=2.73\n",
            "[632 | 1192.36] loss=2.71 avg=2.73\n",
            "[633 | 1194.05] loss=2.91 avg=2.73\n",
            "[634 | 1195.72] loss=2.20 avg=2.73\n",
            "[635 | 1197.39] loss=2.16 avg=2.72\n",
            "[636 | 1199.06] loss=3.37 avg=2.73\n",
            "[637 | 1200.73] loss=2.87 avg=2.73\n",
            "[638 | 1202.39] loss=2.92 avg=2.73\n",
            "[639 | 1204.06] loss=2.47 avg=2.73\n",
            "[640 | 1205.71] loss=2.50 avg=2.73\n",
            "[641 | 1207.37] loss=2.63 avg=2.73\n",
            "[642 | 1209.03] loss=2.83 avg=2.73\n",
            "[643 | 1210.68] loss=2.69 avg=2.73\n",
            "[644 | 1212.34] loss=3.19 avg=2.73\n",
            "[645 | 1214.00] loss=3.26 avg=2.74\n",
            "[646 | 1215.65] loss=3.18 avg=2.74\n",
            "[647 | 1217.30] loss=2.90 avg=2.74\n",
            "[648 | 1218.95] loss=2.13 avg=2.74\n",
            "[649 | 1220.59] loss=2.27 avg=2.73\n",
            "[650 | 1222.24] loss=2.98 avg=2.74\n",
            "[651 | 1223.88] loss=2.74 avg=2.74\n",
            "[652 | 1225.53] loss=3.02 avg=2.74\n",
            "[653 | 1227.16] loss=2.86 avg=2.74\n",
            "[654 | 1228.80] loss=2.74 avg=2.74\n",
            "[655 | 1230.44] loss=2.44 avg=2.74\n",
            "[656 | 1232.08] loss=2.82 avg=2.74\n",
            "[657 | 1233.72] loss=2.48 avg=2.73\n",
            "[658 | 1235.36] loss=2.76 avg=2.73\n",
            "[659 | 1237.00] loss=2.63 avg=2.73\n",
            "[660 | 1238.62] loss=2.71 avg=2.73\n",
            "[661 | 1240.26] loss=2.31 avg=2.73\n",
            "[662 | 1241.89] loss=2.30 avg=2.72\n",
            "[663 | 1243.53] loss=2.42 avg=2.72\n",
            "[664 | 1245.18] loss=3.27 avg=2.73\n",
            "[665 | 1246.82] loss=2.77 avg=2.73\n",
            "[666 | 1248.46] loss=2.76 avg=2.73\n",
            "[667 | 1250.10] loss=3.09 avg=2.73\n",
            "[668 | 1251.74] loss=2.69 avg=2.73\n",
            "[669 | 1253.38] loss=2.71 avg=2.73\n",
            "[670 | 1255.03] loss=2.54 avg=2.73\n",
            "[671 | 1256.67] loss=2.89 avg=2.73\n",
            "[672 | 1258.31] loss=2.80 avg=2.73\n",
            "[673 | 1259.95] loss=2.22 avg=2.73\n",
            "[674 | 1261.60] loss=2.66 avg=2.73\n",
            "[675 | 1263.25] loss=2.56 avg=2.72\n",
            "[676 | 1264.90] loss=2.73 avg=2.72\n",
            "[677 | 1266.55] loss=2.45 avg=2.72\n",
            "[678 | 1268.21] loss=3.07 avg=2.72\n",
            "[679 | 1269.86] loss=2.46 avg=2.72\n",
            "[680 | 1271.52] loss=2.55 avg=2.72\n",
            "[681 | 1273.19] loss=2.66 avg=2.72\n",
            "[682 | 1274.86] loss=3.03 avg=2.72\n",
            "[683 | 1276.54] loss=3.06 avg=2.73\n",
            "[684 | 1278.20] loss=2.89 avg=2.73\n",
            "[685 | 1279.88] loss=2.12 avg=2.72\n",
            "[686 | 1281.56] loss=1.97 avg=2.71\n",
            "[687 | 1283.24] loss=2.15 avg=2.71\n",
            "[688 | 1284.93] loss=2.70 avg=2.71\n",
            "[689 | 1286.60] loss=3.07 avg=2.71\n",
            "[690 | 1288.29] loss=2.53 avg=2.71\n",
            "[691 | 1289.97] loss=2.87 avg=2.71\n",
            "[692 | 1291.66] loss=2.84 avg=2.71\n",
            "[693 | 1293.34] loss=2.51 avg=2.71\n",
            "[694 | 1295.02] loss=2.69 avg=2.71\n",
            "[695 | 1296.70] loss=1.79 avg=2.70\n",
            "[696 | 1298.38] loss=2.77 avg=2.70\n",
            "[697 | 1300.05] loss=2.54 avg=2.70\n",
            "[698 | 1301.72] loss=2.86 avg=2.70\n",
            "[699 | 1303.38] loss=2.66 avg=2.70\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "rod and all the rest of it) to a new era of political discourse.<|endoftext|>“You may take this knife on your mission, but I'm going to take my knives with me.” ―Sharlto Copley [src]\n",
            "\n",
            "The Cone-Crouch was an energy weapon developed by Sharlto Copley during the Battle of Endor and used by him and his former rebel comrades during the Battle of Endor during the Clone Wars. The Cone-Crouch was composed of ten large cylinders arranged in a semicircle. A single cylinder contained ten bolts on its shaft and contained the missile casing inside, while a larger cylinder contained ten smaller cylinders.\n",
            "\n",
            "Appearances Edit\n",
            "\n",
            "Sources Edit\n",
            "\n",
            "Notes and references Edit<|endoftext|>When news broke yesterday that former UFC light heavyweight champion Jon Jones was allegedly offered a fight with Bellator light heavyweight champion Daniel Cormier, it triggered a predictable response from MMA aficionados. The outcry came primarily from those hoping for an immediate rematch between Jones and the champion—a rematch that would be one of the most anticipated fights of the year.\n",
            "\n",
            "One of the UFC's biggest stars came out of retirement and immediately offered the former champ the opportunity to fight for the first time in 14 years.\n",
            "\n",
            "Jones has already made his intentions abundantly clear. His desire is to compete against Cormier—the man who became his greatest opponent when Cormier captured a belt, for real, in 2015—in whatever fashion he sets his mind to.\n",
            "\n",
            "In the last 10 months, Jones has turned his focus to his career and has decided not to fight at a weight that Cormier could handle. Cormier could potentially push Jones' weight up to 170-pounds and make him a monster, but for now Cormier is the choice.\n",
            "\n",
            "From the outside looking in, it may seem like Cormier is the best fighter in the UFC, but the champ certainly isn't the only one to find the division ripe with talent. On the other side of the fence, former champion Benson Henderson is also eager to see Cormier face off against him.\n",
            "\n",
            "This, of course, wouldn't sit well with Jones, who hasn't been too keen on the thought of seeing the champion meet the guy who beat him in the main event of UFC 205. As much as the former champion was looking forward to seeing the rematch in his hometown of Las Vegas, he was ready to see Cormier face off at UFC 205 in San Jose. It seems Cormier is more than willing to make the trip there, as he was a guest on the latest edition of The MMA Hour to discuss the possibility of facing off against the former champion.\n",
            "\n",
            "This may be the first time in 20 years that a champion has stepped out of the cage and into the crowd at an event like UFC 205. The fans were happy to see him there for his scheduled fight against Daniel Cormier last Saturday night, but when Henderson entered the ring, it was as if the lights went out in the building.\n",
            "\n",
            "From the outside looking in, UFC President Dana White and his men may have thought they were having a good time. The lights came on, fans came out on the main stage, and everything was going good for White and the UFC. And then, just like that, things got very dicey.\n",
            "\n",
            "The UFC, which has traditionally been a safe sport, is now facing an existential threat as the fans in San Jose turned on the fans at UFC 205.\n",
            "\n",
            "This has happened before, of course. In fact, the same thing has happened before for White, whose team didn't do a particularly good job of telling the truth around him and who eventually had to step down as the situation evolved.\n",
            "\n",
            "The last thing the UFC needs right now is another scandal involving its most successful star coming out of the cage. Jones has made it clear that he is not willing to step over his line, and he isn't going to settle for anything less than he deserves.\n",
            "\n",
            "Cormier had to know the extent of the animosity he was generating around the world, and the sport he is a part of. The fact that some of his opponents were willing to consider him vulnerable at the hands of a former world champion is a real problem and one that can't be allowed to continue.\n",
            "\n",
            "For now, Jones is still the fighter UFC needs to get its next big event out of the way. He just has to get his act together in a big way and let everyone do their job.\n",
            "\n",
            "The problem is, the way Jones is going about his business, he's getting a little tired of getting his act together. His fans in Las Vegas weren't going to be getting their excitement-fueled feelings ratcheted up to the level they were trying to get by beating the guy who beat them more than a decade ago.\n",
            "\n",
            "It's all so awkward for one of the sport's biggest stars. If he can keep himself in the mindset that he is being treated well, the UFC won't need\n",
            "\n",
            "[700 | 1327.86] loss=3.08 avg=2.71\n",
            "[701 | 1329.49] loss=2.41 avg=2.70\n",
            "[702 | 1331.14] loss=2.62 avg=2.70\n",
            "[703 | 1332.79] loss=2.61 avg=2.70\n",
            "[704 | 1334.44] loss=2.13 avg=2.70\n",
            "[705 | 1336.08] loss=2.56 avg=2.69\n",
            "[706 | 1337.73] loss=3.25 avg=2.70\n",
            "[707 | 1339.38] loss=2.37 avg=2.70\n",
            "[708 | 1341.02] loss=2.86 avg=2.70\n",
            "[709 | 1342.66] loss=2.71 avg=2.70\n",
            "[710 | 1344.30] loss=2.46 avg=2.70\n",
            "[711 | 1345.94] loss=3.07 avg=2.70\n",
            "[712 | 1347.58] loss=3.04 avg=2.70\n",
            "[713 | 1349.22] loss=2.08 avg=2.70\n",
            "[714 | 1350.86] loss=2.55 avg=2.70\n",
            "[715 | 1352.50] loss=2.21 avg=2.69\n",
            "[716 | 1354.14] loss=3.00 avg=2.69\n",
            "[717 | 1355.78] loss=3.00 avg=2.70\n",
            "[718 | 1357.42] loss=2.54 avg=2.70\n",
            "[719 | 1359.06] loss=2.36 avg=2.69\n",
            "[720 | 1360.71] loss=3.06 avg=2.70\n",
            "[721 | 1362.35] loss=2.63 avg=2.69\n",
            "[722 | 1364.01] loss=2.83 avg=2.70\n",
            "[723 | 1365.66] loss=2.80 avg=2.70\n",
            "[724 | 1367.31] loss=2.69 avg=2.70\n",
            "[725 | 1368.96] loss=3.31 avg=2.70\n",
            "[726 | 1370.63] loss=3.05 avg=2.71\n",
            "[727 | 1372.29] loss=2.78 avg=2.71\n",
            "[728 | 1373.95] loss=2.51 avg=2.71\n",
            "[729 | 1375.62] loss=2.58 avg=2.70\n",
            "[730 | 1377.30] loss=2.61 avg=2.70\n",
            "[731 | 1378.98] loss=2.64 avg=2.70\n",
            "[732 | 1380.67] loss=2.50 avg=2.70\n",
            "[733 | 1382.36] loss=2.74 avg=2.70\n",
            "[734 | 1384.05] loss=2.46 avg=2.70\n",
            "[735 | 1385.75] loss=2.82 avg=2.70\n",
            "[736 | 1387.44] loss=3.02 avg=2.70\n",
            "[737 | 1389.13] loss=2.32 avg=2.70\n",
            "[738 | 1390.82] loss=1.92 avg=2.69\n",
            "[739 | 1392.50] loss=2.48 avg=2.69\n",
            "[740 | 1394.18] loss=2.45 avg=2.69\n",
            "[741 | 1395.85] loss=2.90 avg=2.69\n",
            "[742 | 1397.52] loss=2.93 avg=2.69\n",
            "[743 | 1399.19] loss=2.65 avg=2.69\n",
            "[744 | 1400.86] loss=2.25 avg=2.69\n",
            "[745 | 1402.53] loss=2.54 avg=2.69\n",
            "[746 | 1404.20] loss=2.47 avg=2.68\n",
            "[747 | 1405.86] loss=3.10 avg=2.69\n",
            "[748 | 1407.52] loss=2.46 avg=2.68\n",
            "[749 | 1409.18] loss=2.69 avg=2.69\n",
            "[750 | 1410.83] loss=2.92 avg=2.69\n",
            "[751 | 1412.49] loss=2.11 avg=2.68\n",
            "[752 | 1414.14] loss=2.36 avg=2.68\n",
            "[753 | 1415.79] loss=2.77 avg=2.68\n",
            "[754 | 1417.44] loss=2.12 avg=2.67\n",
            "[755 | 1419.10] loss=2.34 avg=2.67\n",
            "[756 | 1420.74] loss=2.51 avg=2.67\n",
            "[757 | 1422.40] loss=2.81 avg=2.67\n",
            "[758 | 1424.04] loss=2.86 avg=2.67\n",
            "[759 | 1425.69] loss=2.35 avg=2.67\n",
            "[760 | 1427.34] loss=2.68 avg=2.67\n",
            "[761 | 1428.98] loss=2.67 avg=2.67\n",
            "[762 | 1430.62] loss=2.68 avg=2.67\n",
            "[763 | 1432.27] loss=2.62 avg=2.67\n",
            "[764 | 1433.91] loss=2.54 avg=2.67\n",
            "[765 | 1435.55] loss=2.98 avg=2.67\n",
            "[766 | 1437.19] loss=2.33 avg=2.67\n",
            "[767 | 1438.84] loss=2.91 avg=2.67\n",
            "[768 | 1440.48] loss=2.81 avg=2.67\n",
            "[769 | 1442.12] loss=2.53 avg=2.67\n",
            "[770 | 1443.76] loss=1.88 avg=2.66\n",
            "[771 | 1445.40] loss=2.56 avg=2.66\n",
            "[772 | 1447.03] loss=2.71 avg=2.66\n",
            "[773 | 1448.67] loss=2.93 avg=2.66\n",
            "[774 | 1450.31] loss=2.53 avg=2.66\n",
            "[775 | 1451.96] loss=3.28 avg=2.67\n",
            "[776 | 1453.60] loss=2.54 avg=2.67\n",
            "[777 | 1455.24] loss=2.64 avg=2.67\n",
            "[778 | 1456.89] loss=2.69 avg=2.67\n",
            "[779 | 1458.54] loss=2.90 avg=2.67\n",
            "[780 | 1460.19] loss=2.37 avg=2.67\n",
            "[781 | 1461.84] loss=2.63 avg=2.67\n",
            "[782 | 1463.49] loss=2.56 avg=2.67\n",
            "[783 | 1465.14] loss=2.11 avg=2.66\n",
            "[784 | 1466.80] loss=3.14 avg=2.66\n",
            "[785 | 1468.47] loss=2.63 avg=2.66\n",
            "[786 | 1470.13] loss=2.60 avg=2.66\n",
            "[787 | 1471.81] loss=2.41 avg=2.66\n",
            "[788 | 1473.48] loss=2.70 avg=2.66\n",
            "[789 | 1475.15] loss=2.60 avg=2.66\n",
            "[790 | 1476.83] loss=1.78 avg=2.65\n",
            "[791 | 1478.51] loss=2.20 avg=2.65\n",
            "[792 | 1480.18] loss=2.69 avg=2.65\n",
            "[793 | 1481.86] loss=2.57 avg=2.65\n",
            "[794 | 1483.53] loss=2.84 avg=2.65\n",
            "[795 | 1485.20] loss=1.89 avg=2.64\n",
            "[796 | 1486.88] loss=2.20 avg=2.64\n",
            "[797 | 1488.57] loss=2.46 avg=2.64\n",
            "[798 | 1490.26] loss=2.74 avg=2.64\n",
            "[799 | 1491.94] loss=2.68 avg=2.64\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " a questionable relationship — to me her husband had better be there and that they were ready to make love and be intimate.\n",
            "\n",
            "Kathleen: I did find that [Lincoln] was pretty much open to the idea of having sex outside the marriage, which I've kind of gotten used to, especially since we were younger. I also thought he [was] very strong — you've got to have a strong man in the relationship, no? I also think he was a little vain, which you have to understand is how you get past the difficulties of marriage [laughs].\n",
            "\n",
            "The other thing that I found interesting, if I may say so, about Lizzie is that she was a great listener to the couple. … When Louis first appeared on camera, I remember listening to their baby talk. She listened to it, and she loved it, and she liked playing up the baby talk.\n",
            "\n",
            "Margo: Is there a story you want to share with us that didn't make it into the episode, and we didn't get to include in the podcast?\n",
            "\n",
            "Kathleen: [Louis] had this moment when he walked up to her on the dance floor, and Lizzie turned her head and he grabbed her face [laughs]. It was a beautiful moment. That had a very, very long-lasting effect on Lizzie — one of the best moments in television as far as what we did with [this moment] and what it could mean to the couple. I think Louis was aware that that moment had a very long-lasting effect on her, and I think Louis just thought of her as the mother of his love.\n",
            "\n",
            "Margo: I think it's fair to say that Lizzie and Louis are both a little bit older than you guys and a little bit wiser, with regard to life. But when they first meet, is there anyone they talk about a little bit more …\n",
            "\n",
            "Kathleen: As far as the relationship goes, Louis and I are much more of an open-minded couple and [than] Lizzie or myself is an open-minded couple.\n",
            "\n",
            "Margo: Do you always think about how far they can go before they fall apart?\n",
            "\n",
            "Kathleen: Absolutely. I think Louis is always like, You know what? Just for me, the relationship between me and Lizzie, that was so far, and he knew that it was probably going to be that long before he would fall apart in love.\n",
            "\n",
            "\n",
            "\n",
            "Devin [Spencer—Editor] is married with three daughters. In a series of columns for the magazine published in September 2016, he talked about the ups and downs of being a dad in the new world of work-life balance and the sacrifices made to keep the family in a house in the family [in Chicago]. He wrote: My story … begins in mid-October of 2013, on the third day of the school year. I'm 20 and a sophomore at Oberlin College, sitting at my desk in my first semester teaching creative writing. All I have in my life is my work and my job, and I have a girlfriend. We've got three young children together, and my day begins and ends with her. There are other students in our room next door to me, who have just arrived from summer camp, working on Saturday afternoons. I'm a part-time student, and I teach as many classes as possible on my lunch hour. For the first time, my day is flexible. As I get older, I realize that there are so many opportunities for me as a teacher and as a husband. I want to have as much fun as possible and be as responsible as possible while also providing for my family. ... There are other moments of stress in my work life. I get a little nervous during the day, and my husband and I get into a lot of arguments. For me, the hardest point was when my day began. That first day of school ended up being the hardest time of my life, because I knew that I was about to be on my own, and I had no one to turn to. I felt I was drowning. I tried to be as responsive as possible to the demands of a new relationship, so that I wouldn't become that person I knew I didn't want to become. “ I felt like I didn’t have much in the way of a future. The school had [a] summer program opening up, but it didn’t include all of the courses, and we were still just starting. If I were to do something, I didn’t know what it would be. I had no money, and a lot of the classes weren’t in English or biology or any of the other subjects.\n",
            "\n",
            "While these challenges presented themselves in every possible way — not just with schoolwork, but with the life decisions that would follow, too — I was able to cope by figuring out how to cope. I started writing to\n",
            "\n",
            "[800 | 1516.47] loss=2.67 avg=2.64\n",
            "[801 | 1518.12] loss=2.29 avg=2.63\n",
            "[802 | 1519.77] loss=2.60 avg=2.63\n",
            "[803 | 1521.42] loss=3.06 avg=2.64\n",
            "[804 | 1523.06] loss=2.36 avg=2.63\n",
            "[805 | 1524.71] loss=2.61 avg=2.63\n",
            "[806 | 1526.35] loss=2.48 avg=2.63\n",
            "[807 | 1527.99] loss=1.99 avg=2.63\n",
            "[808 | 1529.63] loss=2.71 avg=2.63\n",
            "[809 | 1531.28] loss=1.72 avg=2.62\n",
            "[810 | 1532.92] loss=3.21 avg=2.62\n",
            "[811 | 1534.56] loss=2.67 avg=2.62\n",
            "[812 | 1536.20] loss=1.94 avg=2.62\n",
            "[813 | 1537.85] loss=3.29 avg=2.62\n",
            "[814 | 1539.49] loss=2.43 avg=2.62\n",
            "[815 | 1541.13] loss=2.59 avg=2.62\n",
            "[816 | 1542.78] loss=1.45 avg=2.61\n",
            "[817 | 1544.42] loss=3.13 avg=2.62\n",
            "[818 | 1546.05] loss=3.07 avg=2.62\n",
            "[819 | 1547.70] loss=2.68 avg=2.62\n",
            "[820 | 1549.34] loss=2.21 avg=2.62\n",
            "[821 | 1550.98] loss=2.51 avg=2.62\n",
            "[822 | 1552.63] loss=2.76 avg=2.62\n",
            "[823 | 1554.27] loss=2.82 avg=2.62\n",
            "[824 | 1555.91] loss=2.68 avg=2.62\n",
            "[825 | 1557.56] loss=2.75 avg=2.62\n",
            "[826 | 1559.22] loss=2.61 avg=2.62\n",
            "[827 | 1560.87] loss=2.37 avg=2.62\n",
            "[828 | 1562.52] loss=2.77 avg=2.62\n",
            "[829 | 1564.17] loss=2.22 avg=2.62\n",
            "[830 | 1565.83] loss=2.77 avg=2.62\n",
            "[831 | 1567.49] loss=3.26 avg=2.62\n",
            "[832 | 1569.15] loss=2.93 avg=2.63\n",
            "[833 | 1570.81] loss=3.18 avg=2.63\n",
            "[834 | 1572.47] loss=2.56 avg=2.63\n",
            "[835 | 1574.14] loss=2.28 avg=2.63\n",
            "[836 | 1575.81] loss=2.25 avg=2.62\n",
            "[837 | 1577.48] loss=2.21 avg=2.62\n",
            "[838 | 1579.15] loss=2.60 avg=2.62\n",
            "[839 | 1580.83] loss=2.65 avg=2.62\n",
            "[840 | 1582.51] loss=2.38 avg=2.62\n",
            "[841 | 1584.19] loss=3.06 avg=2.62\n",
            "[842 | 1585.87] loss=2.25 avg=2.62\n",
            "[843 | 1587.54] loss=2.87 avg=2.62\n",
            "[844 | 1589.24] loss=2.41 avg=2.62\n",
            "[845 | 1590.92] loss=2.53 avg=2.62\n",
            "[846 | 1592.61] loss=2.44 avg=2.62\n",
            "[847 | 1594.30] loss=3.09 avg=2.62\n",
            "[848 | 1595.97] loss=2.55 avg=2.62\n",
            "[849 | 1597.66] loss=3.00 avg=2.62\n",
            "[850 | 1599.33] loss=2.76 avg=2.63\n",
            "[851 | 1601.00] loss=2.65 avg=2.63\n",
            "[852 | 1602.67] loss=3.27 avg=2.63\n",
            "[853 | 1604.34] loss=2.66 avg=2.63\n",
            "[854 | 1606.00] loss=2.40 avg=2.63\n",
            "[855 | 1607.66] loss=2.86 avg=2.63\n",
            "[856 | 1609.32] loss=2.07 avg=2.63\n",
            "[857 | 1610.98] loss=2.36 avg=2.62\n",
            "[858 | 1612.63] loss=2.33 avg=2.62\n",
            "[859 | 1614.28] loss=2.85 avg=2.62\n",
            "[860 | 1615.93] loss=2.96 avg=2.63\n",
            "[861 | 1617.58] loss=2.73 avg=2.63\n",
            "[862 | 1619.22] loss=2.36 avg=2.62\n",
            "[863 | 1620.85] loss=2.27 avg=2.62\n",
            "[864 | 1622.50] loss=2.94 avg=2.62\n",
            "[865 | 1624.15] loss=2.83 avg=2.63\n",
            "[866 | 1625.79] loss=2.69 avg=2.63\n",
            "[867 | 1627.44] loss=2.58 avg=2.63\n",
            "[868 | 1629.08] loss=2.29 avg=2.62\n",
            "[869 | 1630.73] loss=1.99 avg=2.62\n",
            "[870 | 1632.37] loss=2.39 avg=2.61\n",
            "[871 | 1634.01] loss=2.50 avg=2.61\n",
            "[872 | 1635.65] loss=2.18 avg=2.61\n",
            "[873 | 1637.30] loss=3.04 avg=2.61\n",
            "[874 | 1638.94] loss=2.60 avg=2.61\n",
            "[875 | 1640.57] loss=2.49 avg=2.61\n",
            "[876 | 1642.21] loss=2.70 avg=2.61\n",
            "[877 | 1643.86] loss=2.44 avg=2.61\n",
            "[878 | 1645.49] loss=2.36 avg=2.61\n",
            "[879 | 1647.13] loss=2.32 avg=2.61\n",
            "[880 | 1648.77] loss=2.94 avg=2.61\n",
            "[881 | 1650.41] loss=2.24 avg=2.61\n",
            "[882 | 1652.05] loss=2.41 avg=2.60\n",
            "[883 | 1653.69] loss=2.77 avg=2.61\n",
            "[884 | 1655.33] loss=2.65 avg=2.61\n",
            "[885 | 1656.97] loss=2.33 avg=2.60\n",
            "[886 | 1658.62] loss=2.80 avg=2.61\n",
            "[887 | 1660.26] loss=2.92 avg=2.61\n",
            "[888 | 1661.91] loss=3.07 avg=2.61\n",
            "[889 | 1663.56] loss=2.56 avg=2.61\n",
            "[890 | 1665.21] loss=2.22 avg=2.61\n",
            "[891 | 1666.86] loss=2.50 avg=2.61\n",
            "[892 | 1668.52] loss=2.38 avg=2.61\n",
            "[893 | 1670.18] loss=2.48 avg=2.60\n",
            "[894 | 1671.84] loss=2.76 avg=2.61\n",
            "[895 | 1673.51] loss=2.77 avg=2.61\n",
            "[896 | 1675.18] loss=2.62 avg=2.61\n",
            "[897 | 1676.86] loss=2.23 avg=2.60\n",
            "[898 | 1678.53] loss=2.72 avg=2.60\n",
            "[899 | 1680.20] loss=2.55 avg=2.60\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            ". But there are two more important things to note, one positive and one negative, at this time. First, we find that in the 21st century, science-fiction has been the leading source of entertainment for young adults, and fantasy is more popular among females than male fans. Even as young women continue to dominate as moviegoers, fantasy is still a fairly significant and lucrative genre for women. The fact that so many of the stories presented to them are about women characters also highlights the importance of more diverse genres in entertainment. Second, the results suggest that a significant portion of the younger adult female audience is drawn to the genre by the characters, rather than by the storytelling.\n",
            "\n",
            "The authors' analysis of the data is interesting as well:\n",
            "\n",
            "As more female adults come to expect a female-driven storytelling approach rather than a man’s, more women are drawn to genre fiction. The extent to which females are drawn to genre fiction, however, varies greatly by genre. Women with more general interest in science fiction were significantly more likely to be drawn to fantasy stories than women with more specific interests [in] noir or historical suspense.\n",
            "\n",
            "This seems to reflect a general trend toward more nuanced storylines. The authors concluded that because fantasy is a genre whose stories are told in visual novel-style games and interactive online elements, a narrative more grounded in reality may appeal to women more than other forms of storytelling.\n",
            "\n",
            "So, while there is still more work to be done to advance representation of women in science fiction in the media, the authors note that the field is in a good place.\n",
            "\n",
            "“We are moving in the right direction with an array of excellent new writers and creators, and our stories will be the ones that define the future of the genre in the years to come,” they wrote. It’s great to see fantasy reaching a younger demographic, and I wish the writers and authors who do their best to represent the diversity of the genre would continue to do so.\n",
            "\n",
            "But for now, the work of female creators is simply not enough. “Science-fiction has always been about the female protagonists,” the authors concluded, “but today, more and more, women are being told stories that actually have a dramatic impact on the protagonists, not the story.” If they want to continue to tell compelling feminist stories, they need to have more than a few white protagonists and female protagonists to choose from. They also need to have diverse casts. They need a cast of the same type of people, and a cast of writers to tell the same stories, across the genres.\n",
            "\n",
            "There is also the question of whether the genre will continue to exist as a cultural phenomenon of its own, distinct from other forms of entertainment, if women are not able to work their way into the story. The authors of the new study say that, on balance, there was an increase in young female readers after the series is introduced. But the authors note that there are challenges in trying to convey compelling stories to an audience in the 21st century, and they say that the science-fantasy genre is still undergoing its own structural change, in terms of female representation, that also affects the way story is told. And yet, despite such challenges, they concluded, the series is still popular. That certainly speaks to the strength of the genre; its popularity might reflect the fact that it is not just a male-dominated genre; it can also reflect the fact that it has continued to thrive even without male creators.\n",
            "\n",
            "\n",
            "\n",
            "I used to watch The Twilight Zone, with its recurring nightmare sequences and surrealist framing of everyday situations, in my teens. After my mother died, when I was 19, I started a website devoted to the show, in part because of the way that my grief felt like there should be a way to describe it for people without sounding reductive. And so in December 2012, after watching all but the first episode of the original series, I subscribed to The New York Times’s review of the series on DVD, with the disclaimer that I hadn’t seen the whole show. (Because I was watching it on my own, not in the context of the Times review, I didn’t read the review. But the episode included the first-ever teaser for the new season, and the episode title alone was enough to tell potential viewer to get it on Netflix.)\n",
            "\n",
            "By then, The Twilight Zone had become such a cult classic that television viewers have been playing and replaying scenes of it on repeat for nearly two decades. I was a newbie to the show, and so there weren’t that many familiar faces on that first episode of Season 1. But I still remembered my shock when I learned that the new show was made by an all-female production team, which included two longtime Twilight Zone writers—Judi Goldfuss and Kim Stanley Robinson (who also wrote the series’ first two seasons), and the long-running science-fiction sitcom All in\n",
            "\n",
            "[900 | 1704.80] loss=2.90 avg=2.61\n",
            "[901 | 1706.46] loss=2.60 avg=2.61\n",
            "[902 | 1708.13] loss=2.90 avg=2.61\n",
            "[903 | 1709.79] loss=2.45 avg=2.61\n",
            "[904 | 1711.45] loss=2.62 avg=2.61\n",
            "[905 | 1713.10] loss=2.67 avg=2.61\n",
            "[906 | 1714.76] loss=2.58 avg=2.61\n",
            "[907 | 1716.41] loss=3.00 avg=2.61\n",
            "[908 | 1718.07] loss=1.80 avg=2.60\n",
            "[909 | 1719.72] loss=2.52 avg=2.60\n",
            "[910 | 1721.37] loss=2.49 avg=2.60\n",
            "[911 | 1723.00] loss=2.05 avg=2.60\n",
            "[912 | 1724.64] loss=2.66 avg=2.60\n",
            "[913 | 1726.28] loss=3.16 avg=2.60\n",
            "[914 | 1727.93] loss=2.79 avg=2.60\n",
            "[915 | 1729.57] loss=2.90 avg=2.61\n",
            "[916 | 1731.22] loss=2.00 avg=2.60\n",
            "[917 | 1732.86] loss=2.71 avg=2.60\n",
            "[918 | 1734.50] loss=2.73 avg=2.60\n",
            "[919 | 1736.15] loss=2.58 avg=2.60\n",
            "[920 | 1737.79] loss=2.94 avg=2.61\n",
            "[921 | 1739.43] loss=2.37 avg=2.60\n",
            "[922 | 1741.06] loss=2.98 avg=2.61\n",
            "[923 | 1742.70] loss=2.23 avg=2.60\n",
            "[924 | 1744.33] loss=2.09 avg=2.60\n",
            "[925 | 1745.98] loss=2.73 avg=2.60\n",
            "[926 | 1747.62] loss=1.70 avg=2.59\n",
            "[927 | 1749.25] loss=2.83 avg=2.59\n",
            "[928 | 1750.89] loss=3.04 avg=2.60\n",
            "[929 | 1752.53] loss=3.25 avg=2.61\n",
            "[930 | 1754.17] loss=2.58 avg=2.61\n",
            "[931 | 1755.81] loss=2.61 avg=2.61\n",
            "[932 | 1757.45] loss=3.24 avg=2.61\n",
            "[933 | 1759.10] loss=2.49 avg=2.61\n",
            "[934 | 1760.74] loss=2.05 avg=2.60\n",
            "[935 | 1762.39] loss=2.42 avg=2.60\n",
            "[936 | 1764.04] loss=2.09 avg=2.60\n",
            "[937 | 1765.69] loss=2.15 avg=2.59\n",
            "[938 | 1767.35] loss=2.83 avg=2.60\n",
            "[939 | 1769.01] loss=2.28 avg=2.59\n",
            "[940 | 1770.67] loss=2.59 avg=2.59\n",
            "[941 | 1772.35] loss=2.75 avg=2.59\n",
            "[942 | 1774.02] loss=2.60 avg=2.59\n",
            "[943 | 1775.70] loss=1.41 avg=2.58\n",
            "[944 | 1777.37] loss=2.21 avg=2.58\n",
            "[945 | 1779.06] loss=2.80 avg=2.58\n",
            "[946 | 1780.76] loss=2.07 avg=2.58\n",
            "[947 | 1782.46] loss=2.96 avg=2.58\n",
            "[948 | 1784.16] loss=2.26 avg=2.58\n",
            "[949 | 1785.87] loss=1.89 avg=2.57\n",
            "[950 | 1787.58] loss=2.72 avg=2.57\n",
            "[951 | 1789.28] loss=2.45 avg=2.57\n",
            "[952 | 1790.98] loss=2.41 avg=2.57\n",
            "[953 | 1792.67] loss=3.06 avg=2.57\n",
            "[954 | 1794.37] loss=2.50 avg=2.57\n",
            "[955 | 1796.05] loss=1.91 avg=2.57\n",
            "[956 | 1797.74] loss=3.19 avg=2.57\n",
            "[957 | 1799.41] loss=2.64 avg=2.57\n",
            "[958 | 1801.08] loss=1.82 avg=2.57\n",
            "[959 | 1802.75] loss=2.24 avg=2.56\n",
            "[960 | 1804.42] loss=2.24 avg=2.56\n",
            "[961 | 1806.09] loss=3.11 avg=2.56\n",
            "[962 | 1807.76] loss=2.26 avg=2.56\n",
            "[963 | 1809.42] loss=2.47 avg=2.56\n",
            "[964 | 1811.08] loss=2.31 avg=2.56\n",
            "[965 | 1812.74] loss=1.86 avg=2.55\n",
            "[966 | 1814.40] loss=2.64 avg=2.55\n",
            "[967 | 1816.05] loss=2.17 avg=2.55\n",
            "[968 | 1817.71] loss=2.45 avg=2.55\n",
            "[969 | 1819.37] loss=2.16 avg=2.54\n",
            "[970 | 1821.02] loss=2.50 avg=2.54\n",
            "[971 | 1822.67] loss=2.38 avg=2.54\n",
            "[972 | 1824.32] loss=3.26 avg=2.55\n",
            "[973 | 1825.97] loss=2.23 avg=2.55\n",
            "[974 | 1827.63] loss=2.37 avg=2.54\n",
            "[975 | 1829.27] loss=2.77 avg=2.55\n",
            "[976 | 1830.92] loss=2.54 avg=2.55\n",
            "[977 | 1832.57] loss=2.78 avg=2.55\n",
            "[978 | 1834.21] loss=2.61 avg=2.55\n",
            "[979 | 1835.86] loss=2.38 avg=2.55\n",
            "[980 | 1837.50] loss=2.71 avg=2.55\n",
            "[981 | 1839.15] loss=3.15 avg=2.55\n",
            "[982 | 1840.79] loss=2.75 avg=2.56\n",
            "[983 | 1842.43] loss=2.81 avg=2.56\n",
            "[984 | 1844.07] loss=1.97 avg=2.55\n",
            "[985 | 1845.71] loss=2.49 avg=2.55\n",
            "[986 | 1847.34] loss=1.65 avg=2.54\n",
            "[987 | 1848.98] loss=2.50 avg=2.54\n",
            "[988 | 1850.62] loss=2.79 avg=2.55\n",
            "[989 | 1852.26] loss=2.86 avg=2.55\n",
            "[990 | 1853.90] loss=3.02 avg=2.55\n",
            "[991 | 1855.52] loss=2.88 avg=2.56\n",
            "[992 | 1857.16] loss=2.51 avg=2.56\n",
            "[993 | 1858.79] loss=2.52 avg=2.56\n",
            "[994 | 1860.44] loss=2.30 avg=2.55\n",
            "[995 | 1862.09] loss=2.50 avg=2.55\n",
            "[996 | 1863.73] loss=2.76 avg=2.55\n",
            "[997 | 1865.38] loss=2.98 avg=2.56\n",
            "[998 | 1867.03] loss=3.02 avg=2.56\n",
            "[999 | 1868.69] loss=2.51 avg=2.56\n",
            "Ran 1010 steps.\n",
            "Saving checkpoint/atlantic_entertainment_345/model-1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXCgZs5p8Uo5",
        "colab_type": "code",
        "outputId": "3eb36015-324f-4510-9baf-a181c9b4e7bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "## ideas essays training - with 345\n",
        "!PYTHONPATH=src ./train.py --dataset /content/all_ideas.txt --run_name 'atlantic_ideas_345' --model_name '345M'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0627 18:58:51.886311 140366815840128 deprecation_wrapper.py:119] From /content/gpt-2/src/model.py:147: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
            "\n",
            "W0627 18:58:51.895255 140366815840128 deprecation_wrapper.py:119] From /content/gpt-2/src/memory_saving_gradients.py:13: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n",
            "W0627 18:58:51.986181 140366815840128 deprecation_wrapper.py:119] From ./train.py:88: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "W0627 18:58:51.986577 140366815840128 deprecation_wrapper.py:119] From ./train.py:91: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "2019-06-27 18:58:51.993001: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-06-27 18:58:51.993257: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1ce7100 executing computations on platform Host. Devices:\n",
            "2019-06-27 18:58:51.993288: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2019-06-27 18:58:51.996028: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\n",
            "2019-06-27 18:58:52.152531: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-27 18:58:52.153079: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1ce6840 executing computations on platform CUDA. Devices:\n",
            "2019-06-27 18:58:52.153109: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2019-06-27 18:58:52.153401: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-27 18:58:52.153760: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-06-27 18:58:52.154119: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-06-27 18:58:52.155410: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-06-27 18:58:52.156668: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-06-27 18:58:52.157028: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-06-27 18:58:52.158510: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-06-27 18:58:52.159699: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-06-27 18:58:52.162882: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-06-27 18:58:52.163026: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-27 18:58:52.163448: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-27 18:58:52.163846: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
            "2019-06-27 18:58:52.163914: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-06-27 18:58:52.164867: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-06-27 18:58:52.164896: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n",
            "2019-06-27 18:58:52.164907: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n",
            "2019-06-27 18:58:52.165219: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-27 18:58:52.165697: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-27 18:58:52.166076: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14202 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "W0627 18:58:52.166945 140366815840128 deprecation_wrapper.py:119] From ./train.py:92: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0627 18:59:03.045539 140366815840128 deprecation.py:323] From /content/gpt-2/src/sample.py:65: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "W0627 18:59:03.059544 140366815840128 deprecation.py:323] From /content/gpt-2/src/sample.py:16: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0627 18:59:03.061213 140366815840128 deprecation.py:323] From /content/gpt-2/src/sample.py:70: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.random.categorical` instead.\n",
            "W0627 18:59:03.071156 140366815840128 deprecation_wrapper.py:119] From ./train.py:121: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "W0627 18:59:18.348968 140366815840128 deprecation_wrapper.py:119] From ./train.py:144: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n",
            "W0627 18:59:18.352025 140366815840128 deprecation_wrapper.py:119] From ./train.py:147: The name tf.summary.merge is deprecated. Please use tf.compat.v1.summary.merge instead.\n",
            "\n",
            "W0627 18:59:18.352834 140366815840128 deprecation_wrapper.py:119] From ./train.py:149: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
            "\n",
            "W0627 18:59:18.353613 140366815840128 deprecation_wrapper.py:119] From ./train.py:152: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "Loading checkpoint models/345M/model.ckpt\n",
            "W0627 18:59:32.050755 140366815840128 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "Loading dataset...\n",
            "100% 1/1 [00:03<00:00,  3.67s/it]\n",
            "dataset has 459393 tokens\n",
            "Training...\n",
            "2019-06-27 18:59:49.730904: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n",
            "2019-06-27 18:59:50.430294: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "[1 | 14.03] loss=2.83 avg=2.83\n",
            "[2 | 15.55] loss=2.91 avg=2.87\n",
            "[3 | 17.07] loss=3.00 avg=2.91\n",
            "[4 | 18.60] loss=3.01 avg=2.94\n",
            "[5 | 20.12] loss=3.19 avg=2.99\n",
            "[6 | 21.66] loss=2.91 avg=2.97\n",
            "[7 | 23.20] loss=2.87 avg=2.96\n",
            "[8 | 24.73] loss=3.36 avg=3.01\n",
            "[9 | 26.27] loss=2.86 avg=2.99\n",
            "[10 | 27.82] loss=2.91 avg=2.99\n",
            "[11 | 29.36] loss=3.05 avg=2.99\n",
            "[12 | 30.90] loss=2.66 avg=2.96\n",
            "[13 | 32.45] loss=2.70 avg=2.94\n",
            "[14 | 34.00] loss=2.84 avg=2.93\n",
            "[15 | 35.55] loss=3.14 avg=2.95\n",
            "[16 | 37.11] loss=2.54 avg=2.92\n",
            "[17 | 38.67] loss=2.89 avg=2.92\n",
            "[18 | 40.24] loss=3.08 avg=2.93\n",
            "[19 | 41.81] loss=2.88 avg=2.93\n",
            "[20 | 43.39] loss=3.08 avg=2.93\n",
            "[21 | 44.96] loss=3.03 avg=2.94\n",
            "[22 | 46.53] loss=2.50 avg=2.92\n",
            "[23 | 48.11] loss=3.26 avg=2.93\n",
            "[24 | 49.69] loss=2.73 avg=2.92\n",
            "[25 | 51.27] loss=2.98 avg=2.93\n",
            "[26 | 52.86] loss=3.16 avg=2.94\n",
            "[27 | 54.43] loss=2.67 avg=2.93\n",
            "[28 | 56.02] loss=2.98 avg=2.93\n",
            "[29 | 57.60] loss=3.01 avg=2.93\n",
            "[30 | 59.18] loss=2.92 avg=2.93\n",
            "[31 | 60.77] loss=3.01 avg=2.93\n",
            "[32 | 62.34] loss=2.53 avg=2.92\n",
            "[33 | 63.92] loss=2.76 avg=2.91\n",
            "[34 | 65.50] loss=2.90 avg=2.91\n",
            "[35 | 67.08] loss=3.04 avg=2.92\n",
            "[36 | 68.66] loss=2.90 avg=2.92\n",
            "[37 | 70.24] loss=2.77 avg=2.91\n",
            "[38 | 71.82] loss=3.08 avg=2.92\n",
            "[39 | 73.40] loss=2.73 avg=2.91\n",
            "[40 | 74.98] loss=2.94 avg=2.91\n",
            "[41 | 76.56] loss=2.51 avg=2.90\n",
            "[42 | 78.14] loss=2.63 avg=2.89\n",
            "[43 | 79.73] loss=3.01 avg=2.90\n",
            "[44 | 81.32] loss=3.08 avg=2.90\n",
            "[45 | 82.90] loss=2.70 avg=2.90\n",
            "[46 | 84.48] loss=2.93 avg=2.90\n",
            "[47 | 86.06] loss=2.92 avg=2.90\n",
            "[48 | 87.63] loss=3.12 avg=2.90\n",
            "[49 | 89.22] loss=3.04 avg=2.91\n",
            "[50 | 90.80] loss=2.46 avg=2.90\n",
            "[51 | 92.39] loss=2.53 avg=2.89\n",
            "[52 | 93.97] loss=2.48 avg=2.88\n",
            "[53 | 95.55] loss=2.96 avg=2.88\n",
            "[54 | 97.13] loss=2.96 avg=2.88\n",
            "[55 | 98.71] loss=2.49 avg=2.87\n",
            "[56 | 100.31] loss=2.82 avg=2.87\n",
            "[57 | 101.89] loss=2.62 avg=2.86\n",
            "[58 | 103.48] loss=2.83 avg=2.86\n",
            "[59 | 105.07] loss=3.17 avg=2.87\n",
            "[60 | 106.66] loss=2.80 avg=2.87\n",
            "[61 | 108.25] loss=2.43 avg=2.86\n",
            "[62 | 109.84] loss=2.84 avg=2.86\n",
            "[63 | 111.43] loss=2.97 avg=2.86\n",
            "[64 | 113.02] loss=2.74 avg=2.86\n",
            "[65 | 114.61] loss=2.69 avg=2.86\n",
            "[66 | 116.20] loss=3.15 avg=2.86\n",
            "[67 | 117.79] loss=2.46 avg=2.85\n",
            "[68 | 119.38] loss=3.05 avg=2.86\n",
            "[69 | 120.97] loss=2.81 avg=2.86\n",
            "[70 | 122.56] loss=2.77 avg=2.85\n",
            "[71 | 124.15] loss=3.03 avg=2.86\n",
            "[72 | 125.74] loss=2.82 avg=2.86\n",
            "[73 | 127.33] loss=2.97 avg=2.86\n",
            "[74 | 128.92] loss=3.06 avg=2.86\n",
            "[75 | 130.52] loss=2.73 avg=2.86\n",
            "[76 | 132.12] loss=2.62 avg=2.86\n",
            "[77 | 133.74] loss=2.76 avg=2.85\n",
            "[78 | 135.34] loss=2.26 avg=2.84\n",
            "[79 | 136.95] loss=2.89 avg=2.84\n",
            "[80 | 138.57] loss=2.68 avg=2.84\n",
            "[81 | 140.19] loss=2.80 avg=2.84\n",
            "[82 | 141.82] loss=2.96 avg=2.84\n",
            "[83 | 143.45] loss=2.92 avg=2.84\n",
            "[84 | 145.09] loss=2.81 avg=2.84\n",
            "[85 | 146.71] loss=2.80 avg=2.84\n",
            "[86 | 148.35] loss=2.87 avg=2.84\n",
            "[87 | 149.99] loss=2.95 avg=2.84\n",
            "[88 | 151.62] loss=2.64 avg=2.84\n",
            "[89 | 153.26] loss=2.72 avg=2.84\n",
            "[90 | 154.89] loss=3.03 avg=2.84\n",
            "[91 | 156.52] loss=2.90 avg=2.84\n",
            "[92 | 158.15] loss=2.67 avg=2.84\n",
            "[93 | 159.77] loss=2.85 avg=2.84\n",
            "[94 | 161.39] loss=2.76 avg=2.84\n",
            "[95 | 163.02] loss=3.14 avg=2.84\n",
            "[96 | 164.63] loss=2.99 avg=2.85\n",
            "[97 | 166.25] loss=2.93 avg=2.85\n",
            "[98 | 167.87] loss=2.62 avg=2.84\n",
            "[99 | 169.48] loss=3.12 avg=2.85\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " he would be happy to meet with him. So the whole country saw and heard the same thing. His answer to this was that it would be better for his reputation if he did not meet with him. Then the people of England began to cry, and some of the principal men, especially the King, began to cry on both sides, saying, 'Why, the devil, is it that he does not meet with us? Why do we not wish that he should meet with us? He would come nearer to us by bringing us with him. He would bring us into England.' So they began to cry for him to come. But in every country the people cried harder for him to come and meet them than for him to meet the people of England. Now it became a universal custom that there is a feast to be celebrated at Westminster after sunset, from the morning of Sunday until evening hours. The people went to Westminster, and at the same time they made their supplications, and at night the Church was illuminated. In the year 1613, in the Church after sunset, there was a full moon, and at the same time the Church was illuminated with much glory and beauty, and the people went to Westminster. During the whole time, the King came from Windsor to Westminster in an open carriage. The people gathered there to greet him, and then he went to the Church as the host, and the host then went to the King through the open door, to the King in his carriage, and then they came to Westminster. When he saw the light that was there, in the Church, he stood for a long time and, as he saw the people, he said, 'My beloved people, it is now the hour of our destruction.' So he went in his carriage to the Church, and it was a full day afterward, when the King had come there. And the Church, as it is said, was a fire at the beginning, because it was burned with the light that was, on the whole, in the Church from the time of the consecration until then, and as all the people were gathered there, the fire burnt all the people. On the twentieth day after he entered the Church, he was told, 'My beloved people, you have all gone to your places. This is the most glorious day since the consecration. God will have the grace of blessing your people, and God will have the grace of blessing you. Now, my beloved people, I must speak to you about the greatest things that have happened to you since this is the greatest day since it was consecrated. The most glorious in the Church has come to be the King. In one of these great days, I say, the greatest, the greatest was brought from out of the Kingdom of Heaven.\n",
            "\n",
            "I will now give you a very long quotation from the famous writer, William Shakespeare. 'Tis well, O King of England, to send him to the scaffold,\n",
            "\n",
            "For thou hast sent a thousand,\n",
            "\n",
            "For that he is thy Majesty—\n",
            "\n",
            "If he be slain, thou hast sent his soul:\n",
            "\n",
            "Wherefore, God forbid, if his soul be not slain,\n",
            "\n",
            "It must be slain by another, or by man.\n",
            "\n",
            "When he was slain, he was a thousand\n",
            "\n",
            "Till the very hour of his death:\n",
            "\n",
            "God hath dealt with him through love. The King then was a thousand 'Till the very hour of death; and God could not have dealt with him by his own powers, save he must have brought him to death by another. The King must have brought him to death; but it cannot be otherwise. The first thing is to bring him to life. Then, if he be alive, God will send him to life. But we are in no one's hands to bring him to life. God has not made us in his own image. God must have made us in his own image; it cannot be otherwise.\n",
            "\n",
            "The reason why he was slain there, was to put him there to be a death fit for such a great King; who had had such a great and glorious career, and had done so many great things for the kingdom, and for the People of the kingdom. Wherein is his death fit? To bring him to life by death. In the day of which I have spoken, the day of which it is written thus,\n",
            "\n",
            "A day which shall be remembered,\n",
            "\n",
            "Shall I ever be slain again?\n",
            "\n",
            "'Tis because there were many, and many kingdoms and power, and great men, that slew him on the scaffold. I pray you, it was a great day that befell him. What he died to was a good day for all his kingdoms, and for the king, and for the people of the kingdom.\n",
            "\n",
            "Now he said, 'That is the greatest of God's decrees: that the Kingdom must reign among all the nations. 'When he died on the day of which I speak,\n",
            "\n",
            "[100 | 197.63] loss=3.16 avg=2.85\n",
            "[101 | 199.24] loss=2.95 avg=2.86\n",
            "[102 | 200.85] loss=3.05 avg=2.86\n",
            "[103 | 202.47] loss=3.19 avg=2.86\n",
            "[104 | 204.09] loss=2.98 avg=2.87\n",
            "[105 | 205.70] loss=2.97 avg=2.87\n",
            "[106 | 207.30] loss=3.08 avg=2.87\n",
            "[107 | 208.92] loss=2.74 avg=2.87\n",
            "[108 | 210.54] loss=2.71 avg=2.87\n",
            "[109 | 212.16] loss=2.95 avg=2.87\n",
            "[110 | 213.78] loss=2.64 avg=2.86\n",
            "[111 | 215.40] loss=3.00 avg=2.87\n",
            "[112 | 217.02] loss=2.89 avg=2.87\n",
            "[113 | 218.65] loss=2.81 avg=2.87\n",
            "[114 | 220.27] loss=2.46 avg=2.86\n",
            "[115 | 221.90] loss=2.87 avg=2.86\n",
            "[116 | 223.53] loss=2.80 avg=2.86\n",
            "[117 | 225.16] loss=2.85 avg=2.86\n",
            "[118 | 226.78] loss=2.92 avg=2.86\n",
            "[119 | 228.42] loss=2.69 avg=2.86\n",
            "[120 | 230.06] loss=2.74 avg=2.86\n",
            "[121 | 231.70] loss=2.88 avg=2.86\n",
            "[122 | 233.35] loss=2.42 avg=2.85\n",
            "[123 | 235.00] loss=2.82 avg=2.85\n",
            "[124 | 236.65] loss=2.80 avg=2.85\n",
            "[125 | 238.31] loss=2.59 avg=2.84\n",
            "[126 | 239.96] loss=2.87 avg=2.84\n",
            "[127 | 241.62] loss=2.76 avg=2.84\n",
            "[128 | 243.29] loss=2.75 avg=2.84\n",
            "[129 | 244.96] loss=3.02 avg=2.85\n",
            "[130 | 246.62] loss=2.92 avg=2.85\n",
            "[131 | 248.29] loss=2.81 avg=2.85\n",
            "[132 | 249.96] loss=2.80 avg=2.84\n",
            "[133 | 251.62] loss=3.07 avg=2.85\n",
            "[134 | 253.28] loss=2.81 avg=2.85\n",
            "[135 | 254.94] loss=2.79 avg=2.85\n",
            "[136 | 256.59] loss=2.68 avg=2.84\n",
            "[137 | 258.24] loss=2.92 avg=2.85\n",
            "[138 | 259.89] loss=2.40 avg=2.84\n",
            "[139 | 261.53] loss=2.85 avg=2.84\n",
            "[140 | 263.18] loss=2.56 avg=2.84\n",
            "[141 | 264.81] loss=2.82 avg=2.84\n",
            "[142 | 266.46] loss=3.10 avg=2.84\n",
            "[143 | 268.10] loss=3.03 avg=2.84\n",
            "[144 | 269.74] loss=3.10 avg=2.84\n",
            "[145 | 271.38] loss=2.43 avg=2.84\n",
            "[146 | 273.01] loss=2.77 avg=2.84\n",
            "[147 | 274.65] loss=3.08 avg=2.84\n",
            "[148 | 276.29] loss=2.59 avg=2.84\n",
            "[149 | 277.92] loss=2.60 avg=2.84\n",
            "[150 | 279.55] loss=2.80 avg=2.83\n",
            "[151 | 281.19] loss=2.78 avg=2.83\n",
            "[152 | 282.82] loss=2.68 avg=2.83\n",
            "[153 | 284.45] loss=2.67 avg=2.83\n",
            "[154 | 286.06] loss=2.47 avg=2.83\n",
            "[155 | 287.69] loss=2.52 avg=2.82\n",
            "[156 | 289.32] loss=2.65 avg=2.82\n",
            "[157 | 290.95] loss=2.78 avg=2.82\n",
            "[158 | 292.58] loss=3.07 avg=2.82\n",
            "[159 | 294.21] loss=2.39 avg=2.82\n",
            "[160 | 295.84] loss=2.58 avg=2.81\n",
            "[161 | 297.47] loss=2.90 avg=2.81\n",
            "[162 | 299.11] loss=3.10 avg=2.82\n",
            "[163 | 300.74] loss=3.24 avg=2.82\n",
            "[164 | 302.37] loss=3.01 avg=2.83\n",
            "[165 | 304.01] loss=3.07 avg=2.83\n",
            "[166 | 305.64] loss=2.26 avg=2.82\n",
            "[167 | 307.26] loss=2.79 avg=2.82\n",
            "[168 | 308.89] loss=2.97 avg=2.82\n",
            "[169 | 310.52] loss=2.79 avg=2.82\n",
            "[170 | 312.15] loss=2.61 avg=2.82\n",
            "[171 | 313.79] loss=2.92 avg=2.82\n",
            "[172 | 315.42] loss=2.87 avg=2.82\n",
            "[173 | 317.05] loss=2.56 avg=2.82\n",
            "[174 | 318.70] loss=3.06 avg=2.82\n",
            "[175 | 320.33] loss=2.60 avg=2.82\n",
            "[176 | 321.97] loss=2.71 avg=2.82\n",
            "[177 | 323.61] loss=2.67 avg=2.82\n",
            "[178 | 325.25] loss=2.68 avg=2.81\n",
            "[179 | 326.89] loss=2.79 avg=2.81\n",
            "[180 | 328.54] loss=2.42 avg=2.81\n",
            "[181 | 330.19] loss=2.47 avg=2.81\n",
            "[182 | 331.84] loss=2.98 avg=2.81\n",
            "[183 | 333.50] loss=3.21 avg=2.81\n",
            "[184 | 335.16] loss=2.64 avg=2.81\n",
            "[185 | 336.83] loss=2.56 avg=2.81\n",
            "[186 | 338.50] loss=2.88 avg=2.81\n",
            "[187 | 340.17] loss=2.78 avg=2.81\n",
            "[188 | 341.84] loss=2.57 avg=2.80\n",
            "[189 | 343.52] loss=2.37 avg=2.80\n",
            "[190 | 345.19] loss=2.80 avg=2.80\n",
            "[191 | 346.87] loss=3.25 avg=2.80\n",
            "[192 | 348.55] loss=2.88 avg=2.81\n",
            "[193 | 350.25] loss=2.87 avg=2.81\n",
            "[194 | 351.92] loss=2.54 avg=2.80\n",
            "[195 | 353.61] loss=2.64 avg=2.80\n",
            "[196 | 355.28] loss=2.90 avg=2.80\n",
            "[197 | 356.96] loss=2.82 avg=2.80\n",
            "[198 | 358.63] loss=3.12 avg=2.81\n",
            "[199 | 360.30] loss=2.66 avg=2.80\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " integration.\n",
            "\n",
            "The Trump administration has been accused, particularly by Democrats for its alleged links to Russia, of politicizing and corrupting the process of bringing charges of election interference against Russian operatives to ensure, in a bipartisan way, that the country's democratic process would be protected not by the Justice Department or Congress but by the executive branch. In October 2016, a special counsel was appointed to oversee those investigations.\n",
            "\n",
            "Trump's team has taken extraordinary legal steps to obstruct investigations of the administration on the grounds of the president's power to fire his attorney general, Attorney General Jeff Sessions, and to prevent Congressional investigations into his own actions. When Trump fired Comey in May 2018, the president argued that Comey was not under investigation and he had not told the Senate Judiciary Committee, the leading congressional investigation, that he had not fired him because of his investigation.\n",
            "\n",
            "Some Trump supporters, including many within the Justice Department, now insist that Comey is an agent of Russia. If he is, to have fired him and then, with only a few days left in office, to dismiss him—indicating his loyalty—is not a violation of the law. If it is, his termination seems so bizarre and inexplicable from a political standpoint that Comey is likely to be vindicated within days.\n",
            "\n",
            "But this interpretation of Trump firing Comey was undercut by Sessions' explanation during his confirmation hearing that the president had asked for his guidance on not pursuing Russia investigations in the Oval Office, before he had taken office. That seems like a betrayal on Trump's part, even if Trump then claimed that, at the time, he did not ask for Sessions's advice.\n",
            "\n",
            "While Sessions insisted on the explanation that Trump never asked from him for this sort of support, his explanation, in my view, leaves much to be desired.\n",
            "\n",
            "I know a great many people in legal circles who think that the Constitution allows a president to fire subordinates for political reasons. It does not. That would be the president's constitutional duty, not the president's. But a constitutional rule is merely a tool. A president can also fire the attorney general for any legitimate reason, whether he is motivated by personal animus or to pursue legitimate goals. And even if the reasons are legitimate, the president can still make that decision because he is not within his constitutional constitutional authority to fire the attorney general—although there is a strong presumption of illegality, something Trump may be aware of.\n",
            "\n",
            "It's a simple distinction, if you accept that the constitutional rule governing the president's role is that he can fire the attorney general from within the chain of command and not be held to that standard at all. This will be a central argument about Mueller's appointment under the Trump administration. Even in the abstract, that argument is more problematic in the case of Trump firing the director of the FBI. The attorney general has a fiduciary responsibility to the American public by maintaining public confidence, and a good reason to fire such a high-value public servant. His dismissal may constitute a clear violation of the fiduciary responsibility to promote such confidence, and this could be a reason to fire him even if he had not violated the rule of public respect and thus not been acting in the president's lawful capacity.\n",
            "\n",
            "However, even if he did take the executive action to fire Sessions (which is not quite a conspiracy theory, given Mueller's record of firing, including one for lying to Congress, in the 1990s), Trump could not avoid having fired Sessions under the rule that the attorney general cannot be fired for any reason. In theory, the legal and ethical arguments that go with this interpretation, however, go against the president, and this does not make them persuasive. There are other issues, of course, but here I wish to focus on their potential ramifications. What if the case against Sessions's firing was bolstered by other legal and ethical considerations? If other political considerations also matter more than the president-elect's need for a good reason not to fire him, what about other considerations—the Constitution?\n",
            "\n",
            "That raises the possibility that the President may, in fact, have waived an oath of office and may be impeached for perjury if he quits before a thorough inquiry into the president's legal and ethical conduct, though he may plead guilty, as he has done many times in past episodes. Such an inquiry into his own legal and ethical conduct, as well as any possible involvement with Russian and/or the Trump campaign, could lead to a serious consequence for the president in a court of law.\n",
            "\n",
            "In the context of a president refusing to do the job he was elected to do, it's important that the law apply. If Congress can find that the president has lied and broken his oath of office and has thus violated these ethical and legal norms, the Constitution itself may require impeachment by a two-thirds vote.\n",
            "\n",
            "There are some questions that Trump could raise, such as when and what the House and Senate, rather than the White House, would have to decide. In that case, the constitutional question will be whether Congress has delegated the\n",
            "\n",
            "[200 | 385.00] loss=2.95 avg=2.81\n",
            "[201 | 386.64] loss=2.50 avg=2.80\n",
            "[202 | 388.29] loss=2.86 avg=2.80\n",
            "[203 | 389.93] loss=2.60 avg=2.80\n",
            "[204 | 391.58] loss=3.35 avg=2.81\n",
            "[205 | 393.21] loss=2.60 avg=2.81\n",
            "[206 | 394.85] loss=2.86 avg=2.81\n",
            "[207 | 396.49] loss=2.96 avg=2.81\n",
            "[208 | 398.12] loss=3.28 avg=2.81\n",
            "[209 | 399.76] loss=2.69 avg=2.81\n",
            "[210 | 401.39] loss=2.82 avg=2.81\n",
            "[211 | 403.03] loss=2.67 avg=2.81\n",
            "[212 | 404.66] loss=3.14 avg=2.81\n",
            "[213 | 406.30] loss=2.77 avg=2.81\n",
            "[214 | 407.94] loss=2.84 avg=2.81\n",
            "[215 | 409.57] loss=2.85 avg=2.81\n",
            "[216 | 411.21] loss=2.74 avg=2.81\n",
            "[217 | 412.84] loss=3.27 avg=2.82\n",
            "[218 | 414.48] loss=2.64 avg=2.82\n",
            "[219 | 416.11] loss=2.69 avg=2.82\n",
            "[220 | 417.75] loss=2.95 avg=2.82\n",
            "[221 | 419.38] loss=2.71 avg=2.82\n",
            "[222 | 421.01] loss=2.35 avg=2.81\n",
            "[223 | 422.65] loss=2.79 avg=2.81\n",
            "[224 | 424.29] loss=2.41 avg=2.81\n",
            "[225 | 425.93] loss=3.04 avg=2.81\n",
            "[226 | 427.57] loss=2.94 avg=2.81\n",
            "[227 | 429.21] loss=3.01 avg=2.81\n",
            "[228 | 430.85] loss=2.82 avg=2.81\n",
            "[229 | 432.50] loss=2.94 avg=2.81\n",
            "[230 | 434.15] loss=2.69 avg=2.81\n",
            "[231 | 435.81] loss=2.51 avg=2.81\n",
            "[232 | 437.48] loss=3.43 avg=2.82\n",
            "[233 | 439.15] loss=3.03 avg=2.82\n",
            "[234 | 440.82] loss=2.72 avg=2.82\n",
            "[235 | 442.50] loss=2.55 avg=2.81\n",
            "[236 | 444.18] loss=2.53 avg=2.81\n",
            "[237 | 445.86] loss=2.56 avg=2.81\n",
            "[238 | 447.56] loss=2.63 avg=2.81\n",
            "[239 | 449.24] loss=2.93 avg=2.81\n",
            "[240 | 450.94] loss=2.56 avg=2.80\n",
            "[241 | 452.65] loss=2.89 avg=2.81\n",
            "[242 | 454.35] loss=2.56 avg=2.80\n",
            "[243 | 456.03] loss=2.76 avg=2.80\n",
            "[244 | 457.72] loss=2.60 avg=2.80\n",
            "[245 | 459.41] loss=2.78 avg=2.80\n",
            "[246 | 461.08] loss=2.78 avg=2.80\n",
            "[247 | 462.75] loss=2.69 avg=2.80\n",
            "[248 | 464.42] loss=2.61 avg=2.80\n",
            "[249 | 466.08] loss=2.36 avg=2.79\n",
            "[250 | 467.75] loss=2.83 avg=2.79\n",
            "[251 | 469.42] loss=2.73 avg=2.79\n",
            "[252 | 471.08] loss=2.74 avg=2.79\n",
            "[253 | 472.74] loss=2.92 avg=2.79\n",
            "[254 | 474.39] loss=2.58 avg=2.79\n",
            "[255 | 476.05] loss=2.67 avg=2.79\n",
            "[256 | 477.71] loss=2.90 avg=2.79\n",
            "[257 | 479.35] loss=2.74 avg=2.79\n",
            "[258 | 481.00] loss=2.82 avg=2.79\n",
            "[259 | 482.65] loss=2.60 avg=2.79\n",
            "[260 | 484.30] loss=2.84 avg=2.79\n",
            "[261 | 485.94] loss=2.57 avg=2.79\n",
            "[262 | 487.59] loss=2.88 avg=2.79\n",
            "[263 | 489.23] loss=2.87 avg=2.79\n",
            "[264 | 490.87] loss=2.73 avg=2.79\n",
            "[265 | 492.51] loss=2.34 avg=2.78\n",
            "[266 | 494.16] loss=2.90 avg=2.78\n",
            "[267 | 495.80] loss=2.82 avg=2.78\n",
            "[268 | 497.44] loss=2.84 avg=2.78\n",
            "[269 | 499.07] loss=2.55 avg=2.78\n",
            "[270 | 500.71] loss=2.49 avg=2.78\n",
            "[271 | 502.35] loss=3.02 avg=2.78\n",
            "[272 | 503.99] loss=3.00 avg=2.78\n",
            "[273 | 505.63] loss=3.03 avg=2.79\n",
            "[274 | 507.27] loss=2.44 avg=2.78\n",
            "[275 | 508.90] loss=2.52 avg=2.78\n",
            "[276 | 510.54] loss=2.67 avg=2.78\n",
            "[277 | 512.16] loss=2.88 avg=2.78\n",
            "[278 | 513.79] loss=2.52 avg=2.78\n",
            "[279 | 515.42] loss=2.60 avg=2.78\n",
            "[280 | 517.05] loss=2.94 avg=2.78\n",
            "[281 | 518.69] loss=2.47 avg=2.77\n",
            "[282 | 520.33] loss=2.88 avg=2.77\n",
            "[283 | 521.96] loss=3.15 avg=2.78\n",
            "[284 | 523.61] loss=3.08 avg=2.78\n",
            "[285 | 525.25] loss=2.44 avg=2.78\n",
            "[286 | 526.90] loss=2.80 avg=2.78\n",
            "[287 | 528.54] loss=3.04 avg=2.78\n",
            "[288 | 530.19] loss=2.51 avg=2.78\n",
            "[289 | 531.85] loss=2.40 avg=2.77\n",
            "[290 | 533.50] loss=2.25 avg=2.77\n",
            "[291 | 535.17] loss=2.59 avg=2.77\n",
            "[292 | 536.83] loss=2.27 avg=2.76\n",
            "[293 | 538.50] loss=2.96 avg=2.76\n",
            "[294 | 540.18] loss=2.37 avg=2.76\n",
            "[295 | 541.86] loss=2.55 avg=2.76\n",
            "[296 | 543.54] loss=2.29 avg=2.75\n",
            "[297 | 545.23] loss=2.92 avg=2.75\n",
            "[298 | 546.92] loss=2.86 avg=2.76\n",
            "[299 | 548.63] loss=2.82 avg=2.76\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " was most successful in its pursuit of equality.\"\n",
            "\n",
            "What is also striking about this line of thought is that it doesn't reflect the consensus of most civil rights leaders — namely, that the rights of transgender people are not constitutionally protected.\n",
            "\n",
            "For example, the Democratic Party platform committee approved a resolution Tuesday night in support of an amendment to the U.S. Constitution that would prohibit courts from forcing federal contractors to accommodate transgender people by offering them sex-reassignment surgeries and other treatments in-house.\n",
            "\n",
            "This resolution was also co-sponsored by several prominent civil rights leaders, including President Barack Obama and former President Bill Clinton.\n",
            "\n",
            "The Republican president, Donald Trump, has vowed repeatedly to roll back anti-LGBT measures enacted since the civil rights movement. His transition team did not respond to The Advocate's request to comment on what plans it has made to repeal, as it has threatened, anti-LGBT language in the new administration agenda.\n",
            "\n",
            "If the Trump administration had its way, LGBT Americans would still be barred from health insurance if they are trans. In an interview with The Advocate, Trump suggested that such people should be required to pay for their treatment, even if it meant losing access to government-funded care.\n",
            "\n",
            "\"I'd rather have transgender people have to pay for their own surgery than them have to pay a tax because I don't believe in putting them in. So we could say you have to go to the doctor, but that could be costly and it could also allow a lot of trouble, for example, if people are using the bathroom that doesn't belong to them,\" Trump said. \"I'm the least bit against it.\"\n",
            "\n",
            "On Monday, after the GOP-led Indiana General Assembly passed a law that is not only unconstitutional but would deny religious exemptions to religious employers from providing health insurance to LGBT people, Pence warned the state's citizens not to be taken in by a wave of \"constitutionalist judicial initiatives.\" Pence explained that this would lead the state to enact laws that would not be constitutional — but it would also allow the state Supreme Court to strike the law down as unconstitutional on the grounds that it violates the state constitution.\n",
            "\n",
            "On Friday night, Trump retweeted a conservative website that is run by the son of a former president, and urged followers to sign an anti-LGBT pledge and then email it to lawmakers seeking their endorsement for the so-called so-called Pompeo Doctrine.\n",
            "\n",
            "Trump was responding to a Daily Beast article about two anti-LGBT bills—one approved and the other still before the U.S. Senate—that have received the votes of three Senate Republicans, but no votes from Vice President Mike Pence.\n",
            "\n",
            "The bills are the so-called \"Precious Life and Life Insurance Act\" and the so-called \"Anti-Discrimination Act of 2015,\" respectively. As Pence noted, the latter would require people to pay for life-extension or terminations that are both medically necessary, and that would be paid for by Medicaid or Medicare. Both measures have been dubbed the \"Protect Life Act\" by LGBTQ advocates, but there is little discussion of whether such treatment violates the federal Constitution.\n",
            "\n",
            "One of the bill's supporters is the Christian right evangelical politician and television pundit Michael Fumento. He is the sponsor of this week's Pompeo Doctrine, and the author of the anti-LGBT propaganda pamphlet The Precious Life: Protecting the Precious Lives of LGBT People.\n",
            "\n",
            "On Monday, Fumento tweeted a picture of himself in an anti-LGBT t-shirt, with the hashtag #PompeoMatter, which he referred to as the \"precious life and life insurance act.\" He wrote:\n",
            "\n",
            "Pompeo is a good bill. I support it for the same reasons I oppose the federal ACA. That being the fact that it does and does not give people health coverage for termination or life extension. This includes sex reassignment surgery/pediatric care, including gender reassignment in-hospital surgery and surgery to remove or de-mimic a person's genitalia.\n",
            "\n",
            "Trump has also repeatedly retweeted people who support the Pompeo Doctrine, including a message urging fellow Americans to write to their senators to oppose it. Some of his followers have responded with their own pro-Pompeo messages.\n",
            "\n",
            "As I wrote on Tuesday:\n",
            "\n",
            "Trump likes to tweet his political statements, but he has also been known to retweet what he wants to read. It's not unusual for a presidential candidate to tweet a link to the pro-Pompeo book of the same name, and it isn't uncommon for him to retweet pro-Pompeo tweets. The first president who really tried to be on the pro-LGBT side was Ronald Reagan. That's been one of his strengths. On Wednesday, he retweeted a message that was apparently directed at a former campaign staffer:\n",
            "\n",
            "In August, Trump retweeted a tweet from the pro-LGBT organization PRRI:\n",
            "\n",
            "In the wake of the Supreme\n",
            "\n",
            "[300 | 573.90] loss=2.97 avg=2.76\n",
            "[301 | 575.55] loss=2.59 avg=2.76\n",
            "[302 | 577.22] loss=2.55 avg=2.75\n",
            "[303 | 578.87] loss=2.58 avg=2.75\n",
            "[304 | 580.53] loss=2.43 avg=2.75\n",
            "[305 | 582.19] loss=2.53 avg=2.75\n",
            "[306 | 583.84] loss=2.62 avg=2.75\n",
            "[307 | 585.49] loss=2.74 avg=2.75\n",
            "[308 | 587.14] loss=2.44 avg=2.74\n",
            "[309 | 588.78] loss=2.85 avg=2.74\n",
            "[310 | 590.43] loss=2.54 avg=2.74\n",
            "[311 | 592.08] loss=2.30 avg=2.74\n",
            "[312 | 593.72] loss=2.95 avg=2.74\n",
            "[313 | 595.37] loss=2.78 avg=2.74\n",
            "[314 | 597.01] loss=2.90 avg=2.74\n",
            "[315 | 598.64] loss=2.91 avg=2.74\n",
            "[316 | 600.29] loss=2.47 avg=2.74\n",
            "[317 | 601.93] loss=2.74 avg=2.74\n",
            "[318 | 603.57] loss=2.88 avg=2.74\n",
            "[319 | 605.21] loss=2.82 avg=2.74\n",
            "[320 | 606.85] loss=2.80 avg=2.74\n",
            "[321 | 608.48] loss=2.93 avg=2.74\n",
            "[322 | 610.12] loss=2.40 avg=2.74\n",
            "[323 | 611.76] loss=2.84 avg=2.74\n",
            "[324 | 613.40] loss=2.82 avg=2.74\n",
            "[325 | 615.04] loss=2.56 avg=2.74\n",
            "[326 | 616.68] loss=2.56 avg=2.74\n",
            "[327 | 618.31] loss=2.99 avg=2.74\n",
            "[328 | 619.96] loss=2.71 avg=2.74\n",
            "[329 | 621.60] loss=2.77 avg=2.74\n",
            "[330 | 623.24] loss=2.72 avg=2.74\n",
            "[331 | 624.88] loss=2.23 avg=2.74\n",
            "[332 | 626.52] loss=2.02 avg=2.73\n",
            "[333 | 628.17] loss=2.85 avg=2.73\n",
            "[334 | 629.82] loss=2.94 avg=2.73\n",
            "[335 | 631.48] loss=2.79 avg=2.73\n",
            "[336 | 633.14] loss=2.49 avg=2.73\n",
            "[337 | 634.79] loss=3.21 avg=2.74\n",
            "[338 | 636.46] loss=3.19 avg=2.74\n",
            "[339 | 638.13] loss=3.24 avg=2.75\n",
            "[340 | 639.80] loss=2.15 avg=2.74\n",
            "[341 | 641.48] loss=2.54 avg=2.74\n",
            "[342 | 643.17] loss=2.45 avg=2.73\n",
            "[343 | 644.85] loss=2.73 avg=2.73\n",
            "[344 | 646.54] loss=2.75 avg=2.73\n",
            "[345 | 648.23] loss=2.79 avg=2.73\n",
            "[346 | 649.92] loss=2.28 avg=2.73\n",
            "[347 | 651.62] loss=2.62 avg=2.73\n",
            "[348 | 653.31] loss=2.21 avg=2.72\n",
            "[349 | 654.99] loss=2.96 avg=2.73\n",
            "[350 | 656.68] loss=2.82 avg=2.73\n",
            "[351 | 658.37] loss=2.80 avg=2.73\n",
            "[352 | 660.05] loss=2.63 avg=2.73\n",
            "[353 | 661.73] loss=2.80 avg=2.73\n",
            "[354 | 663.40] loss=2.60 avg=2.73\n",
            "[355 | 665.07] loss=2.65 avg=2.73\n",
            "[356 | 666.73] loss=2.64 avg=2.72\n",
            "[357 | 668.40] loss=2.49 avg=2.72\n",
            "[358 | 670.05] loss=2.76 avg=2.72\n",
            "[359 | 671.72] loss=2.67 avg=2.72\n",
            "[360 | 673.39] loss=2.39 avg=2.72\n",
            "[361 | 675.06] loss=2.71 avg=2.72\n",
            "[362 | 676.71] loss=2.37 avg=2.71\n",
            "[363 | 678.37] loss=2.52 avg=2.71\n",
            "[364 | 680.02] loss=2.85 avg=2.71\n",
            "[365 | 681.68] loss=2.82 avg=2.72\n",
            "[366 | 683.33] loss=2.86 avg=2.72\n",
            "[367 | 684.98] loss=2.62 avg=2.72\n",
            "[368 | 686.63] loss=2.66 avg=2.71\n",
            "[369 | 688.28] loss=2.44 avg=2.71\n",
            "[370 | 689.92] loss=3.30 avg=2.72\n",
            "[371 | 691.55] loss=2.45 avg=2.72\n",
            "[372 | 693.19] loss=2.67 avg=2.71\n",
            "[373 | 694.83] loss=2.75 avg=2.72\n",
            "[374 | 696.48] loss=2.81 avg=2.72\n",
            "[375 | 698.12] loss=3.13 avg=2.72\n",
            "[376 | 699.76] loss=2.85 avg=2.72\n",
            "[377 | 701.40] loss=2.51 avg=2.72\n",
            "[378 | 703.05] loss=2.67 avg=2.72\n",
            "[379 | 704.69] loss=2.34 avg=2.72\n",
            "[380 | 706.33] loss=2.52 avg=2.71\n",
            "[381 | 707.96] loss=2.58 avg=2.71\n",
            "[382 | 709.59] loss=2.99 avg=2.71\n",
            "[383 | 711.23] loss=2.59 avg=2.71\n",
            "[384 | 712.87] loss=3.07 avg=2.72\n",
            "[385 | 714.51] loss=2.71 avg=2.72\n",
            "[386 | 716.15] loss=2.58 avg=2.72\n",
            "[387 | 717.78] loss=2.68 avg=2.72\n",
            "[388 | 719.42] loss=2.60 avg=2.71\n",
            "[389 | 721.06] loss=2.87 avg=2.72\n",
            "[390 | 722.69] loss=2.83 avg=2.72\n",
            "[391 | 724.33] loss=2.77 avg=2.72\n",
            "[392 | 725.98] loss=2.74 avg=2.72\n",
            "[393 | 727.63] loss=2.88 avg=2.72\n",
            "[394 | 729.28] loss=2.76 avg=2.72\n",
            "[395 | 730.94] loss=2.77 avg=2.72\n",
            "[396 | 732.60] loss=2.44 avg=2.72\n",
            "[397 | 734.27] loss=2.57 avg=2.72\n",
            "[398 | 735.94] loss=2.91 avg=2.72\n",
            "[399 | 737.61] loss=2.80 avg=2.72\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " region, the United States has been pursuing a policy of regime change in Yemen that has been accompanied by a steady escalation of its military involvement in the country and its use of U.S. air power in the region. The Obama Administration has been reluctant to acknowledge, even after the Houthis captured Sana'a, that it has been involved in directly supporting the Houthis, or that its decision to arm them became a precondition for the removal of the Yemeni President, who is also Saudi Arabia's designated proxy. For the past six years, President Barack Obama has said nothing about the U.S. role in supporting his failed leadership.\n",
            "\n",
            "The Houthis' victory in Aden was a turning point. The defeat of the Iranian-backed, pro-Iranian Hadi—who was allied with the Houthis in Yemen's first civil war—is also a turning point. It shows that if there ever was a time to admit to a mistake, this is the time.\n",
            "\n",
            "Now we need a new Administration to confront the new Houthi threat and restore the peace and stability that was so long and so badly needed before it was invaded. The president's speech was a warning to the Trump Administrations that he should take a different approach to Yemen and to the rest of the region.\n",
            "\n",
            "The United States must remain deeply concerned about the security situation in Yemen, and must continue to seek a resolution to this conflict. The president should stress that he recognizes the urgent need for a political solution to this conflict, and that the United States would continue to seek diplomatic and technical assistance, and any other appropriate assistance, from the United Nations and all other interested parties to ensure that this conflict is ended peacefully. He also should express his determination to continue to seek a peaceful settlement.\n",
            "\n",
            "Finally, the administration should make clear that he will oppose any attempts to create conditions that facilitate a Houthi or Saleh victory. But Trump should emphasize that he opposes efforts to use force by any individual or foreign actor to force that outcome or otherwise deny him political and economic opportunity. Finally, he should express his belief that military action by other actors would only further escalate the conflict and undermine peace and stability in the region.\n",
            "\n",
            "After all, the Houthis are still trying to seize control of the capital, Sana'a, and their military presence remains strong. We need to focus our attention on rebuilding the war-ravaged country, as well as the broader region, not creating conditions that could allow them to survive. And the Administration needs to do everything it can to help its partners, particularly the international community, put a credible end to the conflict through a negotiated peace agreement with Houthis and Saleh: We cannot continue to arm the Houthis, who are fighting against their own government, as they stand now, when there's no political will to continue to conduct their military conflict, or allow our partners to continue to participate in it.\n",
            "\n",
            "We also cannot let ourselves be divided on Yemen. We cannot maintain an open, honest debate about the status of these conflicts that only serves a foreign power. We must instead put aside our differences, and use our leverage to seek the removal of the political leaders who have been supporting their enemies. The only way the conflict with Houthis has lessened the risk of their becoming a proxy force for Iran and its Arab allies—or of a future proxy force for Iran—is with the backing of the United States and other powers that share our interests such as a political solution to solve this conflict. It is not just our interests, but the interests of all the people in Yemen.\n",
            "\n",
            "In his speech, the president also urged others to join his effort to end the conflict and called on the international community to work with the parties to create a \"political and military transition\" to a stable political process that allows for a comprehensive solution to the conflict.\n",
            "\n",
            "The president deserves credit for being clear in his message. But I remain deeply troubled by the words of his first inaugural address. I'm troubled by it even more by the words of the words I learned from several of my colleagues who spoke at the event: They failed to recognize there are lessons, lessons that ought to be learned. We should have known.<|endoftext|>If you've done so, chances are your head may be spinning. When the United States government announced this week that it would not seek the arrest and conviction of Donald Trump and his administration, many expressed disbelief, frustration and wonder as to why it was so different from the way most other countries handled such cases. How were the government of the free world deciding the charges against Trump, who is a candidate for president? And why was it so different in such cases versus cases in which there was evidence that he was guilty?\n",
            "\n",
            "What the United States government is not pursuing, for obvious reasons, is the alleged conspiracy of Donald Trump to commit or to support conspiracy against the United States. That conspiracy includes, among other things, obstructing the investigation of Russia as well as the investigation of any foreign government attempting to interfere with the election of Donald Trump\n",
            "\n",
            "[400 | 762.28] loss=3.29 avg=2.72\n",
            "[401 | 763.95] loss=2.40 avg=2.72\n",
            "[402 | 765.63] loss=3.20 avg=2.73\n",
            "[403 | 767.30] loss=2.69 avg=2.73\n",
            "[404 | 768.96] loss=2.62 avg=2.72\n",
            "[405 | 770.63] loss=3.03 avg=2.73\n",
            "[406 | 772.29] loss=2.92 avg=2.73\n",
            "[407 | 773.95] loss=2.28 avg=2.72\n",
            "[408 | 775.61] loss=2.58 avg=2.72\n",
            "[409 | 777.26] loss=2.66 avg=2.72\n",
            "[410 | 778.91] loss=2.89 avg=2.72\n",
            "[411 | 780.57] loss=2.61 avg=2.72\n",
            "[412 | 782.22] loss=2.73 avg=2.72\n",
            "[413 | 783.87] loss=2.32 avg=2.72\n",
            "[414 | 785.53] loss=2.77 avg=2.72\n",
            "[415 | 787.18] loss=2.69 avg=2.72\n",
            "[416 | 788.83] loss=2.46 avg=2.72\n",
            "[417 | 790.47] loss=2.75 avg=2.72\n",
            "[418 | 792.11] loss=2.38 avg=2.71\n",
            "[419 | 793.76] loss=2.54 avg=2.71\n",
            "[420 | 795.40] loss=2.63 avg=2.71\n",
            "[421 | 797.04] loss=2.02 avg=2.70\n",
            "[422 | 798.68] loss=2.67 avg=2.70\n",
            "[423 | 800.33] loss=2.36 avg=2.70\n",
            "[424 | 801.97] loss=2.51 avg=2.70\n",
            "[425 | 803.61] loss=2.37 avg=2.69\n",
            "[426 | 805.25] loss=2.77 avg=2.70\n",
            "[427 | 806.89] loss=2.54 avg=2.69\n",
            "[428 | 808.53] loss=2.45 avg=2.69\n",
            "[429 | 810.17] loss=2.55 avg=2.69\n",
            "[430 | 811.81] loss=2.32 avg=2.69\n",
            "[431 | 813.44] loss=2.93 avg=2.69\n",
            "[432 | 815.08] loss=2.79 avg=2.69\n",
            "[433 | 816.72] loss=2.56 avg=2.69\n",
            "[434 | 818.36] loss=2.52 avg=2.69\n",
            "[435 | 820.00] loss=2.77 avg=2.69\n",
            "[436 | 821.63] loss=2.51 avg=2.69\n",
            "[437 | 823.27] loss=2.09 avg=2.68\n",
            "[438 | 824.91] loss=2.48 avg=2.68\n",
            "[439 | 826.54] loss=2.53 avg=2.68\n",
            "[440 | 828.18] loss=2.89 avg=2.68\n",
            "[441 | 829.84] loss=2.96 avg=2.68\n",
            "[442 | 831.48] loss=2.39 avg=2.68\n",
            "[443 | 833.14] loss=2.76 avg=2.68\n",
            "[444 | 834.80] loss=3.06 avg=2.68\n",
            "[445 | 836.46] loss=2.41 avg=2.68\n",
            "[446 | 838.13] loss=2.70 avg=2.68\n",
            "[447 | 839.80] loss=3.00 avg=2.68\n",
            "[448 | 841.48] loss=3.02 avg=2.69\n",
            "[449 | 843.15] loss=2.48 avg=2.69\n",
            "[450 | 844.84] loss=2.63 avg=2.68\n",
            "[451 | 846.53] loss=3.21 avg=2.69\n",
            "[452 | 848.23] loss=2.11 avg=2.68\n",
            "[453 | 849.94] loss=3.19 avg=2.69\n",
            "[454 | 851.65] loss=2.63 avg=2.69\n",
            "[455 | 853.36] loss=2.84 avg=2.69\n",
            "[456 | 855.07] loss=2.82 avg=2.69\n",
            "[457 | 856.78] loss=2.80 avg=2.69\n",
            "[458 | 858.49] loss=2.56 avg=2.69\n",
            "[459 | 860.19] loss=2.70 avg=2.69\n",
            "[460 | 861.88] loss=2.53 avg=2.69\n",
            "[461 | 863.57] loss=3.12 avg=2.69\n",
            "[462 | 865.25] loss=2.61 avg=2.69\n",
            "[463 | 866.94] loss=3.10 avg=2.70\n",
            "[464 | 868.63] loss=2.80 avg=2.70\n",
            "[465 | 870.31] loss=2.40 avg=2.70\n",
            "[466 | 872.00] loss=2.61 avg=2.69\n",
            "[467 | 873.66] loss=2.65 avg=2.69\n",
            "[468 | 875.34] loss=2.60 avg=2.69\n",
            "[469 | 877.03] loss=2.90 avg=2.70\n",
            "[470 | 878.68] loss=2.30 avg=2.69\n",
            "[471 | 880.35] loss=3.26 avg=2.70\n",
            "[472 | 882.02] loss=2.97 avg=2.70\n",
            "[473 | 883.69] loss=3.22 avg=2.71\n",
            "[474 | 885.36] loss=2.90 avg=2.71\n",
            "[475 | 887.03] loss=2.59 avg=2.71\n",
            "[476 | 888.70] loss=2.40 avg=2.70\n",
            "[477 | 890.37] loss=2.64 avg=2.70\n",
            "[478 | 892.04] loss=2.46 avg=2.70\n",
            "[479 | 893.72] loss=2.43 avg=2.70\n",
            "[480 | 895.39] loss=2.81 avg=2.70\n",
            "[481 | 897.05] loss=2.70 avg=2.70\n",
            "[482 | 898.72] loss=2.55 avg=2.70\n",
            "[483 | 900.37] loss=2.68 avg=2.70\n",
            "[484 | 902.04] loss=2.77 avg=2.70\n",
            "[485 | 903.71] loss=2.78 avg=2.70\n",
            "[486 | 905.38] loss=2.99 avg=2.70\n",
            "[487 | 907.05] loss=2.53 avg=2.70\n",
            "[488 | 908.72] loss=2.45 avg=2.70\n",
            "[489 | 910.40] loss=2.97 avg=2.70\n",
            "[490 | 912.07] loss=2.86 avg=2.70\n",
            "[491 | 913.73] loss=1.87 avg=2.69\n",
            "[492 | 915.39] loss=2.56 avg=2.69\n",
            "[493 | 917.06] loss=2.71 avg=2.69\n",
            "[494 | 918.74] loss=2.08 avg=2.69\n",
            "[495 | 920.39] loss=2.52 avg=2.68\n",
            "[496 | 922.06] loss=2.41 avg=2.68\n",
            "[497 | 923.73] loss=2.81 avg=2.68\n",
            "[498 | 925.40] loss=2.61 avg=2.68\n",
            "[499 | 927.07] loss=2.84 avg=2.68\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "overty, income inequality, and the lack of fair access to health care. These problems are not unique to American society. Indeed, many countries also experience high levels of inequality and poverty, but they do not have government in charge and government policies that have such a wide reach. Many countries have had democratic movements with substantial social-democratic support. And many have implemented public-sector wage and benefit reforms, a policy which can reduce inequality significantly.\n",
            "\n",
            "What is missing from this argument is that the United States and other Western democracies should do more to reduce the disparity between rich and poor in their societies. It should also do more to promote equality between different groups of people. However, what the argument is missing is a commitment to achieving the end. Instead, it focuses on the means to achieving equality.\n",
            "\n",
            "This approach misses the point of the argument. The American Dream should be for all Americans to share whatever economic opportunity comes their way. But it also should not be for a government to take from a portion of society, redistribute it, and provide the privilege of doing so. We shouldn't get rich and do better in life just from working hard and striving. Rather, we should do better, work harder, and be more productive.\n",
            "\n",
            "America was founded on a belief that everyone has a chance in life. That vision has grown stronger. In addition to the American Dream, other values of our founding–the idea that all are created equal, that all are entitled to the same opportunity, and the idea that government should treat citizens fairly on the basis of their achievements and hard work–have not been abandoned as our country develops.\n",
            "\n",
            "Moreover, the American Dream is not a distant concept. Americans grew up on that vision, and that vision guides our civic life in many ways. We want to share the American Dream, and we want our public institutions and government to recognize and respect that vision.\n",
            "\n",
            "America should not be a place where a majority of Americans cannot afford to buy enough food, or that poor people cannot afford to live out of their houses, or that working-class people cannot afford to go to college. Instead, America should be a place where everyone can aspire to a fair share of the wealth in society. That goal is what propelled the American Revolution. That goal should guide our future government.\n",
            "\n",
            "And that goal can be achieved by reducing inequality and inequality-related problems while also working to expand the opportunities available to all Americans.\n",
            "\n",
            "There is no simple formula for how to do that, and America should not attempt to change others or to create circumstances in which their values are more popular. But there is a way to do that: By doing what it has done historically, by investing its vast welfare-state budget into programs that help the poor, the working class, the disabled, and the sick.\n",
            "\n",
            "Inequality-related problems can be addressed through targeted spending. Government programs can be restructured, and new institutions built to help solve problems more effectively and to help all Americans. And that means that the United States should spend more to improve the lives of its citizens.\n",
            "\n",
            "\n",
            "\n",
            "The Senate voted 50-50 late Friday to end debate on the nomination of Brett Kavanaugh to the Supreme Court. Republicans had the votes to proceed. The drama was predictable; an institution is broken; Republicans should change it. But if you think the fight is lost, don't wait that long.\n",
            "\n",
            "It will become harder to move the Supreme Court by waiting. In the next few days, the House of Representatives will consider a bill to repeal some of Obamacare. If it passes, and if the presidential vote is even slightly lopsided, the Senate can easily act to repeal Obamacare without the threat of a filibuster—and then confirm the next president.\n",
            "\n",
            "And if Republicans hold the Senate—as they do now, despite a filibuster—the only way to pass the bill will be to block all Democratic moves to use the chamber to filibuster. This means that Republicans cannot amend their plan until at least one of their 60 votes has been cast.\n",
            "\n",
            "This creates a real dilemma for Republicans. They would rather lose than to confirm a liberal ideologue, and if the filibuster is lost they will have their hands on the Supreme Court. They would surely want to prevent Democratic obstruction in the House, even if it means delaying the president-elect's confirmation.\n",
            "\n",
            "But Republicans aren't going to give up. Republicans are going to fight hard to ensure the Trump administration has the power to set the agenda. For this reason, Republicans are also prepared to go to war with Democrats to ensure that Trump is confirmed. The Democrats do not have the will to fight such fights. They aren't weak enough to do so alone. And they are unlikely to relent until they have an absolute majority in the Senate to do so. It is in that moment when the Democrats have the least control that power will be exercised. (Some observers will argue that the filibuster is too weak to be effective in preventing this type of abuse of power to be used against Trump.)\n",
            "\n",
            "Some Democratic\n",
            "\n",
            "[500 | 951.45] loss=2.73 avg=2.68\n",
            "[501 | 953.12] loss=2.77 avg=2.68\n",
            "[502 | 954.80] loss=2.61 avg=2.68\n",
            "[503 | 956.47] loss=2.87 avg=2.69\n",
            "[504 | 958.14] loss=2.74 avg=2.69\n",
            "[505 | 959.81] loss=3.05 avg=2.69\n",
            "[506 | 961.49] loss=2.51 avg=2.69\n",
            "[507 | 963.16] loss=2.63 avg=2.69\n",
            "[508 | 964.83] loss=2.77 avg=2.69\n",
            "[509 | 966.50] loss=2.58 avg=2.69\n",
            "[510 | 968.17] loss=2.34 avg=2.68\n",
            "[511 | 969.83] loss=2.81 avg=2.69\n",
            "[512 | 971.50] loss=2.97 avg=2.69\n",
            "[513 | 973.16] loss=2.21 avg=2.68\n",
            "[514 | 974.82] loss=2.33 avg=2.68\n",
            "[515 | 976.48] loss=2.80 avg=2.68\n",
            "[516 | 978.14] loss=2.87 avg=2.68\n",
            "[517 | 979.80] loss=2.44 avg=2.68\n",
            "[518 | 981.46] loss=2.50 avg=2.68\n",
            "[519 | 983.13] loss=2.87 avg=2.68\n",
            "[520 | 984.79] loss=2.52 avg=2.68\n",
            "[521 | 986.45] loss=2.48 avg=2.68\n",
            "[522 | 988.12] loss=2.63 avg=2.68\n",
            "[523 | 989.79] loss=2.39 avg=2.67\n",
            "[524 | 991.46] loss=2.33 avg=2.67\n",
            "[525 | 993.13] loss=2.59 avg=2.67\n",
            "[526 | 994.80] loss=2.67 avg=2.67\n",
            "[527 | 996.47] loss=2.70 avg=2.67\n",
            "[528 | 998.15] loss=2.68 avg=2.67\n",
            "[529 | 999.82] loss=2.37 avg=2.67\n",
            "[530 | 1001.49] loss=2.56 avg=2.67\n",
            "[531 | 1003.16] loss=2.43 avg=2.66\n",
            "[532 | 1004.85] loss=2.73 avg=2.66\n",
            "[533 | 1006.54] loss=2.44 avg=2.66\n",
            "[534 | 1008.22] loss=2.73 avg=2.66\n",
            "[535 | 1009.88] loss=2.68 avg=2.66\n",
            "[536 | 1011.56] loss=2.84 avg=2.66\n",
            "[537 | 1013.25] loss=2.27 avg=2.66\n",
            "[538 | 1014.94] loss=2.37 avg=2.66\n",
            "[539 | 1016.63] loss=2.74 avg=2.66\n",
            "[540 | 1018.32] loss=2.45 avg=2.66\n",
            "[541 | 1020.02] loss=2.84 avg=2.66\n",
            "[542 | 1021.70] loss=2.75 avg=2.66\n",
            "[543 | 1023.39] loss=2.71 avg=2.66\n",
            "[544 | 1025.08] loss=2.65 avg=2.66\n",
            "[545 | 1026.77] loss=2.79 avg=2.66\n",
            "[546 | 1028.46] loss=2.57 avg=2.66\n",
            "[547 | 1030.13] loss=2.52 avg=2.66\n",
            "[548 | 1031.83] loss=3.02 avg=2.66\n",
            "[549 | 1033.50] loss=2.60 avg=2.66\n",
            "[550 | 1035.20] loss=2.62 avg=2.66\n",
            "[551 | 1036.87] loss=2.98 avg=2.66\n",
            "[552 | 1038.56] loss=2.60 avg=2.66\n",
            "[553 | 1040.22] loss=2.64 avg=2.66\n",
            "[554 | 1041.90] loss=2.46 avg=2.66\n",
            "[555 | 1043.59] loss=3.05 avg=2.67\n",
            "[556 | 1045.27] loss=2.71 avg=2.67\n",
            "[557 | 1046.96] loss=2.62 avg=2.67\n",
            "[558 | 1048.63] loss=2.40 avg=2.66\n",
            "[559 | 1050.32] loss=2.32 avg=2.66\n",
            "[560 | 1051.99] loss=2.96 avg=2.66\n",
            "[561 | 1053.68] loss=2.40 avg=2.66\n",
            "[562 | 1055.36] loss=2.68 avg=2.66\n",
            "[563 | 1057.05] loss=3.09 avg=2.66\n",
            "[564 | 1058.71] loss=2.44 avg=2.66\n",
            "[565 | 1060.38] loss=2.61 avg=2.66\n",
            "[566 | 1062.06] loss=3.13 avg=2.67\n",
            "[567 | 1063.73] loss=2.54 avg=2.66\n",
            "[568 | 1065.42] loss=2.89 avg=2.67\n",
            "[569 | 1067.09] loss=2.28 avg=2.66\n",
            "[570 | 1068.77] loss=2.40 avg=2.66\n",
            "[571 | 1070.45] loss=2.44 avg=2.66\n",
            "[572 | 1072.12] loss=3.05 avg=2.66\n",
            "[573 | 1073.79] loss=2.07 avg=2.66\n",
            "[574 | 1075.46] loss=2.56 avg=2.66\n",
            "[575 | 1077.13] loss=2.64 avg=2.66\n",
            "[576 | 1078.80] loss=2.95 avg=2.66\n",
            "[577 | 1080.47] loss=2.71 avg=2.66\n",
            "[578 | 1082.14] loss=2.36 avg=2.66\n",
            "[579 | 1083.82] loss=2.09 avg=2.65\n",
            "[580 | 1085.49] loss=2.89 avg=2.65\n",
            "[581 | 1087.16] loss=2.66 avg=2.65\n",
            "[582 | 1088.83] loss=2.95 avg=2.66\n",
            "[583 | 1090.50] loss=2.68 avg=2.66\n",
            "[584 | 1092.17] loss=2.80 avg=2.66\n",
            "[585 | 1093.84] loss=2.48 avg=2.66\n",
            "[586 | 1095.51] loss=2.35 avg=2.65\n",
            "[587 | 1097.18] loss=2.32 avg=2.65\n",
            "[588 | 1098.85] loss=2.14 avg=2.64\n",
            "[589 | 1100.53] loss=2.89 avg=2.65\n",
            "[590 | 1102.20] loss=2.51 avg=2.64\n",
            "[591 | 1103.87] loss=2.55 avg=2.64\n",
            "[592 | 1105.54] loss=2.99 avg=2.65\n",
            "[593 | 1107.22] loss=2.43 avg=2.65\n",
            "[594 | 1108.89] loss=2.74 avg=2.65\n",
            "[595 | 1110.56] loss=2.78 avg=2.65\n",
            "[596 | 1112.24] loss=2.67 avg=2.65\n",
            "[597 | 1113.91] loss=3.09 avg=2.65\n",
            "[598 | 1115.59] loss=2.67 avg=2.65\n",
            "[599 | 1117.26] loss=2.30 avg=2.65\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " considerably as any other group of people — not just Muslims, but the poor, the sick, and the disabled.<|endoftext|>I was surprised by how many people responded to this article (not all, but many). I wanted to write something that was as specific as possible and not repeat the same points over and over again, and so I thought that I would post something short and to the point that people could read and understand on their own.\n",
            "\n",
            "There are 2 key points that I wanted to highlight:\n",
            "\n",
            "First, this report contains a very clear statement, written by one of its principal writers, that all Muslim-American citizens who have been convicted of felonies within the U.S. are now American citizens. This was not a comment by the president or by any of his surrogates, but a clear statement of fact by the one person listed as the author: Hounsell.\n",
            "\n",
            "In fact, the only time in the report that people were told that someone was 'incorrectly convicted,' or that someone was 'lying to the court,' was if they spoke negatively about a convicted criminal. In doing so, people were accused of 'not wanting to be a hero.' And if you speak negatively about a convicted criminal, they said, 'you must have been lying to the court.'\n",
            "\n",
            "While I found that argument offensive, I understand how it might be controversial. But a report like this one shouldn't be controversial. (I actually applaud it for what it is. I also believe strongly that the president should be able to speak out when he feels it appropriate, regardless of whatever political or religious sensitivities any of his supporters might have.) In fact, it is essential reading.\n",
            "\n",
            "Second, this is a report that would seem to say, the American citizen who lives and works in the U.S. should be protected with the laws the country is built upon. If your home is in an overwhelmingly Muslim country, which, of course, it is, then you may not be able to get the same rights that you would if you were raised by the majority. The United States has a lot of rights. The legal foundation on which the rights that it offers are founded is a strong civil society. I believe it is wrong when the president is arguing that some people, those who have a criminal record, don't get equal protection of all equal-protection rights at work in America.\n",
            "\n",
            "\n",
            "A former federal prosecutor who served on the panel that recently recommended that President Donald Trump be impeached for obstruction of justice, Bill Barr, has come out with a stunning new essay titled: Let Him Go.\n",
            "\n",
            "Barr, a conservative commentator, tweeted this out today, in response to news that former FBI Director James Comey recommended the indictment of Trump.\n",
            "\n",
            "No, the president didn’t do that:\n",
            "\n",
            "So, Bill, when Comey wrote this letter, he knew full well what Comey was about to write, though he had not yet gotten the memo. But, then again, so did every other Republican on that committee: It was Comey’s decision that he made.\n",
            "\n",
            "He wrote it. That’s all. He told Comey to do so. But now he’s back on the same side of this. Because he was, he insists, complicit in trying to get Comey to write it. I believe him. In all of his legal acumen, in all of his wisdom, in all of his years of experience, Barr, on his own, would never write it. Comey wrote it, of course, so it would not be written. The truth is Barr is being disingenuous here, at best. Comey wrote the letter. Barr knew this, and had nothing to do with Comey’s reasoning. Barr did not write it.\n",
            "\n",
            "There is, I believe, a reason why Trump’s opponents were shocked by his decision. They must view it as a betrayal. The Democrats were not outraged. They have not been angry—in all their rage—all their life for any of the things they have always accused Trump of.\n",
            "\n",
            "The Democrats, from Sanders to the Clintons, have been on the right track from their founding days and at every turn. They have worked tirelessly to make America an equal nation, and so I believe that Barr’s statement that Trump has betrayed everything that it stood for, was a betrayal of everything that he stood for, and he, and this nation, must take responsibility for it.\n",
            "\n",
            "Here’s Barr’s statement:\n",
            "\n",
            "But there is a difference between betrayal and a desire to subvert the legal system. Under the Constitution, the president of the United States is the chief law-enforcement officer, and any claim of illegality based on prosecutorial discretion is a breach of the Constitution. By asserting the constitutional obligation to prosecute all impeachable offenses, the First Amendment is a vital check on the power of the executive branch, and as such is guaranteed by the constitutional guarantees of the office. President Trump is in a\n",
            "\n",
            "[600 | 1141.95] loss=2.64 avg=2.65\n",
            "[601 | 1143.62] loss=2.37 avg=2.65\n",
            "[602 | 1145.29] loss=2.84 avg=2.65\n",
            "[603 | 1146.96] loss=2.79 avg=2.65\n",
            "[604 | 1148.63] loss=2.43 avg=2.65\n",
            "[605 | 1150.30] loss=2.61 avg=2.65\n",
            "[606 | 1151.97] loss=2.24 avg=2.64\n",
            "[607 | 1153.64] loss=2.84 avg=2.64\n",
            "[608 | 1155.30] loss=2.64 avg=2.64\n",
            "[609 | 1156.97] loss=2.30 avg=2.64\n",
            "[610 | 1158.64] loss=2.65 avg=2.64\n",
            "[611 | 1160.31] loss=2.51 avg=2.64\n",
            "[612 | 1161.98] loss=2.94 avg=2.64\n",
            "[613 | 1163.65] loss=2.30 avg=2.64\n",
            "[614 | 1165.32] loss=2.16 avg=2.63\n",
            "[615 | 1166.99] loss=2.71 avg=2.64\n",
            "[616 | 1168.67] loss=2.48 avg=2.63\n",
            "[617 | 1170.33] loss=2.89 avg=2.64\n",
            "[618 | 1172.00] loss=2.33 avg=2.63\n",
            "[619 | 1173.67] loss=2.58 avg=2.63\n",
            "[620 | 1175.35] loss=2.62 avg=2.63\n",
            "[621 | 1177.01] loss=2.80 avg=2.63\n",
            "[622 | 1178.69] loss=2.49 avg=2.63\n",
            "[623 | 1180.35] loss=2.26 avg=2.63\n",
            "[624 | 1182.02] loss=2.65 avg=2.63\n",
            "[625 | 1183.69] loss=2.43 avg=2.63\n",
            "[626 | 1185.36] loss=2.59 avg=2.63\n",
            "[627 | 1187.03] loss=2.45 avg=2.63\n",
            "[628 | 1188.70] loss=2.76 avg=2.63\n",
            "[629 | 1190.36] loss=2.25 avg=2.62\n",
            "[630 | 1192.03] loss=2.47 avg=2.62\n",
            "[631 | 1193.71] loss=2.34 avg=2.62\n",
            "[632 | 1195.38] loss=2.20 avg=2.61\n",
            "[633 | 1197.04] loss=2.38 avg=2.61\n",
            "[634 | 1198.71] loss=2.44 avg=2.61\n",
            "[635 | 1200.39] loss=2.51 avg=2.61\n",
            "[636 | 1202.06] loss=2.96 avg=2.61\n",
            "[637 | 1203.73] loss=2.94 avg=2.62\n",
            "[638 | 1205.38] loss=2.50 avg=2.61\n",
            "[639 | 1207.07] loss=2.70 avg=2.62\n",
            "[640 | 1208.74] loss=2.38 avg=2.61\n",
            "[641 | 1210.41] loss=2.14 avg=2.61\n",
            "[642 | 1212.09] loss=2.41 avg=2.61\n",
            "[643 | 1213.77] loss=2.63 avg=2.61\n",
            "[644 | 1215.44] loss=2.93 avg=2.61\n",
            "[645 | 1217.13] loss=2.09 avg=2.60\n",
            "[646 | 1218.81] loss=2.67 avg=2.61\n",
            "[647 | 1220.50] loss=2.85 avg=2.61\n",
            "[648 | 1222.19] loss=2.13 avg=2.60\n",
            "[649 | 1223.86] loss=2.71 avg=2.60\n",
            "[650 | 1225.56] loss=2.63 avg=2.60\n",
            "[651 | 1227.24] loss=2.28 avg=2.60\n",
            "[652 | 1228.92] loss=2.52 avg=2.60\n",
            "[653 | 1230.61] loss=2.59 avg=2.60\n",
            "[654 | 1232.30] loss=3.40 avg=2.61\n",
            "[655 | 1233.97] loss=2.58 avg=2.61\n",
            "[656 | 1235.66] loss=2.89 avg=2.61\n",
            "[657 | 1237.35] loss=2.38 avg=2.61\n",
            "[658 | 1239.03] loss=2.80 avg=2.61\n",
            "[659 | 1240.72] loss=2.49 avg=2.61\n",
            "[660 | 1242.42] loss=2.61 avg=2.61\n",
            "[661 | 1244.11] loss=2.69 avg=2.61\n",
            "[662 | 1245.80] loss=2.52 avg=2.61\n",
            "[663 | 1247.48] loss=2.37 avg=2.61\n",
            "[664 | 1249.17] loss=2.92 avg=2.61\n",
            "[665 | 1250.85] loss=2.61 avg=2.61\n",
            "[666 | 1252.54] loss=2.35 avg=2.61\n",
            "[667 | 1254.23] loss=2.42 avg=2.61\n",
            "[668 | 1255.92] loss=2.84 avg=2.61\n",
            "[669 | 1257.61] loss=2.83 avg=2.61\n",
            "[670 | 1259.30] loss=2.82 avg=2.61\n",
            "[671 | 1260.99] loss=2.53 avg=2.61\n",
            "[672 | 1262.66] loss=3.05 avg=2.62\n",
            "[673 | 1264.35] loss=2.87 avg=2.62\n",
            "[674 | 1266.04] loss=2.97 avg=2.62\n",
            "[675 | 1267.73] loss=2.88 avg=2.62\n",
            "[676 | 1269.41] loss=2.07 avg=2.62\n",
            "[677 | 1271.10] loss=2.34 avg=2.62\n",
            "[678 | 1272.78] loss=2.61 avg=2.62\n",
            "[679 | 1274.46] loss=2.35 avg=2.61\n",
            "[680 | 1276.15] loss=2.47 avg=2.61\n",
            "[681 | 1277.82] loss=2.38 avg=2.61\n",
            "[682 | 1279.51] loss=2.72 avg=2.61\n",
            "[683 | 1281.17] loss=2.67 avg=2.61\n",
            "[684 | 1282.87] loss=2.48 avg=2.61\n",
            "[685 | 1284.54] loss=2.20 avg=2.61\n",
            "[686 | 1286.23] loss=2.40 avg=2.60\n",
            "[687 | 1287.92] loss=2.92 avg=2.61\n",
            "[688 | 1289.59] loss=2.14 avg=2.60\n",
            "[689 | 1291.26] loss=2.63 avg=2.60\n",
            "[690 | 1292.93] loss=2.93 avg=2.61\n",
            "[691 | 1294.60] loss=3.06 avg=2.61\n",
            "[692 | 1296.29] loss=2.63 avg=2.61\n",
            "[693 | 1297.97] loss=2.82 avg=2.61\n",
            "[694 | 1299.65] loss=2.29 avg=2.61\n",
            "[695 | 1301.33] loss=2.91 avg=2.61\n",
            "[696 | 1303.00] loss=2.11 avg=2.61\n",
            "[697 | 1304.68] loss=2.13 avg=2.60\n",
            "[698 | 1306.36] loss=2.18 avg=2.60\n",
            "[699 | 1308.04] loss=2.96 avg=2.60\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " paperwork, as well as some of the records that remain classified, could cause serious damage to the national security.\n",
            "\n",
            "It would also be a slap in the face to the American people who stand by the Constitution and the rule of law.\n",
            "\n",
            "This article is part of “The Speech Wars,” a project supported by the Charles Koch Foundation, the Reporters Committee for the Freedom of the Press, and the Fetzer Institute’s False Narrative Project.\n",
            "\n",
            "\n",
            "\n",
            "Since Donald Trump won the 2016 presidential election, much controversy has surrounded the role of Russia in the Trump campaign’s efforts to influence it. Now that Donald Trump has been sworn in as the 45th president of the United States, many on both the left and the right are seeking to find answers to our country’s most pressing national security questions.\n",
            "\n",
            "President Donald Trump, who has since become a close ally of Russian President Vladimir Putin, may have personally received instructions from the Russian government in the form of classified information—for both the Russians and the Trump campaign. Trump has acknowledged that he shared this classified information with the Russians during the campaign. Yet now that he’s the president, the questions that are raised remain unanswered.\n",
            "\n",
            "ADVERTISEMENT\n",
            "\n",
            "We know that Russia is using cyberattacks and disinformation campaigns—a growing problem worldwide—to undermine American democracy. We know that Russia is now targeting our institutions on a daily basis, using a variety of mediums, including social media, to spread disinformation and attack political opponents. We know that Russia has been trying to subvert the U.S. presidential election—which, at this point, looks even more like it is a proxy war. And we know that Russia continues to use its cyberweapons against U.S. institutions such as the Democratic National Committee, and the American electoral process.\n",
            "\n",
            "In fact, President Barack Obama’s administration concluded in March 2017 that Russia’s cyberattacks had been far more extensive than previously acknowledged—and that the damage done to the U.S. election campaign would be as severe as the damage caused by the Stuxnet malware attack in 2010. The Trump administration, when faced with evidence of Russia’s actions, responded with a denial like none we have ever seen before. This was a calculated strategy.\n",
            "\n",
            "Yet despite the evidence of Russia manipulating our election for the direct benefit of Donald Trump, we remain unable to bring our nations together—especially when it comes to our national-security institutions.\n",
            "\n",
            "Until an agreement is reached with Russia to end the disinformation campaign launched by President Donald Trump, we will continue to face a constant barrage of misinformation and fake news that undermines the public’s trust in the system we have built to hold our leaders accountable.\n",
            "\n",
            "These false narratives do not just damage our democratic institutions. They also create opportunities for those who wish to harm our democratic institutions—such as pro-Russian terrorist organizations, who seek to sow division and discord both inside our countries and around the world.\n",
            "\n",
            "And when our systems are targeted by foreign actors seeking to undermine democracy directly and indirectly, we will fail in our most important mission: to defend the United States. We will not stand for this.\n",
            "\n",
            "\n",
            "\n",
            "In the United States, the Republican Party’s support for Donald Trump has been based in large part on the belief that an outsider could fix our country’s problems.\n",
            "\n",
            "The GOP’s rejection of Trump’s agenda is not some aberrant behavior, it is a natural extension of who they are. The Republican Party’s support for Trump rests on a belief that the president was elected for a specific set of reasons and should follow them. The majority of Americans think the president is doing a pretty good job. He is succeeding thanks to policies pursued by this administration, not because of them. Americans are fed up with Washington corruption and the corruption of both parties.\n",
            "\n",
            "Republicans and Democrats disagree on key policy initiatives, but the result of this disagreement is that no one party can govern as a majority with one set of priorities.\n",
            "\n",
            "Trump did not emerge as the Republican nominee in 2016 out of some grand conspiracy. He did not emerge from a grassroots movement with a new set of ideas or bold initiatives. In fact, during the primaries, most of the Republican candidates agreed on a lot—and many of their primary policies align with the positions of the majority of voters.\n",
            "\n",
            "The fact that Trump is the undisputed GOP candidate does not mean that a majority of Republican voters agree with Trump. A plurality of Republican voters—at least 50 percent—support Hillary Clinton, although that is a fraction of the Republican electorate. A majority of Republicans, in fact, do not even consider Donald Trump as a serious candidate for president. Many feel that their preferences are irrelevant.\n",
            "\n",
            "Even if they did not identify with the Republican Party on any policy issue, a broad majority of Republicans are uncomfortable with Trump. In fact, Trump is viewed unfavorably by majorities of Republicans in every major poll, by a\n",
            "\n",
            "[700 | 1332.80] loss=2.32 avg=2.60\n",
            "[701 | 1334.48] loss=2.53 avg=2.60\n",
            "[702 | 1336.15] loss=2.22 avg=2.59\n",
            "[703 | 1337.82] loss=2.73 avg=2.60\n",
            "[704 | 1339.49] loss=2.32 avg=2.59\n",
            "[705 | 1341.16] loss=2.31 avg=2.59\n",
            "[706 | 1342.83] loss=2.51 avg=2.59\n",
            "[707 | 1344.48] loss=2.50 avg=2.59\n",
            "[708 | 1346.15] loss=2.65 avg=2.59\n",
            "[709 | 1347.80] loss=2.77 avg=2.59\n",
            "[710 | 1349.46] loss=2.82 avg=2.59\n",
            "[711 | 1351.13] loss=2.43 avg=2.59\n",
            "[712 | 1352.79] loss=2.82 avg=2.59\n",
            "[713 | 1354.46] loss=2.24 avg=2.59\n",
            "[714 | 1356.13] loss=2.69 avg=2.59\n",
            "[715 | 1357.80] loss=2.64 avg=2.59\n",
            "[716 | 1359.45] loss=2.13 avg=2.59\n",
            "[717 | 1361.12] loss=2.61 avg=2.59\n",
            "[718 | 1362.80] loss=2.17 avg=2.58\n",
            "[719 | 1364.47] loss=2.25 avg=2.58\n",
            "[720 | 1366.14] loss=2.66 avg=2.58\n",
            "[721 | 1367.81] loss=2.40 avg=2.58\n",
            "[722 | 1369.48] loss=2.43 avg=2.58\n",
            "[723 | 1371.16] loss=2.52 avg=2.58\n",
            "[724 | 1372.85] loss=2.66 avg=2.58\n",
            "[725 | 1374.52] loss=2.44 avg=2.58\n",
            "[726 | 1376.20] loss=2.80 avg=2.58\n",
            "[727 | 1377.87] loss=2.48 avg=2.58\n",
            "[728 | 1379.56] loss=2.75 avg=2.58\n",
            "[729 | 1381.24] loss=2.51 avg=2.58\n",
            "[730 | 1382.93] loss=3.11 avg=2.58\n",
            "[731 | 1384.62] loss=2.01 avg=2.58\n",
            "[732 | 1386.29] loss=2.60 avg=2.58\n",
            "[733 | 1387.99] loss=2.29 avg=2.58\n",
            "[734 | 1389.68] loss=2.52 avg=2.58\n",
            "[735 | 1391.37] loss=2.37 avg=2.57\n",
            "[736 | 1393.06] loss=2.69 avg=2.57\n",
            "[737 | 1394.75] loss=2.59 avg=2.57\n",
            "[738 | 1396.44] loss=2.59 avg=2.57\n",
            "[739 | 1398.11] loss=2.40 avg=2.57\n",
            "[740 | 1399.80] loss=2.68 avg=2.57\n",
            "[741 | 1401.48] loss=2.36 avg=2.57\n",
            "[742 | 1403.17] loss=2.51 avg=2.57\n",
            "[743 | 1404.85] loss=2.40 avg=2.57\n",
            "[744 | 1406.53] loss=2.47 avg=2.57\n",
            "[745 | 1408.22] loss=2.47 avg=2.57\n",
            "[746 | 1409.89] loss=2.85 avg=2.57\n",
            "[747 | 1411.58] loss=2.45 avg=2.57\n",
            "[748 | 1413.27] loss=2.29 avg=2.57\n",
            "[749 | 1414.96] loss=1.88 avg=2.56\n",
            "[750 | 1416.63] loss=2.04 avg=2.55\n",
            "[751 | 1418.32] loss=2.65 avg=2.56\n",
            "[752 | 1420.01] loss=2.30 avg=2.55\n",
            "[753 | 1421.68] loss=2.54 avg=2.55\n",
            "[754 | 1423.37] loss=2.17 avg=2.55\n",
            "[755 | 1425.04] loss=1.89 avg=2.54\n",
            "[756 | 1426.73] loss=3.04 avg=2.55\n",
            "[757 | 1428.41] loss=2.32 avg=2.54\n",
            "[758 | 1430.09] loss=3.06 avg=2.55\n",
            "[759 | 1431.76] loss=2.69 avg=2.55\n",
            "[760 | 1433.44] loss=2.79 avg=2.55\n",
            "[761 | 1435.12] loss=2.47 avg=2.55\n",
            "[762 | 1436.80] loss=2.22 avg=2.55\n",
            "[763 | 1438.47] loss=2.88 avg=2.55\n",
            "[764 | 1440.15] loss=1.97 avg=2.55\n",
            "[765 | 1441.82] loss=2.59 avg=2.55\n",
            "[766 | 1443.50] loss=2.10 avg=2.54\n",
            "[767 | 1445.19] loss=2.28 avg=2.54\n",
            "[768 | 1446.86] loss=2.60 avg=2.54\n",
            "[769 | 1448.54] loss=2.69 avg=2.54\n",
            "[770 | 1450.21] loss=2.28 avg=2.54\n",
            "[771 | 1451.89] loss=2.79 avg=2.54\n",
            "[772 | 1453.57] loss=2.21 avg=2.54\n",
            "[773 | 1455.25] loss=2.40 avg=2.54\n",
            "[774 | 1456.93] loss=2.76 avg=2.54\n",
            "[775 | 1458.60] loss=2.42 avg=2.54\n",
            "[776 | 1460.29] loss=2.86 avg=2.54\n",
            "[777 | 1461.97] loss=2.56 avg=2.54\n",
            "[778 | 1463.66] loss=2.36 avg=2.54\n",
            "[779 | 1465.34] loss=1.75 avg=2.53\n",
            "[780 | 1467.01] loss=2.45 avg=2.53\n",
            "[781 | 1468.68] loss=2.82 avg=2.53\n",
            "[782 | 1470.36] loss=2.68 avg=2.54\n",
            "[783 | 1472.04] loss=1.94 avg=2.53\n",
            "[784 | 1473.71] loss=3.06 avg=2.53\n",
            "[785 | 1475.40] loss=2.11 avg=2.53\n",
            "[786 | 1477.07] loss=2.50 avg=2.53\n",
            "[787 | 1478.74] loss=2.53 avg=2.53\n",
            "[788 | 1480.41] loss=2.45 avg=2.53\n",
            "[789 | 1482.10] loss=2.84 avg=2.53\n",
            "[790 | 1483.76] loss=2.84 avg=2.54\n",
            "[791 | 1485.44] loss=1.99 avg=2.53\n",
            "[792 | 1487.11] loss=2.17 avg=2.53\n",
            "[793 | 1488.79] loss=2.40 avg=2.53\n",
            "[794 | 1490.47] loss=3.01 avg=2.53\n",
            "[795 | 1492.14] loss=1.99 avg=2.53\n",
            "[796 | 1493.82] loss=2.61 avg=2.53\n",
            "[797 | 1495.51] loss=2.70 avg=2.53\n",
            "[798 | 1497.18] loss=1.94 avg=2.52\n",
            "[799 | 1498.87] loss=2.14 avg=2.52\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " Hispanic communities. He is now running for president to protect those communities.\n",
            "\n",
            "This narrative, which is being disseminated by Donald Trump Jr., is also consistent with Trump’s rhetoric. When he was in office, he famously declared that he had no intention, by any means, of letting immigration laws determine the country’s borders. Since the current president has begun allowing those same borders to be modified, Trump has called for an end to immigration. His public words echo earlier positions that Trump took at his 1987 announcement speech, in which he announced: “We’ll no longer accept new immigrants unless they are willing to respect our laws and be treated fairly.”\n",
            "\n",
            "In contrast to the Trump and Romney campaigns, which have emphasized the importance of securing the border and enforcing laws, Trump is focusing instead on policies that would make people feel safer, and that would allow them to return to their home country. His positions are starkly different from Romney’s. Romney offered a policy framework that, among other things, included, among other things, imposing a temporary ban on the flow of refugees, restricting the number of Syrian refugees admitted to the United States, and imposing an annual cap on Syrian refugee admissions.\n",
            "\n",
            "Trump, on the other hand, refuses to take a position on Syrian refugees, and he has even proposed a temporary ban on Muslims entering the United States. Under Trump’s proposal, those with strong jihadist views would be banned from entering the United States, and those who engage in “selective persecution” would be prevented. Trump is trying to impose a regime of terror–a regime that would leave many citizens fearful that such beliefs might not be permissible in public. Trump is also running for president because of fears that immigrants are breaking the nation’s laws and are the causes of violence.\n",
            "\n",
            "Trump, and Republicans, are now operating on a thesis that borders must not exist at all. A Trump strategy would see people returning to their home countries if they are turned away on the southern border as a result of their immigration status. Once they are in, they would go through the process of seeking asylum as soon as possible and then seek to reunite with family and families in their home country, where they would be welcomed and treated fairly. Those in the United States would then vote to make their return possible. This approach is consistent with Trump’s earlier rhetoric that suggested that once a country is deemed secure, immigration would then be controlled and immigration levels would decline. He has argued before, and again now, that there must be no walls between nations.\n",
            "\n",
            "This narrative is the same narrative that Obama deployed in his 2008 campaign announcement speech, which proclaimed, “This is not simply about the border. We are a nation at war against terrorism. We are in a war with radical Islam. If we are honest, these are the most important issues of our time.”\n",
            "\n",
            "The Clinton campaign has taken a different approach. In November 2008, Clinton said that she felt “an obligation to act” on immigration and other issues. Obama then said that he’d only support a path to citizenship; Clinton then said that she felt “an obligation” and would advocate for “common ground.” The Clinton campaign also has employed a framing strategy that is meant to convince voters that Clinton's agenda would be consistent with the interests of voters in other parts of the country: she would move to the center on issues like abortion, guns, and immigration.\n",
            "\n",
            "Clinton and Obama are both relying on framing. Obama, as an avid campaigner, tried to tie herself to the Democratic presidential nominee. In 2012, Obama used his closing speech in Iowa to promise voters that he would “move beyond my own view that there are things we can do together ‘to pass a constitutional amendment to define marriage as between one man and one woman,’’ “establish a national day to honor those who gave their lives in the World Trade Center.” Trump, by contrast, has no clear position on these issues. He has not offered a policy platform on what issues he would support to vote for him but has instead offered only a promise or two–such as, in his response to the Orlando tragedy, that he would “be open to ‘a constitutional amendment on how to deal with crime’’ and that “I’ll try to move on to other issues.”\n",
            "\n",
            "What are the limits to which Clinton and Trump are willing to break from this narrative? As I have argued previously, the evidence only supports the view that immigration does pose a major barrier to the integration of the United States into a global society. It is true that immigration is associated with immigration flows and that, therefore, immigration policies tend to have large effects on economic growth. But Clinton’s argument that immigration policies will produce benefits has not been tested over long periods of time. In fact, it is entirely possible that immigration reduces GDP growth, which can have\n",
            "\n",
            "[800 | 1523.58] loss=2.34 avg=2.52\n",
            "[801 | 1525.24] loss=2.59 avg=2.52\n",
            "[802 | 1526.91] loss=2.37 avg=2.52\n",
            "[803 | 1528.57] loss=2.15 avg=2.51\n",
            "[804 | 1530.22] loss=3.07 avg=2.52\n",
            "[805 | 1531.88] loss=2.53 avg=2.52\n",
            "[806 | 1533.54] loss=2.91 avg=2.52\n",
            "[807 | 1535.19] loss=2.04 avg=2.52\n",
            "[808 | 1536.84] loss=2.62 avg=2.52\n",
            "[809 | 1538.49] loss=2.46 avg=2.52\n",
            "[810 | 1540.14] loss=2.44 avg=2.52\n",
            "[811 | 1541.80] loss=2.27 avg=2.51\n",
            "[812 | 1543.45] loss=2.14 avg=2.51\n",
            "[813 | 1545.11] loss=2.71 avg=2.51\n",
            "[814 | 1546.76] loss=2.60 avg=2.51\n",
            "[815 | 1548.42] loss=2.30 avg=2.51\n",
            "[816 | 1550.08] loss=2.84 avg=2.51\n",
            "[817 | 1551.73] loss=3.10 avg=2.52\n",
            "[818 | 1553.39] loss=2.20 avg=2.52\n",
            "[819 | 1555.06] loss=2.54 avg=2.52\n",
            "[820 | 1556.73] loss=2.66 avg=2.52\n",
            "[821 | 1558.41] loss=2.82 avg=2.52\n",
            "[822 | 1560.06] loss=2.36 avg=2.52\n",
            "[823 | 1561.73] loss=2.60 avg=2.52\n",
            "[824 | 1563.40] loss=2.75 avg=2.52\n",
            "[825 | 1565.07] loss=2.41 avg=2.52\n",
            "[826 | 1566.74] loss=2.01 avg=2.52\n",
            "[827 | 1568.41] loss=2.52 avg=2.52\n",
            "[828 | 1570.09] loss=2.63 avg=2.52\n",
            "[829 | 1571.78] loss=2.34 avg=2.52\n",
            "[830 | 1573.47] loss=2.16 avg=2.51\n",
            "[831 | 1575.15] loss=1.92 avg=2.51\n",
            "[832 | 1576.84] loss=2.41 avg=2.51\n",
            "[833 | 1578.51] loss=2.48 avg=2.51\n",
            "[834 | 1580.20] loss=2.52 avg=2.51\n",
            "[835 | 1581.88] loss=1.96 avg=2.50\n",
            "[836 | 1583.57] loss=2.54 avg=2.50\n",
            "[837 | 1585.26] loss=2.59 avg=2.50\n",
            "[838 | 1586.94] loss=2.20 avg=2.50\n",
            "[839 | 1588.64] loss=2.73 avg=2.50\n",
            "[840 | 1590.32] loss=3.43 avg=2.51\n",
            "[841 | 1592.00] loss=2.57 avg=2.51\n",
            "[842 | 1593.68] loss=2.65 avg=2.51\n",
            "[843 | 1595.36] loss=2.46 avg=2.51\n",
            "[844 | 1597.04] loss=2.24 avg=2.51\n",
            "[845 | 1598.73] loss=2.31 avg=2.51\n",
            "[846 | 1600.40] loss=2.51 avg=2.51\n",
            "[847 | 1602.07] loss=2.34 avg=2.51\n",
            "[848 | 1603.74] loss=2.39 avg=2.50\n",
            "[849 | 1605.42] loss=3.24 avg=2.51\n",
            "[850 | 1607.10] loss=2.08 avg=2.51\n",
            "[851 | 1608.78] loss=2.51 avg=2.51\n",
            "[852 | 1610.46] loss=2.98 avg=2.51\n",
            "[853 | 1612.13] loss=2.25 avg=2.51\n",
            "[854 | 1613.80] loss=2.18 avg=2.51\n",
            "[855 | 1615.48] loss=2.80 avg=2.51\n",
            "[856 | 1617.14] loss=1.95 avg=2.50\n",
            "[857 | 1618.81] loss=2.49 avg=2.50\n",
            "[858 | 1620.48] loss=2.46 avg=2.50\n",
            "[859 | 1622.15] loss=2.04 avg=2.50\n",
            "[860 | 1623.82] loss=2.22 avg=2.50\n",
            "[861 | 1625.49] loss=2.06 avg=2.49\n",
            "[862 | 1627.16] loss=2.55 avg=2.49\n",
            "[863 | 1628.83] loss=2.57 avg=2.49\n",
            "[864 | 1630.51] loss=2.83 avg=2.50\n",
            "[865 | 1632.17] loss=2.55 avg=2.50\n",
            "[866 | 1633.84] loss=2.58 avg=2.50\n",
            "[867 | 1635.52] loss=3.07 avg=2.50\n",
            "[868 | 1637.19] loss=2.00 avg=2.50\n",
            "[869 | 1638.86] loss=2.71 avg=2.50\n",
            "[870 | 1640.53] loss=2.67 avg=2.50\n",
            "[871 | 1642.20] loss=2.94 avg=2.51\n",
            "[872 | 1643.88] loss=2.24 avg=2.50\n",
            "[873 | 1645.55] loss=2.65 avg=2.50\n",
            "[874 | 1647.23] loss=2.67 avg=2.51\n",
            "[875 | 1648.90] loss=2.63 avg=2.51\n",
            "[876 | 1650.57] loss=3.28 avg=2.52\n",
            "[877 | 1652.24] loss=2.44 avg=2.51\n",
            "[878 | 1653.91] loss=2.52 avg=2.51\n",
            "[879 | 1655.59] loss=2.55 avg=2.51\n",
            "[880 | 1657.26] loss=2.52 avg=2.52\n",
            "[881 | 1658.93] loss=2.34 avg=2.51\n",
            "[882 | 1660.60] loss=2.36 avg=2.51\n",
            "[883 | 1662.27] loss=2.21 avg=2.51\n",
            "[884 | 1663.94] loss=2.60 avg=2.51\n",
            "[885 | 1665.61] loss=2.65 avg=2.51\n",
            "[886 | 1667.28] loss=2.17 avg=2.51\n",
            "[887 | 1668.95] loss=2.43 avg=2.51\n",
            "[888 | 1670.63] loss=2.44 avg=2.51\n",
            "[889 | 1672.30] loss=2.65 avg=2.51\n",
            "[890 | 1673.97] loss=2.36 avg=2.51\n",
            "[891 | 1675.64] loss=2.44 avg=2.51\n",
            "[892 | 1677.31] loss=2.36 avg=2.50\n",
            "[893 | 1678.98] loss=1.97 avg=2.50\n",
            "[894 | 1680.65] loss=2.75 avg=2.50\n",
            "[895 | 1682.32] loss=2.52 avg=2.50\n",
            "[896 | 1683.99] loss=2.72 avg=2.50\n",
            "[897 | 1685.66] loss=2.29 avg=2.50\n",
            "[898 | 1687.33] loss=2.50 avg=2.50\n",
            "[899 | 1689.00] loss=2.02 avg=2.50\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " Biden and his vice presidential pick, Mike Pence\n",
            "\n",
            "• The RNC’s decision to remove the RNC website, apparently after complaints from many GOP volunteers that the DNC website was “sabotaging ‘the integrity of our voting,’\n",
            "\n",
            "• DNC staffers’ decision to remove our website, after the RNC said they had done so. … Many of the RNC site editors were outed earlier this week after it was announced that their social-media accounts had been hacked. … [T]he RNC’s decision to pull, in this case, the website and put up the Vice Presidential contenders and their supporters’ own website is quite alarming, not just because of the potential for political damage, but because, of course, the DNC is trying to undermine our democratic institutions. In this case, I’m sure the RNC would take the same approach, only it wouldn’t be taken seriously.\n",
            "\n",
            "“It’s hard to see the RNC standing up to any of these attacks in a way that would withstand the scrutiny of the Ocasio-Cortez campaign,” he concludes.\n",
            "\n",
            "“A little while back in this same column, [Nancy] Carr wrote a column headlined “Trump has the support of the RNC.” That’s true, according to the RNC. And it’s a good column, although I think there is a difference between “Trump’s the support of the RNC,” as Politico reported, and “the support of the RNC.”\n",
            "\n",
            "“There’s a crucial difference,” says a Republican strategist who discussed the issues in the post. “The RNC and Trump are united around two very clear principles—the separation of powers and the independence of the executive branch.” “The RNC has never objected to his policies,” he told me. “They’ve never said he’s untrustworthy.”\n",
            "\n",
            "The strategist thinks that the DNC’s actions are part of this strategy. “This is not a strategic decision.” “And Trump does not consider it strategic,” he continues. “It’s a political decision.” A Republican operative tells me that “the RNC doesn’t understand the purpose of the site,” so they removed it for the simple fact that there weren’t many Republican Trump voters. “They said there were people who were just not interested in Trump.”\n",
            "\n",
            "But what about the rest of the GOP? As for Trump’s team’s attacks on the RNC and Trump specifically, I would argue that they are all well-founded. The attacks are a response to a statement that Trump made in Iowa in 2000. It was about how he had won the Iowa caucuses. “There was no way I could ever get that state,” he said that night, and he lost both of his closest rivals: Mike Huckabee and Ron Paul. “I was behind all that. And I was responsible for that,” Trump boasted. “I was leading in a lot of primaries,” and so he was confident he would win the nomination. He was wrong.\n",
            "\n",
            "There was a time, too, when it would have been very difficult for the GOP to separate itself from Trump. Back then, he was a big celebrity. He held rallies for President George H. W. Bush. He made appearances in major cities. To the GOP’s benefit, though, he did not have enough power and support to make his views a central issue. He was an entertainer, and a celebrity. You could say, rightly, that he was a money grubber, feeding off the publicity and the fame. Republicans in the 1980s didn’t have that luxury.\n",
            "\n",
            "Yet even here, they have some reason to hope. In March, a conservative lawyer in Utah, John Yoo, was sued by Trump University for more than $35 million. The case went to trial a few months after the controversy first erupted. In an open-ended complaint, the case claimed that Trump had committed fraud. He said, it asserted, “That a program for real estate investment education had been … fraudulently and maliciously defrauded by real estate salesmen for their own personal gain, and that millions of dollars of false advertising of worthless courses had been spread on social media to lure unsuspecting students to give fraudulent offers of education benefits, “the ‘university’s own personal enrichment” programs. “I also have extensive real estate and financial transactions, and I had … opportunities for employment, and in fact had a home equity line of credit provided by the university and a long-term loan agreement with it.”\n",
            "\n",
            "Trump denied the case’s allegations. He maintained that Trump University “was nothing\n",
            "\n",
            "[900 | 1713.77] loss=2.30 avg=2.49\n",
            "[901 | 1715.44] loss=2.36 avg=2.49\n",
            "[902 | 1717.11] loss=3.04 avg=2.50\n",
            "[903 | 1718.78] loss=2.11 avg=2.49\n",
            "[904 | 1720.44] loss=1.69 avg=2.49\n",
            "[905 | 1722.11] loss=2.73 avg=2.49\n",
            "[906 | 1723.78] loss=2.45 avg=2.49\n",
            "[907 | 1725.45] loss=2.16 avg=2.49\n",
            "[908 | 1727.11] loss=2.98 avg=2.49\n",
            "[909 | 1728.78] loss=3.19 avg=2.50\n",
            "[910 | 1730.45] loss=2.85 avg=2.50\n",
            "[911 | 1732.11] loss=2.85 avg=2.50\n",
            "[912 | 1733.78] loss=2.29 avg=2.50\n",
            "[913 | 1735.45] loss=2.72 avg=2.50\n",
            "[914 | 1737.11] loss=2.23 avg=2.50\n",
            "[915 | 1738.79] loss=2.10 avg=2.50\n",
            "[916 | 1740.46] loss=2.56 avg=2.50\n",
            "[917 | 1742.13] loss=2.71 avg=2.50\n",
            "[918 | 1743.80] loss=3.05 avg=2.51\n",
            "[919 | 1745.47] loss=2.54 avg=2.51\n",
            "[920 | 1747.16] loss=2.34 avg=2.50\n",
            "[921 | 1748.83] loss=2.49 avg=2.50\n",
            "[922 | 1750.51] loss=3.01 avg=2.51\n",
            "[923 | 1752.19] loss=2.61 avg=2.51\n",
            "[924 | 1753.88] loss=2.37 avg=2.51\n",
            "[925 | 1755.56] loss=2.60 avg=2.51\n",
            "[926 | 1757.24] loss=2.74 avg=2.51\n",
            "[927 | 1758.93] loss=2.44 avg=2.51\n",
            "[928 | 1760.60] loss=2.41 avg=2.51\n",
            "[929 | 1762.29] loss=2.11 avg=2.51\n",
            "[930 | 1763.98] loss=2.92 avg=2.51\n",
            "[931 | 1765.67] loss=2.35 avg=2.51\n",
            "[932 | 1767.35] loss=2.35 avg=2.51\n",
            "[933 | 1769.04] loss=2.67 avg=2.51\n",
            "[934 | 1770.73] loss=2.56 avg=2.51\n",
            "[935 | 1772.42] loss=2.15 avg=2.51\n",
            "[936 | 1774.11] loss=1.81 avg=2.50\n",
            "[937 | 1775.77] loss=2.73 avg=2.50\n",
            "[938 | 1777.45] loss=2.47 avg=2.50\n",
            "[939 | 1779.14] loss=2.62 avg=2.50\n",
            "[940 | 1780.82] loss=2.07 avg=2.50\n",
            "[941 | 1782.49] loss=3.03 avg=2.50\n",
            "[942 | 1784.17] loss=2.66 avg=2.50\n",
            "[943 | 1785.85] loss=2.63 avg=2.51\n",
            "[944 | 1787.53] loss=2.69 avg=2.51\n",
            "[945 | 1789.21] loss=2.56 avg=2.51\n",
            "[946 | 1790.88] loss=2.62 avg=2.51\n",
            "[947 | 1792.57] loss=2.51 avg=2.51\n",
            "[948 | 1794.24] loss=2.24 avg=2.51\n",
            "[949 | 1795.92] loss=2.23 avg=2.50\n",
            "[950 | 1797.61] loss=2.60 avg=2.51\n",
            "[951 | 1799.27] loss=2.67 avg=2.51\n",
            "[952 | 1800.95] loss=2.45 avg=2.51\n",
            "[953 | 1802.62] loss=2.12 avg=2.50\n",
            "[954 | 1804.29] loss=2.65 avg=2.50\n",
            "[955 | 1805.96] loss=2.71 avg=2.51\n",
            "[956 | 1807.63] loss=2.38 avg=2.50\n",
            "[957 | 1809.30] loss=2.14 avg=2.50\n",
            "[958 | 1810.97] loss=3.26 avg=2.51\n",
            "[959 | 1812.63] loss=2.39 avg=2.51\n",
            "[960 | 1814.30] loss=1.94 avg=2.50\n",
            "[961 | 1815.97] loss=2.08 avg=2.50\n",
            "[962 | 1817.65] loss=2.68 avg=2.50\n",
            "[963 | 1819.32] loss=2.51 avg=2.50\n",
            "[964 | 1821.01] loss=1.90 avg=2.49\n",
            "[965 | 1822.68] loss=2.74 avg=2.50\n",
            "[966 | 1824.35] loss=2.48 avg=2.50\n",
            "[967 | 1826.02] loss=2.45 avg=2.50\n",
            "[968 | 1827.69] loss=2.56 avg=2.50\n",
            "[969 | 1829.36] loss=2.40 avg=2.49\n",
            "[970 | 1831.03] loss=2.28 avg=2.49\n",
            "[971 | 1832.70] loss=2.30 avg=2.49\n",
            "[972 | 1834.37] loss=2.22 avg=2.49\n",
            "[973 | 1836.05] loss=2.29 avg=2.49\n",
            "[974 | 1837.73] loss=1.96 avg=2.48\n",
            "[975 | 1839.40] loss=2.37 avg=2.48\n",
            "[976 | 1841.07] loss=2.38 avg=2.48\n",
            "[977 | 1842.74] loss=2.47 avg=2.48\n",
            "[978 | 1844.41] loss=2.44 avg=2.48\n",
            "[979 | 1846.09] loss=2.03 avg=2.47\n",
            "[980 | 1847.77] loss=3.08 avg=2.48\n",
            "[981 | 1849.45] loss=2.45 avg=2.48\n",
            "[982 | 1851.14] loss=2.65 avg=2.48\n",
            "[983 | 1852.82] loss=2.53 avg=2.48\n",
            "[984 | 1854.50] loss=2.82 avg=2.49\n",
            "[985 | 1856.18] loss=2.32 avg=2.48\n",
            "[986 | 1857.86] loss=2.04 avg=2.48\n",
            "[987 | 1859.55] loss=2.57 avg=2.48\n",
            "[988 | 1861.23] loss=2.35 avg=2.48\n",
            "[989 | 1862.93] loss=3.01 avg=2.48\n",
            "[990 | 1864.60] loss=2.34 avg=2.48\n",
            "[991 | 1866.29] loss=2.26 avg=2.48\n",
            "[992 | 1867.98] loss=2.43 avg=2.48\n",
            "[993 | 1869.65] loss=2.88 avg=2.48\n",
            "[994 | 1871.32] loss=2.14 avg=2.48\n",
            "[995 | 1873.01] loss=2.44 avg=2.48\n",
            "[996 | 1874.70] loss=2.50 avg=2.48\n",
            "[997 | 1876.37] loss=2.42 avg=2.48\n",
            "[998 | 1878.05] loss=1.83 avg=2.47\n",
            "[999 | 1879.73] loss=2.69 avg=2.48\n",
            "Ran 1010 steps.\n",
            "Saving checkpoint/atlantic_ideas_345/model-1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHsAW2op8Uh9",
        "colab_type": "code",
        "outputId": "063adb9b-2ae4-4187-e6c7-b768d29e994e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "## international essays training - with 345\n",
        "!PYTHONPATH=src ./train.py --dataset /content/all_international.txt --run_name 'atlantic_international_345' --model_name '345M'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0627 19:31:16.338305 139982513698688 deprecation_wrapper.py:119] From /content/gpt-2/src/model.py:147: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
            "\n",
            "W0627 19:31:16.348274 139982513698688 deprecation_wrapper.py:119] From /content/gpt-2/src/memory_saving_gradients.py:13: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n",
            "W0627 19:31:16.450475 139982513698688 deprecation_wrapper.py:119] From ./train.py:88: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "W0627 19:31:16.450879 139982513698688 deprecation_wrapper.py:119] From ./train.py:91: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "2019-06-27 19:31:16.457513: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-06-27 19:31:16.457789: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x29d7100 executing computations on platform Host. Devices:\n",
            "2019-06-27 19:31:16.457840: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2019-06-27 19:31:16.460260: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\n",
            "2019-06-27 19:31:16.617292: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-27 19:31:16.618132: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x29d6840 executing computations on platform CUDA. Devices:\n",
            "2019-06-27 19:31:16.618170: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2019-06-27 19:31:16.618540: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-27 19:31:16.619141: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-06-27 19:31:16.619957: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-06-27 19:31:16.621441: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-06-27 19:31:16.622891: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-06-27 19:31:16.623268: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-06-27 19:31:16.625188: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-06-27 19:31:16.626665: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-06-27 19:31:16.629963: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-06-27 19:31:16.630100: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-27 19:31:16.630521: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-27 19:31:16.630893: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
            "2019-06-27 19:31:16.630953: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-06-27 19:31:16.631996: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-06-27 19:31:16.632022: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n",
            "2019-06-27 19:31:16.632032: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n",
            "2019-06-27 19:31:16.632365: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-27 19:31:16.632774: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-27 19:31:16.633134: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14202 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "W0627 19:31:16.633959 139982513698688 deprecation_wrapper.py:119] From ./train.py:92: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0627 19:31:27.835078 139982513698688 deprecation.py:323] From /content/gpt-2/src/sample.py:65: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "W0627 19:31:27.849889 139982513698688 deprecation.py:323] From /content/gpt-2/src/sample.py:16: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0627 19:31:27.851558 139982513698688 deprecation.py:323] From /content/gpt-2/src/sample.py:70: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.random.categorical` instead.\n",
            "W0627 19:31:27.862165 139982513698688 deprecation_wrapper.py:119] From ./train.py:121: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "W0627 19:31:43.505291 139982513698688 deprecation_wrapper.py:119] From ./train.py:144: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n",
            "W0627 19:31:43.508292 139982513698688 deprecation_wrapper.py:119] From ./train.py:147: The name tf.summary.merge is deprecated. Please use tf.compat.v1.summary.merge instead.\n",
            "\n",
            "W0627 19:31:43.509174 139982513698688 deprecation_wrapper.py:119] From ./train.py:149: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
            "\n",
            "W0627 19:31:43.509923 139982513698688 deprecation_wrapper.py:119] From ./train.py:152: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "Loading checkpoint models/345M/model.ckpt\n",
            "W0627 19:31:56.744505 139982513698688 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "Loading dataset...\n",
            "100% 1/1 [00:02<00:00,  2.18s/it]\n",
            "dataset has 277295 tokens\n",
            "Training...\n",
            "2019-06-27 19:32:11.719559: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n",
            "2019-06-27 19:32:12.434042: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "[1 | 12.91] loss=2.90 avg=2.90\n",
            "[2 | 14.50] loss=2.71 avg=2.80\n",
            "[3 | 16.11] loss=3.00 avg=2.87\n",
            "[4 | 17.71] loss=2.79 avg=2.85\n",
            "[5 | 19.33] loss=2.59 avg=2.80\n",
            "[6 | 20.95] loss=2.80 avg=2.80\n",
            "[7 | 22.58] loss=2.94 avg=2.82\n",
            "[8 | 24.21] loss=2.80 avg=2.81\n",
            "[9 | 25.85] loss=2.87 avg=2.82\n",
            "[10 | 27.50] loss=2.76 avg=2.81\n",
            "[11 | 29.15] loss=2.86 avg=2.82\n",
            "[12 | 30.80] loss=2.87 avg=2.82\n",
            "[13 | 32.47] loss=2.89 avg=2.83\n",
            "[14 | 34.14] loss=2.78 avg=2.82\n",
            "[15 | 35.81] loss=2.91 avg=2.83\n",
            "[16 | 37.49] loss=2.90 avg=2.84\n",
            "[17 | 39.18] loss=2.82 avg=2.83\n",
            "[18 | 40.86] loss=2.76 avg=2.83\n",
            "[19 | 42.57] loss=2.96 avg=2.84\n",
            "[20 | 44.28] loss=2.44 avg=2.82\n",
            "[21 | 46.00] loss=2.84 avg=2.82\n",
            "[22 | 47.72] loss=2.77 avg=2.81\n",
            "[23 | 49.43] loss=2.71 avg=2.81\n",
            "[24 | 51.17] loss=2.51 avg=2.80\n",
            "[25 | 52.90] loss=2.77 avg=2.79\n",
            "[26 | 54.64] loss=3.08 avg=2.81\n",
            "[27 | 56.35] loss=2.87 avg=2.81\n",
            "[28 | 58.06] loss=2.84 avg=2.81\n",
            "[29 | 59.76] loss=2.66 avg=2.80\n",
            "[30 | 61.47] loss=2.61 avg=2.80\n",
            "[31 | 63.16] loss=2.55 avg=2.79\n",
            "[32 | 64.87] loss=2.66 avg=2.78\n",
            "[33 | 66.55] loss=2.59 avg=2.78\n",
            "[34 | 68.22] loss=2.50 avg=2.77\n",
            "[35 | 69.89] loss=2.61 avg=2.76\n",
            "[36 | 71.56] loss=2.57 avg=2.76\n",
            "[37 | 73.23] loss=2.65 avg=2.75\n",
            "[38 | 74.89] loss=2.75 avg=2.75\n",
            "[39 | 76.54] loss=2.53 avg=2.75\n",
            "[40 | 78.20] loss=2.57 avg=2.74\n",
            "[41 | 79.86] loss=2.85 avg=2.74\n",
            "[42 | 81.52] loss=2.51 avg=2.74\n",
            "[43 | 83.17] loss=2.70 avg=2.74\n",
            "[44 | 84.82] loss=2.49 avg=2.73\n",
            "[45 | 86.46] loss=2.94 avg=2.73\n",
            "[46 | 88.12] loss=2.64 avg=2.73\n",
            "[47 | 89.76] loss=2.96 avg=2.74\n",
            "[48 | 91.41] loss=2.49 avg=2.73\n",
            "[49 | 93.05] loss=2.50 avg=2.73\n",
            "[50 | 94.70] loss=2.99 avg=2.73\n",
            "[51 | 96.34] loss=2.93 avg=2.74\n",
            "[52 | 97.97] loss=2.48 avg=2.73\n",
            "[53 | 99.61] loss=2.24 avg=2.72\n",
            "[54 | 101.25] loss=2.44 avg=2.71\n",
            "[55 | 102.89] loss=2.96 avg=2.72\n",
            "[56 | 104.54] loss=3.05 avg=2.73\n",
            "[57 | 106.17] loss=2.60 avg=2.72\n",
            "[58 | 107.82] loss=2.57 avg=2.72\n",
            "[59 | 109.46] loss=2.82 avg=2.72\n",
            "[60 | 111.10] loss=2.89 avg=2.73\n",
            "[61 | 112.75] loss=2.40 avg=2.72\n",
            "[62 | 114.39] loss=2.43 avg=2.71\n",
            "[63 | 116.04] loss=2.46 avg=2.71\n",
            "[64 | 117.70] loss=2.26 avg=2.70\n",
            "[65 | 119.35] loss=2.21 avg=2.69\n",
            "[66 | 121.01] loss=2.90 avg=2.69\n",
            "[67 | 122.67] loss=2.67 avg=2.69\n",
            "[68 | 124.33] loss=2.67 avg=2.69\n",
            "[69 | 126.00] loss=2.18 avg=2.68\n",
            "[70 | 127.67] loss=2.57 avg=2.68\n",
            "[71 | 129.34] loss=2.25 avg=2.67\n",
            "[72 | 131.01] loss=2.79 avg=2.67\n",
            "[73 | 132.68] loss=2.81 avg=2.67\n",
            "[74 | 134.35] loss=2.62 avg=2.67\n",
            "[75 | 136.02] loss=2.42 avg=2.67\n",
            "[76 | 137.69] loss=2.74 avg=2.67\n",
            "[77 | 139.37] loss=2.66 avg=2.67\n",
            "[78 | 141.03] loss=2.72 avg=2.67\n",
            "[79 | 142.70] loss=2.71 avg=2.67\n",
            "[80 | 144.37] loss=2.88 avg=2.68\n",
            "[81 | 146.04] loss=2.39 avg=2.67\n",
            "[82 | 147.71] loss=2.83 avg=2.67\n",
            "[83 | 149.37] loss=2.14 avg=2.66\n",
            "[84 | 151.04] loss=2.51 avg=2.66\n",
            "[85 | 152.70] loss=2.46 avg=2.66\n",
            "[86 | 154.36] loss=2.73 avg=2.66\n",
            "[87 | 156.01] loss=2.89 avg=2.66\n",
            "[88 | 157.68] loss=2.71 avg=2.66\n",
            "[89 | 159.34] loss=2.60 avg=2.66\n",
            "[90 | 161.00] loss=2.58 avg=2.66\n",
            "[91 | 162.66] loss=2.02 avg=2.65\n",
            "[92 | 164.32] loss=2.73 avg=2.65\n",
            "[93 | 165.98] loss=2.67 avg=2.65\n",
            "[94 | 167.64] loss=2.10 avg=2.64\n",
            "[95 | 169.30] loss=2.71 avg=2.64\n",
            "[96 | 170.95] loss=2.37 avg=2.64\n",
            "[97 | 172.61] loss=1.92 avg=2.63\n",
            "[98 | 174.27] loss=1.74 avg=2.61\n",
            "[99 | 175.92] loss=2.39 avg=2.61\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " lived in Russia during the Soviet era, she said to the BBC, because she didn't understand how Putin could have had the energy to continue to rule, while her mother was still alive. \"But if he doesn't do what is needed for the future of Russia…the problem is going to become larger than Russia.\"\n",
            "\n",
            "As a young person of the generation that was born after the collapse of communism, Nadezhda, now 45, saw its country deteriorate. \"I saw the devastation and what happened with those who wanted to leave,\" she said. Many of them sought refuge during the turmoil, seeking asylum in Europe or in the United States. Her generation went to university, and she said it was difficult to have a normal life.\n",
            "\n",
            "In the early 1980s, when she was 23, Nadezhda's family moved to Moscow after its official end in 1991, in what has been dubbed the Soviet-Russian Bloc. Since then, the city has become a breeding ground for Putin's opponents, and for other radicals, including those inspired by his worldview.\n",
            "\n",
            "Today, Nadezhda, who is married, lives with her husband in a flat in a Moscow suburb that is home to a church and a preschool. She said the city is full of anti-Kremlin young Russians who are seeking asylum. She said she has spent much of her time watching her children, who are young enough to be her own.\n",
            "\n",
            "\"I tell them, 'No, leave Russia, because Putin doesn't want you around,'\" said Nadezhda. \"And they'll listen to me. There is nothing in Russia that supports you, except Putin.\"\n",
            "\n",
            "But she said she is worried about her grandchildren, who will be growing up in a country that remains deeply corrupt. Putin is, after all, the head of Russia, and his rule will continue.\n",
            "\n",
            "That's why Nadezhda said she thinks young people should stay and continue to fight for Russia. \"I can't leave. It's part and parcel to what is going on here. It's not that I don't want to live here. But my generation is here and it is my generation that I'm fighting at. It is my generation that will defend Russia.\"\n",
            "\n",
            "* * *\n",
            "\n",
            "In Russia, we have an entire worldview. It seems we are constantly questioning who we are. In some ways, that's how we grow up here.\n",
            "\n",
            "Growing up in one of Moscow's old neighborhoods, I remember how I would look at the sky from my window as my father, a retired nuclear physicist, put on one of his last radio lectures. I remember his gentle manner, his fondness for stories, and his belief in the future of science.\n",
            "\n",
            "When I think of Russia's future, I see a future populated by the most creative minds in the country, the best doctors and engineers, the most inspiring artists and thinkers. When I remember the end of the Soviet Union, I see a future of deep division in a land long marked by trust and collaboration.\n",
            "\n",
            "I remember not having lived in Russia for so long. I think of that time when I was little, when I was afraid and quiet and never told anyone what I did not want to talk about. I feel the fear and sadness I know Russia now, where my parents were able to get by in a harsh world but where I was forced to live in the shadow of their loss.\n",
            "\n",
            "Of the many voices in the room, I heard none. My father could not speak at the end of his speech.\n",
            "\n",
            "I don't know what those days will bring, but I believe their world is closer than it has ever been.\n",
            "\n",
            "For people here, life in Russia is not going well; we're constantly told it's the worst of the worst. But for me—as a young Moscow resident looking to rebuild my life with my own hands—that is hardly surprising. In many ways, everything I see and feel here is what I remember and what I want to see and feel.\n",
            "\n",
            "I'm not sure what Russia will be, but I know it will be far less of something.<|endoftext|>This is my first ever experience with a K&N 12-32 gauge, but it is a lot like my previous experiences with the Glock 43 and Kimber LC9, and it seems to work wonderfully. The K&N model I got from Magpul is the same as the Magpul G36 and the MCX, with the MCX-12 model adding a 12-inch barrel that is more or less identical to that of the G36-10 (in both length and width). A K&N model has \"MOLLE\" straps attached to the front and the back, while the one in my Kimber is just plastic tabs that connect the back to the mag well as you pull on it. This makes for a much better grip on my firearm. I do not know if the Magpul K-11 comes with a Magp\n",
            "\n",
            "[100 | 203.69] loss=2.74 avg=2.61\n",
            "[101 | 205.36] loss=2.50 avg=2.61\n",
            "[102 | 207.03] loss=3.12 avg=2.62\n",
            "[103 | 208.70] loss=2.56 avg=2.62\n",
            "[104 | 210.37] loss=2.42 avg=2.61\n",
            "[105 | 212.04] loss=2.76 avg=2.62\n",
            "[106 | 213.71] loss=2.31 avg=2.61\n",
            "[107 | 215.38] loss=2.76 avg=2.61\n",
            "[108 | 217.04] loss=2.68 avg=2.62\n",
            "[109 | 218.71] loss=2.24 avg=2.61\n",
            "[110 | 220.38] loss=2.56 avg=2.61\n",
            "[111 | 222.05] loss=2.37 avg=2.61\n",
            "[112 | 223.72] loss=2.49 avg=2.60\n",
            "[113 | 225.39] loss=2.46 avg=2.60\n",
            "[114 | 227.06] loss=2.55 avg=2.60\n",
            "[115 | 228.72] loss=2.66 avg=2.60\n",
            "[116 | 230.39] loss=1.99 avg=2.59\n",
            "[117 | 232.05] loss=2.52 avg=2.59\n",
            "[118 | 233.72] loss=2.88 avg=2.60\n",
            "[119 | 235.39] loss=2.62 avg=2.60\n",
            "[120 | 237.06] loss=2.46 avg=2.59\n",
            "[121 | 238.72] loss=2.73 avg=2.60\n",
            "[122 | 240.39] loss=2.45 avg=2.59\n",
            "[123 | 242.05] loss=2.56 avg=2.59\n",
            "[124 | 243.72] loss=2.50 avg=2.59\n",
            "[125 | 245.38] loss=2.53 avg=2.59\n",
            "[126 | 247.04] loss=2.69 avg=2.59\n",
            "[127 | 248.71] loss=2.89 avg=2.60\n",
            "[128 | 250.38] loss=2.51 avg=2.60\n",
            "[129 | 252.04] loss=2.67 avg=2.60\n",
            "[130 | 253.71] loss=2.76 avg=2.60\n",
            "[131 | 255.39] loss=2.20 avg=2.59\n",
            "[132 | 257.05] loss=2.64 avg=2.59\n",
            "[133 | 258.73] loss=2.77 avg=2.60\n",
            "[134 | 260.40] loss=2.35 avg=2.59\n",
            "[135 | 262.07] loss=1.63 avg=2.58\n",
            "[136 | 263.75] loss=2.47 avg=2.58\n",
            "[137 | 265.41] loss=2.58 avg=2.58\n",
            "[138 | 267.08] loss=1.74 avg=2.57\n",
            "[139 | 268.75] loss=2.41 avg=2.57\n",
            "[140 | 270.43] loss=3.04 avg=2.57\n",
            "[141 | 272.10] loss=2.86 avg=2.58\n",
            "[142 | 273.77] loss=2.77 avg=2.58\n",
            "[143 | 275.44] loss=2.47 avg=2.58\n",
            "[144 | 277.11] loss=2.44 avg=2.57\n",
            "[145 | 278.78] loss=1.59 avg=2.56\n",
            "[146 | 280.45] loss=2.75 avg=2.56\n",
            "[147 | 282.12] loss=2.59 avg=2.56\n",
            "[148 | 283.79] loss=2.34 avg=2.56\n",
            "[149 | 285.46] loss=2.21 avg=2.56\n",
            "[150 | 287.13] loss=2.66 avg=2.56\n",
            "[151 | 288.79] loss=3.04 avg=2.56\n",
            "[152 | 290.46] loss=2.39 avg=2.56\n",
            "[153 | 292.13] loss=2.20 avg=2.56\n",
            "[154 | 293.80] loss=1.95 avg=2.55\n",
            "[155 | 295.47] loss=2.86 avg=2.55\n",
            "[156 | 297.14] loss=2.15 avg=2.55\n",
            "[157 | 298.82] loss=2.70 avg=2.55\n",
            "[158 | 300.48] loss=2.56 avg=2.55\n",
            "[159 | 302.16] loss=2.55 avg=2.55\n",
            "[160 | 303.83] loss=1.90 avg=2.54\n",
            "[161 | 305.50] loss=2.66 avg=2.54\n",
            "[162 | 307.17] loss=2.34 avg=2.54\n",
            "[163 | 308.84] loss=2.00 avg=2.54\n",
            "[164 | 310.51] loss=2.36 avg=2.53\n",
            "[165 | 312.18] loss=2.33 avg=2.53\n",
            "[166 | 313.86] loss=2.48 avg=2.53\n",
            "[167 | 315.53] loss=1.25 avg=2.51\n",
            "[168 | 317.20] loss=2.35 avg=2.51\n",
            "[169 | 318.86] loss=2.91 avg=2.52\n",
            "[170 | 320.53] loss=2.79 avg=2.52\n",
            "[171 | 322.21] loss=2.32 avg=2.52\n",
            "[172 | 323.88] loss=2.73 avg=2.52\n",
            "[173 | 325.55] loss=2.43 avg=2.52\n",
            "[174 | 327.22] loss=2.42 avg=2.52\n",
            "[175 | 328.89] loss=2.66 avg=2.52\n",
            "[176 | 330.56] loss=2.78 avg=2.52\n",
            "[177 | 332.23] loss=2.57 avg=2.52\n",
            "[178 | 333.90] loss=2.23 avg=2.52\n",
            "[179 | 335.57] loss=2.71 avg=2.52\n",
            "[180 | 337.24] loss=2.57 avg=2.52\n",
            "[181 | 338.91] loss=2.58 avg=2.52\n",
            "[182 | 340.58] loss=2.47 avg=2.52\n",
            "[183 | 342.26] loss=2.92 avg=2.53\n",
            "[184 | 343.94] loss=2.48 avg=2.53\n",
            "[185 | 345.61] loss=2.45 avg=2.53\n",
            "[186 | 347.30] loss=2.83 avg=2.53\n",
            "[187 | 348.98] loss=2.08 avg=2.52\n",
            "[188 | 350.66] loss=2.20 avg=2.52\n",
            "[189 | 352.33] loss=2.53 avg=2.52\n",
            "[190 | 354.02] loss=1.16 avg=2.50\n",
            "[191 | 355.69] loss=2.50 avg=2.50\n",
            "[192 | 357.37] loss=2.52 avg=2.51\n",
            "[193 | 359.05] loss=1.85 avg=2.50\n",
            "[194 | 360.74] loss=2.67 avg=2.50\n",
            "[195 | 362.42] loss=2.42 avg=2.50\n",
            "[196 | 364.10] loss=1.92 avg=2.49\n",
            "[197 | 365.77] loss=2.57 avg=2.49\n",
            "[198 | 367.45] loss=1.93 avg=2.49\n",
            "[199 | 369.14] loss=2.46 avg=2.49\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "f. 2.19-5-9\n",
            "\n",
            "7.02 (5/08-5/09)\n",
            "\n",
            "2.9 (3/08-4/09)\n",
            "\n",
            "1.9 (10/01-11/04)\n",
            "\n",
            "2.1 (12/18-26/05)\n",
            "\n",
            "1.8 (10/02-5/06)\n",
            "\n",
            "2.2 –\n",
            "\n",
            "Source: https://github, github.com/mccoy/wtc.nodes . The above data comes from https://github.com/mccoy-dc/wtc.nodes_data_2nd_hand-release. To add or view other data from these nodes, use the search function in the table.\n",
            "\n",
            "Data provided by: https://openqa.io/wpthread/t/wtc-the-wasteland-of-the-people/2776\n",
            "\n",
            "Source: https://github.com/mccoy-dc/wtc.nodes.data_2nd_hand-release. To add or view other data from these nodes, use the search function in the table.\n",
            "\n",
            "Source: https://github.com/mccoy-dc/wtc.nodes.data_2nd_hand-release. To add or view other data from these nodes, use the search function in the table.\n",
            "\n",
            "Source: https://github.com/mccoy-dc/wtc.nodes.data_2nd_hand-release. To add or view other data from these nodes, use the search function in the table.\n",
            "\n",
            "Source: https://github.com/mccoy-dc/wtc.nodes.data_2nd_hand-release. To add or view other data from these nodes, use the search function in the table.\n",
            "\n",
            "Source: https://github.com/mccoy-dc/wtc.nodes.data_2nd_hand-release. To add or view other data from these nodes, use the search function in the table.\n",
            "\n",
            "Source: https://github.com/mccoy-dc/wtc.nodes.data_2nd_hand-release. To add or view other data from these nodes, use the search function in the table.\n",
            "\n",
            "Source: https://github.com/mccoy-dc/wtc.nodes.data_2nd_hand-release. To add or view other data from these nodes, use the search function in the table.\n",
            "\n",
            "Source: https://github.com/mccoy-dc/wtc.nodes.data_2nd_hand-release. To add or view other data from these nodes, use the search function in the table.\n",
            "\n",
            "Source: https://github.com/mccoy-dc/wtc.nodes.data_2nd_hand-release. To add or view other data from these nodes, use the search function in the table.\n",
            "\n",
            "Source: https://github.com/mccoy-dc/wtc.nodes.data_2nd_hand-release. To add or view other data from these nodes, use the search function in the table.\n",
            "\n",
            "Source: https://github.com/mccoy-dc/wtc.nodes.data_2nd_hand-release. To add or view other data from these nodes, use the search function in the table.\n",
            "\n",
            "Source: https://github.com/mccoy-dc/wtc.nodes.data_2nd_hand-release. To add or view other data from these nodes, use the search function in the table.\n",
            "\n",
            "Source: https://github.com/mccoy-dc/wtc.nodes.data_2nd_hand-release. To add or view other data from these nodes, use the search function in the table.\n",
            "\n",
            "Source: https://github.com/mccoy-dc/wtc.nodes.data_2nd_hand-release. To add or view other data from these nodes, use the search function in the table.\n",
            "\n",
            "Source: https://github.com/mccoy-dc/wtc.nodes.data_2nd_hand-release. To add or view other data from these nodes, use the search function in the table.\n",
            "\n",
            "1.5 (6/05-6/06)\n",
            "\n",
            "2.2 (8/08-8/09)\n",
            "\n",
            "3.1 (8/01-11/02)\n",
            "\n",
            "Source: https://github.com/mccoy-dc/wtc.nodes_data_2nd_hand-release. To add or view other data from these nodes, use the search function in the table.\n",
            "\n",
            "Source: https://\n",
            "\n",
            "[200 | 393.67] loss=2.54 avg=2.49\n",
            "[201 | 395.33] loss=2.64 avg=2.49\n",
            "[202 | 397.00] loss=1.96 avg=2.48\n",
            "[203 | 398.64] loss=2.90 avg=2.49\n",
            "[204 | 400.31] loss=2.54 avg=2.49\n",
            "[205 | 401.97] loss=2.63 avg=2.49\n",
            "[206 | 403.63] loss=2.40 avg=2.49\n",
            "[207 | 405.29] loss=2.68 avg=2.49\n",
            "[208 | 406.95] loss=2.74 avg=2.49\n",
            "[209 | 408.61] loss=2.42 avg=2.49\n",
            "[210 | 410.27] loss=2.47 avg=2.49\n",
            "[211 | 411.93] loss=2.68 avg=2.49\n",
            "[212 | 413.60] loss=1.97 avg=2.49\n",
            "[213 | 415.27] loss=2.76 avg=2.49\n",
            "[214 | 416.94] loss=2.87 avg=2.50\n",
            "[215 | 418.61] loss=2.75 avg=2.50\n",
            "[216 | 420.29] loss=2.74 avg=2.50\n",
            "[217 | 421.96] loss=2.76 avg=2.50\n",
            "[218 | 423.63] loss=2.50 avg=2.50\n",
            "[219 | 425.30] loss=1.71 avg=2.50\n",
            "[220 | 426.97] loss=3.01 avg=2.50\n",
            "[221 | 428.64] loss=1.82 avg=2.49\n",
            "[222 | 430.31] loss=1.50 avg=2.48\n",
            "[223 | 431.98] loss=2.71 avg=2.48\n",
            "[224 | 433.66] loss=2.93 avg=2.49\n",
            "[225 | 435.33] loss=2.87 avg=2.49\n",
            "[226 | 437.01] loss=2.52 avg=2.49\n",
            "[227 | 438.69] loss=2.08 avg=2.49\n",
            "[228 | 440.37] loss=2.26 avg=2.49\n",
            "[229 | 442.04] loss=2.40 avg=2.49\n",
            "[230 | 443.73] loss=2.36 avg=2.48\n",
            "[231 | 445.40] loss=2.32 avg=2.48\n",
            "[232 | 447.09] loss=2.78 avg=2.49\n",
            "[233 | 448.77] loss=2.17 avg=2.48\n",
            "[234 | 450.45] loss=0.76 avg=2.46\n",
            "[235 | 452.12] loss=1.33 avg=2.45\n",
            "[236 | 453.81] loss=2.71 avg=2.45\n",
            "[237 | 455.50] loss=2.55 avg=2.46\n",
            "[238 | 457.17] loss=2.72 avg=2.46\n",
            "[239 | 458.85] loss=2.22 avg=2.46\n",
            "[240 | 460.54] loss=2.70 avg=2.46\n",
            "[241 | 462.21] loss=2.33 avg=2.46\n",
            "[242 | 463.89] loss=2.28 avg=2.45\n",
            "[243 | 465.57] loss=2.18 avg=2.45\n",
            "[244 | 467.25] loss=2.69 avg=2.45\n",
            "[245 | 468.93] loss=2.74 avg=2.46\n",
            "[246 | 470.60] loss=2.33 avg=2.46\n",
            "[247 | 472.27] loss=2.06 avg=2.45\n",
            "[248 | 473.94] loss=2.83 avg=2.46\n",
            "[249 | 475.61] loss=2.58 avg=2.46\n",
            "[250 | 477.28] loss=2.85 avg=2.46\n",
            "[251 | 478.96] loss=2.59 avg=2.46\n",
            "[252 | 480.64] loss=2.33 avg=2.46\n",
            "[253 | 482.31] loss=2.57 avg=2.46\n",
            "[254 | 484.00] loss=2.76 avg=2.47\n",
            "[255 | 485.68] loss=2.32 avg=2.46\n",
            "[256 | 487.37] loss=2.94 avg=2.47\n",
            "[257 | 489.04] loss=3.03 avg=2.48\n",
            "[258 | 490.73] loss=2.49 avg=2.48\n",
            "[259 | 492.40] loss=2.38 avg=2.47\n",
            "[260 | 494.09] loss=2.50 avg=2.47\n",
            "[261 | 495.76] loss=2.29 avg=2.47\n",
            "[262 | 497.43] loss=3.00 avg=2.48\n",
            "[263 | 499.10] loss=2.62 avg=2.48\n",
            "[264 | 500.78] loss=1.22 avg=2.47\n",
            "[265 | 502.47] loss=2.96 avg=2.47\n",
            "[266 | 504.15] loss=2.76 avg=2.47\n",
            "[267 | 505.83] loss=1.59 avg=2.47\n",
            "[268 | 507.51] loss=2.61 avg=2.47\n",
            "[269 | 509.19] loss=2.64 avg=2.47\n",
            "[270 | 510.86] loss=2.68 avg=2.47\n",
            "[271 | 512.55] loss=2.69 avg=2.47\n",
            "[272 | 514.23] loss=2.48 avg=2.47\n",
            "[273 | 515.92] loss=2.99 avg=2.48\n",
            "[274 | 517.60] loss=2.88 avg=2.48\n",
            "[275 | 519.27] loss=2.63 avg=2.49\n",
            "[276 | 520.94] loss=2.06 avg=2.48\n",
            "[277 | 522.62] loss=2.59 avg=2.48\n",
            "[278 | 524.29] loss=2.25 avg=2.48\n",
            "[279 | 525.96] loss=1.08 avg=2.46\n",
            "[280 | 527.65] loss=2.40 avg=2.46\n",
            "[281 | 529.32] loss=2.00 avg=2.46\n",
            "[282 | 531.00] loss=2.63 avg=2.46\n",
            "[283 | 532.69] loss=1.67 avg=2.45\n",
            "[284 | 534.36] loss=2.52 avg=2.45\n",
            "[285 | 536.05] loss=2.32 avg=2.45\n",
            "[286 | 537.73] loss=2.66 avg=2.45\n",
            "[287 | 539.41] loss=2.33 avg=2.45\n",
            "[288 | 541.10] loss=1.86 avg=2.45\n",
            "[289 | 542.77] loss=2.36 avg=2.44\n",
            "[290 | 544.46] loss=2.33 avg=2.44\n",
            "[291 | 546.15] loss=2.42 avg=2.44\n",
            "[292 | 547.83] loss=2.51 avg=2.44\n",
            "[293 | 549.50] loss=2.75 avg=2.45\n",
            "[294 | 551.18] loss=2.47 avg=2.45\n",
            "[295 | 552.86] loss=2.28 avg=2.45\n",
            "[296 | 554.56] loss=1.87 avg=2.44\n",
            "[297 | 556.23] loss=2.49 avg=2.44\n",
            "[298 | 557.91] loss=2.87 avg=2.44\n",
            "[299 | 559.59] loss=2.78 avg=2.45\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            ".\n",
            "\n",
            "\n",
            "The city was struck by the sudden spate of deadly explosions and shootings, leading to fears of a \"bloodbath\" and \"massacre.\" When the attacks took place, people gathered around cell radios for reassurance. A local journalist reported that the attacks had taken place in a neighborhood of a predominantly Muslim community.\n",
            "\n",
            "What was not yet known at the time was that the men suspected of the attacks were all born in Istanbul.\n",
            "\n",
            "\n",
            "— With assistance by Kavik Caglayan\n",
            "\n",
            "\n",
            "A large number of citizens in the Turkish city of Ankara were evacuated on Tuesday, in response to rumors of a possible terrorist attack, amid escalating fears of a terrorist attack by Kurdish forces in Iraqi Kurdistan. The move came after two Turkish men were detained on Sunday by Iraqi government authorities for allegedly supporting the outlawed Kurdistan Workers' Party (PKK), a group designated as a terrorist group by Turkey.\n",
            "\n",
            "\n",
            "The incidents are just the latest in a series of events that have prompted a sense of fear and anxiety in Turkish society, following the arrest of an American woman on Sunday and two Kurdish suspects detained on Tuesday.\n",
            "\n",
            "\n",
            "Turkey, which has close ties to the United States, has been one of those countries' strongest allies in the country since Erdogan assumed office more than two decades ago—since when has there been a stronger presence of American citizens from around the world in Turkey than there is inside the United States? For a long time, no one spoke about their absence in Turkey's history and culture, but things changed in 2015, when Erdogan and his then-top adviser Necdet Ozel publicly stated that Turkey would soon become the first country in the world to bar American citizens from entering the country indefinitely.\n",
            "\n",
            "\n",
            "Today, the fear of being removed from Turkey and denied entry because of your race, religion, nationality, ethnicity, or political beliefs is becoming a familiar voice of reason on both sides. The two people detained Sunday have been released but remain in their custody.\n",
            "\n",
            "\n",
            "The events in Ankara have caused the Turkish government to scale back its travel restrictions, and it has also begun a public debate on the potential of a possible terrorist attack—an argument largely dismissed by American citizens in Turkey.\n",
            "\n",
            "\n",
            "As the government has reacted to the Ankara incidents, an influx of citizens from Syria, Pakistan, Afghanistan, Iraq, Afghanistan, Libya, and Yemen have also begun arriving in the United States, with the most recent visit by a Somali refugee being the most recent to arrive.\n",
            "\n",
            "\n",
            "As the situation in Iraq and Syria has worsened, U.S. government travel advisories have been removed from Turkish airports, barring citizens of the former Muslim states from visiting, while the United States has withdrawn its full-time embassy in Ankara. Ankara remains, however, one of the most sensitive countries in the world to U.S. security—and some citizens fear the threat of a terrorist incident.\n",
            "\n",
            "— With assistance by Genna Hejduk\n",
            "\n",
            "\n",
            "The man suspected of carrying out the deadliest terrorist attack in Turkish history has been named as Mehmet Ozol, a member of the Muslim Ahmetdin Peker party and a former army general who led a coup against Prime Minister Recep Tayyip Erdogan in the early '90s.\n",
            "\n",
            "\n",
            "According to state-run news agency Anadolu, Ozol is the man, believed to be the mastermind of a string of bombings, shootings, and knife attacks that targeted Istanbul at least five times between 1999 and 2005. One incident saw a car plow through a line of soldiers on Istanbul University, the country's busiest university, killing 44 people. Another saw two knives slashed through pedestrians on Gezi Park, Istanbul's largest park and a site of protests against the military coup against Erdogan in 2014. And there were also attacks in 2013, when a truck packed with explosives exploded in the main market, killing at least 21 people, most of which were shoppers at a clothing store there.\n",
            "\n",
            "\n",
            "Ozol remained a key figure in Turkey for years after the military launched its coup against the then-prime minister in 2013, and was a member of Erdogan's new ruling party until he was appointed prime minister last year—and was sworn in in the presence of Erdogan—along with military strongman Lt. Gen. Cem Öztürk, who later rose to his own prominence, despite having served as Erdoğan's prime minister and then as Erdogan's chief of staff.\n",
            "\n",
            "\n",
            "The Turkish government has taken steps to ensure that there is no repeat of the Istanbul attack, including canceling Turkish citizenship of those implicated in the attacks, and imposing a travel ban on those believed to have been in the country before the July 15 coup and since—a similar move that was in place after the 2013 Istanbul attacks.\n",
            "\n",
            "\n",
            "— Follow Sarah on Twitter\n",
            "\n",
            "Send tips to shad@ dailycallernewsfoundation.org.\n",
            "\n",
            "Content created by The Daily Caller News Foundation is available without charge to any eligible news publisher that can provide a large audience. For licensing opportunities of our original content, please contact licensing@dailycallernewsfoundation\n",
            "\n",
            "[300 | 584.47] loss=2.32 avg=2.45\n",
            "[301 | 586.14] loss=2.53 avg=2.45\n",
            "[302 | 587.80] loss=2.11 avg=2.44\n",
            "[303 | 589.46] loss=1.19 avg=2.43\n",
            "[304 | 591.13] loss=2.26 avg=2.43\n",
            "[305 | 592.79] loss=2.27 avg=2.43\n",
            "[306 | 594.45] loss=2.72 avg=2.43\n",
            "[307 | 596.11] loss=3.09 avg=2.44\n",
            "[308 | 597.77] loss=2.33 avg=2.44\n",
            "[309 | 599.43] loss=1.72 avg=2.43\n",
            "[310 | 601.09] loss=2.72 avg=2.43\n",
            "[311 | 602.76] loss=2.38 avg=2.43\n",
            "[312 | 604.42] loss=2.76 avg=2.43\n",
            "[313 | 606.08] loss=2.45 avg=2.43\n",
            "[314 | 607.74] loss=2.32 avg=2.43\n",
            "[315 | 609.42] loss=2.78 avg=2.44\n",
            "[316 | 611.09] loss=0.98 avg=2.42\n",
            "[317 | 612.76] loss=2.35 avg=2.42\n",
            "[318 | 614.43] loss=2.01 avg=2.42\n",
            "[319 | 616.10] loss=1.75 avg=2.41\n",
            "[320 | 617.77] loss=0.84 avg=2.39\n",
            "[321 | 619.44] loss=2.62 avg=2.40\n",
            "[322 | 621.11] loss=2.66 avg=2.40\n",
            "[323 | 622.80] loss=0.57 avg=2.38\n",
            "[324 | 624.47] loss=0.58 avg=2.36\n",
            "[325 | 626.15] loss=2.55 avg=2.36\n",
            "[326 | 627.82] loss=2.53 avg=2.36\n",
            "[327 | 629.49] loss=1.23 avg=2.35\n",
            "[328 | 631.18] loss=1.94 avg=2.35\n",
            "[329 | 632.87] loss=2.73 avg=2.35\n",
            "[330 | 634.54] loss=2.92 avg=2.36\n",
            "[331 | 636.21] loss=2.45 avg=2.36\n",
            "[332 | 637.89] loss=2.34 avg=2.36\n",
            "[333 | 639.58] loss=2.86 avg=2.36\n",
            "[334 | 641.26] loss=2.51 avg=2.37\n",
            "[335 | 642.94] loss=2.75 avg=2.37\n",
            "[336 | 644.61] loss=2.00 avg=2.37\n",
            "[337 | 646.28] loss=2.09 avg=2.36\n",
            "[338 | 647.97] loss=2.52 avg=2.36\n",
            "[339 | 649.64] loss=0.24 avg=2.34\n",
            "[340 | 651.32] loss=2.47 avg=2.34\n",
            "[341 | 653.01] loss=2.38 avg=2.34\n",
            "[342 | 654.68] loss=2.54 avg=2.35\n",
            "[343 | 656.36] loss=2.96 avg=2.35\n",
            "[344 | 658.03] loss=0.71 avg=2.34\n",
            "[345 | 659.70] loss=2.01 avg=2.33\n",
            "[346 | 661.37] loss=2.66 avg=2.34\n",
            "[347 | 663.05] loss=2.10 avg=2.33\n",
            "[348 | 664.72] loss=2.20 avg=2.33\n",
            "[349 | 666.39] loss=2.31 avg=2.33\n",
            "[350 | 668.06] loss=2.72 avg=2.34\n",
            "[351 | 669.73] loss=2.18 avg=2.33\n",
            "[352 | 671.40] loss=2.46 avg=2.34\n",
            "[353 | 673.07] loss=2.23 avg=2.33\n",
            "[354 | 674.74] loss=2.58 avg=2.34\n",
            "[355 | 676.41] loss=1.58 avg=2.33\n",
            "[356 | 678.08] loss=0.97 avg=2.32\n",
            "[357 | 679.75] loss=2.61 avg=2.32\n",
            "[358 | 681.42] loss=0.48 avg=2.30\n",
            "[359 | 683.09] loss=2.50 avg=2.30\n",
            "[360 | 684.76] loss=2.47 avg=2.30\n",
            "[361 | 686.43] loss=1.51 avg=2.30\n",
            "[362 | 688.10] loss=2.94 avg=2.30\n",
            "[363 | 689.77] loss=2.78 avg=2.31\n",
            "[364 | 691.44] loss=2.44 avg=2.31\n",
            "[365 | 693.11] loss=2.45 avg=2.31\n",
            "[366 | 694.78] loss=2.11 avg=2.31\n",
            "[367 | 696.45] loss=1.95 avg=2.30\n",
            "[368 | 698.12] loss=1.21 avg=2.29\n",
            "[369 | 699.79] loss=1.82 avg=2.29\n",
            "[370 | 701.45] loss=0.59 avg=2.27\n",
            "[371 | 703.12] loss=2.76 avg=2.28\n",
            "[372 | 704.79] loss=2.41 avg=2.28\n",
            "[373 | 706.46] loss=2.41 avg=2.28\n",
            "[374 | 708.13] loss=2.23 avg=2.28\n",
            "[375 | 709.80] loss=2.50 avg=2.28\n",
            "[376 | 711.48] loss=2.23 avg=2.28\n",
            "[377 | 713.14] loss=2.78 avg=2.28\n",
            "[378 | 714.81] loss=2.73 avg=2.29\n",
            "[379 | 716.48] loss=0.45 avg=2.27\n",
            "[380 | 718.15] loss=1.32 avg=2.26\n",
            "[381 | 719.82] loss=1.00 avg=2.25\n",
            "[382 | 721.49] loss=0.34 avg=2.23\n",
            "[383 | 723.16] loss=2.57 avg=2.23\n",
            "[384 | 724.83] loss=2.37 avg=2.23\n",
            "[385 | 726.50] loss=2.35 avg=2.23\n",
            "[386 | 728.17] loss=2.64 avg=2.24\n",
            "[387 | 729.84] loss=2.61 avg=2.24\n",
            "[388 | 731.50] loss=2.11 avg=2.24\n",
            "[389 | 733.17] loss=2.35 avg=2.24\n",
            "[390 | 734.85] loss=2.35 avg=2.24\n",
            "[391 | 736.52] loss=1.55 avg=2.24\n",
            "[392 | 738.19] loss=2.87 avg=2.24\n",
            "[393 | 739.87] loss=0.34 avg=2.22\n",
            "[394 | 741.55] loss=2.12 avg=2.22\n",
            "[395 | 743.22] loss=2.57 avg=2.23\n",
            "[396 | 744.89] loss=2.38 avg=2.23\n",
            "[397 | 746.57] loss=2.24 avg=2.23\n",
            "[398 | 748.25] loss=2.47 avg=2.23\n",
            "[399 | 749.92] loss=2.21 avg=2.23\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " international leaders would have to agree.\n",
            "\n",
            "In May, the United Nations voted to call for the suspension of all new sanctions against Burma because it was using \"cruel, inhumane, and in some instances even illegal\" practices to quell unrest there. The move will give Beijing time to negotiate on issues related to the island nation that Beijing says have been raised, including human rights and food safety standards.\n",
            "\n",
            "The United States also remains a firm ally of Burma, but has repeatedly indicated that it may consider taking stronger action against Burma than it has so far. This week, the State Department released a statement saying that the administration is concerned about alleged human rights violations there and that the United States has expressed concerns to government officials that violations might be continuing, though those complaints have not been substantiated.\n",
            "\n",
            "In the meantime, Burmese troops have moved in — in places where their crackdown has had the most success, including villages and towns where local leaders have appealed for international assistance — and have expanded the size of the operation and strengthened their control.\n",
            "\n",
            "\"For Burma, where this is the war, the only path for the Burmese remains to continue this bloody war,\" said Nyanas. \"The United States, and more and more other countries will have to consider whether that's a good path. And whether it is the path to peace in the long run. We need a long-term solution.\"\n",
            "\n",
            "\n",
            "\n",
            "BEIJING — China has continued to bolster its hold atop the seas, and its strategy is to use the islands that it has seized to exert its power in ways the rest of the world has not seen. These advances will be under pressure in the coming days, as the People's Liberation Army considers its next moves on disputed territories in the South China Sea, and will be an ongoing challenge for the U.S.-led alliance that has been committed to upholding freedom of navigation and overflight.\n",
            "\n",
            "China's latest moves are being carefully watched in the U.S. and elsewhere, as Washington and its allies have been pressing China on these issues in their efforts to pressure it into reform. The U.S. is also stepping up its pressure on Beijing in ways it has long not done.\n",
            "\n",
            "Since taking power only a year ago, Chinese President Xi Jinping has continued to expand China's reach to choke down trade in the South China Sea and bolster its control over these islands, with Beijing providing military and logistics support to its neighbors. Although much of this has come with a caveat: Since 2015, when China began asserting its claims to some of these islands with extensive building and land reclamation, Washington has been under increasing pressure to exercise pressure on Beijing on other issues.\n",
            "\n",
            "That could be especially true in the run-up to the ASEAN summit, scheduled in Manila this week. Beijing has been one of the few Southeast Asian countries unwilling to take any steps that might be seen as threatening its influence in the Asia-Pacific. So the U.S. and other allies, and particularly those in the Philippines and Vietnam, are unlikely to accept any Chinese moves that go beyond land reclamation, militarization, and reclamation. That could open a wider rift between the region and that of the U.S., already struggling to find consensus — despite growing economic ties and military commitments — and a U.S. president facing calls by allies in the region and the White House to take even deeper steps to stop China.\n",
            "\n",
            "This is the second recent time China has expanded its control of the South China Sea, after it announced in September it would militarize some of its reclaimed islands, including Subi Reef, and build airstrips and other structures there. While the new reclamation could be seen as a test run for what those reclamation and militarization plans will look like, both have also drawn U.S. criticism at the same time. And a U.S. military deployment to the area will not be inevitable.\n",
            "\n",
            "Still, the United States and its allies will be watching to see whether it can manage China's efforts on contested territory — how its actions are viewed by others, including China, and whether the U.S. and its partners in the Asia Pacific have the will or capacity to act. The alliance may be too much for the Chinese leadership, although they may hope others will soon find that challenge.\n",
            "\n",
            "\"This is China, this is their strategy. It has not worked. It has not been implemented,\" said Niyaz Jamil, executive director of the U.S.-based Institute for the Study of War, and a former U.S. Navy officer in the Asian Pacific, which now provides expertise to the Pentagon on the region, as well as to other countries.\n",
            "\n",
            "One major area that China is expected to keep its eyes on are its islands, which it recently reclaimed in the South China Sea, which are almost entirely uncharted and disputed territory. That makes China as much a global power as the Philippines, Vietnam, or any other country, although the Philippines, whose navy\n",
            "\n",
            "[400 | 774.60] loss=2.36 avg=2.23\n",
            "[401 | 776.27] loss=2.90 avg=2.24\n",
            "[402 | 777.94] loss=0.27 avg=2.22\n",
            "[403 | 779.61] loss=2.42 avg=2.22\n",
            "[404 | 781.28] loss=2.10 avg=2.22\n",
            "[405 | 782.94] loss=2.34 avg=2.22\n",
            "[406 | 784.60] loss=2.54 avg=2.22\n",
            "[407 | 786.26] loss=2.49 avg=2.23\n",
            "[408 | 787.92] loss=1.61 avg=2.22\n",
            "[409 | 789.59] loss=2.01 avg=2.22\n",
            "[410 | 791.26] loss=2.27 avg=2.22\n",
            "[411 | 792.92] loss=2.28 avg=2.22\n",
            "[412 | 794.59] loss=2.30 avg=2.22\n",
            "[413 | 796.26] loss=0.72 avg=2.20\n",
            "[414 | 797.94] loss=2.76 avg=2.21\n",
            "[415 | 799.61] loss=1.86 avg=2.21\n",
            "[416 | 801.28] loss=2.79 avg=2.21\n",
            "[417 | 802.95] loss=2.40 avg=2.21\n",
            "[418 | 804.63] loss=2.82 avg=2.22\n",
            "[419 | 806.30] loss=2.19 avg=2.22\n",
            "[420 | 807.97] loss=2.17 avg=2.22\n",
            "[421 | 809.64] loss=2.28 avg=2.22\n",
            "[422 | 811.32] loss=1.36 avg=2.21\n",
            "[423 | 813.00] loss=2.78 avg=2.22\n",
            "[424 | 814.68] loss=2.63 avg=2.22\n",
            "[425 | 816.36] loss=2.49 avg=2.22\n",
            "[426 | 818.04] loss=2.19 avg=2.22\n",
            "[427 | 819.73] loss=2.62 avg=2.23\n",
            "[428 | 821.41] loss=2.44 avg=2.23\n",
            "[429 | 823.09] loss=2.67 avg=2.23\n",
            "[430 | 824.78] loss=2.82 avg=2.24\n",
            "[431 | 826.47] loss=2.18 avg=2.24\n",
            "[432 | 828.14] loss=2.58 avg=2.24\n",
            "[433 | 829.83] loss=2.57 avg=2.25\n",
            "[434 | 831.52] loss=2.50 avg=2.25\n",
            "[435 | 833.21] loss=2.25 avg=2.25\n",
            "[436 | 834.88] loss=2.32 avg=2.25\n",
            "[437 | 836.56] loss=2.22 avg=2.25\n",
            "[438 | 838.24] loss=2.47 avg=2.25\n",
            "[439 | 839.92] loss=2.28 avg=2.25\n",
            "[440 | 841.59] loss=2.41 avg=2.25\n",
            "[441 | 843.27] loss=2.69 avg=2.26\n",
            "[442 | 844.94] loss=2.55 avg=2.26\n",
            "[443 | 846.61] loss=0.72 avg=2.25\n",
            "[444 | 848.28] loss=2.35 avg=2.25\n",
            "[445 | 849.96] loss=2.12 avg=2.25\n",
            "[446 | 851.62] loss=0.16 avg=2.22\n",
            "[447 | 853.30] loss=2.38 avg=2.23\n",
            "[448 | 854.97] loss=2.59 avg=2.23\n",
            "[449 | 856.64] loss=2.21 avg=2.23\n",
            "[450 | 858.30] loss=2.49 avg=2.23\n",
            "[451 | 859.99] loss=2.57 avg=2.24\n",
            "[452 | 861.66] loss=2.40 avg=2.24\n",
            "[453 | 863.34] loss=2.23 avg=2.24\n",
            "[454 | 865.01] loss=2.40 avg=2.24\n",
            "[455 | 866.69] loss=2.27 avg=2.24\n",
            "[456 | 868.37] loss=2.50 avg=2.24\n",
            "[457 | 870.03] loss=2.62 avg=2.25\n",
            "[458 | 871.71] loss=2.66 avg=2.25\n",
            "[459 | 873.38] loss=2.21 avg=2.25\n",
            "[460 | 875.05] loss=1.83 avg=2.24\n",
            "[461 | 876.72] loss=2.44 avg=2.25\n",
            "[462 | 878.39] loss=2.15 avg=2.25\n",
            "[463 | 880.06] loss=2.34 avg=2.25\n",
            "[464 | 881.73] loss=2.04 avg=2.24\n",
            "[465 | 883.41] loss=0.25 avg=2.22\n",
            "[466 | 885.09] loss=2.59 avg=2.23\n",
            "[467 | 886.75] loss=2.62 avg=2.23\n",
            "[468 | 888.42] loss=1.95 avg=2.23\n",
            "[469 | 890.09] loss=2.71 avg=2.23\n",
            "[470 | 891.76] loss=2.56 avg=2.24\n",
            "[471 | 893.43] loss=2.52 avg=2.24\n",
            "[472 | 895.09] loss=2.06 avg=2.24\n",
            "[473 | 896.76] loss=2.35 avg=2.24\n",
            "[474 | 898.42] loss=2.22 avg=2.24\n",
            "[475 | 900.09] loss=1.08 avg=2.23\n",
            "[476 | 901.74] loss=2.25 avg=2.23\n",
            "[477 | 903.40] loss=1.96 avg=2.23\n",
            "[478 | 905.05] loss=2.43 avg=2.23\n",
            "[479 | 906.71] loss=2.58 avg=2.23\n",
            "[480 | 908.35] loss=2.56 avg=2.23\n",
            "[481 | 910.00] loss=2.53 avg=2.24\n",
            "[482 | 911.65] loss=2.28 avg=2.24\n",
            "[483 | 913.29] loss=1.42 avg=2.23\n",
            "[484 | 914.93] loss=2.32 avg=2.23\n",
            "[485 | 916.58] loss=1.91 avg=2.23\n",
            "[486 | 918.23] loss=2.73 avg=2.23\n",
            "[487 | 919.88] loss=0.25 avg=2.21\n",
            "[488 | 921.53] loss=2.49 avg=2.21\n",
            "[489 | 923.17] loss=2.08 avg=2.21\n",
            "[490 | 924.82] loss=1.88 avg=2.21\n",
            "[491 | 926.47] loss=2.03 avg=2.21\n",
            "[492 | 928.12] loss=2.52 avg=2.21\n",
            "[493 | 929.76] loss=2.74 avg=2.22\n",
            "[494 | 931.42] loss=2.82 avg=2.22\n",
            "[495 | 933.06] loss=2.39 avg=2.22\n",
            "[496 | 934.71] loss=2.39 avg=2.23\n",
            "[497 | 936.36] loss=2.08 avg=2.22\n",
            "[498 | 938.00] loss=1.94 avg=2.22\n",
            "[499 | 939.65] loss=2.15 avg=2.22\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " fight to prevent the loss of Crimea, including a referendum on joining Russia — which the West says Ukraine should hold.\n",
            "\n",
            "That support, coupled with Putin's desire to build bridges across former Soviet bloc lands, has drawn in Moscow and bolstered Russia-friendly governments in Eastern Europe, including the Baltic states of Estonia and Lithuania. The two countries are at least now looking, in the hope that the new president, Petro Poroshenko, will offer Moscow at least some sort of support for a cease-fire and a commitment to further trade and diplomatic measures.\n",
            "\n",
            "But in Ukraine, Putin has made clear that he wants to keep the Crimean Peninsula as a way station on Russia's Silk Road project and that maintaining it as part of Ukraine would make him a key partner in a larger effort to replace Russian influence in Europe.\n",
            "\n",
            "In Moscow today, most of the people I spoke with were in agreement that Putin could do as he wants to Ukraine. But at the same time, they said, they don't want Russia to get involved militarily with the rest of Ukraine. And they also said that if Russian troops in Crimea decided to move to the southern Russian city of Rostov-on-Don or to the restive region of South Ossetia — Russian-speaking regions of Georgia that have been occupied by the Georgian government — they would almost certainly be welcomed.\n",
            "\n",
            "And as a last resort, some Russian speakers said, they fear that the Kremlin might resort to force to bring about regime change in Georgia, a policy that has been criticized by the West.\n",
            "\n",
            "Still, many people I spoke with, and a large segment of Russian-speakers in Ukraine, said they would like to see Putin back the country's pro-Russian faction in political, economic, or military affairs, as well. Some said they would like to see the Russians to get rid of Ukraine, too.\n",
            "\n",
            "***\n",
            "\n",
            "The Ukrainian crisis has become emblematic of the growing divide between East and West. From the United States to Europe and the former Soviet Union, citizens have come together not just to question U.S. foreign policy and the U.S. role in the world, but in an unprecedented way, to push back against a perceived threat to their country.\n",
            "\n",
            "To understand that sense of danger, you would think that the United States would have moved quickly to contain the consequences of a breach of NATO's eastern flank. But U.S. officials have resisted the notion of the alliance shifting its eastern flank, even as Moscow appears bent on annexing the Crimea by force. In one stark sign, when it came to responding to possible military escalation by Russia, the White House said it would seek Russian military support without directly supporting the separatists in eastern Ukraine. (Washington has not done so in almost a decade.)\n",
            "\n",
            "Ukraine is one of several countries in Eastern Europe that have been caught between the West and Russia. It is a strange situation that has arisen after the cold war and the collapse of the Soviet Union.\n",
            "\n",
            "But this kind of confrontation — between Russia and the West in the 21st century, and between Russia and itself on the global stage — is a thing of the past. Instead, the focus of the Ukrainian conflict is on the question of whether Washington can continue its military campaign in Afghanistan, and whether it even has the political means to do what it is now undertaking in Afghanistan.\n",
            "\n",
            "The Obama administration believes the answer to both questions is no. Russian President Vladi­mir Putin has said that in his view the United States is waging a war on Russia, and has responded to the United States' military campaign by saying that no such war is necessary.\n",
            "\n",
            "The problem is that it is not clear what Washington expects Russia to do in the interim. The United States and others would like to see Putin back Ukraine as part of Russia, but Putin himself has insisted that he will only take full full control of Crimea if the outside world accepts Crimea as part of Russia — something that would mean his own takeover of Ukraine, which does not appear likely.\n",
            "\n",
            "Western governments have begun to come together against Russia, especially with the Crimean and now the southeastern Ukrainian region of Donetsk. In the past, Ukrainian leaders have said that if Moscow wants to hold back the flow of refugees across the land border, Ukraine should not join the European Union. On Monday, Poroshenko said that as a result of sanctions and Western sanctions against Russia over its ongoing military campaign, Ukraine would not join the EU.\n",
            "\n",
            "And in Ukraine, the pro-Russian separatists have shown signs of slowing their advance at the border with the Russian regions of Luhansk and Donetsk, which have suffered repeated heavy fighting. If the situation improves on the Russian side, Kiev and the West hope that the next step will be a ceasefire rather than a full-fledged full-scale conflict. But it is not clear that the separatists would permit Moscow to take full control of their territory, and Moscow would likely prefer a peaceful settlement to allow a greater amount of time to resolve the situation.\n",
            "\n",
            "The Crimea crisis has created\n",
            "\n",
            "[500 | 964.37] loss=0.14 avg=2.20\n",
            "[501 | 966.03] loss=0.14 avg=2.18\n",
            "[502 | 967.70] loss=1.86 avg=2.18\n",
            "[503 | 969.37] loss=2.12 avg=2.18\n",
            "[504 | 971.04] loss=2.31 avg=2.18\n",
            "[505 | 972.71] loss=2.36 avg=2.18\n",
            "[506 | 974.38] loss=2.49 avg=2.18\n",
            "[507 | 976.05] loss=2.17 avg=2.18\n",
            "[508 | 977.73] loss=2.18 avg=2.18\n",
            "[509 | 979.40] loss=2.09 avg=2.18\n",
            "[510 | 981.07] loss=2.55 avg=2.18\n",
            "[511 | 982.75] loss=1.95 avg=2.18\n",
            "[512 | 984.43] loss=2.29 avg=2.18\n",
            "[513 | 986.10] loss=2.09 avg=2.18\n",
            "[514 | 987.78] loss=2.14 avg=2.18\n",
            "[515 | 989.45] loss=2.14 avg=2.18\n",
            "[516 | 991.12] loss=1.93 avg=2.18\n",
            "[517 | 992.80] loss=3.20 avg=2.19\n",
            "[518 | 994.48] loss=2.13 avg=2.19\n",
            "[519 | 996.16] loss=1.99 avg=2.19\n",
            "[520 | 997.84] loss=2.26 avg=2.19\n",
            "[521 | 999.52] loss=2.15 avg=2.19\n",
            "[522 | 1001.19] loss=2.14 avg=2.19\n",
            "[523 | 1002.88] loss=1.97 avg=2.18\n",
            "[524 | 1004.56] loss=2.53 avg=2.19\n",
            "[525 | 1006.24] loss=1.90 avg=2.18\n",
            "[526 | 1007.93] loss=2.30 avg=2.19\n",
            "[527 | 1009.61] loss=2.53 avg=2.19\n",
            "[528 | 1011.28] loss=2.53 avg=2.19\n",
            "[529 | 1012.97] loss=2.37 avg=2.19\n",
            "[530 | 1014.66] loss=2.32 avg=2.20\n",
            "[531 | 1016.33] loss=1.86 avg=2.19\n",
            "[532 | 1018.02] loss=2.59 avg=2.20\n",
            "[533 | 1019.71] loss=2.29 avg=2.20\n",
            "[534 | 1021.38] loss=2.01 avg=2.20\n",
            "[535 | 1023.07] loss=2.19 avg=2.20\n",
            "[536 | 1024.74] loss=2.65 avg=2.20\n",
            "[537 | 1026.43] loss=2.65 avg=2.20\n",
            "[538 | 1028.12] loss=2.10 avg=2.20\n",
            "[539 | 1029.79] loss=2.05 avg=2.20\n",
            "[540 | 1031.46] loss=1.95 avg=2.20\n",
            "[541 | 1033.14] loss=2.24 avg=2.20\n",
            "[542 | 1034.83] loss=2.43 avg=2.20\n",
            "[543 | 1036.50] loss=2.03 avg=2.20\n",
            "[544 | 1038.17] loss=2.25 avg=2.20\n",
            "[545 | 1039.84] loss=1.91 avg=2.20\n",
            "[546 | 1041.53] loss=2.12 avg=2.20\n",
            "[547 | 1043.20] loss=1.92 avg=2.19\n",
            "[548 | 1044.87] loss=1.63 avg=2.19\n",
            "[549 | 1046.54] loss=2.46 avg=2.19\n",
            "[550 | 1048.21] loss=2.43 avg=2.19\n",
            "[551 | 1049.89] loss=2.67 avg=2.20\n",
            "[552 | 1051.56] loss=1.89 avg=2.20\n",
            "[553 | 1053.23] loss=1.00 avg=2.18\n",
            "[554 | 1054.90] loss=1.88 avg=2.18\n",
            "[555 | 1056.57] loss=2.60 avg=2.18\n",
            "[556 | 1058.25] loss=2.57 avg=2.19\n",
            "[557 | 1059.92] loss=2.26 avg=2.19\n",
            "[558 | 1061.59] loss=2.00 avg=2.19\n",
            "[559 | 1063.26] loss=0.19 avg=2.17\n",
            "[560 | 1064.94] loss=2.61 avg=2.17\n",
            "[561 | 1066.61] loss=2.42 avg=2.17\n",
            "[562 | 1068.28] loss=2.75 avg=2.18\n",
            "[563 | 1069.95] loss=2.72 avg=2.19\n",
            "[564 | 1071.63] loss=2.06 avg=2.18\n",
            "[565 | 1073.30] loss=2.49 avg=2.19\n",
            "[566 | 1074.97] loss=0.33 avg=2.17\n",
            "[567 | 1076.64] loss=2.99 avg=2.18\n",
            "[568 | 1078.31] loss=0.20 avg=2.16\n",
            "[569 | 1079.98] loss=2.36 avg=2.16\n",
            "[570 | 1081.64] loss=0.26 avg=2.14\n",
            "[571 | 1083.30] loss=2.58 avg=2.14\n",
            "[572 | 1084.97] loss=1.96 avg=2.14\n",
            "[573 | 1086.64] loss=2.41 avg=2.15\n",
            "[574 | 1088.31] loss=2.34 avg=2.15\n",
            "[575 | 1089.98] loss=2.47 avg=2.15\n",
            "[576 | 1091.65] loss=2.22 avg=2.15\n",
            "[577 | 1093.32] loss=2.07 avg=2.15\n",
            "[578 | 1094.99] loss=2.23 avg=2.15\n",
            "[579 | 1096.66] loss=2.67 avg=2.16\n",
            "[580 | 1098.33] loss=2.50 avg=2.16\n",
            "[581 | 1100.00] loss=0.81 avg=2.15\n",
            "[582 | 1101.67] loss=1.84 avg=2.14\n",
            "[583 | 1103.34] loss=2.59 avg=2.15\n",
            "[584 | 1105.01] loss=2.60 avg=2.15\n",
            "[585 | 1106.68] loss=2.36 avg=2.15\n",
            "[586 | 1108.34] loss=2.29 avg=2.16\n",
            "[587 | 1110.01] loss=2.66 avg=2.16\n",
            "[588 | 1111.68] loss=1.69 avg=2.16\n",
            "[589 | 1113.35] loss=2.35 avg=2.16\n",
            "[590 | 1115.02] loss=1.88 avg=2.15\n",
            "[591 | 1116.69] loss=1.25 avg=2.15\n",
            "[592 | 1118.36] loss=2.83 avg=2.15\n",
            "[593 | 1120.03] loss=1.41 avg=2.15\n",
            "[594 | 1121.71] loss=2.81 avg=2.15\n",
            "[595 | 1123.38] loss=2.49 avg=2.16\n",
            "[596 | 1125.05] loss=2.09 avg=2.15\n",
            "[597 | 1126.73] loss=1.07 avg=2.14\n",
            "[598 | 1128.40] loss=2.26 avg=2.14\n",
            "[599 | 1130.07] loss=0.18 avg=2.13\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " behind all these concerns is that these institutions have not been able to work efficiently and effectively. While the government has implemented various reforms, including the restructuring of the oil industry, oil prices remain stubbornly high and India needs to be cautious without a stable price. There is also uncertainty caused by the Indian government's decision to allow the release of India and Pakistan’s highest-security prisoners into China in December.\n",
            "\n",
            "The most optimistic forecast for India is that, with an economic growth rate of at least 3 percent this year, the country also has the potential to achieve at least another five percent growth rate, with relatively low inflation and relatively high personal consumption expenditures, allowing India to remain among the strongest growth performers even during the tough times. Even then, though, the government is still short by about five percent of the national budget and a mere 5 percent of the budget allocated for education.\n",
            "\n",
            "A more pessimistic scenario is that the government fails to meet its target. The IMF has warned India in the past that failure to do so could have a dramatic adverse impact on its growth prospects.\n",
            "\n",
            "Yet despite economic challenges, Modi and his government are nonetheless proud of their reform effort. This is due in large part to Modi’s performance as an economic leader. In just one year, Modi has been credited with the creation of more than 7,000 billion renminbi ($100 billion) in investment, an amount equal to nearly three percent of China’s gross domestic product (GDP). This is also a fraction of China’s annual GDP, which has more than doubled in the last 20 years.\n",
            "\n",
            "In the country’s most recent National Accounts, India ranked second in the world among countries in terms of total investment and investment in the last decade, ahead even of China.\n",
            "\n",
            "But what has made Modi such a success even as it has been the case for decades is his belief in a “Bharat” vision of India as an open, democratic, secular society that is also economically and environmentally sound. This vision of the country is what has made him such a powerful and popular leader in the first place.\n",
            "\n",
            "For Modi to become Prime Minister for longer than would be necessary would be a victory not just for the BJP, but for an anti-corruption movement that has had far-reaching consequences over corruption and the use of bribes in India’s economy. By his own account and even his own personal experience, it has been harder for him to remain the champion of the poor in Indian society who he has for so long been.\n",
            "\n",
            "Still, Modi’s success has been mostly due to what was left of the opposition parties, as well as a small minority of Congress and BJP loyalists who are willing to compromise with the state governments and the ruling party to allow greater use of the government as a way to curry favor with the media.\n",
            "\n",
            "But this would be a Pyrrhic victory.\n",
            "\n",
            "At least four of the former Congress and two of the Trinamool Congress governments collapsed due to internal corruption or the need for constitutional clarity. The ruling BJP has faced similar challenges, though it may not be immune to it as it has sought to make India “proper” to its core ideals of democracy and secularism since its formation, and in that sense the ruling Bharatiya Janata Party (BJP) has had an even easier time in power.\n",
            "\n",
            "Even without some of these crises and struggles, Modi’s rise has been propelled on multiple fronts. The central bank has acted by keeping the rate of interest at ultra-low levels for the first time since its inception, and has kept the central bank cash holdings at a level that is considered inflation-free. More importantly, the BJP government has ensured that the government’s finances can be audited, while the central bank has announced monetary policies to ensure maximum monetary efficacy.\n",
            "\n",
            "The most obvious positive consequence could be the rise in government spending, as much as half of which has already happened. It is expected that spending will average 7 percent this year (as opposed to the current 8.9 percent inflation rate) and 6.5 percent in 2017 (as opposed to the current 7.7 percent). The real question with regard to how the trend will play out, though, will be with the economy as growth is clearly slowing to the slowest pace in almost a decade. This is expected to have a major impact on the inflation target but, in an effort to ease concerns, the government is trying to keep the inflation within the target range by reducing the fiscal deficit. This effort is not without its own problems as the government is also using the fiscal expansion as a leverage to lower interest rates.\n",
            "\n",
            "This trend should see a recovery in the economy eventually as the government, though, needs to be careful to ensure that even as growth picks up, inflation does not rise higher than 3 percent.\n",
            "\n",
            "The political and economic crisis has come at a cost for India. With economic growth at only 1.3 percent\n",
            "\n",
            "[600 | 1154.42] loss=2.80 avg=2.13\n",
            "[601 | 1156.09] loss=0.62 avg=2.12\n",
            "[602 | 1157.75] loss=2.61 avg=2.12\n",
            "[603 | 1159.42] loss=0.93 avg=2.11\n",
            "[604 | 1161.09] loss=2.35 avg=2.11\n",
            "[605 | 1162.75] loss=2.15 avg=2.11\n",
            "[606 | 1164.41] loss=2.61 avg=2.12\n",
            "[607 | 1166.07] loss=1.75 avg=2.11\n",
            "[608 | 1167.74] loss=2.62 avg=2.12\n",
            "[609 | 1169.41] loss=2.01 avg=2.12\n",
            "[610 | 1171.07] loss=2.17 avg=2.12\n",
            "[611 | 1172.74] loss=1.81 avg=2.12\n",
            "[612 | 1174.40] loss=2.62 avg=2.12\n",
            "[613 | 1176.08] loss=1.84 avg=2.12\n",
            "[614 | 1177.75] loss=1.48 avg=2.11\n",
            "[615 | 1179.42] loss=2.64 avg=2.12\n",
            "[616 | 1181.09] loss=2.42 avg=2.12\n",
            "[617 | 1182.76] loss=2.17 avg=2.12\n",
            "[618 | 1184.43] loss=2.13 avg=2.12\n",
            "[619 | 1186.10] loss=2.15 avg=2.12\n",
            "[620 | 1187.78] loss=2.30 avg=2.12\n",
            "[621 | 1189.46] loss=2.12 avg=2.12\n",
            "[622 | 1191.14] loss=2.23 avg=2.12\n",
            "[623 | 1192.82] loss=2.73 avg=2.13\n",
            "[624 | 1194.51] loss=2.21 avg=2.13\n",
            "[625 | 1196.18] loss=2.92 avg=2.14\n",
            "[626 | 1197.87] loss=1.90 avg=2.14\n",
            "[627 | 1199.54] loss=2.33 avg=2.14\n",
            "[628 | 1201.22] loss=1.73 avg=2.13\n",
            "[629 | 1202.91] loss=2.13 avg=2.13\n",
            "[630 | 1204.61] loss=0.09 avg=2.11\n",
            "[631 | 1206.28] loss=1.19 avg=2.10\n",
            "[632 | 1207.97] loss=2.21 avg=2.10\n",
            "[633 | 1209.66] loss=2.14 avg=2.11\n",
            "[634 | 1211.34] loss=2.41 avg=2.11\n",
            "[635 | 1213.03] loss=2.05 avg=2.11\n",
            "[636 | 1214.70] loss=2.68 avg=2.11\n",
            "[637 | 1216.37] loss=0.87 avg=2.10\n",
            "[638 | 1218.06] loss=2.03 avg=2.10\n",
            "[639 | 1219.73] loss=1.70 avg=2.10\n",
            "[640 | 1221.40] loss=2.00 avg=2.10\n",
            "[641 | 1223.08] loss=2.40 avg=2.10\n",
            "[642 | 1224.74] loss=2.22 avg=2.10\n",
            "[643 | 1226.41] loss=2.59 avg=2.10\n",
            "[644 | 1228.08] loss=1.83 avg=2.10\n",
            "[645 | 1229.75] loss=2.72 avg=2.11\n",
            "[646 | 1231.42] loss=2.02 avg=2.11\n",
            "[647 | 1233.09] loss=2.14 avg=2.11\n",
            "[648 | 1234.76] loss=2.28 avg=2.11\n",
            "[649 | 1236.43] loss=1.63 avg=2.10\n",
            "[650 | 1238.10] loss=1.22 avg=2.10\n",
            "[651 | 1239.77] loss=1.60 avg=2.09\n",
            "[652 | 1241.44] loss=1.97 avg=2.09\n",
            "[653 | 1243.11] loss=2.26 avg=2.09\n",
            "[654 | 1244.77] loss=0.72 avg=2.08\n",
            "[655 | 1246.44] loss=2.36 avg=2.08\n",
            "[656 | 1248.11] loss=1.58 avg=2.08\n",
            "[657 | 1249.78] loss=2.65 avg=2.08\n",
            "[658 | 1251.45] loss=2.27 avg=2.08\n",
            "[659 | 1253.12] loss=1.90 avg=2.08\n",
            "[660 | 1254.79] loss=1.14 avg=2.07\n",
            "[661 | 1256.46] loss=2.55 avg=2.08\n",
            "[662 | 1258.14] loss=2.83 avg=2.08\n",
            "[663 | 1259.81] loss=2.42 avg=2.09\n",
            "[664 | 1261.50] loss=0.10 avg=2.07\n",
            "[665 | 1263.17] loss=1.77 avg=2.06\n",
            "[666 | 1264.85] loss=1.66 avg=2.06\n",
            "[667 | 1266.52] loss=2.30 avg=2.06\n",
            "[668 | 1268.19] loss=1.49 avg=2.06\n",
            "[669 | 1269.86] loss=2.18 avg=2.06\n",
            "[670 | 1271.54] loss=1.88 avg=2.06\n",
            "[671 | 1273.21] loss=0.44 avg=2.04\n",
            "[672 | 1274.89] loss=2.47 avg=2.04\n",
            "[673 | 1276.56] loss=2.22 avg=2.05\n",
            "[674 | 1278.23] loss=2.46 avg=2.05\n",
            "[675 | 1279.90] loss=2.02 avg=2.05\n",
            "[676 | 1281.57] loss=2.11 avg=2.05\n",
            "[677 | 1283.25] loss=0.63 avg=2.04\n",
            "[678 | 1284.94] loss=0.99 avg=2.03\n",
            "[679 | 1286.61] loss=1.82 avg=2.02\n",
            "[680 | 1288.28] loss=2.15 avg=2.03\n",
            "[681 | 1289.97] loss=2.04 avg=2.03\n",
            "[682 | 1291.66] loss=2.08 avg=2.03\n",
            "[683 | 1293.33] loss=2.26 avg=2.03\n",
            "[684 | 1295.02] loss=2.67 avg=2.03\n",
            "[685 | 1296.71] loss=2.31 avg=2.04\n",
            "[686 | 1298.38] loss=2.26 avg=2.04\n",
            "[687 | 1300.07] loss=1.99 avg=2.04\n",
            "[688 | 1301.74] loss=2.04 avg=2.04\n",
            "[689 | 1303.43] loss=1.75 avg=2.04\n",
            "[690 | 1305.12] loss=2.03 avg=2.04\n",
            "[691 | 1306.81] loss=2.35 avg=2.04\n",
            "[692 | 1308.49] loss=2.20 avg=2.04\n",
            "[693 | 1310.17] loss=1.87 avg=2.04\n",
            "[694 | 1311.85] loss=1.74 avg=2.04\n",
            "[695 | 1313.54] loss=2.07 avg=2.04\n",
            "[696 | 1315.21] loss=3.12 avg=2.05\n",
            "[697 | 1316.90] loss=2.38 avg=2.05\n",
            "[698 | 1318.57] loss=2.45 avg=2.05\n",
            "[699 | 1320.26] loss=2.50 avg=2.06\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " with a series of articles about the political and social problems that they were grappling with, such as drug abuse and gun violence in African-American communities. And the articles were edited by the same group of writers that ran the New York Times, Vox, and The Atlantic — as well as BuzzFeed, The Huffington Post, Slate, and The Atlantic.\n",
            "\n",
            "What do these papers publish is sometimes a mix of fact and opinion. They often cite sources. And they publish articles on subjects that many Americans might consider politically controversial, such as same-sex marriage, immigration, and the environment.\n",
            "\n",
            "But at the same time, their content has a political undercurrent: the occasional photo of an armadillo, an iconic species of lizard from the Permian region of central Mexico, was shared more than 200,000 times on Facebook in the weeks leading up to the election. It echoed the posts of groups like Stop The Islamization of America and the Traditional Values Coalition, two conservative advocacy organizations, as well as the Trump campaign. Both organizations and their followers have touted themselves as nonpartisan publications.\n",
            "\n",
            "The idea that the media are partisan entities at all — to use the term they themselves used to describe themselves — is, at best, a misnomer. They write primarily about political issues, and are largely pro-Republican or are simply against Democratic policies and candidates. In reality, the media are more closely aligned with political groups on the left and to a lesser extent the right, as well as often publishing stories that are at odds with their own.\n",
            "\n",
            "These groups are using the news they present to try to shape American society in a certain way.\n",
            "\n",
            "In August, the American Enterprise Institute, a think tank in Washington, D.C., issued a report titled Influence, Insulosity, and Power: How the American Media Distort Facts To Help Political Ideologies. In one passage, the report argued that the rise of the left has helped fuel the rise of a type of online media called the so-called echo chamber: The echo chamber, the authors say, is populated by people who share the media outlet's viewpoint, but who in fact do not share it. The problem is, they add, they don't share it very effectively.\n",
            "\n",
            "The report came under fire for its assertion, based on a 2017 study, that news organizations had a 33 percent to 59 percent bias in favor of leftist causes. The study also cited a 2016 survey that found a 20 percent to 25 percent bias against Republican politicians. And it argued that news organizations are in league with political actors by distorting and amplifying evidence to advance their narrative. (The studies have not been independently verified.)\n",
            "\n",
            "A similar analysis released in early January by the Media Research Center, a conservative media monitoring organization, cited a University of Wisconsin-Madison study that found the bias of the media was twice as large as that of the public as a whole. (The study was conducted using the Social Similarity Index, a measure of how similar someone is to others in relation to how important their views are to those they are speaking about. It cannot distinguish among news outlets or other people, because the people they speak of are the organizations or other people.)\n",
            "\n",
            "\"There doesn't seem to be any evidence that the American media distort any facts they report, or even that they believe that they should,\" Brian C. Kelley, the MRC's executive vice president, told me. \"The point is that the media may very well be doing exactly what its agenda is, but there is no evidence of this.\n",
            "\n",
            "\"This is why what is being proposed is so much more than what is being implemented. The media want to have your attention while their goal is actually to distort facts to their own advantage and their own agenda.\"\n",
            "\n",
            "It is a goal that goes against the grain of some of the most passionate and engaged elements of the news media. These are groups such as the Trump campaign, the White House, and think tanks that they see as anti-Trump or even hostile. At the same time, the media are often not interested in having an ideological or political conflict of interest: They will often report stories, whether the reporting is based on fact, opinion, or anecdote, unless they face the prospect of significant conflict of interest if they do not. In this way, they function as the political wing of the very news organizations that they report on.\n",
            "\n",
            "Even mainstream publications, which have for years used their reporters to be journalists and not to have an ideological agenda, have found themselves the subjects of more invective and satire than anything reported by the news media. This has been in part a function of the ways that they report, with many of them employing newsmakers, such as CNN contributor Don Lemon, to provide context instead of simply quoting them. In 2017, a Vox editor, Ta-Nehisi Coates, was criticized when he cited Coates when making points during a news segment about the president’s immigration policy. Coates said that when he was a student at Ford\n",
            "\n",
            "[700 | 1344.87] loss=2.09 avg=2.06\n",
            "[701 | 1346.53] loss=1.87 avg=2.06\n",
            "[702 | 1348.19] loss=2.49 avg=2.06\n",
            "[703 | 1349.86] loss=2.06 avg=2.06\n",
            "[704 | 1351.51] loss=1.73 avg=2.06\n",
            "[705 | 1353.18] loss=1.59 avg=2.05\n",
            "[706 | 1354.83] loss=2.02 avg=2.05\n",
            "[707 | 1356.50] loss=2.37 avg=2.06\n",
            "[708 | 1358.16] loss=1.89 avg=2.06\n",
            "[709 | 1359.82] loss=2.24 avg=2.06\n",
            "[710 | 1361.48] loss=2.00 avg=2.06\n",
            "[711 | 1363.14] loss=2.38 avg=2.06\n",
            "[712 | 1364.81] loss=2.61 avg=2.07\n",
            "[713 | 1366.47] loss=2.07 avg=2.07\n",
            "[714 | 1368.14] loss=2.63 avg=2.07\n",
            "[715 | 1369.81] loss=2.71 avg=2.08\n",
            "[716 | 1371.48] loss=1.68 avg=2.07\n",
            "[717 | 1373.15] loss=0.68 avg=2.06\n",
            "[718 | 1374.82] loss=2.20 avg=2.06\n",
            "[719 | 1376.49] loss=2.04 avg=2.06\n",
            "[720 | 1378.16] loss=2.42 avg=2.06\n",
            "[721 | 1379.83] loss=2.65 avg=2.07\n",
            "[722 | 1381.50] loss=1.89 avg=2.07\n",
            "[723 | 1383.17] loss=2.24 avg=2.07\n",
            "[724 | 1384.86] loss=2.00 avg=2.07\n",
            "[725 | 1386.53] loss=1.94 avg=2.07\n",
            "[726 | 1388.22] loss=2.34 avg=2.07\n",
            "[727 | 1389.89] loss=2.51 avg=2.07\n",
            "[728 | 1391.58] loss=2.09 avg=2.07\n",
            "[729 | 1393.26] loss=0.29 avg=2.06\n",
            "[730 | 1394.95] loss=2.21 avg=2.06\n",
            "[731 | 1396.64] loss=1.95 avg=2.06\n",
            "[732 | 1398.32] loss=2.00 avg=2.06\n",
            "[733 | 1400.01] loss=2.09 avg=2.06\n",
            "[734 | 1401.68] loss=0.08 avg=2.04\n",
            "[735 | 1403.37] loss=0.14 avg=2.02\n",
            "[736 | 1405.05] loss=2.42 avg=2.02\n",
            "[737 | 1406.72] loss=2.30 avg=2.03\n",
            "[738 | 1408.40] loss=2.42 avg=2.03\n",
            "[739 | 1410.07] loss=1.43 avg=2.02\n",
            "[740 | 1411.74] loss=2.61 avg=2.03\n",
            "[741 | 1413.42] loss=2.03 avg=2.03\n",
            "[742 | 1415.09] loss=2.03 avg=2.03\n",
            "[743 | 1416.75] loss=2.21 avg=2.03\n",
            "[744 | 1418.43] loss=1.91 avg=2.03\n",
            "[745 | 1420.10] loss=2.47 avg=2.03\n",
            "[746 | 1421.78] loss=2.40 avg=2.04\n",
            "[747 | 1423.45] loss=2.42 avg=2.04\n",
            "[748 | 1425.13] loss=2.12 avg=2.04\n",
            "[749 | 1426.79] loss=0.43 avg=2.03\n",
            "[750 | 1428.46] loss=2.12 avg=2.03\n",
            "[751 | 1430.15] loss=0.11 avg=2.01\n",
            "[752 | 1431.82] loss=2.58 avg=2.01\n",
            "[753 | 1433.49] loss=2.32 avg=2.02\n",
            "[754 | 1435.16] loss=2.19 avg=2.02\n",
            "[755 | 1436.83] loss=2.25 avg=2.02\n",
            "[756 | 1438.50] loss=1.89 avg=2.02\n",
            "[757 | 1440.17] loss=1.65 avg=2.02\n",
            "[758 | 1441.84] loss=2.19 avg=2.02\n",
            "[759 | 1443.51] loss=2.19 avg=2.02\n",
            "[760 | 1445.18] loss=2.42 avg=2.02\n",
            "[761 | 1446.85] loss=1.89 avg=2.02\n",
            "[762 | 1448.52] loss=1.76 avg=2.02\n",
            "[763 | 1450.19] loss=2.41 avg=2.02\n",
            "[764 | 1451.86] loss=0.10 avg=2.00\n",
            "[765 | 1453.53] loss=2.09 avg=2.00\n",
            "[766 | 1455.20] loss=1.82 avg=2.00\n",
            "[767 | 1456.87] loss=2.02 avg=2.00\n",
            "[768 | 1458.54] loss=0.09 avg=1.98\n",
            "[769 | 1460.21] loss=2.24 avg=1.99\n",
            "[770 | 1461.88] loss=0.07 avg=1.97\n",
            "[771 | 1463.55] loss=2.11 avg=1.97\n",
            "[772 | 1465.23] loss=2.46 avg=1.97\n",
            "[773 | 1466.90] loss=2.29 avg=1.98\n",
            "[774 | 1468.57] loss=2.11 avg=1.98\n",
            "[775 | 1470.24] loss=1.88 avg=1.98\n",
            "[776 | 1471.91] loss=1.60 avg=1.97\n",
            "[777 | 1473.59] loss=0.12 avg=1.95\n",
            "[778 | 1475.26] loss=3.01 avg=1.97\n",
            "[779 | 1476.94] loss=1.72 avg=1.96\n",
            "[780 | 1478.61] loss=2.46 avg=1.97\n",
            "[781 | 1480.29] loss=2.43 avg=1.97\n",
            "[782 | 1481.97] loss=2.22 avg=1.98\n",
            "[783 | 1483.64] loss=2.05 avg=1.98\n",
            "[784 | 1485.30] loss=1.90 avg=1.98\n",
            "[785 | 1486.98] loss=2.49 avg=1.98\n",
            "[786 | 1488.65] loss=0.10 avg=1.96\n",
            "[787 | 1490.32] loss=2.02 avg=1.96\n",
            "[788 | 1491.99] loss=0.07 avg=1.94\n",
            "[789 | 1493.66] loss=1.99 avg=1.94\n",
            "[790 | 1495.33] loss=2.18 avg=1.95\n",
            "[791 | 1497.00] loss=1.99 avg=1.95\n",
            "[792 | 1498.67] loss=2.05 avg=1.95\n",
            "[793 | 1500.34] loss=1.82 avg=1.95\n",
            "[794 | 1502.01] loss=2.63 avg=1.95\n",
            "[795 | 1503.68] loss=2.15 avg=1.95\n",
            "[796 | 1505.35] loss=2.12 avg=1.96\n",
            "[797 | 1507.02] loss=2.97 avg=1.97\n",
            "[798 | 1508.69] loss=1.61 avg=1.96\n",
            "[799 | 1510.36] loss=1.30 avg=1.96\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " Lanka (Tajikistan) and China, the United States’s closest ally and trade partner, to investigate whether its citizens were subject to the Trump administration’s travel ban.\n",
            "\n",
            "It’s unclear how the Trump administration plans to respond to any such investigation—if any—taking place in the United States. The administration released a brief statement on the matter, in which it said it “will review all of the facts” surrounding the ban.\n",
            "\n",
            "The Trump administration has also taken steps to appease its most conservative members in Congress. Attorney General Jeff Sessions said this week that he had instructed his deputy to “immediately review ‘any and all reviews” that have been made of the Executive Order.\n",
            "\n",
            "That review could result in the Justice Department changing how it prosecutes terrorism-related cases and other matters, according to two people familiar with the matter, but that might not be enough to quell the public’s fury. “It’s not the end of the world,” says Michael Kugelman, a professor of international law and public affairs at Georgetown University’s law school. “If I have to start having to rethink everything, I could.”\n",
            "\n",
            "Not every ban has sparked a national outpouring of protest. In February, after a man with a knife attacked passengers aboard a passenger train in Würzburg, Germany, a number of German Muslim organizations, led by the Confederation of German Islamic Societies, sent an open letter to then-President Donald Trump, demanding that the administration “immediately cease and desist from this ban”.\n",
            "\n",
            "The ban sparked a backlash too. After the attack, passengers on a train in the same area were urged to stay away from the area and train stations.\n",
            "\n",
            "The Trump administration’s response to the Würzburg attack has drawn sharp criticism in Germany and elsewhere at the expense of national security. Here are some of the ways in which the German government is reacting:\n",
            "\n",
            "The order “broke protocol” and posed a security threat. The ban was announced in a press release, not by the president himself. A spokeswoman for Merkel on Friday called it a “despicable action” and said it was “completely unacceptable to treat all refugees and migrants badly by banning them from entering the United States.”\n",
            "\n",
            "The order broke protocol, too. When it came out, Germany was already grappling with a wave of refugee tensions following the refugee crisis that began in 2015. During the Trump administration’s first month in office, an influx of people fleeing the Middle East and Africa, mainly from Iraq and Syria, hit Germany hard, with more than 1 million refugees arriving in 2016. As well as causing some friction, the travel ban didn’t take into consideration the broader refugee situation in Europe. As German Chancellor Angela Merkel said this week, the majority of refugees who have arrived in Germany so far this year have been from a single country: Syria.\n",
            "\n",
            "Merkel has since ordered a review of the refugee program, which could lead the country to be asked to take more refugees. The review isn’t scheduled to be completed until Jan. 31, two days after the inauguration of Donald Trump as president.\n",
            "\n",
            "Trump said that he would be happy to review the ban “if I get to do a full review,” the first time the president has been asked about it. Merkel has also said that it isn’t her place to debate whether it is right to register refugees or not.\n",
            "\n",
            "A full review could take months, but so far, no one is arguing that way. This week alone, as Trump’s comments on the Würzburg incident have sparked controversy, Twitter and Facebook have removed some of the content that the German government is citing as violating its policy. At least one other social-media site, Instapundit, has also removed some of the content that the government is citing as violating its policy.\n",
            "\n",
            "Still, the backlash is nothing new—even though the Würzburg attack was condemned in the United States, and as such was not covered by German media, people who live in the country, and many journalists in Germany have voiced concerns that the ban will cause further violence by immigrants.\n",
            "\n",
            "Ahead of the Würzburg attack, the mood in Germany was one of renewed vigor on the part of the country’s Muslim community, which had been on alert for any signs of danger after a spate of far-right extremist attacks in the country. But since Trump’s remarks Sunday morning, the country has descended into the kind of anger that has been seen in Europe in the aftermath of terrorist attacks.\n",
            "\n",
            "\n",
            "\n",
            "ISTANBUL—When Huseyin Celik visited his friend and colleague Fethullah Gulen to thank him for helping him become President of Pennsylvania, the veteran politician, who lives in a secluded enclave of\n",
            "\n",
            "[800 | 1535.28] loss=1.98 avg=1.96\n",
            "[801 | 1536.94] loss=1.63 avg=1.95\n",
            "[802 | 1538.62] loss=0.78 avg=1.94\n",
            "[803 | 1540.28] loss=1.87 avg=1.94\n",
            "[804 | 1541.94] loss=0.11 avg=1.92\n",
            "[805 | 1543.61] loss=0.05 avg=1.90\n",
            "[806 | 1545.28] loss=2.32 avg=1.91\n",
            "[807 | 1546.94] loss=2.06 avg=1.91\n",
            "[808 | 1548.61] loss=2.21 avg=1.91\n",
            "[809 | 1550.28] loss=2.89 avg=1.92\n",
            "[810 | 1551.95] loss=2.17 avg=1.92\n",
            "[811 | 1553.62] loss=1.30 avg=1.92\n",
            "[812 | 1555.28] loss=2.51 avg=1.92\n",
            "[813 | 1556.94] loss=2.28 avg=1.93\n",
            "[814 | 1558.61] loss=2.40 avg=1.93\n",
            "[815 | 1560.28] loss=2.02 avg=1.93\n",
            "[816 | 1561.95] loss=2.14 avg=1.94\n",
            "[817 | 1563.62] loss=1.54 avg=1.93\n",
            "[818 | 1565.29] loss=2.07 avg=1.93\n",
            "[819 | 1566.96] loss=2.36 avg=1.94\n",
            "[820 | 1568.63] loss=2.49 avg=1.94\n",
            "[821 | 1570.30] loss=1.96 avg=1.94\n",
            "[822 | 1571.97] loss=1.77 avg=1.94\n",
            "[823 | 1573.64] loss=2.27 avg=1.94\n",
            "[824 | 1575.32] loss=2.51 avg=1.95\n",
            "[825 | 1576.98] loss=2.58 avg=1.96\n",
            "[826 | 1578.65] loss=2.71 avg=1.96\n",
            "[827 | 1580.32] loss=2.40 avg=1.97\n",
            "[828 | 1582.00] loss=1.87 avg=1.97\n",
            "[829 | 1583.67] loss=2.08 avg=1.97\n",
            "[830 | 1585.33] loss=0.77 avg=1.96\n",
            "[831 | 1587.01] loss=1.94 avg=1.96\n",
            "[832 | 1588.68] loss=1.93 avg=1.96\n",
            "[833 | 1590.35] loss=2.00 avg=1.96\n",
            "[834 | 1592.03] loss=2.20 avg=1.96\n",
            "[835 | 1593.72] loss=1.82 avg=1.96\n",
            "[836 | 1595.39] loss=2.71 avg=1.97\n",
            "[837 | 1597.06] loss=1.73 avg=1.96\n",
            "[838 | 1598.73] loss=2.47 avg=1.97\n",
            "[839 | 1600.41] loss=1.83 avg=1.97\n",
            "[840 | 1602.08] loss=1.95 avg=1.97\n",
            "[841 | 1603.77] loss=0.41 avg=1.95\n",
            "[842 | 1605.45] loss=1.78 avg=1.95\n",
            "[843 | 1607.14] loss=2.28 avg=1.95\n",
            "[844 | 1608.80] loss=1.95 avg=1.95\n",
            "[845 | 1610.48] loss=2.24 avg=1.96\n",
            "[846 | 1612.17] loss=1.30 avg=1.95\n",
            "[847 | 1613.85] loss=1.62 avg=1.95\n",
            "[848 | 1615.54] loss=1.98 avg=1.95\n",
            "[849 | 1617.21] loss=2.52 avg=1.95\n",
            "[850 | 1618.90] loss=2.23 avg=1.95\n",
            "[851 | 1620.58] loss=0.05 avg=1.94\n",
            "[852 | 1622.27] loss=2.15 avg=1.94\n",
            "[853 | 1623.96] loss=2.72 avg=1.95\n",
            "[854 | 1625.65] loss=2.11 avg=1.95\n",
            "[855 | 1627.32] loss=2.60 avg=1.95\n",
            "[856 | 1629.01] loss=2.45 avg=1.96\n",
            "[857 | 1630.68] loss=2.05 avg=1.96\n",
            "[858 | 1632.36] loss=2.08 avg=1.96\n",
            "[859 | 1634.04] loss=1.92 avg=1.96\n",
            "[860 | 1635.72] loss=1.73 avg=1.96\n",
            "[861 | 1637.39] loss=1.92 avg=1.96\n",
            "[862 | 1639.06] loss=2.06 avg=1.96\n",
            "[863 | 1640.74] loss=1.63 avg=1.96\n",
            "[864 | 1642.42] loss=2.81 avg=1.96\n",
            "[865 | 1644.09] loss=2.36 avg=1.97\n",
            "[866 | 1645.76] loss=0.09 avg=1.95\n",
            "[867 | 1647.45] loss=2.22 avg=1.95\n",
            "[868 | 1649.11] loss=2.54 avg=1.96\n",
            "[869 | 1650.79] loss=2.66 avg=1.96\n",
            "[870 | 1652.47] loss=0.49 avg=1.95\n",
            "[871 | 1654.15] loss=1.86 avg=1.95\n",
            "[872 | 1655.84] loss=2.39 avg=1.95\n",
            "[873 | 1657.51] loss=1.63 avg=1.95\n",
            "[874 | 1659.20] loss=0.56 avg=1.94\n",
            "[875 | 1660.87] loss=1.47 avg=1.93\n",
            "[876 | 1662.54] loss=2.00 avg=1.93\n",
            "[877 | 1664.21] loss=2.30 avg=1.94\n",
            "[878 | 1665.88] loss=2.34 avg=1.94\n",
            "[879 | 1667.55] loss=2.19 avg=1.94\n",
            "[880 | 1669.23] loss=2.33 avg=1.95\n",
            "[881 | 1670.90] loss=1.61 avg=1.94\n",
            "[882 | 1672.57] loss=0.10 avg=1.92\n",
            "[883 | 1674.25] loss=0.08 avg=1.91\n",
            "[884 | 1675.92] loss=2.41 avg=1.91\n",
            "[885 | 1677.59] loss=2.16 avg=1.91\n",
            "[886 | 1679.26] loss=1.99 avg=1.91\n",
            "[887 | 1680.94] loss=0.08 avg=1.90\n",
            "[888 | 1682.61] loss=2.02 avg=1.90\n",
            "[889 | 1684.28] loss=2.03 avg=1.90\n",
            "[890 | 1685.96] loss=2.20 avg=1.90\n",
            "[891 | 1687.63] loss=0.05 avg=1.88\n",
            "[892 | 1689.30] loss=2.38 avg=1.89\n",
            "[893 | 1690.97] loss=0.87 avg=1.88\n",
            "[894 | 1692.64] loss=1.74 avg=1.88\n",
            "[895 | 1694.31] loss=2.53 avg=1.88\n",
            "[896 | 1695.98] loss=1.89 avg=1.88\n",
            "[897 | 1697.65] loss=0.12 avg=1.87\n",
            "[898 | 1699.32] loss=0.56 avg=1.85\n",
            "[899 | 1700.99] loss=1.24 avg=1.85\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " watch the game.\n",
            "\n",
            "The biggest impact for teams that want to compete in this market will be on Wednesday night. The Warriors, who've won nine in a row, will host the Celtics, who beat them to two of three meetings this season. Afterward, Golden State will host the Cavaliers in Cleveland, who are without Kyrie Irving (rest), Tristan Thompson (thumb fracture) and J.R. Smith (left ankle sprain), and are without Richard Jefferson (ankle).\n",
            "\n",
            "The Warriors lost 116-94 to Boston in Game 1 of the 2017 NBA Finals. Since then, they've worn down the Celtics in three straight meetings, but Irving (hamstring) missed most of Golden State's win in Cleveland. Smith (hamstring) was questionable again, and could be out some time as well; Iguodala has also been questionable recently, though he seems to be making a comeback. Coach Steve Kerr said that Stephen Curry would likely start Thursday's game, but it's anyone's guess what role Curry would play in a four-guard lineup.\n",
            "\n",
            "The Warriors (20-4 this season) are tied for the best record in the Western Conference and sixth in the NBA in points per game. They're also the only team in the West left in the Eastern Conference.\n",
            "\n",
            "In the NBA, you can argue all you want about which team is better—the Golden State Warriors or LeBron James and the Cleveland Cavaliers? In this case, the argument is more interesting.\n",
            "\n",
            "The Warriors have won 10 straight games overall, one in which they fell to the San Antonio Spurs in six games in the first round of the playoffs this season. They're 9-1 when LeBron is playing, tied with the 2002-03 Miami Heat.\n",
            "\n",
            "How do the Cleveland Cavaliers compare? In a word: win. They've won just three of the past 10 meetings, each time beating the Warriors by an average of 10.3 points. Golden State last won 10 games in Cleveland in late February, an 88-88 victory that snapped a 17-game winning streak.\n",
            "\n",
            "Why are they winning? James has emerged as the team MVP following a remarkable rookie season that saw him win MVP at the end of the 2016-17 season while helping the Cleveland Cavaliers to their first playoff appearance in 19 years. The Warriors, meanwhile, have added Kevin Durant as well as some key veterans for a team that reportedly wants to trade Stephen Curry.\n",
            "\n",
            "Here are three takeaways:\n",
            "\n",
            "1. Golden State Doesn’t Need The Bigger Prize. The Warriors won the NBA title last season and were one game ahead of the Cleveland Cavaliers in the Western Conference standings after a grueling five-game series against the Chicago Bulls and eventual champions Golden State.\n",
            "\n",
            "With such a dominant roster and James leading Golden State to the title, how could everyone in the league possibly disagree with the Warriors?\n",
            "\n",
            "Then again, how could anyone disagree with the Cavs heading into the 2016 offseason? They added LeBron James, Kevin Love, Iman Shumpert, Tristan Thompson, and more. They re-signed Kevin Love, then acquired Klay Thompson, Iman Shumpert, and J.R. Smith in the LeBron James blockbuster. Then they dealt Love, Thompson, Shumpert, and Smith to the New York Knicks in the Lance Stephenson package. They traded Smith to the Lakers in the Pau Gasol deal. And now they're headed into the 2017 offseason with a champion, a new stadium, and a new television deal.\n",
            "\n",
            "Sure, the Warriors don’t have James, but those don’t automatically give them a better team than Cleveland’s. The Cavs have LeBron James and are headed to a title. Golden State has James, Kevin Durant, Stephen Curry, Klay Thompson, Draymond Green, and Draymond Green, and are headed to a title.\n",
            "\n",
            "Plus, the Warriors were the favorites last season. In fact, they were the best team in the Western Conference before James left.\n",
            "\n",
            "In three games heading into Game 6 of the Western Conference finals, the Warriors blew leads of 20, 19, 21 points. In three games heading into Game 7 of the Western Conference finals, the Warriors blew leads of 17, 18, 17 points.\n",
            "\n",
            "The season's never over, folks.\n",
            "\n",
            "2. The Cavaliers Are Still Good, But Their Streak Of 5 Games In A Row Is Over. They won 5 games in a row from 5 to 6 June 14–17, but the Dallas Mavericks won six games in a row from 6 to 7 June 22–June 28. The last time the Cavaliers won six straight games in a row was on Tuesday, June 14, 2006. They lost 4 to 2 to the Celtics in the 2008 NBA Finals. That team was LeBron James and Kyrie Irving. That game started a streak of 12 straight wins for the Cavaliers.\n",
            "\n",
            "3. LeBron’s Career High Is 21. He played in all 82 games for the Cleveland\n",
            "\n",
            "[900 | 1725.34] loss=1.72 avg=1.84\n",
            "[901 | 1727.01] loss=2.19 avg=1.85\n",
            "[902 | 1728.67] loss=1.99 avg=1.85\n",
            "[903 | 1730.33] loss=1.94 avg=1.85\n",
            "[904 | 1732.00] loss=1.88 avg=1.85\n",
            "[905 | 1733.65] loss=2.58 avg=1.86\n",
            "[906 | 1735.31] loss=2.18 avg=1.86\n",
            "[907 | 1736.96] loss=2.27 avg=1.87\n",
            "[908 | 1738.61] loss=2.87 avg=1.88\n",
            "[909 | 1740.26] loss=1.73 avg=1.87\n",
            "[910 | 1741.92] loss=2.05 avg=1.88\n",
            "[911 | 1743.57] loss=2.22 avg=1.88\n",
            "[912 | 1745.22] loss=1.72 avg=1.88\n",
            "[913 | 1746.87] loss=1.74 avg=1.88\n",
            "[914 | 1748.53] loss=1.53 avg=1.87\n",
            "[915 | 1750.18] loss=1.49 avg=1.87\n",
            "[916 | 1751.84] loss=2.12 avg=1.87\n",
            "[917 | 1753.49] loss=2.02 avg=1.87\n",
            "[918 | 1755.15] loss=2.05 avg=1.87\n",
            "[919 | 1756.81] loss=2.03 avg=1.88\n",
            "[920 | 1758.47] loss=1.55 avg=1.87\n",
            "[921 | 1760.14] loss=2.29 avg=1.88\n",
            "[922 | 1761.81] loss=1.90 avg=1.88\n",
            "[923 | 1763.49] loss=0.08 avg=1.86\n",
            "[924 | 1765.16] loss=1.96 avg=1.86\n",
            "[925 | 1766.83] loss=2.26 avg=1.86\n",
            "[926 | 1768.50] loss=1.69 avg=1.86\n",
            "[927 | 1770.18] loss=2.44 avg=1.87\n",
            "[928 | 1771.85] loss=0.07 avg=1.85\n",
            "[929 | 1773.52] loss=3.13 avg=1.86\n",
            "[930 | 1775.19] loss=1.33 avg=1.86\n",
            "[931 | 1776.87] loss=2.92 avg=1.87\n",
            "[932 | 1778.56] loss=2.39 avg=1.87\n",
            "[933 | 1780.24] loss=1.40 avg=1.87\n",
            "[934 | 1781.93] loss=0.06 avg=1.85\n",
            "[935 | 1783.62] loss=2.34 avg=1.86\n",
            "[936 | 1785.31] loss=2.03 avg=1.86\n",
            "[937 | 1787.00] loss=1.63 avg=1.86\n",
            "[938 | 1788.68] loss=1.87 avg=1.86\n",
            "[939 | 1790.37] loss=1.59 avg=1.85\n",
            "[940 | 1792.06] loss=2.34 avg=1.86\n",
            "[941 | 1793.74] loss=1.46 avg=1.85\n",
            "[942 | 1795.41] loss=0.06 avg=1.84\n",
            "[943 | 1797.08] loss=2.15 avg=1.84\n",
            "[944 | 1798.75] loss=0.08 avg=1.82\n",
            "[945 | 1800.43] loss=1.31 avg=1.82\n",
            "[946 | 1802.10] loss=2.29 avg=1.82\n",
            "[947 | 1803.77] loss=2.20 avg=1.82\n",
            "[948 | 1805.44] loss=0.83 avg=1.81\n",
            "[949 | 1807.12] loss=1.93 avg=1.82\n",
            "[950 | 1808.80] loss=0.25 avg=1.80\n",
            "[951 | 1810.47] loss=2.39 avg=1.81\n",
            "[952 | 1812.14] loss=2.11 avg=1.81\n",
            "[953 | 1813.81] loss=0.03 avg=1.79\n",
            "[954 | 1815.49] loss=2.00 avg=1.79\n",
            "[955 | 1817.16] loss=1.77 avg=1.79\n",
            "[956 | 1818.83] loss=1.05 avg=1.79\n",
            "[957 | 1820.50] loss=2.20 avg=1.79\n",
            "[958 | 1822.18] loss=2.17 avg=1.79\n",
            "[959 | 1823.85] loss=1.69 avg=1.79\n",
            "[960 | 1825.52] loss=1.99 avg=1.79\n",
            "[961 | 1827.19] loss=2.20 avg=1.80\n",
            "[962 | 1828.86] loss=0.12 avg=1.78\n",
            "[963 | 1830.53] loss=1.97 avg=1.78\n",
            "[964 | 1832.20] loss=1.82 avg=1.78\n",
            "[965 | 1833.86] loss=0.09 avg=1.77\n",
            "[966 | 1835.53] loss=0.93 avg=1.76\n",
            "[967 | 1837.20] loss=2.38 avg=1.77\n",
            "[968 | 1838.87] loss=1.21 avg=1.76\n",
            "[969 | 1840.54] loss=2.40 avg=1.77\n",
            "[970 | 1842.21] loss=2.12 avg=1.77\n",
            "[971 | 1843.88] loss=2.45 avg=1.78\n",
            "[972 | 1845.55] loss=2.00 avg=1.78\n",
            "[973 | 1847.22] loss=1.87 avg=1.78\n",
            "[974 | 1848.90] loss=1.95 avg=1.78\n",
            "[975 | 1850.56] loss=1.64 avg=1.78\n",
            "[976 | 1852.23] loss=1.63 avg=1.78\n",
            "[977 | 1853.90] loss=2.55 avg=1.79\n",
            "[978 | 1855.58] loss=2.31 avg=1.79\n",
            "[979 | 1857.25] loss=1.65 avg=1.79\n",
            "[980 | 1858.92] loss=0.07 avg=1.77\n",
            "[981 | 1860.59] loss=0.12 avg=1.76\n",
            "[982 | 1862.26] loss=1.84 avg=1.76\n",
            "[983 | 1863.94] loss=2.03 avg=1.76\n",
            "[984 | 1865.63] loss=2.04 avg=1.76\n",
            "[985 | 1867.30] loss=1.12 avg=1.76\n",
            "[986 | 1868.97] loss=1.77 avg=1.76\n",
            "[987 | 1870.65] loss=1.59 avg=1.75\n",
            "[988 | 1872.32] loss=0.66 avg=1.74\n",
            "[989 | 1874.01] loss=0.63 avg=1.73\n",
            "[990 | 1875.68] loss=2.24 avg=1.74\n",
            "[991 | 1877.36] loss=2.03 avg=1.74\n",
            "[992 | 1879.03] loss=2.22 avg=1.75\n",
            "[993 | 1880.71] loss=2.16 avg=1.75\n",
            "[994 | 1882.38] loss=2.42 avg=1.76\n",
            "[995 | 1884.05] loss=0.04 avg=1.74\n",
            "[996 | 1885.72] loss=1.65 avg=1.74\n",
            "[997 | 1887.39] loss=1.73 avg=1.74\n",
            "[998 | 1889.06] loss=2.21 avg=1.74\n",
            "[999 | 1890.74] loss=0.07 avg=1.73\n",
            "Ran 1010 steps.\n",
            "Saving checkpoint/atlantic_international_345/model-1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uly598Th8UfE",
        "colab_type": "code",
        "outputId": "32fa32d6-a6a4-4cfb-ed6c-64426034fd15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "## health essays training - with 345\n",
        "!PYTHONPATH=src ./train.py --dataset /content/all_health.txt --run_name 'atlantic_health_345' --model_name '345M'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0627 20:03:50.572252 140538845390720 deprecation_wrapper.py:119] From /content/gpt-2/src/model.py:147: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
            "\n",
            "W0627 20:03:50.580771 140538845390720 deprecation_wrapper.py:119] From /content/gpt-2/src/memory_saving_gradients.py:13: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n",
            "W0627 20:03:50.676445 140538845390720 deprecation_wrapper.py:119] From ./train.py:88: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "W0627 20:03:50.676766 140538845390720 deprecation_wrapper.py:119] From ./train.py:91: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "2019-06-27 20:03:50.683209: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-06-27 20:03:50.683482: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2dc9100 executing computations on platform Host. Devices:\n",
            "2019-06-27 20:03:50.683528: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2019-06-27 20:03:50.686133: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\n",
            "2019-06-27 20:03:50.841043: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-27 20:03:50.841606: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2dc8840 executing computations on platform CUDA. Devices:\n",
            "2019-06-27 20:03:50.841638: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2019-06-27 20:03:50.841931: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-27 20:03:50.842279: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-06-27 20:03:50.842726: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-06-27 20:03:50.844105: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-06-27 20:03:50.845424: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-06-27 20:03:50.845865: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-06-27 20:03:50.847299: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-06-27 20:03:50.848674: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-06-27 20:03:50.851834: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-06-27 20:03:50.851976: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-27 20:03:50.852410: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-27 20:03:50.852748: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
            "2019-06-27 20:03:50.852811: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-06-27 20:03:50.853791: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-06-27 20:03:50.853819: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n",
            "2019-06-27 20:03:50.853832: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n",
            "2019-06-27 20:03:50.854151: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-27 20:03:50.854567: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-27 20:03:50.854936: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14202 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "W0627 20:03:50.855808 140538845390720 deprecation_wrapper.py:119] From ./train.py:92: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0627 20:04:02.052771 140538845390720 deprecation.py:323] From /content/gpt-2/src/sample.py:65: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "W0627 20:04:02.068847 140538845390720 deprecation.py:323] From /content/gpt-2/src/sample.py:16: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0627 20:04:02.070712 140538845390720 deprecation.py:323] From /content/gpt-2/src/sample.py:70: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.random.categorical` instead.\n",
            "W0627 20:04:02.082603 140538845390720 deprecation_wrapper.py:119] From ./train.py:121: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "W0627 20:04:17.715703 140538845390720 deprecation_wrapper.py:119] From ./train.py:144: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n",
            "W0627 20:04:17.718882 140538845390720 deprecation_wrapper.py:119] From ./train.py:147: The name tf.summary.merge is deprecated. Please use tf.compat.v1.summary.merge instead.\n",
            "\n",
            "W0627 20:04:17.719756 140538845390720 deprecation_wrapper.py:119] From ./train.py:149: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
            "\n",
            "W0627 20:04:17.720689 140538845390720 deprecation_wrapper.py:119] From ./train.py:152: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "Loading checkpoint models/345M/model.ckpt\n",
            "W0627 20:04:30.717108 140538845390720 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "Loading dataset...\n",
            "100% 1/1 [00:03<00:00,  3.15s/it]\n",
            "dataset has 409394 tokens\n",
            "Training...\n",
            "2019-06-27 20:04:46.719927: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n",
            "2019-06-27 20:04:47.417396: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "[1 | 12.99] loss=2.71 avg=2.71\n",
            "[2 | 14.58] loss=3.15 avg=2.93\n",
            "[3 | 16.21] loss=2.98 avg=2.95\n",
            "[4 | 17.82] loss=3.39 avg=3.06\n",
            "[5 | 19.44] loss=3.01 avg=3.05\n",
            "[6 | 21.07] loss=3.01 avg=3.04\n",
            "[7 | 22.70] loss=3.22 avg=3.07\n",
            "[8 | 24.34] loss=2.89 avg=3.05\n",
            "[9 | 25.98] loss=2.67 avg=3.00\n",
            "[10 | 27.64] loss=2.77 avg=2.98\n",
            "[11 | 29.29] loss=2.87 avg=2.97\n",
            "[12 | 30.95] loss=2.68 avg=2.94\n",
            "[13 | 32.63] loss=3.05 avg=2.95\n",
            "[14 | 34.30] loss=2.94 avg=2.95\n",
            "[15 | 35.97] loss=2.45 avg=2.91\n",
            "[16 | 37.69] loss=3.17 avg=2.93\n",
            "[17 | 39.40] loss=2.78 avg=2.92\n",
            "[18 | 41.11] loss=3.01 avg=2.93\n",
            "[19 | 42.83] loss=2.99 avg=2.93\n",
            "[20 | 44.54] loss=2.47 avg=2.91\n",
            "[21 | 46.28] loss=2.61 avg=2.89\n",
            "[22 | 48.01] loss=2.85 avg=2.89\n",
            "[23 | 49.74] loss=2.84 avg=2.89\n",
            "[24 | 51.47] loss=2.79 avg=2.88\n",
            "[25 | 53.20] loss=2.99 avg=2.89\n",
            "[26 | 54.93] loss=2.63 avg=2.87\n",
            "[27 | 56.66] loss=2.80 avg=2.87\n",
            "[28 | 58.38] loss=2.91 avg=2.87\n",
            "[29 | 60.09] loss=2.94 avg=2.88\n",
            "[30 | 61.79] loss=2.91 avg=2.88\n",
            "[31 | 63.49] loss=2.77 avg=2.87\n",
            "[32 | 65.18] loss=2.59 avg=2.86\n",
            "[33 | 66.88] loss=2.62 avg=2.85\n",
            "[34 | 68.55] loss=3.03 avg=2.86\n",
            "[35 | 70.24] loss=2.50 avg=2.85\n",
            "[36 | 71.91] loss=2.86 avg=2.85\n",
            "[37 | 73.57] loss=2.61 avg=2.84\n",
            "[38 | 75.24] loss=2.64 avg=2.83\n",
            "[39 | 76.91] loss=3.01 avg=2.84\n",
            "[40 | 78.57] loss=2.98 avg=2.84\n",
            "[41 | 80.24] loss=2.96 avg=2.85\n",
            "[42 | 81.89] loss=2.94 avg=2.85\n",
            "[43 | 83.54] loss=2.73 avg=2.85\n",
            "[44 | 85.20] loss=2.78 avg=2.85\n",
            "[45 | 86.85] loss=2.55 avg=2.84\n",
            "[46 | 88.50] loss=2.99 avg=2.84\n",
            "[47 | 90.15] loss=3.06 avg=2.85\n",
            "[48 | 91.80] loss=3.10 avg=2.85\n",
            "[49 | 93.45] loss=2.84 avg=2.85\n",
            "[50 | 95.10] loss=2.98 avg=2.86\n",
            "[51 | 96.75] loss=2.55 avg=2.85\n",
            "[52 | 98.40] loss=2.85 avg=2.85\n",
            "[53 | 100.04] loss=2.56 avg=2.84\n",
            "[54 | 101.68] loss=2.55 avg=2.83\n",
            "[55 | 103.33] loss=3.33 avg=2.85\n",
            "[56 | 104.97] loss=2.59 avg=2.84\n",
            "[57 | 106.62] loss=2.48 avg=2.83\n",
            "[58 | 108.27] loss=2.55 avg=2.83\n",
            "[59 | 109.92] loss=2.91 avg=2.83\n",
            "[60 | 111.58] loss=3.00 avg=2.83\n",
            "[61 | 113.23] loss=2.87 avg=2.83\n",
            "[62 | 114.89] loss=3.15 avg=2.84\n",
            "[63 | 116.55] loss=2.83 avg=2.84\n",
            "[64 | 118.23] loss=3.33 avg=2.85\n",
            "[65 | 119.89] loss=3.03 avg=2.85\n",
            "[66 | 121.56] loss=2.94 avg=2.86\n",
            "[67 | 123.23] loss=2.75 avg=2.85\n",
            "[68 | 124.89] loss=3.14 avg=2.86\n",
            "[69 | 126.56] loss=3.09 avg=2.86\n",
            "[70 | 128.23] loss=2.89 avg=2.86\n",
            "[71 | 129.91] loss=2.76 avg=2.86\n",
            "[72 | 131.58] loss=2.53 avg=2.86\n",
            "[73 | 133.25] loss=2.60 avg=2.85\n",
            "[74 | 134.93] loss=2.84 avg=2.85\n",
            "[75 | 136.61] loss=2.88 avg=2.85\n",
            "[76 | 138.29] loss=2.62 avg=2.85\n",
            "[77 | 139.97] loss=2.55 avg=2.84\n",
            "[78 | 141.64] loss=2.86 avg=2.84\n",
            "[79 | 143.32] loss=2.84 avg=2.84\n",
            "[80 | 145.00] loss=2.56 avg=2.84\n",
            "[81 | 146.69] loss=2.79 avg=2.84\n",
            "[82 | 148.36] loss=2.81 avg=2.83\n",
            "[83 | 150.03] loss=2.85 avg=2.84\n",
            "[84 | 151.69] loss=2.94 avg=2.84\n",
            "[85 | 153.36] loss=3.00 avg=2.84\n",
            "[86 | 155.03] loss=2.50 avg=2.83\n",
            "[87 | 156.70] loss=2.76 avg=2.83\n",
            "[88 | 158.37] loss=2.62 avg=2.83\n",
            "[89 | 160.04] loss=2.71 avg=2.83\n",
            "[90 | 161.70] loss=2.78 avg=2.83\n",
            "[91 | 163.38] loss=2.83 avg=2.83\n",
            "[92 | 165.05] loss=2.64 avg=2.82\n",
            "[93 | 166.71] loss=2.85 avg=2.82\n",
            "[94 | 168.37] loss=2.74 avg=2.82\n",
            "[95 | 170.05] loss=3.13 avg=2.83\n",
            "[96 | 171.71] loss=3.12 avg=2.83\n",
            "[97 | 173.37] loss=2.66 avg=2.83\n",
            "[98 | 175.03] loss=2.83 avg=2.83\n",
            "[99 | 176.70] loss=2.67 avg=2.83\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " much the exact opposite to what was portrayed in the movie, so it stands to reason that they're not nearly the greatest of roommates.\n",
            "\n",
            "\n",
            "The fact is, there's never any doubt about who our best friends are. From the time we're kids we're constantly bombarded by references. Whether it's a school cafeteria, the couch, the sofa, the dining room, the living room, or our bedrooms, people have tried to define who is or should be our closest friend. The only limit we can put is whether or not that person is also a person.\n",
            "\n",
            "\n",
            "A few years back, I found myself in a strange situation. We had moved to the same town, the one I had just learned I was moving to. Despite having not spoken to each other in months, we had become good friends. For a while, we were able to spend more time together on the phone (we call it \"hanging out\"), but we still mostly hung out behind my desk in a quiet office, at the local library.\n",
            "\n",
            "\n",
            "As I looked through the pages of my copy of The Casual Vacancy, I couldn't help but think about my friendship with my neighbor. The book was about people who have broken up with each other, and was about their journeys through the process. And yet, I couldn't help but wonder if it was my neighbor that I was hanging out with — or maybe it was my roommate.\n",
            "\n",
            "\n",
            "The story was fascinating, and the concept was fascinating. Why do people stay friends and friends stay friends and friends stay friends? Are there any fundamental differences? Is there just a few fundamental differences?\n",
            "\n",
            "\n",
            "So I went back to bookstores, trying to find books that had anything resembling an answer to these questions, when I saw The Casual Vacancy.\n",
            "\n",
            "\n",
            "While I had read many books about romance before reading The Casual Vacancy, this one seemed to be an exception. As a romantic romance, the book seems to do more than simply portray a relationship ending — it portrays its characters. The characters in The Casual Vacancy are people who, though at least superficially similar, aren't really much alike in the way that we think of friends. Instead, we tend to associate them as the same person. Even the most obvious similarities that I can remember of the characters were minor, like the way they wore the same clothes or used the same cars.\n",
            "\n",
            "\n",
            "And yet, this book seemed to depict a relationship entirely different. That's because the characters are described, first and foremost, in their romantic relationships. While the characters in The Casual Vacancy do mostly see each other from a practical and superficial perspective, the characters in The Casual Vacancy aren't so superficial. They're real, complicated people.\n",
            "\n",
            "\n",
            "So I asked for the book's sequel to understand where the romance really began, and what the relationships were like at the time I had the book in my hands. When I received my email, the reply was immediate. \"The Casual Vacancy, Part 2. What am I reading? Yes.\"\n",
            "\n",
            "The Casual Vacancy, Part 2 follows a story that began with, and is still followed by, this quote: \"So that when you are at the end of something, you know, you are there in some way. You are not in some kind of fugue state.\"\n",
            "\n",
            "\n",
            "You see, this thing about the end of things is not completely crazy nonsense. In fact, sometimes we do have a pretty definitive end. It's almost an axiom of literature, one that seems to come down a little bit more consistently with the facts than the fancy fiction we see on screens. But in a more literal sense than if you wanted it to, the beginning of things could be a beginning. We could just as well know that a child born is a child born. In that case, we also know that something is already a beginning that we aren't entirely sure what that is yet.\n",
            "\n",
            "\n",
            "But you have to realize that some things are truly beginning, and that there will always be something at the end of them. In the beginning of something, there can be a beginning. At the beginning. There is always a beginning. And so, in a similar way, to what I said on the first question, there can be a beginning of friendships.\n",
            "\n",
            "The Casual Vacancy, Part 2 focuses on the relationship between an old man and a young woman, but even on any given day, you will likely have a couple of conversations on the phone that have you thinking about the things you said after the meeting ended. It could be the conversation about how you met your neighbor, and how your roommate ended up talking about him when she met him, or it could be the discussion about how you talked about her, but whether you feel close enough that you feel like you have to put a person you love closer to you, or it could be when you have a talk about where your relationship got into trouble, when you told the other person you were in love with someone else, you start to feel close enough\n",
            "\n",
            "[100 | 204.21] loss=3.32 avg=2.83\n",
            "[101 | 205.89] loss=2.44 avg=2.83\n",
            "[102 | 207.57] loss=2.84 avg=2.83\n",
            "[103 | 209.25] loss=2.72 avg=2.83\n",
            "[104 | 210.93] loss=3.02 avg=2.83\n",
            "[105 | 212.60] loss=2.72 avg=2.83\n",
            "[106 | 214.27] loss=2.93 avg=2.83\n",
            "[107 | 215.95] loss=2.45 avg=2.82\n",
            "[108 | 217.62] loss=2.84 avg=2.82\n",
            "[109 | 219.29] loss=2.78 avg=2.82\n",
            "[110 | 220.96] loss=2.62 avg=2.82\n",
            "[111 | 222.63] loss=2.75 avg=2.82\n",
            "[112 | 224.30] loss=3.40 avg=2.83\n",
            "[113 | 225.97] loss=2.70 avg=2.83\n",
            "[114 | 227.64] loss=3.00 avg=2.83\n",
            "[115 | 229.31] loss=2.78 avg=2.83\n",
            "[116 | 230.98] loss=2.71 avg=2.83\n",
            "[117 | 232.64] loss=2.75 avg=2.83\n",
            "[118 | 234.32] loss=2.81 avg=2.83\n",
            "[119 | 236.00] loss=2.82 avg=2.83\n",
            "[120 | 237.66] loss=2.74 avg=2.82\n",
            "[121 | 239.33] loss=2.68 avg=2.82\n",
            "[122 | 241.00] loss=2.65 avg=2.82\n",
            "[123 | 242.66] loss=2.68 avg=2.82\n",
            "[124 | 244.34] loss=2.69 avg=2.82\n",
            "[125 | 246.01] loss=2.85 avg=2.82\n",
            "[126 | 247.68] loss=2.80 avg=2.82\n",
            "[127 | 249.35] loss=2.66 avg=2.81\n",
            "[128 | 251.02] loss=3.10 avg=2.82\n",
            "[129 | 252.69] loss=2.58 avg=2.81\n",
            "[130 | 254.36] loss=2.78 avg=2.81\n",
            "[131 | 256.04] loss=2.53 avg=2.81\n",
            "[132 | 257.71] loss=2.89 avg=2.81\n",
            "[133 | 259.38] loss=2.78 avg=2.81\n",
            "[134 | 261.06] loss=2.88 avg=2.81\n",
            "[135 | 262.73] loss=2.61 avg=2.81\n",
            "[136 | 264.41] loss=2.81 avg=2.81\n",
            "[137 | 266.08] loss=2.64 avg=2.81\n",
            "[138 | 267.75] loss=2.56 avg=2.80\n",
            "[139 | 269.42] loss=2.56 avg=2.80\n",
            "[140 | 271.10] loss=2.70 avg=2.80\n",
            "[141 | 272.77] loss=2.78 avg=2.80\n",
            "[142 | 274.44] loss=2.83 avg=2.80\n",
            "[143 | 276.11] loss=2.81 avg=2.80\n",
            "[144 | 277.78] loss=2.93 avg=2.80\n",
            "[145 | 279.45] loss=2.64 avg=2.80\n",
            "[146 | 281.14] loss=2.71 avg=2.80\n",
            "[147 | 282.81] loss=2.93 avg=2.80\n",
            "[148 | 284.48] loss=2.89 avg=2.80\n",
            "[149 | 286.16] loss=2.87 avg=2.80\n",
            "[150 | 287.83] loss=2.72 avg=2.80\n",
            "[151 | 289.52] loss=2.75 avg=2.80\n",
            "[152 | 291.21] loss=2.77 avg=2.80\n",
            "[153 | 292.88] loss=2.60 avg=2.80\n",
            "[154 | 294.57] loss=2.33 avg=2.79\n",
            "[155 | 296.25] loss=2.70 avg=2.79\n",
            "[156 | 297.93] loss=2.73 avg=2.79\n",
            "[157 | 299.61] loss=2.64 avg=2.79\n",
            "[158 | 301.30] loss=2.73 avg=2.79\n",
            "[159 | 302.99] loss=2.62 avg=2.78\n",
            "[160 | 304.69] loss=2.83 avg=2.78\n",
            "[161 | 306.38] loss=2.77 avg=2.78\n",
            "[162 | 308.06] loss=2.83 avg=2.79\n",
            "[163 | 309.76] loss=2.82 avg=2.79\n",
            "[164 | 311.44] loss=2.81 avg=2.79\n",
            "[165 | 313.13] loss=2.93 avg=2.79\n",
            "[166 | 314.82] loss=2.76 avg=2.79\n",
            "[167 | 316.49] loss=2.66 avg=2.79\n",
            "[168 | 318.18] loss=2.54 avg=2.78\n",
            "[169 | 319.85] loss=2.52 avg=2.78\n",
            "[170 | 321.54] loss=2.67 avg=2.78\n",
            "[171 | 323.21] loss=2.62 avg=2.78\n",
            "[172 | 324.89] loss=2.61 avg=2.77\n",
            "[173 | 326.55] loss=2.73 avg=2.77\n",
            "[174 | 328.23] loss=2.59 avg=2.77\n",
            "[175 | 329.90] loss=2.61 avg=2.77\n",
            "[176 | 331.57] loss=2.69 avg=2.77\n",
            "[177 | 333.24] loss=2.61 avg=2.77\n",
            "[178 | 334.90] loss=2.83 avg=2.77\n",
            "[179 | 336.58] loss=2.32 avg=2.76\n",
            "[180 | 338.25] loss=2.98 avg=2.76\n",
            "[181 | 339.92] loss=2.96 avg=2.77\n",
            "[182 | 341.60] loss=2.67 avg=2.77\n",
            "[183 | 343.26] loss=2.72 avg=2.77\n",
            "[184 | 344.93] loss=3.15 avg=2.77\n",
            "[185 | 346.61] loss=2.80 avg=2.77\n",
            "[186 | 348.28] loss=2.42 avg=2.77\n",
            "[187 | 349.95] loss=2.75 avg=2.77\n",
            "[188 | 351.62] loss=2.86 avg=2.77\n",
            "[189 | 353.30] loss=2.78 avg=2.77\n",
            "[190 | 354.96] loss=2.81 avg=2.77\n",
            "[191 | 356.65] loss=2.57 avg=2.77\n",
            "[192 | 358.33] loss=2.41 avg=2.76\n",
            "[193 | 360.02] loss=2.47 avg=2.76\n",
            "[194 | 361.70] loss=2.79 avg=2.76\n",
            "[195 | 363.38] loss=2.59 avg=2.76\n",
            "[196 | 365.07] loss=2.96 avg=2.76\n",
            "[197 | 366.76] loss=2.84 avg=2.76\n",
            "[198 | 368.44] loss=2.72 avg=2.76\n",
            "[199 | 370.12] loss=2.76 avg=2.76\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            ", a group of people in the South Korean town of Gangwak, in which many of the migrants live, was so desperate that it could offer them a better chance to get here than anyone else, a local man with an infectious laugh said.\n",
            "\n",
            "He told me that every town has its different story: The village with the best roads always win hands down. On the street, men who could barely walk were offering their arms in the other direction. Some men were running away so that they could make it, others were dying. Some people walked out of the village because they had no choice. Others were waiting for the village bus. For some of the younger guys in the neighborhood, the chance to work an odd job wasn't something they should pass up. A lot of them were going through a divorce they had to get through without help. In one local pub he said, his face took on a sour note. 'If someone was coming to us to say, \"We don't deserve any money,\"' he said, 'or \"You've got to go back to your country\" or \"You have to go back to your country,\"' there were always these people who say they came to us because of money.\n",
            "\n",
            "But, as one man who knew the situation in many local towns from the internet told me a few weeks ago, we've also come to expect these people who do come through. We get it. Some towns with high unemployment rates just won't accept the refugees, and the refugees have to have their lives in order. And it is here that the problem started in earnest. The South Koreans are famous for their high numbers of refugees: They're the world's biggest exporters, the biggest recipients of foreign aid, the best exporters in the world. The country also has the greatest human-development index of all (the measure goes on to include health), which also goes on to include poverty rates. And it's been a good story over the centuries, the stuff of folktales, movies and comic strips. South Korea was the cradle of the nation. Its prosperity and diversity are celebrated and celebrated in the most bizarre and bizarre ways. It is also one of the world's poorest countries, and the highest unemployment rate in the world—especially in the rural areas.\n",
            "\n",
            "There have been studies that attempt to examine the causes of poverty—which, by the way, usually are tied to lack of free time, which is another name for jobs—but the people who have put together these studies have been hit up by some of our own society. The studies usually find that poverty is down in the rural areas, while in the urban areas people are actually better off. However, they rarely look in the whole data set, which is where the interesting stuff happens.\n",
            "\n",
            "In this study, a team of researchers led by Robert Cimino of the University of Utah tried to look at both regions. They didn't quite reach all of their conclusions, but they did find a couple of unexpected findings. For one thing, South Koreans are happier if they live in a city than they are if they live in a rural area, and the happiest cities are often the ones that have lots of jobs to begin with. So the idea that these refugees are happier overall has a tendency to go to the opposite extreme.\n",
            "\n",
            "Also, living in a urban state can make it easier for people to move back to the country once they have become settled. As I mentioned earlier, South Korea is the world's largest exporter of goods, so if its people move to a new place so they can earn the same incomes there, they're more likely to stay. So, in other words, this study suggests that, as a country, South Korea really wants the people it has to be.\n",
            "\n",
            "Cimino and his colleagues wanted to find out, in a different way, and found that it isn't necessarily hard to find. Refugees are not like the refugees who came before, in other words. They're also not the people who might look like refugees to outsiders, but who come for reasons of desperation. And as long as immigrants are willing to change their country of origin, it doesn't matter much when we're talking about refugees. Most have no idea what their country is, and no intentions of going back. The country in which they've now settled can be quite nice. The country isn't exactly where they like to live, after all. Or a place so bad that they'd like to go back. If they had the choice, they would just leave. For what reasons? I don't know the answer. But, if they want to get as far away from a country as possible, here they can stay.\n",
            "\n",
            "I'm still not convinced that people who want a better life tend to move, or leave, because their country is bad. The South Koreans are pretty wealthy. Even people living in rural areas can afford to get through a divorce and get the money they need later in a rural country. They also can afford to live\n",
            "\n",
            "[200 | 394.65] loss=2.35 avg=2.75\n",
            "[201 | 396.32] loss=2.75 avg=2.75\n",
            "[202 | 397.99] loss=2.55 avg=2.75\n",
            "[203 | 399.67] loss=2.36 avg=2.75\n",
            "[204 | 401.34] loss=3.16 avg=2.75\n",
            "[205 | 403.00] loss=2.67 avg=2.75\n",
            "[206 | 404.67] loss=2.59 avg=2.75\n",
            "[207 | 406.34] loss=2.71 avg=2.75\n",
            "[208 | 408.01] loss=2.68 avg=2.75\n",
            "[209 | 409.67] loss=2.86 avg=2.75\n",
            "[210 | 411.33] loss=2.60 avg=2.75\n",
            "[211 | 413.00] loss=3.17 avg=2.75\n",
            "[212 | 414.66] loss=2.71 avg=2.75\n",
            "[213 | 416.33] loss=2.94 avg=2.75\n",
            "[214 | 417.99] loss=2.76 avg=2.75\n",
            "[215 | 419.66] loss=2.83 avg=2.76\n",
            "[216 | 421.33] loss=2.62 avg=2.75\n",
            "[217 | 423.00] loss=2.98 avg=2.76\n",
            "[218 | 424.66] loss=2.44 avg=2.75\n",
            "[219 | 426.34] loss=2.42 avg=2.75\n",
            "[220 | 428.00] loss=2.27 avg=2.74\n",
            "[221 | 429.68] loss=2.76 avg=2.74\n",
            "[222 | 431.35] loss=2.88 avg=2.75\n",
            "[223 | 433.02] loss=2.79 avg=2.75\n",
            "[224 | 434.71] loss=2.89 avg=2.75\n",
            "[225 | 436.38] loss=3.01 avg=2.75\n",
            "[226 | 438.05] loss=3.28 avg=2.76\n",
            "[227 | 439.73] loss=2.48 avg=2.75\n",
            "[228 | 441.41] loss=2.82 avg=2.75\n",
            "[229 | 443.10] loss=2.27 avg=2.75\n",
            "[230 | 444.78] loss=2.49 avg=2.75\n",
            "[231 | 446.47] loss=2.72 avg=2.75\n",
            "[232 | 448.16] loss=2.79 avg=2.75\n",
            "[233 | 449.85] loss=2.58 avg=2.74\n",
            "[234 | 451.53] loss=2.89 avg=2.75\n",
            "[235 | 453.22] loss=2.71 avg=2.75\n",
            "[236 | 454.91] loss=2.37 avg=2.74\n",
            "[237 | 456.60] loss=2.89 avg=2.74\n",
            "[238 | 458.29] loss=2.86 avg=2.74\n",
            "[239 | 459.97] loss=2.89 avg=2.75\n",
            "[240 | 461.66] loss=2.84 avg=2.75\n",
            "[241 | 463.34] loss=2.92 avg=2.75\n",
            "[242 | 465.01] loss=2.58 avg=2.75\n",
            "[243 | 466.71] loss=2.54 avg=2.74\n",
            "[244 | 468.39] loss=2.89 avg=2.75\n",
            "[245 | 470.07] loss=2.89 avg=2.75\n",
            "[246 | 471.74] loss=2.64 avg=2.75\n",
            "[247 | 473.42] loss=2.58 avg=2.74\n",
            "[248 | 475.09] loss=2.42 avg=2.74\n",
            "[249 | 476.77] loss=2.45 avg=2.74\n",
            "[250 | 478.46] loss=2.77 avg=2.74\n",
            "[251 | 480.13] loss=2.74 avg=2.74\n",
            "[252 | 481.82] loss=2.48 avg=2.74\n",
            "[253 | 483.51] loss=2.51 avg=2.73\n",
            "[254 | 485.18] loss=2.57 avg=2.73\n",
            "[255 | 486.85] loss=2.72 avg=2.73\n",
            "[256 | 488.53] loss=2.49 avg=2.73\n",
            "[257 | 490.21] loss=3.24 avg=2.73\n",
            "[258 | 491.89] loss=2.51 avg=2.73\n",
            "[259 | 493.58] loss=2.53 avg=2.73\n",
            "[260 | 495.25] loss=2.82 avg=2.73\n",
            "[261 | 496.92] loss=2.38 avg=2.73\n",
            "[262 | 498.59] loss=2.70 avg=2.73\n",
            "[263 | 500.27] loss=2.80 avg=2.73\n",
            "[264 | 501.95] loss=2.67 avg=2.73\n",
            "[265 | 503.62] loss=2.80 avg=2.73\n",
            "[266 | 505.30] loss=2.36 avg=2.72\n",
            "[267 | 506.98] loss=2.28 avg=2.72\n",
            "[268 | 508.65] loss=2.59 avg=2.72\n",
            "[269 | 510.32] loss=3.02 avg=2.72\n",
            "[270 | 511.99] loss=3.04 avg=2.72\n",
            "[271 | 513.67] loss=2.48 avg=2.72\n",
            "[272 | 515.35] loss=3.06 avg=2.72\n",
            "[273 | 517.02] loss=2.66 avg=2.72\n",
            "[274 | 518.69] loss=2.78 avg=2.72\n",
            "[275 | 520.36] loss=2.64 avg=2.72\n",
            "[276 | 522.03] loss=2.82 avg=2.73\n",
            "[277 | 523.71] loss=2.41 avg=2.72\n",
            "[278 | 525.38] loss=2.33 avg=2.72\n",
            "[279 | 527.05] loss=2.76 avg=2.72\n",
            "[280 | 528.72] loss=2.77 avg=2.72\n",
            "[281 | 530.39] loss=2.67 avg=2.72\n",
            "[282 | 532.05] loss=3.16 avg=2.72\n",
            "[283 | 533.72] loss=2.84 avg=2.72\n",
            "[284 | 535.39] loss=2.58 avg=2.72\n",
            "[285 | 537.07] loss=2.64 avg=2.72\n",
            "[286 | 538.74] loss=2.78 avg=2.72\n",
            "[287 | 540.41] loss=2.29 avg=2.72\n",
            "[288 | 542.09] loss=2.56 avg=2.72\n",
            "[289 | 543.76] loss=2.14 avg=2.71\n",
            "[290 | 545.43] loss=2.50 avg=2.71\n",
            "[291 | 547.11] loss=2.97 avg=2.71\n",
            "[292 | 548.77] loss=2.72 avg=2.71\n",
            "[293 | 550.44] loss=2.40 avg=2.71\n",
            "[294 | 552.11] loss=2.40 avg=2.70\n",
            "[295 | 553.78] loss=2.50 avg=2.70\n",
            "[296 | 555.45] loss=2.96 avg=2.70\n",
            "[297 | 557.12] loss=2.77 avg=2.71\n",
            "[298 | 558.79] loss=2.37 avg=2.70\n",
            "[299 | 560.46] loss=2.31 avg=2.70\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            ", so this probably won't come as any surprise.\n",
            "\n",
            "But still, it's good to see that the company has a very clear-cut approach to its products. In addition to the most popular VR headsets, Oculus has released a range of accessories that have helped to popularize virtual reality as a consumer product.\n",
            "\n",
            "The new Oculus Connect line of wireless headphones was developed with the aim of helping developers build better virtual reality games. They will be available soon to Oculus Rift users who bought the company's Rift device.\n",
            "\n",
            "Oculus Connect will be the company's first VR headset with a built-in mic—it was originally a phone accessory that users had to download in order to function. The company plans to offer a standalone accessory for developers to develop with at least next year, Oculus co-founder Palmer Luckey told the Guardian.\n",
            "\n",
            "In addition to the microphone, the headset also includes a wireless charging port, an audio stream, and Wi-Fi. These features are standard on HTC Vive headsets, but Oculus Connect will be different enough that people will be able to use the headset while also recording and streaming VR experiences.\n",
            "\n",
            "A similar feature from Oculus Touch was used to create a live-streamed preview of the release of its latest game, Inception, back in November. The company says many Rift owners are already using Oculus Touch to record their virtual-reality experiences, but Luckey said a new version of the headset will be released with improved audio quality.\n",
            "\n",
            "\"There is a difference between audio quality and audio quality but in principle you could imagine better quality audio and better visual quality—but we don't know yet what they will be,\" he said.\n",
            "\n",
            "The new headsets will likely become a key factor in any sales competition for headset makers to differentiate, and Oculus won't reveal how much the microphone price will go up from the company's current offer.\n",
            "\n",
            "The company has yet to confirm the price of the new headset, however.\n",
            "\n",
            "\"Our goal in using a microphone is to enable immersive, and the more immersive you mean, the less you have to pay attention to the device,\" Luckey told the Guardian. \"The idea is to not have to be on your toes with our product, which has a very specific user experience. The idea is to take the user mind and use the device in a less distracting way.\"\n",
            "\n",
            "Oculus has been working with a number media partners in its effort to help make VR more of a viable medium. There will be an extensive media line up this year aimed at making VR better for the masses, and Luckey said that the company is working \"all the way to the bottom\" for new and interesting content partners.\n",
            "\n",
            "The new headsets have already been used as part of an Oculus Media team in Seattle, where a session ran for a handful of journalists. It was unclear if this was just a demonstration or if any of the participants were there to watch the live stream from another location.\n",
            "\n",
            "Oculus, which plans the first of the line of headsets with a $3,000 price tag, was already selling the technology directly to VR consumers, who won them through an additional $50 price on top of the headset price.\n",
            "\n",
            "At the opening of its developer meeting on Tuesday, Luckey said that the company expects to sell 100,000 headsets this year, with the expectation that it will sell two million more by the end of the year.\n",
            "\n",
            "The company has also been working with the likes of YouTube star James Corden to help further make the industry more appealing for gamers and non-gamers alike. Luckey said that this was \"one of the most anticipated products on Twitter when we came out of the meeting. Our CEO was in the first class. I walked into a meeting with the CEO, and that was in person.\"\n",
            "\n",
            "In addition to the headset's mic, Oculus Connect also includes a microphone jack. It's the first time that Oculus has included a microphone jack, which has been a crucial part of headset technology to facilitate wireless communication from the headset to the user.\n",
            "\n",
            "Oculus also talked up the company's camera, which is now known as a positional camera, and said it is capable of recording 2,000 times faster than previous cameras.\n",
            "\n",
            "This also isn't the first time that there have been claims that the camera is capable of faster, higher quality photos over the headset. An Oculus Rift developer kit released in 2016 included some kind of photo filter that users had to manually filter out before being able to view a photo.\n",
            "\n",
            "Oculus Connect is designed with a camera that is as close to a point-and-shoot camera as the company can muster, according to Luckey. Because of this, it will also be able to record 1080p videos at a lower resolution than its competitors.\n",
            "\n",
            "Oculus will also be able to capture better-than-ever images, and there are a host of other features that Luckey says are coming soon for it, such as a wireless controller, and VR head strap, and a\n",
            "\n",
            "[300 | 584.82] loss=2.20 avg=2.69\n",
            "[301 | 586.48] loss=2.53 avg=2.69\n",
            "[302 | 588.16] loss=2.58 avg=2.69\n",
            "[303 | 589.83] loss=2.24 avg=2.68\n",
            "[304 | 591.50] loss=2.56 avg=2.68\n",
            "[305 | 593.17] loss=3.16 avg=2.69\n",
            "[306 | 594.84] loss=2.32 avg=2.68\n",
            "[307 | 596.50] loss=2.79 avg=2.69\n",
            "[308 | 598.18] loss=2.36 avg=2.68\n",
            "[309 | 599.86] loss=3.13 avg=2.69\n",
            "[310 | 601.53] loss=2.60 avg=2.69\n",
            "[311 | 603.20] loss=2.85 avg=2.69\n",
            "[312 | 604.87] loss=2.28 avg=2.68\n",
            "[313 | 606.55] loss=2.57 avg=2.68\n",
            "[314 | 608.22] loss=2.60 avg=2.68\n",
            "[315 | 609.89] loss=2.74 avg=2.68\n",
            "[316 | 611.57] loss=2.90 avg=2.68\n",
            "[317 | 613.24] loss=2.63 avg=2.68\n",
            "[318 | 614.90] loss=2.69 avg=2.68\n",
            "[319 | 616.58] loss=3.08 avg=2.69\n",
            "[320 | 618.25] loss=2.34 avg=2.68\n",
            "[321 | 619.92] loss=2.69 avg=2.68\n",
            "[322 | 621.59] loss=2.35 avg=2.68\n",
            "[323 | 623.25] loss=2.73 avg=2.68\n",
            "[324 | 624.93] loss=2.30 avg=2.68\n",
            "[325 | 626.60] loss=2.59 avg=2.68\n",
            "[326 | 628.27] loss=3.06 avg=2.68\n",
            "[327 | 629.94] loss=2.37 avg=2.68\n",
            "[328 | 631.61] loss=2.49 avg=2.68\n",
            "[329 | 633.28] loss=2.60 avg=2.67\n",
            "[330 | 634.95] loss=2.80 avg=2.68\n",
            "[331 | 636.62] loss=2.85 avg=2.68\n",
            "[332 | 638.30] loss=2.48 avg=2.68\n",
            "[333 | 639.96] loss=2.41 avg=2.67\n",
            "[334 | 641.63] loss=2.43 avg=2.67\n",
            "[335 | 643.30] loss=2.49 avg=2.67\n",
            "[336 | 644.97] loss=2.84 avg=2.67\n",
            "[337 | 646.64] loss=2.31 avg=2.67\n",
            "[338 | 648.32] loss=2.66 avg=2.67\n",
            "[339 | 650.01] loss=2.77 avg=2.67\n",
            "[340 | 651.68] loss=2.41 avg=2.66\n",
            "[341 | 653.36] loss=2.72 avg=2.67\n",
            "[342 | 655.03] loss=2.44 avg=2.66\n",
            "[343 | 656.72] loss=2.97 avg=2.67\n",
            "[344 | 658.39] loss=2.61 avg=2.67\n",
            "[345 | 660.07] loss=2.57 avg=2.66\n",
            "[346 | 661.74] loss=2.66 avg=2.66\n",
            "[347 | 663.41] loss=2.41 avg=2.66\n",
            "[348 | 665.08] loss=2.66 avg=2.66\n",
            "[349 | 666.76] loss=2.62 avg=2.66\n",
            "[350 | 668.43] loss=2.37 avg=2.66\n",
            "[351 | 670.11] loss=2.97 avg=2.66\n",
            "[352 | 671.78] loss=2.79 avg=2.66\n",
            "[353 | 673.45] loss=2.60 avg=2.66\n",
            "[354 | 675.13] loss=2.54 avg=2.66\n",
            "[355 | 676.80] loss=2.91 avg=2.66\n",
            "[356 | 678.48] loss=3.02 avg=2.67\n",
            "[357 | 680.15] loss=2.60 avg=2.67\n",
            "[358 | 681.83] loss=3.06 avg=2.67\n",
            "[359 | 683.51] loss=2.13 avg=2.66\n",
            "[360 | 685.18] loss=2.58 avg=2.66\n",
            "[361 | 686.86] loss=2.58 avg=2.66\n",
            "[362 | 688.54] loss=2.61 avg=2.66\n",
            "[363 | 690.22] loss=2.61 avg=2.66\n",
            "[364 | 691.90] loss=2.76 avg=2.66\n",
            "[365 | 693.57] loss=2.58 avg=2.66\n",
            "[366 | 695.25] loss=2.24 avg=2.66\n",
            "[367 | 696.93] loss=2.58 avg=2.66\n",
            "[368 | 698.60] loss=2.72 avg=2.66\n",
            "[369 | 700.29] loss=2.59 avg=2.66\n",
            "[370 | 701.96] loss=2.54 avg=2.66\n",
            "[371 | 703.63] loss=2.77 avg=2.66\n",
            "[372 | 705.32] loss=2.71 avg=2.66\n",
            "[373 | 707.01] loss=2.64 avg=2.66\n",
            "[374 | 708.68] loss=2.40 avg=2.65\n",
            "[375 | 710.35] loss=2.68 avg=2.65\n",
            "[376 | 712.04] loss=2.37 avg=2.65\n",
            "[377 | 713.71] loss=2.43 avg=2.65\n",
            "[378 | 715.39] loss=2.60 avg=2.65\n",
            "[379 | 717.08] loss=2.33 avg=2.65\n",
            "[380 | 718.76] loss=2.68 avg=2.65\n",
            "[381 | 720.44] loss=2.18 avg=2.64\n",
            "[382 | 722.12] loss=2.32 avg=2.64\n",
            "[383 | 723.80] loss=2.90 avg=2.64\n",
            "[384 | 725.47] loss=2.50 avg=2.64\n",
            "[385 | 727.17] loss=2.64 avg=2.64\n",
            "[386 | 728.84] loss=2.45 avg=2.64\n",
            "[387 | 730.51] loss=2.45 avg=2.64\n",
            "[388 | 732.20] loss=2.54 avg=2.63\n",
            "[389 | 733.87] loss=2.47 avg=2.63\n",
            "[390 | 735.55] loss=2.44 avg=2.63\n",
            "[391 | 737.22] loss=2.45 avg=2.63\n",
            "[392 | 738.90] loss=2.36 avg=2.63\n",
            "[393 | 740.58] loss=2.93 avg=2.63\n",
            "[394 | 742.26] loss=2.81 avg=2.63\n",
            "[395 | 743.93] loss=2.76 avg=2.63\n",
            "[396 | 745.61] loss=2.82 avg=2.63\n",
            "[397 | 747.28] loss=2.58 avg=2.63\n",
            "[398 | 748.95] loss=2.36 avg=2.63\n",
            "[399 | 750.64] loss=2.83 avg=2.63\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " of blood of a dead horse found in a remote area in Alaska has led to a national hunt for the creature that once roamed the Earth.\n",
            "\n",
            "The horse was found on Friday along a 2,200-foot cliff on the Narrows near Wupatka Lake near Fairbanks, Alaska. But the National Park Service has since decided—perhaps coincidentally—that the missing person is in fact the horse, Hunds, the Park Service said on its Facebook page, as it announced that it was seeking her safe return.\n",
            "\n",
            "Advertisement\n",
            "\n",
            "The Horse has not been seen since she was found at 9 o'clock at night this morning on the cliff. And her disappearance has been shrouded in mystery ever since, with no clues from either authorities or the public about who that person might be, how they died, or if there is a suspect in the case.\n",
            "\n",
            "As of yet, no one has said anything publicly about what led authorities to the person, and the search on the cliff has only begun.\n",
            "\n",
            "\"We really want to find her. We want to track her down so we can reunite with her. We want to find out how she ended up on the cliff and bring her back to Alaska in a safe way until we can say we found her,\" said Bryce Pembrey, who is with the park service and has been tasked with the task.\n",
            "\n",
            "Pembrey did say that the park service is currently making arrangements to send out an investigator to the area. So far, there have been no indications of a connection between the horse and the Missing Person, so there's a possibility that the animal is responsible for her recovery.\n",
            "\n",
            "But this is where the speculation comes in. Hunds had been reported missing last month after she was found at a campground just outside Fairbanks, but no information was released by the Park Service. And while it's not common practice for the search to be publicized without a cause for an investigation, Pembrey said that it wasn't unusual for the Park Service to do this because there is often some type of unusual event that makes people go missing.\n",
            "\n",
            "\"That's not unusual. We have a big area of the park that has been closed to the public so there was no media release that would really have a broad basis for what happened,\" he told Fusion. \"Our hope is that with the public outcry, and the fact that we are looking for this particular horse, then there will be an appropriate response by the local law enforcement.\"\n",
            "\n",
            "Advertisement\n",
            "\n",
            "The person or persons responsible will be named in a blog entry of the National Park Service.\n",
            "\n",
            "Pembrey told Fusion that the horse recovery team has already had contact with the family of the horse, but that he wasn't able to share what information they're already sharing with the public. The public wasn't told about this until this morning.\n",
            "\n",
            "As of 6:23 am, WXTV–a local news station in Fairbanks–reported that Hunds had been located.\n",
            "\n",
            "According to Pembrey, the horse has \"strong ties to Fairbanks.\" The park is known for its rugged scenery and isolation, which would have made it easy to keep her safe, even on such a treacherous cliff. But Hunds is an orphaned horse and has suffered serious injuries.\n",
            "\n",
            "The horse is believed to have been taken by a person known to the family, and she is believed to be in good health.\n",
            "\n",
            "Advertisement\n",
            "\n",
            "Pembrey believes there may actually be some connection between the horse and Hunds.\n",
            "\n",
            "\"Her story is one that we have heard from many people. She has lived in Fairbanks. Many of her owners have been in the same area. Her mother is from [Balsam], in the same state as the family, and she has many family friends who visit her on a regular basis,\" he said of Hunds.\n",
            "\n",
            "But that connection, Pembrey explained, is still speculative at this point.\n",
            "\n",
            "\"We continue to talk with the friends of the horse who are doing everything they can to reunite her with her family. They can't seem to find any leads in those locations. We're trying our best,\" he said.\n",
            "\n",
            "The Park Service did say that they are planning to send an expedited search team to the area next week, and that it is also going to be doing its own search efforts, as well as helping with the recovery efforts.\n",
            "\n",
            "\"It is too dangerous for her to remain on the cliff face in an isolated area,\" Pembrey said. \"We believe that we've found the animal. The National Park Service has announced that she has been located. This is a once in a lifetime opportunity for us and our public. We're going to give everyone we can help and make sure they're safe, but we want Hunds back in the wild as soon as possible.\"\n",
            "\n",
            "In a statement, a Facebook friend of the horse reportedly stated that she has\n",
            "\n",
            "[400 | 775.02] loss=2.60 avg=2.63\n",
            "[401 | 776.69] loss=2.83 avg=2.63\n",
            "[402 | 778.35] loss=2.44 avg=2.63\n",
            "[403 | 780.02] loss=2.91 avg=2.64\n",
            "[404 | 781.68] loss=2.39 avg=2.63\n",
            "[405 | 783.34] loss=2.79 avg=2.63\n",
            "[406 | 785.01] loss=2.95 avg=2.64\n",
            "[407 | 786.67] loss=2.95 avg=2.64\n",
            "[408 | 788.33] loss=2.99 avg=2.64\n",
            "[409 | 789.99] loss=2.41 avg=2.64\n",
            "[410 | 791.65] loss=3.12 avg=2.65\n",
            "[411 | 793.32] loss=2.46 avg=2.65\n",
            "[412 | 794.98] loss=2.45 avg=2.64\n",
            "[413 | 796.66] loss=2.55 avg=2.64\n",
            "[414 | 798.32] loss=2.77 avg=2.64\n",
            "[415 | 800.00] loss=2.30 avg=2.64\n",
            "[416 | 801.66] loss=2.97 avg=2.64\n",
            "[417 | 803.33] loss=2.67 avg=2.64\n",
            "[418 | 805.01] loss=2.51 avg=2.64\n",
            "[419 | 806.68] loss=2.68 avg=2.64\n",
            "[420 | 808.36] loss=2.32 avg=2.64\n",
            "[421 | 810.04] loss=2.46 avg=2.64\n",
            "[422 | 811.72] loss=2.57 avg=2.64\n",
            "[423 | 813.40] loss=2.75 avg=2.64\n",
            "[424 | 815.08] loss=2.51 avg=2.64\n",
            "[425 | 816.76] loss=2.53 avg=2.64\n",
            "[426 | 818.45] loss=2.16 avg=2.63\n",
            "[427 | 820.14] loss=2.53 avg=2.63\n",
            "[428 | 821.83] loss=2.75 avg=2.63\n",
            "[429 | 823.52] loss=2.80 avg=2.63\n",
            "[430 | 825.21] loss=2.36 avg=2.63\n",
            "[431 | 826.90] loss=2.13 avg=2.63\n",
            "[432 | 828.60] loss=2.56 avg=2.62\n",
            "[433 | 830.28] loss=2.49 avg=2.62\n",
            "[434 | 831.97] loss=2.57 avg=2.62\n",
            "[435 | 833.64] loss=2.34 avg=2.62\n",
            "[436 | 835.33] loss=2.75 avg=2.62\n",
            "[437 | 837.02] loss=2.32 avg=2.62\n",
            "[438 | 838.69] loss=2.05 avg=2.61\n",
            "[439 | 840.37] loss=2.64 avg=2.61\n",
            "[440 | 842.05] loss=2.37 avg=2.61\n",
            "[441 | 843.73] loss=2.51 avg=2.61\n",
            "[442 | 845.42] loss=2.68 avg=2.61\n",
            "[443 | 847.10] loss=2.26 avg=2.61\n",
            "[444 | 848.78] loss=2.69 avg=2.61\n",
            "[445 | 850.47] loss=2.75 avg=2.61\n",
            "[446 | 852.16] loss=2.60 avg=2.61\n",
            "[447 | 853.84] loss=2.89 avg=2.61\n",
            "[448 | 855.52] loss=2.60 avg=2.61\n",
            "[449 | 857.19] loss=2.85 avg=2.61\n",
            "[450 | 858.87] loss=2.69 avg=2.61\n",
            "[451 | 860.56] loss=2.69 avg=2.62\n",
            "[452 | 862.23] loss=2.31 avg=2.61\n",
            "[453 | 863.90] loss=2.50 avg=2.61\n",
            "[454 | 865.57] loss=2.63 avg=2.61\n",
            "[455 | 867.24] loss=2.67 avg=2.61\n",
            "[456 | 868.92] loss=2.29 avg=2.61\n",
            "[457 | 870.61] loss=2.48 avg=2.61\n",
            "[458 | 872.28] loss=2.27 avg=2.60\n",
            "[459 | 873.97] loss=2.29 avg=2.60\n",
            "[460 | 875.65] loss=2.51 avg=2.60\n",
            "[461 | 877.32] loss=2.54 avg=2.60\n",
            "[462 | 878.99] loss=2.43 avg=2.60\n",
            "[463 | 880.67] loss=2.09 avg=2.59\n",
            "[464 | 882.34] loss=2.42 avg=2.59\n",
            "[465 | 884.02] loss=2.33 avg=2.59\n",
            "[466 | 885.68] loss=2.48 avg=2.59\n",
            "[467 | 887.36] loss=2.70 avg=2.59\n",
            "[468 | 889.03] loss=2.84 avg=2.59\n",
            "[469 | 890.70] loss=2.62 avg=2.59\n",
            "[470 | 892.37] loss=2.63 avg=2.59\n",
            "[471 | 894.05] loss=2.32 avg=2.59\n",
            "[472 | 895.72] loss=2.44 avg=2.59\n",
            "[473 | 897.40] loss=2.58 avg=2.59\n",
            "[474 | 899.08] loss=2.21 avg=2.58\n",
            "[475 | 900.74] loss=2.40 avg=2.58\n",
            "[476 | 902.41] loss=2.54 avg=2.58\n",
            "[477 | 904.09] loss=2.34 avg=2.58\n",
            "[478 | 905.76] loss=3.03 avg=2.58\n",
            "[479 | 907.42] loss=2.51 avg=2.58\n",
            "[480 | 909.10] loss=2.60 avg=2.58\n",
            "[481 | 910.77] loss=2.55 avg=2.58\n",
            "[482 | 912.45] loss=2.49 avg=2.58\n",
            "[483 | 914.12] loss=2.59 avg=2.58\n",
            "[484 | 915.79] loss=2.90 avg=2.58\n",
            "[485 | 917.46] loss=2.38 avg=2.58\n",
            "[486 | 919.14] loss=2.58 avg=2.58\n",
            "[487 | 920.81] loss=2.55 avg=2.58\n",
            "[488 | 922.47] loss=2.96 avg=2.59\n",
            "[489 | 924.14] loss=2.44 avg=2.58\n",
            "[490 | 925.81] loss=2.15 avg=2.58\n",
            "[491 | 927.48] loss=2.92 avg=2.58\n",
            "[492 | 929.15] loss=2.58 avg=2.58\n",
            "[493 | 930.82] loss=2.72 avg=2.58\n",
            "[494 | 932.49] loss=2.79 avg=2.59\n",
            "[495 | 934.16] loss=2.46 avg=2.59\n",
            "[496 | 935.84] loss=2.21 avg=2.58\n",
            "[497 | 937.50] loss=2.76 avg=2.58\n",
            "[498 | 939.18] loss=2.59 avg=2.58\n",
            "[499 | 940.85] loss=2.54 avg=2.58\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " also still a little unclear: will it work like an electric toothbrush, or will it be more like a tooth brush?\n",
            "\n",
            "Advertisement\n",
            "\n",
            "\n",
            "In the meantime, the more reliable alternative seems to be a small plastic bowl that can hold a little bit more liquid than a conventional toothbrush. I took that up at the dentist's office. I had previously been convinced that toothbrushes should be less likely to get dirty when you brush with one than with two, but when the doctor gave me the opportunity to try my new toothbrush, I wasn't sure yet. The dentist said I could probably wash my hands with the bowl anyway. This wasn't good news. Toothbrushes, especially the kind available at health-food stores, are expensive. I had to make do with a plastic plastic spatula to scrub some of the liquid residue from my teeth.\n",
            "\n",
            "The dentist's appointment was very informative. He had to use an iPad to give me his instructions. I'm told that the iPad is not as reliable as a traditional computer. One problem with the iPad is that you sometimes have to turn it to look, and it's harder to figure out where the arrow on your tablet points. In addition, many dentists offer more advanced products that offer more options for brushing, including a small bowl that can hold as much liquid as an electric toothbrush. (The dentists suggested a bowl for me.) That was good information. But it didn't tell me nearly as much as what the dentist told me.\n",
            "\n",
            "While the dentist told me about two types of liquid brushes designed specifically for the job, neither were especially popular at this past year's show. (I was told that the bowl works great as an electric toothbrush.) Instead, the most popular liquid toothpastes are flavored with other substances such as baking soda to make them more hygienic as well as cheaper, convenient options that are typically sold at health-food stores. Some of the people I spoke with that had tried a different kind of toothpastes had said that there was a certain taste that appealed to them. Other people I spoke with didn't like that the toothpastes looked or smelled like they were made of silicone. One man told me that at a recent dentist appointment, he thought my toothbrush resembled a plastic blender.\n",
            "\n",
            "The toothbrushes that seemed best for the job, for instance, were more expensive. A plastic spatula, offered for about $10, seemed like a better choice because it didn't look like a toothbrush had been used: You could stick your brush or spatula directly into the bowl. (A plastic-and-coconut toothpaste can be found in most health foods stores at some price point.) But the spatula was only $2 and could be used in one direction only.\n",
            "\n",
            "I went to the dentist, which was not a huge surprise, because for the first time in at least a decade, I haven't had a dentist for years. This is one of those moments that you hope a dentist doesn't happen to strike up a conversation at. But that has to be the first thing you do for a dentist. And then, for the first time in years, I went for a toothbrush.\n",
            "\n",
            "I'm not the first person to complain about teeth, especially in this country. Most people assume that, if you don't have problems with your teeth, you need to have an issue with your dental care.\n",
            "\n",
            "But this is an area where the scientific community is really taking a stab, taking stock of both issues. A 2017 study published in The American Journal of Dental Medicine in an article described how dentists have moved quickly to respond to an explosion in complaints related to sugary and salty toothbrushes: More than 30 percent of dental practices in the United States said they would stop providing sugary toothbrushes immediately from 2018, The New York Times reported.\n",
            "\n",
            "I'll take that as a sign that people of all ages want more options when choosing toothbrushes.\n",
            "\n",
            "My dentist was excited that I was having a dental procedure and was glad that I opted in to a dental practice that offered all the options that they did, for a price, including toothbrushes. My appointment with my dentist took just a few minutes. I was in the office, and my dentist was ready to begin.\n",
            "\n",
            "The first thing the doctor did was tell me what a toothbrush is. \"A toothbrush … is this …,\" said the doctor gesturing at the toothbrush. I didn't have a clear idea of what to make of that statement. Do I have a toothbrush of my own? Did she just assume I was on my own because I didn't have a choice? At first, I kind of knew that, because I don't ever have one, and it feels like every time I look in the mirror, my teeth hurt.\n",
            "\n",
            "Advertisement\n",
            "\n",
            "\n",
            "I think about that statement all the time. How did I get to that point in my life? When people\n",
            "\n",
            "[500 | 965.10] loss=2.76 avg=2.59\n",
            "[501 | 966.77] loss=2.73 avg=2.59\n",
            "[502 | 968.44] loss=2.31 avg=2.58\n",
            "[503 | 970.11] loss=2.41 avg=2.58\n",
            "[504 | 971.78] loss=2.83 avg=2.58\n",
            "[505 | 973.45] loss=2.66 avg=2.59\n",
            "[506 | 975.11] loss=2.56 avg=2.58\n",
            "[507 | 976.78] loss=2.33 avg=2.58\n",
            "[508 | 978.44] loss=2.88 avg=2.59\n",
            "[509 | 980.11] loss=2.42 avg=2.58\n",
            "[510 | 981.78] loss=2.65 avg=2.58\n",
            "[511 | 983.45] loss=2.40 avg=2.58\n",
            "[512 | 985.12] loss=3.10 avg=2.59\n",
            "[513 | 986.79] loss=2.18 avg=2.58\n",
            "[514 | 988.46] loss=2.61 avg=2.58\n",
            "[515 | 990.13] loss=2.25 avg=2.58\n",
            "[516 | 991.81] loss=2.63 avg=2.58\n",
            "[517 | 993.48] loss=2.68 avg=2.58\n",
            "[518 | 995.15] loss=2.73 avg=2.58\n",
            "[519 | 996.82] loss=3.01 avg=2.59\n",
            "[520 | 998.49] loss=2.51 avg=2.59\n",
            "[521 | 1000.16] loss=2.58 avg=2.59\n",
            "[522 | 1001.83] loss=2.50 avg=2.59\n",
            "[523 | 1003.50] loss=2.51 avg=2.59\n",
            "[524 | 1005.17] loss=2.57 avg=2.59\n",
            "[525 | 1006.84] loss=2.23 avg=2.58\n",
            "[526 | 1008.52] loss=2.54 avg=2.58\n",
            "[527 | 1010.19] loss=2.44 avg=2.58\n",
            "[528 | 1011.86] loss=2.66 avg=2.58\n",
            "[529 | 1013.53] loss=2.64 avg=2.58\n",
            "[530 | 1015.21] loss=2.85 avg=2.58\n",
            "[531 | 1016.88] loss=2.75 avg=2.59\n",
            "[532 | 1018.55] loss=2.41 avg=2.58\n",
            "[533 | 1020.22] loss=2.40 avg=2.58\n",
            "[534 | 1021.90] loss=2.37 avg=2.58\n",
            "[535 | 1023.58] loss=2.63 avg=2.58\n",
            "[536 | 1025.25] loss=2.66 avg=2.58\n",
            "[537 | 1026.92] loss=2.49 avg=2.58\n",
            "[538 | 1028.59] loss=2.74 avg=2.58\n",
            "[539 | 1030.25] loss=2.61 avg=2.58\n",
            "[540 | 1031.93] loss=2.61 avg=2.58\n",
            "[541 | 1033.61] loss=2.46 avg=2.58\n",
            "[542 | 1035.28] loss=2.71 avg=2.58\n",
            "[543 | 1036.95] loss=2.33 avg=2.58\n",
            "[544 | 1038.62] loss=2.60 avg=2.58\n",
            "[545 | 1040.29] loss=2.90 avg=2.58\n",
            "[546 | 1041.95] loss=2.37 avg=2.58\n",
            "[547 | 1043.63] loss=2.53 avg=2.58\n",
            "[548 | 1045.32] loss=2.34 avg=2.58\n",
            "[549 | 1046.99] loss=2.60 avg=2.58\n",
            "[550 | 1048.68] loss=2.73 avg=2.58\n",
            "[551 | 1050.37] loss=2.47 avg=2.58\n",
            "[552 | 1052.04] loss=2.67 avg=2.58\n",
            "[553 | 1053.71] loss=2.69 avg=2.58\n",
            "[554 | 1055.40] loss=2.25 avg=2.58\n",
            "[555 | 1057.08] loss=2.33 avg=2.58\n",
            "[556 | 1058.77] loss=2.67 avg=2.58\n",
            "[557 | 1060.45] loss=2.22 avg=2.57\n",
            "[558 | 1062.14] loss=2.72 avg=2.57\n",
            "[559 | 1063.83] loss=2.69 avg=2.58\n",
            "[560 | 1065.52] loss=2.13 avg=2.57\n",
            "[561 | 1067.19] loss=2.02 avg=2.57\n",
            "[562 | 1068.89] loss=2.43 avg=2.56\n",
            "[563 | 1070.57] loss=2.63 avg=2.56\n",
            "[564 | 1072.25] loss=2.23 avg=2.56\n",
            "[565 | 1073.94] loss=2.69 avg=2.56\n",
            "[566 | 1075.62] loss=2.97 avg=2.57\n",
            "[567 | 1077.30] loss=2.09 avg=2.56\n",
            "[568 | 1078.97] loss=2.68 avg=2.56\n",
            "[569 | 1080.65] loss=2.55 avg=2.56\n",
            "[570 | 1082.32] loss=2.48 avg=2.56\n",
            "[571 | 1083.99] loss=2.11 avg=2.56\n",
            "[572 | 1085.67] loss=2.57 avg=2.56\n",
            "[573 | 1087.34] loss=2.45 avg=2.56\n",
            "[574 | 1089.01] loss=2.63 avg=2.56\n",
            "[575 | 1090.68] loss=2.41 avg=2.56\n",
            "[576 | 1092.35] loss=2.79 avg=2.56\n",
            "[577 | 1094.02] loss=2.06 avg=2.55\n",
            "[578 | 1095.69] loss=2.24 avg=2.55\n",
            "[579 | 1097.36] loss=2.28 avg=2.55\n",
            "[580 | 1099.03] loss=2.73 avg=2.55\n",
            "[581 | 1100.70] loss=2.72 avg=2.55\n",
            "[582 | 1102.37] loss=2.37 avg=2.55\n",
            "[583 | 1104.05] loss=2.83 avg=2.55\n",
            "[584 | 1105.72] loss=2.44 avg=2.55\n",
            "[585 | 1107.39] loss=2.14 avg=2.55\n",
            "[586 | 1109.06] loss=2.66 avg=2.55\n",
            "[587 | 1110.72] loss=2.40 avg=2.55\n",
            "[588 | 1112.39] loss=2.47 avg=2.55\n",
            "[589 | 1114.06] loss=2.86 avg=2.55\n",
            "[590 | 1115.73] loss=2.55 avg=2.55\n",
            "[591 | 1117.40] loss=2.27 avg=2.55\n",
            "[592 | 1119.08] loss=2.74 avg=2.55\n",
            "[593 | 1120.76] loss=2.70 avg=2.55\n",
            "[594 | 1122.44] loss=2.57 avg=2.55\n",
            "[595 | 1124.11] loss=2.27 avg=2.55\n",
            "[596 | 1125.80] loss=2.33 avg=2.54\n",
            "[597 | 1127.47] loss=2.77 avg=2.55\n",
            "[598 | 1129.14] loss=2.51 avg=2.55\n",
            "[599 | 1130.81] loss=2.78 avg=2.55\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            ". The same goes for the ones that I know of that are available outside the United States. For most people, these might be their preferred brand names. For more than 20 years, I, too,'ve used a family of mouthwash products that I think were first made by using ingredients from different species. But these ingredients—both shampoo and conditioner—have evolved in this country to work better together when used in tandem.\n",
            "\n",
            "If you're interested in the difference between shampoo and conditioner, you can also read a quick primer on the difference here.\n",
            "\n",
            "If you had to take a guess as to which might be the more appropriate name, it sounds like shampoo may be the one you'd use more often; conditioner is pretty much the name of the game in the health department. The one thing you don't want in your handbag is shampoo.\n",
            "\n",
            "\n",
            "This post appears courtesy of our partners at Healthline.\n",
            "\n",
            "\n",
            "** A previous version of this post mischaracterized the term \"biofuels,\" which is a process where biofuels are extracted from plant matter. The term was actually intended to reflect the process.\n",
            "\n",
            "\n",
            "<|endoftext|>This post looks back a little bit to how my family was affected by the 2012 Sandy Hook Elementary School shooting. That was a couple years ago, and we moved back to New Jersey. We didn't have any children, but our husband and I still knew how to handle the sudden change in our apartment.\n",
            "\n",
            "I didn't know what to do. I was a working mother of a 6-week-old and a new job, and I was tired of living in that apartment. I figured my mom would make an easy transition to a different apartment, so we called it quits. Then my husband went to a different office, went into work, and was told he was no longer welcome there. This was not something he expected.\n",
            "\n",
            "So when he told me his new boss was going to tell him he could leave the building and never come back, I was a little nervous. I thought he was just going to miss my usual apartment and never come to visit me. But then he showed up, looked around the place, and saw how much life he was losing on the weekends and in the mornings. He was really looking forward to going back out here one day.\n",
            "\n",
            "Finally, it happened. He told me that he was moving in next door, and I started to accept his decision. After all, I'm a good neighbor; I'm an old grandmother, and I have had this whole apartment thing for 30 years. The apartment got bigger, and it took us longer to get it cleaned, so we had to find another place.\n",
            "\n",
            "Then, two weeks ago, my husband moved in next door, and I moved in. Two weeks ago, I saw one of his new coworkers. Two weeks ago, I called the landlord—a lawyer's office, apparently—and called the police. Two weeks ago, I moved into a new home.\n",
            "\n",
            "Our neighborhood has been shaken, our apartment complex closed, and so have our neighbors. We couldn't sleep at night, even when we wanted to.\n",
            "\n",
            "The first time someone asked me about Sandy Hook, I just shrugged my shoulders and said I didn't know, and there are some pretty tough questions you need to ask. They didn't sound like the kids I watched before the 2012 shooting.\n",
            "\n",
            "My son, now 18, is in sixth grade. I was a year ahead of him, but the other kids just got there earlier. We used the same daycare center. I remember we got the same teacher during his first year, but she made sure he was in line for his first year of preschool.\n",
            "\n",
            "I miss seeing him, and he makes jokes, but he always knows what to do or say. I know he'll go far. He just won't be that kid, you know?\n",
            "\n",
            "My husband and I moved to my daughter-in-law's place in 2016. My daughter-in-law moved to her own apartment, and we live together. She and my son have had their own apartment, and while they haven't had a neighbor since they moved, they still come over for birthday parties, and my son comes over for the anniversary of our daughter-in-law's death.\n",
            "\n",
            "A few weeks ago, they were talking about moving in together, and my daughter-in-law said she was thinking the same thing and wanted to see it through. She and she are very close, and it's hard for her not to have a neighbor, but she has to deal with a lot of stuff herself.\n",
            "\n",
            "The other kids always try to do things together. They go to school together. My daughter-in-law can usually be seen by only her friends and relatives, and they always seem to be with each other. I always find people funny. She has tons of videos, so she enjoys watching movies with them, and we talk about what we had for\n",
            "\n",
            "[600 | 1155.50] loss=2.49 avg=2.55\n",
            "[601 | 1157.17] loss=2.22 avg=2.55\n",
            "[602 | 1158.85] loss=2.48 avg=2.54\n",
            "[603 | 1160.52] loss=2.30 avg=2.54\n",
            "[604 | 1162.19] loss=2.52 avg=2.54\n",
            "[605 | 1163.86] loss=2.36 avg=2.54\n",
            "[606 | 1165.52] loss=2.78 avg=2.54\n",
            "[607 | 1167.20] loss=2.67 avg=2.54\n",
            "[608 | 1168.87] loss=2.55 avg=2.54\n",
            "[609 | 1170.54] loss=2.69 avg=2.55\n",
            "[610 | 1172.20] loss=2.94 avg=2.55\n",
            "[611 | 1173.87] loss=2.69 avg=2.55\n",
            "[612 | 1175.54] loss=2.37 avg=2.55\n",
            "[613 | 1177.22] loss=2.68 avg=2.55\n",
            "[614 | 1178.88] loss=2.28 avg=2.55\n",
            "[615 | 1180.55] loss=2.28 avg=2.54\n",
            "[616 | 1182.22] loss=2.19 avg=2.54\n",
            "[617 | 1183.90] loss=2.82 avg=2.54\n",
            "[618 | 1185.56] loss=2.01 avg=2.54\n",
            "[619 | 1187.23] loss=2.52 avg=2.54\n",
            "[620 | 1188.91] loss=2.35 avg=2.54\n",
            "[621 | 1190.58] loss=2.58 avg=2.54\n",
            "[622 | 1192.26] loss=2.37 avg=2.54\n",
            "[623 | 1193.94] loss=2.58 avg=2.54\n",
            "[624 | 1195.63] loss=2.69 avg=2.54\n",
            "[625 | 1197.30] loss=2.31 avg=2.54\n",
            "[626 | 1198.99] loss=2.17 avg=2.53\n",
            "[627 | 1200.66] loss=2.37 avg=2.53\n",
            "[628 | 1202.35] loss=2.10 avg=2.53\n",
            "[629 | 1204.03] loss=2.83 avg=2.53\n",
            "[630 | 1205.72] loss=2.54 avg=2.53\n",
            "[631 | 1207.41] loss=2.27 avg=2.53\n",
            "[632 | 1209.10] loss=2.77 avg=2.53\n",
            "[633 | 1210.78] loss=2.19 avg=2.53\n",
            "[634 | 1212.48] loss=2.40 avg=2.52\n",
            "[635 | 1214.17] loss=2.50 avg=2.52\n",
            "[636 | 1215.84] loss=2.43 avg=2.52\n",
            "[637 | 1217.52] loss=2.75 avg=2.52\n",
            "[638 | 1219.21] loss=2.09 avg=2.52\n",
            "[639 | 1220.90] loss=2.08 avg=2.52\n",
            "[640 | 1222.57] loss=2.33 avg=2.51\n",
            "[641 | 1224.25] loss=2.44 avg=2.51\n",
            "[642 | 1225.93] loss=2.53 avg=2.51\n",
            "[643 | 1227.62] loss=2.16 avg=2.51\n",
            "[644 | 1229.30] loss=3.05 avg=2.52\n",
            "[645 | 1230.99] loss=2.85 avg=2.52\n",
            "[646 | 1232.68] loss=2.41 avg=2.52\n",
            "[647 | 1234.37] loss=3.22 avg=2.52\n",
            "[648 | 1236.05] loss=2.44 avg=2.52\n",
            "[649 | 1237.75] loss=2.18 avg=2.52\n",
            "[650 | 1239.43] loss=2.12 avg=2.52\n",
            "[651 | 1241.11] loss=2.25 avg=2.51\n",
            "[652 | 1242.80] loss=2.08 avg=2.51\n",
            "[653 | 1244.49] loss=2.49 avg=2.51\n",
            "[654 | 1246.18] loss=2.71 avg=2.51\n",
            "[655 | 1247.85] loss=1.94 avg=2.51\n",
            "[656 | 1249.54] loss=2.24 avg=2.50\n",
            "[657 | 1251.23] loss=2.52 avg=2.50\n",
            "[658 | 1252.92] loss=2.41 avg=2.50\n",
            "[659 | 1254.60] loss=2.62 avg=2.50\n",
            "[660 | 1256.29] loss=2.65 avg=2.50\n",
            "[661 | 1257.98] loss=2.83 avg=2.51\n",
            "[662 | 1259.65] loss=2.69 avg=2.51\n",
            "[663 | 1261.34] loss=1.82 avg=2.50\n",
            "[664 | 1263.03] loss=2.12 avg=2.50\n",
            "[665 | 1264.71] loss=2.10 avg=2.50\n",
            "[666 | 1266.39] loss=2.12 avg=2.49\n",
            "[667 | 1268.06] loss=2.49 avg=2.49\n",
            "[668 | 1269.75] loss=2.27 avg=2.49\n",
            "[669 | 1271.42] loss=2.51 avg=2.49\n",
            "[670 | 1273.10] loss=2.32 avg=2.49\n",
            "[671 | 1274.78] loss=2.67 avg=2.49\n",
            "[672 | 1276.46] loss=2.50 avg=2.49\n",
            "[673 | 1278.14] loss=2.53 avg=2.49\n",
            "[674 | 1279.83] loss=2.19 avg=2.49\n",
            "[675 | 1281.51] loss=2.40 avg=2.49\n",
            "[676 | 1283.18] loss=1.91 avg=2.48\n",
            "[677 | 1284.85] loss=2.95 avg=2.48\n",
            "[678 | 1286.53] loss=1.93 avg=2.48\n",
            "[679 | 1288.20] loss=2.08 avg=2.48\n",
            "[680 | 1289.87] loss=2.23 avg=2.47\n",
            "[681 | 1291.55] loss=2.36 avg=2.47\n",
            "[682 | 1293.22] loss=2.83 avg=2.48\n",
            "[683 | 1294.89] loss=2.34 avg=2.47\n",
            "[684 | 1296.56] loss=2.50 avg=2.47\n",
            "[685 | 1298.25] loss=2.71 avg=2.48\n",
            "[686 | 1299.92] loss=2.40 avg=2.48\n",
            "[687 | 1301.59] loss=2.41 avg=2.48\n",
            "[688 | 1303.26] loss=2.41 avg=2.47\n",
            "[689 | 1304.93] loss=2.48 avg=2.47\n",
            "[690 | 1306.60] loss=2.32 avg=2.47\n",
            "[691 | 1308.27] loss=2.45 avg=2.47\n",
            "[692 | 1309.94] loss=2.98 avg=2.48\n",
            "[693 | 1311.61] loss=2.33 avg=2.48\n",
            "[694 | 1313.28] loss=2.19 avg=2.47\n",
            "[695 | 1314.95] loss=2.37 avg=2.47\n",
            "[696 | 1316.62] loss=2.98 avg=2.48\n",
            "[697 | 1318.29] loss=2.22 avg=2.47\n",
            "[698 | 1319.96] loss=2.04 avg=2.47\n",
            "[699 | 1321.63] loss=2.89 avg=2.47\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "�\n",
            "\n",
            "BELLEVUE--A young mother in the United States has been told to cover up her vagina after being threatened with an STD.\n",
            "\n",
            "In a letter, Jessica Schipper, from the state of Washington, was given a three-day stay of deportation after allegedly posting a picture on Instagram of bare vagina on July 2, according to the Washington Post.\n",
            "\n",
            "Schipper reportedly was asked to remove the tampon she was to use during her stay at a hospital. She also was asked to remove her tampon, which the letter stated caused her \"continued distress.\"\n",
            "\n",
            "The post, which has since been taken down, appeared to describe a hospital where pregnant women were being tested and treated for sexually transmitted diseases, according to the Post.\n",
            "\n",
            "Authorities said they received the letter in response to Schipper's Instagram post of the location of a Planned Parenthood medical facility in Seattle, Washington.\n",
            "\n",
            "Schipper, now 21 weeks pregnant with her son, has been living in Oregon and says she was asked to leave the country.\n",
            "\n",
            "\"It was so upsetting—it was just so shocking,\" she told the Post. \"I was like, 'Yeah, I might end up in prison for this. God no.' It was just not fair, honestly.\"\n",
            "\n",
            "Officials at the hospital Schipper is visiting said they were unable to comment on the matter.\n",
            "\n",
            "Although Schipper said her visit was voluntary, her situation raised important questions about sexual consent and health care access in the United States. According to the Huffington Post, the decision to withhold the medical exam could be thought of as an act of discrimination because Schipper is an undocumented immigrant.\n",
            "\n",
            "According to the letter from the American Civil Liberties Union of Washington, Schipper was placed in a detention facility for six months and deported to her home country of the Philippines. The letter cites Schipper as an example of someone who has \"failing medical exams while incarcerated and not being deported, or whose doctor is not performing medical exams after they've been incarcerated.\"\n",
            "\n",
            "The letter further stated that Schipper was charged with unlawful entry and refused her rights under the Immigration and Nationality Act, a federal law that protects U.S. citizens and legal permanent residents, and has been charged with criminal trespass and violated her detention.\n",
            "\n",
            "This incident, and a wave of similar cases, has prompted federal and state officials to take a closer look at what the federal government deems fair treatment of undocumented immigrants. On Friday, the Department of Homeland Security released a memo that outlines some recommendations for immigration law and asks states to implement them. Among its recommendations: that hospitals be required to obtain consent from women before performing health screenings that could lead to STDs and other pregnancy-related complications.\n",
            "\n",
            "Health officials have struggled to figure out how to provide women with adequate medical care and protect them from potentially dangerous infections or complications if they come into direct contact with medical professionals or patients who aren't covered by health insurance. A study published last fall showed that nearly one third of undocumented residents report getting their first prenatal examination at a hospital — nearly three times as many as undocumented patients who reported receiving the same examination in the previous year. It's unclear how many women have received other kinds of prenatal appointments at health centers.\n",
            "\n",
            "The American College of Obstetricians and Gynecologists wrote in January: \"The medical profession faces a complex problem regarding the medical liability that arises from the unlawful entry of undocumented individuals into the United States. … Doctors are generally not considered agents of the State in these circumstances and are therefore not required to conduct “medical examinations” of women for immigration purposes.\n",
            "\n",
            "In cases like that of Schipper, the hospital where the checkup is being performed is also not in compliance with federal or state law. The California Department of Public Health said Thursday that it was monitoring the situation with the hospital to make sure it complies with all requirements of the state statute. It was not clear whether that was a result of the federal government responding to the Schippers' post or if the state was simply following the federal recommendations.\n",
            "\n",
            "In this case, the hospital has been told to follow the standard of care that hospitals are expected to follow. Other hospitals have been told to provide \"no examination,\" which would mean a clean bill of health only.\n",
            "\n",
            "\n",
            "\n",
            "Federal agents descended on a home in the rural area of Pottawattamie County, Pennsylvania, on Tuesday to investigate a report of an assault by a woman on her 7-year-old son.\n",
            "\n",
            "According to the Washington County Sheriff's Office, the Sheriff's Office and the Drug Enforcement Administration entered the home last week and found two female victims suffering from \"an apparent assault.\" One was in critical condition, and the other was listed in stable condition.\n",
            "\n",
            "The sheriff's office and DEA agents arrived at the home after receiving a phone call from a woman who said she had observed her 14-year-old son being restrained by the child's grandmother. The grandmother alerted police, and agents arrived, as well as the sheriff,\n",
            "\n",
            "[700 | 1346.54] loss=2.32 avg=2.47\n",
            "[701 | 1348.21] loss=2.25 avg=2.47\n",
            "[702 | 1349.88] loss=2.82 avg=2.47\n",
            "[703 | 1351.57] loss=2.08 avg=2.47\n",
            "[704 | 1353.24] loss=2.65 avg=2.47\n",
            "[705 | 1354.92] loss=3.27 avg=2.48\n",
            "[706 | 1356.59] loss=2.68 avg=2.48\n",
            "[707 | 1358.26] loss=2.25 avg=2.48\n",
            "[708 | 1359.94] loss=2.56 avg=2.48\n",
            "[709 | 1361.61] loss=2.42 avg=2.48\n",
            "[710 | 1363.27] loss=2.68 avg=2.48\n",
            "[711 | 1364.94] loss=2.42 avg=2.48\n",
            "[712 | 1366.61] loss=2.34 avg=2.48\n",
            "[713 | 1368.28] loss=2.59 avg=2.48\n",
            "[714 | 1369.95] loss=2.10 avg=2.48\n",
            "[715 | 1371.62] loss=2.90 avg=2.48\n",
            "[716 | 1373.29] loss=2.75 avg=2.48\n",
            "[717 | 1374.96] loss=2.03 avg=2.48\n",
            "[718 | 1376.63] loss=2.25 avg=2.48\n",
            "[719 | 1378.30] loss=2.45 avg=2.48\n",
            "[720 | 1379.97] loss=2.09 avg=2.47\n",
            "[721 | 1381.63] loss=2.06 avg=2.47\n",
            "[722 | 1383.30] loss=2.95 avg=2.47\n",
            "[723 | 1384.96] loss=2.36 avg=2.47\n",
            "[724 | 1386.64] loss=2.99 avg=2.48\n",
            "[725 | 1388.32] loss=2.16 avg=2.47\n",
            "[726 | 1389.99] loss=2.19 avg=2.47\n",
            "[727 | 1391.66] loss=2.25 avg=2.47\n",
            "[728 | 1393.33] loss=2.28 avg=2.47\n",
            "[729 | 1395.00] loss=2.33 avg=2.47\n",
            "[730 | 1396.67] loss=1.83 avg=2.46\n",
            "[731 | 1398.34] loss=2.50 avg=2.46\n",
            "[732 | 1400.01] loss=2.79 avg=2.46\n",
            "[733 | 1401.68] loss=2.20 avg=2.46\n",
            "[734 | 1403.35] loss=2.50 avg=2.46\n",
            "[735 | 1405.02] loss=2.58 avg=2.46\n",
            "[736 | 1406.70] loss=1.90 avg=2.46\n",
            "[737 | 1408.37] loss=2.59 avg=2.46\n",
            "[738 | 1410.04] loss=2.62 avg=2.46\n",
            "[739 | 1411.71] loss=1.80 avg=2.45\n",
            "[740 | 1413.39] loss=2.09 avg=2.45\n",
            "[741 | 1415.06] loss=2.23 avg=2.45\n",
            "[742 | 1416.73] loss=2.54 avg=2.45\n",
            "[743 | 1418.40] loss=2.25 avg=2.45\n",
            "[744 | 1420.08] loss=2.33 avg=2.45\n",
            "[745 | 1421.75] loss=2.67 avg=2.45\n",
            "[746 | 1423.42] loss=2.61 avg=2.45\n",
            "[747 | 1425.10] loss=2.29 avg=2.45\n",
            "[748 | 1426.77] loss=2.10 avg=2.44\n",
            "[749 | 1428.44] loss=2.25 avg=2.44\n",
            "[750 | 1430.11] loss=2.54 avg=2.44\n",
            "[751 | 1431.78] loss=2.39 avg=2.44\n",
            "[752 | 1433.45] loss=2.51 avg=2.44\n",
            "[753 | 1435.12] loss=2.80 avg=2.45\n",
            "[754 | 1436.79] loss=2.58 avg=2.45\n",
            "[755 | 1438.46] loss=2.13 avg=2.45\n",
            "[756 | 1440.13] loss=2.42 avg=2.44\n",
            "[757 | 1441.80] loss=2.36 avg=2.44\n",
            "[758 | 1443.48] loss=2.45 avg=2.44\n",
            "[759 | 1445.15] loss=2.47 avg=2.44\n",
            "[760 | 1446.82] loss=2.49 avg=2.44\n",
            "[761 | 1448.49] loss=2.76 avg=2.45\n",
            "[762 | 1450.16] loss=2.20 avg=2.45\n",
            "[763 | 1451.83] loss=2.45 avg=2.45\n",
            "[764 | 1453.50] loss=2.48 avg=2.45\n",
            "[765 | 1455.17] loss=2.29 avg=2.44\n",
            "[766 | 1456.83] loss=2.05 avg=2.44\n",
            "[767 | 1458.50] loss=2.03 avg=2.44\n",
            "[768 | 1460.17] loss=2.14 avg=2.43\n",
            "[769 | 1461.84] loss=2.69 avg=2.44\n",
            "[770 | 1463.51] loss=2.29 avg=2.43\n",
            "[771 | 1465.18] loss=2.48 avg=2.43\n",
            "[772 | 1466.85] loss=1.99 avg=2.43\n",
            "[773 | 1468.53] loss=2.08 avg=2.43\n",
            "[774 | 1470.20] loss=2.88 avg=2.43\n",
            "[775 | 1471.87] loss=2.65 avg=2.43\n",
            "[776 | 1473.55] loss=2.78 avg=2.44\n",
            "[777 | 1475.23] loss=2.55 avg=2.44\n",
            "[778 | 1476.90] loss=2.57 avg=2.44\n",
            "[779 | 1478.57] loss=2.89 avg=2.44\n",
            "[780 | 1480.24] loss=2.44 avg=2.44\n",
            "[781 | 1481.92] loss=2.47 avg=2.44\n",
            "[782 | 1483.59] loss=2.16 avg=2.44\n",
            "[783 | 1485.26] loss=2.81 avg=2.44\n",
            "[784 | 1486.94] loss=2.19 avg=2.44\n",
            "[785 | 1488.61] loss=3.02 avg=2.45\n",
            "[786 | 1490.29] loss=2.30 avg=2.45\n",
            "[787 | 1491.96] loss=2.60 avg=2.45\n",
            "[788 | 1493.64] loss=2.47 avg=2.45\n",
            "[789 | 1495.32] loss=2.32 avg=2.45\n",
            "[790 | 1497.00] loss=2.12 avg=2.44\n",
            "[791 | 1498.67] loss=2.41 avg=2.44\n",
            "[792 | 1500.34] loss=2.37 avg=2.44\n",
            "[793 | 1502.02] loss=1.96 avg=2.44\n",
            "[794 | 1503.69] loss=3.07 avg=2.44\n",
            "[795 | 1505.37] loss=2.50 avg=2.44\n",
            "[796 | 1507.05] loss=2.15 avg=2.44\n",
            "[797 | 1508.74] loss=2.14 avg=2.44\n",
            "[798 | 1510.41] loss=2.29 avg=2.44\n",
            "[799 | 1512.09] loss=2.61 avg=2.44\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " Nikes. They look like the sleek, sporty design of the iPhone 6S Plus. In fact, only four cars sold today are Nissan Leafs—the two models with the 2018 aerodynamics. A fifth car was recently sold as a prelaunch event for the Model 3, which Nissan says will arrive in the car-buying range of 2019 and would have an expected range of between 1,000 and 1,500 miles. A Leaf can be had for as little as $35,000 if you are lucky.\n",
            "\n",
            "While the vehicles look so exotic to an average consumer, though, they are essentially fuel-efficient taxis, for the most part. To a car salesman, a car is simply a medium-sized, low-mileage vehicle. To Nissan and others, however, many consumers view cars as a luxury. For this reason, buyers tend to have varying levels of familiarity with the components of a car. The Nissan Leaf is a well-rounded vehicle, but many buyers seem to want something slightly different, and even though there aren't any easy answers, Nissan appears to be planning to address this.\n",
            "\n",
            "When I asked if Nissan was considering introducing a battery option for the 2018 Leaf, Jassim said the company is \"still in the process\" of developing the technology. The company has also invested significantly in developing a range of different electric-vehicle charging and hydrogen-powered vehicles, each based on its own battery technology. But so far, Nissan and several other automakers are largely focused on the smaller, plug-in-hybrid vehicles: the Bolt EV and the C45.\n",
            "\n",
            "As I learned from my time with its early models, the 2016 Nissan Leaf is a car that seems to get lost in the shuffle of American life. To many drivers unfamiliar with this generation of EVs, the Leaf is more of a plug-in car. But the Leaf isn't all about the cars. It's about a social and environmental problem—the rise of mass transit, mass transportation options that aren;t what most people want when they want to drive. The Leaf does not offer many of the convenience options that fuel-efficient light rail, rapid transit, or even more expensive carpooling offer. Instead, the Leaf works best when it becomes a bridge between cars and people, between car and city, between commuting and leisure—at which point, Nissan seems to say, it can be replaced by something more efficient, more luxurious.\n",
            "\n",
            "That something, I could find no indication of. While there is a distinct gap between the Leaf and its electric-vehicle competitors when it comes to the electric-car technology that underlies its electric structure, Nissan is clearly working on a plan for a hybrid electric-vehicle that will bring people in from the road to a carpool or carpool’s share station. According to Jassim, Nissan has a goal of having every Leaf on the road be “as good as or better than a plug-in or plug-plus hybrid.” What that entails is unclear. “At the moment we don’t know—that’s why the development costs have been so high,” Jassim said, referring to the $70,000 that it takes to develop a new electric car.\n",
            "\n",
            "Tesla’s electric Model S is currently “the best-selling electric car in the world,” and according to Edmunds.com, it’s still the best-selling car for electric cars in the United States. Tesla’s sales of its Model S sedans and the Model X luxury crossover have been soaring in recent months, and the company has said for months that it intends to offer its drivers “free electricity” and “free charging” in cities. On the road, Tesla’s Model S electric cars can go from “first to 100 percent in less than an hour” of charging, the company says. And as Jassim, the Nissan Leaf’s owner, pointed out, the battery capacity of the Leaf is only “40 kilowatt-hours.”\n",
            "\n",
            "I asked Jassim whether Nissan had any plans to sell any Leaf cars outside of the United States. “We are still in the process of developing a range of options for the 2018 Leaf,” he said. “We could have it in several other countries,” in other words. “But I think the main market for us is in the United States.”\n",
            "\n",
            "That point is echoed by Nissan's chief sales agent, Dan Atherton, in a statement provided to me by the company: “The primary market for the 2018 Leaf is the United States, including California.”\n",
            "\n",
            "When I asked Nissan spokesman Joe Agyeman whether Nissan planned to sell the Leaf in more than just the United States, Agyeman said, “We do not have any product announcements that indicate additional markets pending launch.”\n",
            "\n",
            "\n",
            "\n",
            "[800 | 1536.84] loss=3.03 avg=2.44\n",
            "[801 | 1538.50] loss=2.56 avg=2.45\n",
            "[802 | 1540.18] loss=2.34 avg=2.45\n",
            "[803 | 1541.85] loss=2.27 avg=2.44\n",
            "[804 | 1543.50] loss=2.28 avg=2.44\n",
            "[805 | 1545.17] loss=2.04 avg=2.44\n",
            "[806 | 1546.84] loss=2.06 avg=2.43\n",
            "[807 | 1548.51] loss=2.53 avg=2.43\n",
            "[808 | 1550.17] loss=1.79 avg=2.43\n",
            "[809 | 1551.84] loss=2.07 avg=2.42\n",
            "[810 | 1553.51] loss=1.80 avg=2.42\n",
            "[811 | 1555.18] loss=2.27 avg=2.42\n",
            "[812 | 1556.85] loss=2.63 avg=2.42\n",
            "[813 | 1558.53] loss=2.35 avg=2.42\n",
            "[814 | 1560.20] loss=2.15 avg=2.42\n",
            "[815 | 1561.87] loss=2.44 avg=2.42\n",
            "[816 | 1563.54] loss=2.78 avg=2.42\n",
            "[817 | 1565.20] loss=2.50 avg=2.42\n",
            "[818 | 1566.88] loss=2.50 avg=2.42\n",
            "[819 | 1568.55] loss=1.84 avg=2.42\n",
            "[820 | 1570.22] loss=1.85 avg=2.41\n",
            "[821 | 1571.89] loss=2.40 avg=2.41\n",
            "[822 | 1573.58] loss=2.70 avg=2.41\n",
            "[823 | 1575.25] loss=2.50 avg=2.41\n",
            "[824 | 1576.92] loss=2.87 avg=2.42\n",
            "[825 | 1578.61] loss=1.97 avg=2.41\n",
            "[826 | 1580.29] loss=2.79 avg=2.42\n",
            "[827 | 1581.97] loss=2.80 avg=2.42\n",
            "[828 | 1583.67] loss=2.22 avg=2.42\n",
            "[829 | 1585.36] loss=2.60 avg=2.42\n",
            "[830 | 1587.05] loss=2.16 avg=2.42\n",
            "[831 | 1588.74] loss=1.87 avg=2.41\n",
            "[832 | 1590.43] loss=2.31 avg=2.41\n",
            "[833 | 1592.13] loss=1.95 avg=2.41\n",
            "[834 | 1593.82] loss=2.39 avg=2.41\n",
            "[835 | 1595.51] loss=2.30 avg=2.41\n",
            "[836 | 1597.19] loss=2.30 avg=2.40\n",
            "[837 | 1598.88] loss=2.26 avg=2.40\n",
            "[838 | 1600.57] loss=2.87 avg=2.41\n",
            "[839 | 1602.26] loss=2.49 avg=2.41\n",
            "[840 | 1603.95] loss=1.83 avg=2.40\n",
            "[841 | 1605.64] loss=1.82 avg=2.40\n",
            "[842 | 1607.32] loss=2.53 avg=2.40\n",
            "[843 | 1609.00] loss=2.37 avg=2.40\n",
            "[844 | 1610.67] loss=2.74 avg=2.40\n",
            "[845 | 1612.35] loss=2.10 avg=2.40\n",
            "[846 | 1614.04] loss=2.44 avg=2.40\n",
            "[847 | 1615.72] loss=2.64 avg=2.40\n",
            "[848 | 1617.40] loss=2.79 avg=2.41\n",
            "[849 | 1619.08] loss=2.46 avg=2.41\n",
            "[850 | 1620.77] loss=2.53 avg=2.41\n",
            "[851 | 1622.45] loss=2.62 avg=2.41\n",
            "[852 | 1624.13] loss=2.02 avg=2.41\n",
            "[853 | 1625.80] loss=2.29 avg=2.40\n",
            "[854 | 1627.47] loss=2.77 avg=2.41\n",
            "[855 | 1629.16] loss=1.81 avg=2.40\n",
            "[856 | 1630.84] loss=1.64 avg=2.39\n",
            "[857 | 1632.51] loss=2.20 avg=2.39\n",
            "[858 | 1634.20] loss=2.73 avg=2.40\n",
            "[859 | 1635.86] loss=2.40 avg=2.40\n",
            "[860 | 1637.54] loss=2.40 avg=2.40\n",
            "[861 | 1639.21] loss=2.52 avg=2.40\n",
            "[862 | 1640.88] loss=2.30 avg=2.40\n",
            "[863 | 1642.55] loss=2.65 avg=2.40\n",
            "[864 | 1644.24] loss=2.30 avg=2.40\n",
            "[865 | 1645.91] loss=2.38 avg=2.40\n",
            "[866 | 1647.59] loss=2.61 avg=2.40\n",
            "[867 | 1649.26] loss=2.56 avg=2.40\n",
            "[868 | 1650.94] loss=2.36 avg=2.40\n",
            "[869 | 1652.60] loss=2.28 avg=2.40\n",
            "[870 | 1654.27] loss=2.19 avg=2.40\n",
            "[871 | 1655.95] loss=2.32 avg=2.40\n",
            "[872 | 1657.62] loss=2.46 avg=2.40\n",
            "[873 | 1659.29] loss=1.91 avg=2.39\n",
            "[874 | 1660.96] loss=2.52 avg=2.39\n",
            "[875 | 1662.63] loss=2.15 avg=2.39\n",
            "[876 | 1664.30] loss=2.11 avg=2.39\n",
            "[877 | 1665.98] loss=2.03 avg=2.39\n",
            "[878 | 1667.65] loss=2.45 avg=2.39\n",
            "[879 | 1669.33] loss=2.76 avg=2.39\n",
            "[880 | 1671.00] loss=2.23 avg=2.39\n",
            "[881 | 1672.67] loss=2.65 avg=2.39\n",
            "[882 | 1674.34] loss=2.61 avg=2.39\n",
            "[883 | 1676.01] loss=2.10 avg=2.39\n",
            "[884 | 1677.68] loss=2.34 avg=2.39\n",
            "[885 | 1679.35] loss=2.09 avg=2.39\n",
            "[886 | 1681.02] loss=2.17 avg=2.38\n",
            "[887 | 1682.69] loss=2.59 avg=2.39\n",
            "[888 | 1684.36] loss=2.23 avg=2.38\n",
            "[889 | 1686.02] loss=2.21 avg=2.38\n",
            "[890 | 1687.69] loss=2.79 avg=2.39\n",
            "[891 | 1689.36] loss=2.33 avg=2.39\n",
            "[892 | 1691.04] loss=2.57 avg=2.39\n",
            "[893 | 1692.71] loss=2.30 avg=2.39\n",
            "[894 | 1694.38] loss=1.86 avg=2.38\n",
            "[895 | 1696.06] loss=2.61 avg=2.38\n",
            "[896 | 1697.73] loss=2.21 avg=2.38\n",
            "[897 | 1699.39] loss=3.18 avg=2.39\n",
            "[898 | 1701.07] loss=2.16 avg=2.39\n",
            "[899 | 1702.74] loss=2.07 avg=2.39\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            ". Many of them—some as young as four or five years old—will then face a new and potentially life-threatening problem.\n",
            "\n",
            "According to Dr. Mark Goldsmith, a pediatrician who has treated many of the children and their parents who have come forward as part of this lawsuit, the syndrome is a neurodevelopmental disorder called POTS, in which the body inflates, with no known causes. “‘I’m sure a lot of them were just taking a pain pill and their body thought it’d take them anywhere,” says Dr. Goldsmith, a professor of psychiatry and pediatrics at New York University Langone Health.\n",
            "\n",
            "For some, POTS’s severity can become severe enough to require the use of an oxygen tank or defibrillator; for those who don’t respond to traditional interventions, Goldsmith suggests they become “anatomically disabled,” unable to breathe through their nose or mouth—a process called cystotomy. When this happens, the airway is pushed inward and blood flow to the brain is cut off. When that happens, many children suffer permanent brain damage.\n",
            "\n",
            "By contrast, “nope,” says Silverman. When all else fails, he’s still fine, he says. “I’m fine, I can still drive,” he says during an interview with The New York Times. “I’m fine,” he adds, when his doctor says he has brain surgery.\n",
            "\n",
            "\n",
            "\n",
            "In 2013, I was working as an independent consultant for a pharmaceutical company, and I attended meetings at a hotel conference room. I had never before met someone there who had been treated for POTS. She was blond and slim, but her face was puffy, made of wrinkles and scars. Her skin, I noticed, was covered in pustules. She had to wear an oxygen mask for the rest of the trip. Her doctor at the conference room, who had also gone blind, said he didn’t know how to treat the pustules.\n",
            "\n",
            "The physician explained that the pustules were a sign of an autoimmune disease, and that if the medicine she was taking helped, she might be able to live life without them.\n",
            "\n",
            "To help people like her, the best treatment depends on their genetic makeup, Dr. Thomas Blanchard, a medical oncologist at the University of California at San Francisco, told me. The more genes for the disease associated proteins, often identified with immune system-boosting chemicals, and the more people around them have the diseases, the higher the chance of getting it.\n",
            "\n",
            "That information, from his research, informs the way most people approach treatment today. The odds of getting it rise for everybody, so if you’re a Caucasian with a family history of autoimmune disease, for instance, chances are pretty good that a doctor will want to see you in a hospital. And that means having a physician’s assistant at the end of the day.\n",
            "\n",
            "Blanchard and a few other medical professionals in his field have been warning that the way in which we view autoimmune conditions has gone too far. When we think back on a patient, we’re often not so much thinking about the disease’s cause as we’re thinking about how to treat it. But the way we view our bodies is a critical element of our understanding of how we can best treat them. A doctor’s office is a gathering place for all those body parts, which is the worst thing that can happen, Blanchard says. “It is the only place where the immune system is allowed to go, the body is exposed to toxins,” he told me.\n",
            "\n",
            "These bodies are highly innervated and exquisitely sensitive. “The inflammatory response is almost too far to come from something that is very foreign,” Blanchard told me. “You can make an animal go blind with two shots of insulin.” In some patients, there are no drugs to fight the disease. So the body has to eat and excrete toxins to survive, which puts it under extreme stress, and that can result in a full-blown autoimmune reaction, which can lead to chronic inflammation and death.\n",
            "\n",
            "I learned this firsthand in 2014 when I went to the doctor for a family-owned business—a barber shop, a nail salon, a beauty supply store with makeup and nail products. Although the owner was well-liked by her customers, she couldn’t take her meds because of the illness she’d been suffering with for 10 years. Her daughter came into the office the other day to tell her this morning. “How many times have you asked to take your meds this week?” she asked her daughter. “One last time,” the daughter said, and the mother, who was still\n",
            "\n",
            "[900 | 1727.57] loss=2.19 avg=2.38\n",
            "[901 | 1729.25] loss=2.25 avg=2.38\n",
            "[902 | 1730.93] loss=2.22 avg=2.38\n",
            "[903 | 1732.60] loss=1.93 avg=2.38\n",
            "[904 | 1734.27] loss=2.22 avg=2.37\n",
            "[905 | 1735.94] loss=2.51 avg=2.38\n",
            "[906 | 1737.61] loss=2.61 avg=2.38\n",
            "[907 | 1739.28] loss=2.41 avg=2.38\n",
            "[908 | 1740.95] loss=1.89 avg=2.37\n",
            "[909 | 1742.62] loss=2.12 avg=2.37\n",
            "[910 | 1744.29] loss=1.80 avg=2.37\n",
            "[911 | 1745.96] loss=3.00 avg=2.37\n",
            "[912 | 1747.64] loss=2.34 avg=2.37\n",
            "[913 | 1749.31] loss=1.97 avg=2.37\n",
            "[914 | 1751.00] loss=1.96 avg=2.36\n",
            "[915 | 1752.68] loss=2.35 avg=2.36\n",
            "[916 | 1754.35] loss=2.26 avg=2.36\n",
            "[917 | 1756.05] loss=1.85 avg=2.36\n",
            "[918 | 1757.72] loss=2.66 avg=2.36\n",
            "[919 | 1759.39] loss=2.15 avg=2.36\n",
            "[920 | 1761.07] loss=2.78 avg=2.36\n",
            "[921 | 1762.75] loss=2.41 avg=2.36\n",
            "[922 | 1764.42] loss=1.85 avg=2.36\n",
            "[923 | 1766.09] loss=3.01 avg=2.36\n",
            "[924 | 1767.77] loss=2.34 avg=2.36\n",
            "[925 | 1769.44] loss=1.84 avg=2.36\n",
            "[926 | 1771.13] loss=1.96 avg=2.35\n",
            "[927 | 1772.80] loss=2.46 avg=2.36\n",
            "[928 | 1774.47] loss=2.83 avg=2.36\n",
            "[929 | 1776.15] loss=2.20 avg=2.36\n",
            "[930 | 1777.83] loss=2.50 avg=2.36\n",
            "[931 | 1779.52] loss=2.60 avg=2.36\n",
            "[932 | 1781.19] loss=1.98 avg=2.36\n",
            "[933 | 1782.85] loss=2.55 avg=2.36\n",
            "[934 | 1784.53] loss=2.51 avg=2.36\n",
            "[935 | 1786.20] loss=2.26 avg=2.36\n",
            "[936 | 1787.87] loss=2.71 avg=2.36\n",
            "[937 | 1789.54] loss=2.22 avg=2.36\n",
            "[938 | 1791.21] loss=2.26 avg=2.36\n",
            "[939 | 1792.87] loss=2.17 avg=2.36\n",
            "[940 | 1794.54] loss=2.46 avg=2.36\n",
            "[941 | 1796.21] loss=2.73 avg=2.36\n",
            "[942 | 1797.87] loss=2.50 avg=2.37\n",
            "[943 | 1799.55] loss=2.20 avg=2.36\n",
            "[944 | 1801.22] loss=2.57 avg=2.37\n",
            "[945 | 1802.89] loss=1.98 avg=2.36\n",
            "[946 | 1804.56] loss=2.46 avg=2.36\n",
            "[947 | 1806.23] loss=2.34 avg=2.36\n",
            "[948 | 1807.91] loss=2.69 avg=2.37\n",
            "[949 | 1809.57] loss=2.40 avg=2.37\n",
            "[950 | 1811.25] loss=2.31 avg=2.37\n",
            "[951 | 1812.92] loss=2.54 avg=2.37\n",
            "[952 | 1814.60] loss=2.28 avg=2.37\n",
            "[953 | 1816.26] loss=2.57 avg=2.37\n",
            "[954 | 1817.94] loss=2.18 avg=2.37\n",
            "[955 | 1819.61] loss=1.97 avg=2.36\n",
            "[956 | 1821.28] loss=2.76 avg=2.37\n",
            "[957 | 1822.96] loss=2.41 avg=2.37\n",
            "[958 | 1824.63] loss=1.87 avg=2.36\n",
            "[959 | 1826.30] loss=1.82 avg=2.36\n",
            "[960 | 1827.98] loss=2.16 avg=2.36\n",
            "[961 | 1829.65] loss=2.57 avg=2.36\n",
            "[962 | 1831.32] loss=2.15 avg=2.36\n",
            "[963 | 1833.00] loss=2.18 avg=2.35\n",
            "[964 | 1834.67] loss=2.63 avg=2.36\n",
            "[965 | 1836.34] loss=2.30 avg=2.36\n",
            "[966 | 1838.01] loss=3.01 avg=2.36\n",
            "[967 | 1839.69] loss=2.25 avg=2.36\n",
            "[968 | 1841.37] loss=2.42 avg=2.36\n",
            "[969 | 1843.04] loss=2.40 avg=2.36\n",
            "[970 | 1844.69] loss=2.37 avg=2.36\n",
            "[971 | 1846.37] loss=2.78 avg=2.37\n",
            "[972 | 1848.04] loss=2.35 avg=2.37\n",
            "[973 | 1849.71] loss=2.13 avg=2.36\n",
            "[974 | 1851.38] loss=1.98 avg=2.36\n",
            "[975 | 1853.06] loss=2.53 avg=2.36\n",
            "[976 | 1854.74] loss=2.05 avg=2.36\n",
            "[977 | 1856.40] loss=2.36 avg=2.36\n",
            "[978 | 1858.07] loss=1.68 avg=2.35\n",
            "[979 | 1859.75] loss=2.13 avg=2.35\n",
            "[980 | 1861.43] loss=1.66 avg=2.34\n",
            "[981 | 1863.10] loss=2.33 avg=2.34\n",
            "[982 | 1864.77] loss=2.52 avg=2.34\n",
            "[983 | 1866.45] loss=2.21 avg=2.34\n",
            "[984 | 1868.12] loss=2.09 avg=2.34\n",
            "[985 | 1869.79] loss=2.20 avg=2.34\n",
            "[986 | 1871.46] loss=1.85 avg=2.33\n",
            "[987 | 1873.13] loss=2.10 avg=2.33\n",
            "[988 | 1874.80] loss=2.46 avg=2.33\n",
            "[989 | 1876.47] loss=2.56 avg=2.34\n",
            "[990 | 1878.13] loss=2.37 avg=2.34\n",
            "[991 | 1879.80] loss=2.83 avg=2.34\n",
            "[992 | 1881.47] loss=2.34 avg=2.34\n",
            "[993 | 1883.13] loss=2.59 avg=2.34\n",
            "[994 | 1884.82] loss=2.53 avg=2.35\n",
            "[995 | 1886.49] loss=2.44 avg=2.35\n",
            "[996 | 1888.17] loss=2.61 avg=2.35\n",
            "[997 | 1889.84] loss=2.56 avg=2.35\n",
            "[998 | 1891.51] loss=1.67 avg=2.34\n",
            "[999 | 1893.19] loss=1.69 avg=2.34\n",
            "Ran 1010 steps.\n",
            "Saving checkpoint/atlantic_health_345/model-1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDv9jHF88UNz",
        "colab_type": "code",
        "outputId": "068ea857-6ff2-42c6-fbbc-2e197ad9a414",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "## gutenberg training\n",
        "!PYTHONPATH=src ./train.py --dataset /content/all_gutenberg.txt --run_name 'gutenberg_345' --model_name '345M'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0627 20:36:30.005469 140712750081920 deprecation_wrapper.py:119] From /content/gpt-2/src/model.py:147: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
            "\n",
            "W0627 20:36:30.015077 140712750081920 deprecation_wrapper.py:119] From /content/gpt-2/src/memory_saving_gradients.py:13: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n",
            "W0627 20:36:30.110814 140712750081920 deprecation_wrapper.py:119] From ./train.py:88: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "W0627 20:36:30.111193 140712750081920 deprecation_wrapper.py:119] From ./train.py:91: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "2019-06-27 20:36:30.117927: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-06-27 20:36:30.118206: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x3065100 executing computations on platform Host. Devices:\n",
            "2019-06-27 20:36:30.118244: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2019-06-27 20:36:30.120795: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\n",
            "2019-06-27 20:36:30.334810: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-27 20:36:30.335426: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x3064840 executing computations on platform CUDA. Devices:\n",
            "2019-06-27 20:36:30.335469: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2019-06-27 20:36:30.335802: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-27 20:36:30.336192: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-06-27 20:36:30.336659: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-06-27 20:36:30.338029: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-06-27 20:36:30.339389: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-06-27 20:36:30.339815: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-06-27 20:36:30.341482: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-06-27 20:36:30.342617: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-06-27 20:36:30.345991: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-06-27 20:36:30.346165: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-27 20:36:30.346594: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-27 20:36:30.346922: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
            "2019-06-27 20:36:30.346985: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-06-27 20:36:30.348000: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-06-27 20:36:30.348027: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n",
            "2019-06-27 20:36:30.348038: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n",
            "2019-06-27 20:36:30.348362: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-27 20:36:30.348788: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-27 20:36:30.349151: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14202 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "W0627 20:36:30.350052 140712750081920 deprecation_wrapper.py:119] From ./train.py:92: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0627 20:36:41.731405 140712750081920 deprecation.py:323] From /content/gpt-2/src/sample.py:65: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "W0627 20:36:41.746438 140712750081920 deprecation.py:323] From /content/gpt-2/src/sample.py:16: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0627 20:36:41.748188 140712750081920 deprecation.py:323] From /content/gpt-2/src/sample.py:70: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.random.categorical` instead.\n",
            "W0627 20:36:41.759184 140712750081920 deprecation_wrapper.py:119] From ./train.py:121: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "W0627 20:36:58.165217 140712750081920 deprecation_wrapper.py:119] From ./train.py:144: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n",
            "W0627 20:36:58.168479 140712750081920 deprecation_wrapper.py:119] From ./train.py:147: The name tf.summary.merge is deprecated. Please use tf.compat.v1.summary.merge instead.\n",
            "\n",
            "W0627 20:36:58.169441 140712750081920 deprecation_wrapper.py:119] From ./train.py:149: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
            "\n",
            "W0627 20:36:58.170280 140712750081920 deprecation_wrapper.py:119] From ./train.py:152: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "Loading checkpoint models/345M/model.ckpt\n",
            "W0627 20:37:12.236109 140712750081920 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "Loading dataset...\n",
            "100% 1/1 [00:30<00:00, 30.52s/it]\n",
            "dataset has 5265062 tokens\n",
            "Training...\n",
            "2019-06-27 20:37:56.624056: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n",
            "2019-06-27 20:37:57.367278: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "[1 | 13.91] loss=3.67 avg=3.67\n",
            "[2 | 15.50] loss=3.61 avg=3.64\n",
            "[3 | 17.10] loss=3.19 avg=3.49\n",
            "[4 | 18.71] loss=3.07 avg=3.38\n",
            "[5 | 20.31] loss=3.14 avg=3.33\n",
            "[6 | 21.93] loss=3.14 avg=3.30\n",
            "[7 | 23.54] loss=2.99 avg=3.25\n",
            "[8 | 25.16] loss=3.19 avg=3.25\n",
            "[9 | 26.79] loss=3.19 avg=3.24\n",
            "[10 | 28.42] loss=2.82 avg=3.20\n",
            "[11 | 30.06] loss=2.87 avg=3.16\n",
            "[12 | 31.70] loss=2.89 avg=3.14\n",
            "[13 | 33.34] loss=2.87 avg=3.12\n",
            "[14 | 35.00] loss=3.67 avg=3.16\n",
            "[15 | 36.66] loss=2.97 avg=3.15\n",
            "[16 | 38.32] loss=3.20 avg=3.15\n",
            "[17 | 39.99] loss=2.78 avg=3.13\n",
            "[18 | 41.68] loss=3.13 avg=3.13\n",
            "[19 | 43.37] loss=2.53 avg=3.09\n",
            "[20 | 45.06] loss=2.79 avg=3.08\n",
            "[21 | 46.76] loss=3.10 avg=3.08\n",
            "[22 | 48.46] loss=2.62 avg=3.05\n",
            "[23 | 50.17] loss=2.96 avg=3.05\n",
            "[24 | 51.89] loss=2.80 avg=3.04\n",
            "[25 | 53.59] loss=3.33 avg=3.05\n",
            "[26 | 55.33] loss=2.77 avg=3.04\n",
            "[27 | 57.06] loss=2.75 avg=3.03\n",
            "[28 | 58.78] loss=3.15 avg=3.03\n",
            "[29 | 60.52] loss=3.01 avg=3.03\n",
            "[30 | 62.25] loss=0.64 avg=2.94\n",
            "[31 | 63.95] loss=2.92 avg=2.94\n",
            "[32 | 65.68] loss=2.93 avg=2.94\n",
            "[33 | 67.38] loss=3.13 avg=2.95\n",
            "[34 | 69.07] loss=3.22 avg=2.95\n",
            "[35 | 70.76] loss=2.84 avg=2.95\n",
            "[36 | 72.43] loss=2.91 avg=2.95\n",
            "[37 | 74.12] loss=2.73 avg=2.94\n",
            "[38 | 75.80] loss=3.47 avg=2.96\n",
            "[39 | 77.47] loss=3.29 avg=2.97\n",
            "[40 | 79.15] loss=3.05 avg=2.97\n",
            "[41 | 80.81] loss=2.93 avg=2.97\n",
            "[42 | 82.47] loss=2.90 avg=2.97\n",
            "[43 | 84.13] loss=2.61 avg=2.96\n",
            "[44 | 85.78] loss=3.18 avg=2.96\n",
            "[45 | 87.43] loss=3.01 avg=2.97\n",
            "[46 | 89.08] loss=3.38 avg=2.98\n",
            "[47 | 90.73] loss=3.28 avg=2.99\n",
            "[48 | 92.37] loss=3.31 avg=2.99\n",
            "[49 | 94.01] loss=2.68 avg=2.99\n",
            "[50 | 95.65] loss=3.31 avg=2.99\n",
            "[51 | 97.29] loss=2.94 avg=2.99\n",
            "[52 | 98.94] loss=3.00 avg=2.99\n",
            "[53 | 100.58] loss=2.67 avg=2.98\n",
            "[54 | 102.21] loss=2.49 avg=2.97\n",
            "[55 | 103.85] loss=3.21 avg=2.98\n",
            "[56 | 105.49] loss=2.85 avg=2.98\n",
            "[57 | 107.12] loss=3.70 avg=2.99\n",
            "[58 | 108.76] loss=2.68 avg=2.99\n",
            "[59 | 110.40] loss=3.00 avg=2.99\n",
            "[60 | 112.05] loss=2.61 avg=2.98\n",
            "[61 | 113.70] loss=3.09 avg=2.98\n",
            "[62 | 115.34] loss=2.65 avg=2.97\n",
            "[63 | 116.99] loss=2.97 avg=2.97\n",
            "[64 | 118.64] loss=2.94 avg=2.97\n",
            "[65 | 120.29] loss=2.96 avg=2.97\n",
            "[66 | 121.95] loss=2.90 avg=2.97\n",
            "[67 | 123.61] loss=3.12 avg=2.97\n",
            "[68 | 125.26] loss=2.67 avg=2.97\n",
            "[69 | 126.92] loss=3.08 avg=2.97\n",
            "[70 | 128.59] loss=2.74 avg=2.96\n",
            "[71 | 130.25] loss=3.09 avg=2.97\n",
            "[72 | 131.92] loss=3.43 avg=2.98\n",
            "[73 | 133.59] loss=3.20 avg=2.98\n",
            "[74 | 135.26] loss=3.57 avg=2.99\n",
            "[75 | 136.94] loss=2.34 avg=2.98\n",
            "[76 | 138.61] loss=3.07 avg=2.98\n",
            "[77 | 140.27] loss=3.17 avg=2.98\n",
            "[78 | 141.95] loss=3.05 avg=2.99\n",
            "[79 | 143.62] loss=2.98 avg=2.99\n",
            "[80 | 145.29] loss=2.57 avg=2.98\n",
            "[81 | 146.97] loss=3.39 avg=2.99\n",
            "[82 | 148.64] loss=2.79 avg=2.98\n",
            "[83 | 150.32] loss=3.34 avg=2.99\n",
            "[84 | 151.99] loss=2.97 avg=2.99\n",
            "[85 | 153.65] loss=2.47 avg=2.98\n",
            "[86 | 155.32] loss=3.17 avg=2.98\n",
            "[87 | 156.99] loss=3.09 avg=2.98\n",
            "[88 | 158.66] loss=3.16 avg=2.99\n",
            "[89 | 160.33] loss=3.28 avg=2.99\n",
            "[90 | 162.00] loss=3.15 avg=2.99\n",
            "[91 | 163.66] loss=3.31 avg=3.00\n",
            "[92 | 165.33] loss=3.13 avg=3.00\n",
            "[93 | 166.99] loss=2.72 avg=3.00\n",
            "[94 | 168.65] loss=1.11 avg=2.97\n",
            "[95 | 170.32] loss=2.54 avg=2.96\n",
            "[96 | 171.99] loss=3.06 avg=2.96\n",
            "[97 | 173.65] loss=2.96 avg=2.96\n",
            "[98 | 175.31] loss=2.94 avg=2.96\n",
            "[99 | 176.97] loss=1.17 avg=2.93\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " on the floor in front of the camera, while a large pile of clothes rested on the ground before him, indicating where she had gotten them in the past.\n",
            "\n",
            "\"That was some interesting footage, I'd heard of those two a lot; they seemed to know where the exit was, they might have been able to identify the spot, and maybe even the door.\"\n",
            "\n",
            "He sighed as he took a quick drag of his cigarette. \"I don't doubt it, Miss Evans is a smart girl, I can't wait to see what kind of girls she'll be.\"\n",
            "\n",
            "She shook her head, \"I've got to get to class.\" Without having any words to say or even say it on the surface, she turned and walked away from him. He followed her quietly after and, seeing that it was impossible for him to escape his gaze, he followed after her.\n",
            "\n",
            "He reached the exit in a few minutes, but after exiting the lift he found that the door had locked, and he had to push through the revolving door to access the main hall. The hall became narrower as he passed through, and at one end the door was no longer there.\n",
            "\n",
            "\"There it is, now it's too late for any better solution.\"\n",
            "\n",
            "Just past an open door he made for the stairs, walking up them, which were somewhat bumpy; he eventually reached the first landing. The hallway opened into a small office area where a desk stood.\n",
            "\n",
            "It had windows in it; they were facing the left and right of the main corridor, it appeared to be a small living space. He turned in to see a man in his mid-30's sitting on the opposite end of the room with long, flowing, grey hair. He seemed to be employed. He was wearing a formal suit, but the man on the other side of the room appeared to be in a more casual attire, his outfit was loose and casual, it seemed to be something she had seen more than once, but had never seen her own husband in before.\n",
            "\n",
            "She wore a pale red blouse and was wearing her hair tied back at her side, a simple white frock coat with a white trim was hanging over her shoulders, and a dark green scarf hung down her back and over her head.\n",
            "This was his wife.\n",
            "\n",
            "Her husband stood in plain sight on her side, he was wearing a loose fitting, light blue shirt, black trousers and a pair of dark blue boots. He was also wearing a blue dress around the waist, and he was standing by her window, with a large red, white, blue, and green umbrella sitting on his lap as he was reading on his book.\n",
            "\n",
            "He had blonde hair, with a dark brown headband. He had worn glasses, but they did not seem to be present. He also had a dark brown leather briefcase, with black trim on the sides, and a few pieces of paper, each containing about an inch of parchment on the front. Around his neck was a light pink cloth bag, with a green card pocket, holding his keys and a few papers.\n",
            "\n",
            "He also wore a dark green, short sleeved shirt, and an open grey suit jacket, with a large brown belt around his waist, and a black, blue, and green pocket square.\n",
            "\n",
            "He stood before her with an air of authority, and his wife wore a heavy leather skirt. As he stood before her and looked into the eyes of his own wife, he looked like he wanted to go away, as she looked like she had never been alone before.\n",
            "\n",
            "He seemed to be in his own head, not a part of any living thing, his eyes were fixed on hers and his face looked grave, and yet he looked very much at ease. Not being in that moment that the whole world around her changed, but only her own gaze. She gazed back.\n",
            "\n",
            "She sat down, looking at the papers, which she took from the paper box and threw away.\n",
            "\n",
            "He looked confused and lost. His mind did not seem fully able to process that his wife was sitting against the door of her own private office.\n",
            "\n",
            "The man behind the desk appeared to be a female. She was wearing a very formal dress, with a yellow gown over it, with several necklaces tied on at one side and a white, gold fringed dress around her waist. She also had a gold, white lace bracelet on her right wrist, and two diamonds in her collar.\n",
            "\n",
            "She was looking directly at him, her eyes staring intently at his.\n",
            "\n",
            "He was feeling very anxious, yet feeling slightly relieved that he had met with his wife, she was a very important person to him, but it was a feeling to which the latter had yet to react; she had not made up her mind yet what she planned to do, and he decided to wait for a few moments to let her decide it himself. She had said that on Wednesday morning he should call on her as early\n",
            "\n",
            "[100 | 204.92] loss=2.73 avg=2.93\n",
            "[101 | 206.59] loss=2.54 avg=2.92\n",
            "[102 | 208.26] loss=2.72 avg=2.92\n",
            "[103 | 209.95] loss=2.95 avg=2.92\n",
            "[104 | 211.60] loss=3.40 avg=2.93\n",
            "[105 | 213.27] loss=3.14 avg=2.93\n",
            "[106 | 214.94] loss=3.05 avg=2.93\n",
            "[107 | 216.61] loss=3.30 avg=2.94\n",
            "[108 | 218.28] loss=3.02 avg=2.94\n",
            "[109 | 219.95] loss=2.70 avg=2.94\n",
            "[110 | 221.63] loss=3.22 avg=2.94\n",
            "[111 | 223.30] loss=2.79 avg=2.94\n",
            "[112 | 224.96] loss=2.41 avg=2.93\n",
            "[113 | 226.64] loss=2.81 avg=2.93\n",
            "[114 | 228.30] loss=3.26 avg=2.93\n",
            "[115 | 229.97] loss=3.23 avg=2.94\n",
            "[116 | 231.65] loss=2.77 avg=2.94\n",
            "[117 | 233.32] loss=2.68 avg=2.93\n",
            "[118 | 234.99] loss=3.33 avg=2.94\n",
            "[119 | 236.66] loss=2.99 avg=2.94\n",
            "[120 | 238.33] loss=3.49 avg=2.95\n",
            "[121 | 240.00] loss=2.60 avg=2.94\n",
            "[122 | 241.67] loss=3.69 avg=2.95\n",
            "[123 | 243.35] loss=2.91 avg=2.95\n",
            "[124 | 245.02] loss=2.77 avg=2.95\n",
            "[125 | 246.69] loss=2.84 avg=2.95\n",
            "[126 | 248.37] loss=2.99 avg=2.95\n",
            "[127 | 250.04] loss=2.84 avg=2.95\n",
            "[128 | 251.71] loss=2.88 avg=2.95\n",
            "[129 | 253.39] loss=3.69 avg=2.96\n",
            "[130 | 255.06] loss=2.54 avg=2.95\n",
            "[131 | 256.73] loss=2.47 avg=2.94\n",
            "[132 | 258.40] loss=3.61 avg=2.95\n",
            "[133 | 260.07] loss=2.66 avg=2.95\n",
            "[134 | 261.74] loss=2.98 avg=2.95\n",
            "[135 | 263.43] loss=2.30 avg=2.94\n",
            "[136 | 265.10] loss=2.81 avg=2.94\n",
            "[137 | 266.77] loss=2.72 avg=2.94\n",
            "[138 | 268.44] loss=3.08 avg=2.94\n",
            "[139 | 270.11] loss=2.46 avg=2.93\n",
            "[140 | 271.78] loss=2.83 avg=2.93\n",
            "[141 | 273.45] loss=3.18 avg=2.93\n",
            "[142 | 275.12] loss=2.98 avg=2.93\n",
            "[143 | 276.79] loss=2.69 avg=2.93\n",
            "[144 | 278.48] loss=2.73 avg=2.93\n",
            "[145 | 280.15] loss=3.24 avg=2.93\n",
            "[146 | 281.83] loss=3.41 avg=2.94\n",
            "[147 | 283.50] loss=3.02 avg=2.94\n",
            "[148 | 285.16] loss=2.94 avg=2.94\n",
            "[149 | 286.83] loss=3.54 avg=2.95\n",
            "[150 | 288.50] loss=3.70 avg=2.96\n",
            "[151 | 290.17] loss=2.57 avg=2.95\n",
            "[152 | 291.83] loss=3.06 avg=2.95\n",
            "[153 | 293.51] loss=2.80 avg=2.95\n",
            "[154 | 295.18] loss=2.92 avg=2.95\n",
            "[155 | 296.85] loss=3.21 avg=2.95\n",
            "[156 | 298.53] loss=2.70 avg=2.95\n",
            "[157 | 300.21] loss=2.45 avg=2.94\n",
            "[158 | 301.88] loss=3.18 avg=2.95\n",
            "[159 | 303.56] loss=2.42 avg=2.94\n",
            "[160 | 305.23] loss=2.24 avg=2.93\n",
            "[161 | 306.91] loss=3.39 avg=2.94\n",
            "[162 | 308.58] loss=3.33 avg=2.94\n",
            "[163 | 310.25] loss=2.87 avg=2.94\n",
            "[164 | 311.93] loss=1.92 avg=2.93\n",
            "[165 | 313.60] loss=2.76 avg=2.93\n",
            "[166 | 315.27] loss=3.42 avg=2.93\n",
            "[167 | 316.95] loss=2.51 avg=2.93\n",
            "[168 | 318.62] loss=2.71 avg=2.93\n",
            "[169 | 320.29] loss=2.98 avg=2.93\n",
            "[170 | 321.96] loss=2.78 avg=2.92\n",
            "[171 | 323.63] loss=3.16 avg=2.93\n",
            "[172 | 325.30] loss=2.85 avg=2.93\n",
            "[173 | 326.99] loss=2.79 avg=2.92\n",
            "[174 | 328.66] loss=2.39 avg=2.92\n",
            "[175 | 330.33] loss=2.88 avg=2.92\n",
            "[176 | 332.00] loss=2.76 avg=2.92\n",
            "[177 | 333.69] loss=3.10 avg=2.92\n",
            "[178 | 335.36] loss=2.76 avg=2.92\n",
            "[179 | 337.04] loss=3.10 avg=2.92\n",
            "[180 | 338.73] loss=3.24 avg=2.92\n",
            "[181 | 340.40] loss=3.31 avg=2.93\n",
            "[182 | 342.09] loss=2.99 avg=2.93\n",
            "[183 | 343.76] loss=3.22 avg=2.93\n",
            "[184 | 345.43] loss=3.12 avg=2.93\n",
            "[185 | 347.10] loss=2.78 avg=2.93\n",
            "[186 | 348.77] loss=3.11 avg=2.93\n",
            "[187 | 350.44] loss=3.12 avg=2.94\n",
            "[188 | 352.11] loss=2.86 avg=2.93\n",
            "[189 | 353.79] loss=2.74 avg=2.93\n",
            "[190 | 355.47] loss=3.22 avg=2.94\n",
            "[191 | 357.14] loss=2.67 avg=2.93\n",
            "[192 | 358.81] loss=3.17 avg=2.94\n",
            "[193 | 360.48] loss=2.86 avg=2.93\n",
            "[194 | 362.15] loss=0.66 avg=2.91\n",
            "[195 | 363.83] loss=2.84 avg=2.91\n",
            "[196 | 365.51] loss=2.75 avg=2.91\n",
            "[197 | 367.19] loss=2.96 avg=2.91\n",
            "[198 | 368.86] loss=3.34 avg=2.91\n",
            "[199 | 370.53] loss=2.95 avg=2.91\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " never taken a holiday,\" said Mr. Raffel.\n",
            "\n",
            "\"And you?\" replied Mrs. Raffel. For the longest moment, this question struck her.\n",
            "\n",
            "\"Mrs. Raffel. You must be an ugly woman,\" said he, in an angry strain.\n",
            "\n",
            "\"What do you mean?\" asked Mrs. Raffel, in a voice that seemed to be bursting into tears.\n",
            "\n",
            "\"I mean that you are ugly. If you should ever see myself—I don't know—you would certainly think what I have told you is all true. Perhaps, I'll think it so. I will be ashamed, and I shall not tell you.\"\n",
            "\n",
            "Mrs. Raffel took his hand by the hand, and turned the other cheek to him in a tender and sympathetic gesture.\n",
            "\n",
            "\"I've been here two years now,\" he said.\n",
            "\n",
            "\"Why, how long did that last for?\"\n",
            "\n",
            "\"For one year, to be exact. When I was at home.\"\n",
            "\n",
            "\"That,\" said Mrs. Raffel, with a feeling that seemed to leave a little distance between them, \"I have always regarded as the end of my journey.\"\n",
            "\n",
            "He had a deep and sad look in her eyes, and felt a deep feeling of regret.\n",
            "\n",
            "\"Oh, I know it isn't over,\" said she, \"but a new beginning is beginning to come.\"\n",
            "\n",
            "\"My love!\" cried Mr. Raffel, at the end of his countenance that seemed to express great emotion. \"I had always thought my new home had been my own! I knew it was to begin with. So long had I known it! What I felt for you must have been the same! But you have always brought me to tears, Miss Raffel, and I feel, dear old Miss Raffel, this sorrow for nothing but to see you go, and for it to be long.\"\n",
            "\n",
            "He felt he was so near tears that he could hardly stop them.\n",
            "\n",
            "\"I know, I know,\" said Mrs. Raffel, in a voice that said more than he could well express. \"But you must always have something to tell me, Mr. Raffel, you know.\"\n",
            "\n",
            "\"I understand,\" said he. \"Oh, and I understand very little, do I?\"\n",
            "\n",
            "Mrs. Raffel was the child of the poor; she saw how little Mr. Raffel could learn from her. She felt her heart break as he spoke.\n",
            "\n",
            "\"What have you done to me?\" she said, but in a voice that made him believe his own heart was breaking. She said, and she believed her own face. She spoke a sad and mournful voice. Her soul seemed to be so worn and weary, and so worn out of life, that it was not possible she could think that she was glad to hear such a sorrowful thing, and yet felt sorrowful and sad when a person spoke of such a thing to her. She sat down, tears filling her eyes; Mr. Raffel had to sit back again from the tears he was holding.\n",
            "\n",
            "\"It is such a long-winded tale. I don't care.\" she said, holding up her hand to silence him.\n",
            "\n",
            "\"I'm sorry for you,\" he said, with tears and a look of deep regret upon his face.\n",
            "\n",
            "Mrs. Raffel was so fond of him that she could hardly remember not having been at home.\n",
            "\n",
            "He was a man of large stature, with a large and well toned face and a full beard, well shaven and in good health and active; his body and face, though thickly wrinkled, were still handsome and handsomely framed, and he had a full, shining chin that had never yet been dry, and a charming beard that was still in his hair.\n",
            "\n",
            "\"I've had a long and unhappy life, too,\" said he, feeling that such a short one, and feeling that it was not an easy life to live.\n",
            "\n",
            "\"It is a bad one for you to have had,\" said Mrs. Raffel, with a warm affectionate interest, \"but we're better off, for you and I both have good health; you will do you no harm, we will have nothing but good health. As I am sure you must have considered, it takes a great deal to get on at home.\"\n",
            "\n",
            "\"I'm glad you think so,\" said Mr. Raffel. \"In any case, I was not very happy even here. At home I hardly got one hour at a time, and you knew I was sorry to go away, the very first day I knew we were coming here.\"\n",
            "\n",
            "\"But now I know I'm happy, too, Mr. Raffel—you mean I'm glad?\"\n",
            "\n",
            "He was silent about his home life; Mrs. Raffel would have\n",
            "\n",
            "[200 | 395.20] loss=3.32 avg=2.92\n",
            "[201 | 396.86] loss=3.29 avg=2.92\n",
            "[202 | 398.53] loss=2.15 avg=2.91\n",
            "[203 | 400.20] loss=3.42 avg=2.92\n",
            "[204 | 401.87] loss=3.10 avg=2.92\n",
            "[205 | 403.54] loss=2.83 avg=2.92\n",
            "[206 | 405.21] loss=2.67 avg=2.92\n",
            "[207 | 406.87] loss=2.89 avg=2.92\n",
            "[208 | 408.53] loss=2.63 avg=2.91\n",
            "[209 | 410.19] loss=2.80 avg=2.91\n",
            "[210 | 411.86] loss=3.28 avg=2.92\n",
            "[211 | 413.52] loss=2.53 avg=2.91\n",
            "[212 | 415.19] loss=2.68 avg=2.91\n",
            "[213 | 416.87] loss=2.63 avg=2.90\n",
            "[214 | 418.54] loss=3.02 avg=2.91\n",
            "[215 | 420.21] loss=3.74 avg=2.92\n",
            "[216 | 421.88] loss=3.44 avg=2.92\n",
            "[217 | 423.55] loss=3.18 avg=2.92\n",
            "[218 | 425.22] loss=2.94 avg=2.92\n",
            "[219 | 426.89] loss=2.54 avg=2.92\n",
            "[220 | 428.56] loss=3.21 avg=2.92\n",
            "[221 | 430.23] loss=2.78 avg=2.92\n",
            "[222 | 431.90] loss=2.48 avg=2.92\n",
            "[223 | 433.58] loss=2.81 avg=2.92\n",
            "[224 | 435.26] loss=3.49 avg=2.92\n",
            "[225 | 436.94] loss=2.97 avg=2.92\n",
            "[226 | 438.61] loss=2.36 avg=2.92\n",
            "[227 | 440.28] loss=2.87 avg=2.92\n",
            "[228 | 441.96] loss=0.18 avg=2.89\n",
            "[229 | 443.65] loss=3.01 avg=2.89\n",
            "[230 | 445.33] loss=2.79 avg=2.89\n",
            "[231 | 447.01] loss=2.92 avg=2.89\n",
            "[232 | 448.70] loss=3.13 avg=2.89\n",
            "[233 | 450.39] loss=3.44 avg=2.89\n",
            "[234 | 452.09] loss=3.39 avg=2.90\n",
            "[235 | 453.76] loss=2.62 avg=2.90\n",
            "[236 | 455.46] loss=2.75 avg=2.90\n",
            "[237 | 457.14] loss=2.72 avg=2.89\n",
            "[238 | 458.82] loss=2.82 avg=2.89\n",
            "[239 | 460.51] loss=2.87 avg=2.89\n",
            "[240 | 462.18] loss=2.81 avg=2.89\n",
            "[241 | 463.88] loss=2.65 avg=2.89\n",
            "[242 | 465.56] loss=2.93 avg=2.89\n",
            "[243 | 467.24] loss=3.05 avg=2.89\n",
            "[244 | 468.93] loss=2.69 avg=2.89\n",
            "[245 | 470.59] loss=3.11 avg=2.89\n",
            "[246 | 472.27] loss=2.77 avg=2.89\n",
            "[247 | 473.96] loss=2.21 avg=2.88\n",
            "[248 | 475.62] loss=3.30 avg=2.89\n",
            "[249 | 477.30] loss=3.19 avg=2.89\n",
            "[250 | 478.97] loss=3.00 avg=2.89\n",
            "[251 | 480.63] loss=2.63 avg=2.89\n",
            "[252 | 482.30] loss=3.25 avg=2.89\n",
            "[253 | 483.98] loss=3.03 avg=2.89\n",
            "[254 | 485.64] loss=3.04 avg=2.90\n",
            "[255 | 487.33] loss=2.95 avg=2.90\n",
            "[256 | 489.01] loss=2.89 avg=2.90\n",
            "[257 | 490.69] loss=3.67 avg=2.90\n",
            "[258 | 492.37] loss=2.31 avg=2.90\n",
            "[259 | 494.04] loss=3.16 avg=2.90\n",
            "[260 | 495.73] loss=2.73 avg=2.90\n",
            "[261 | 497.41] loss=2.66 avg=2.90\n",
            "[262 | 499.10] loss=3.17 avg=2.90\n",
            "[263 | 500.77] loss=3.17 avg=2.90\n",
            "[264 | 502.45] loss=2.83 avg=2.90\n",
            "[265 | 504.14] loss=2.91 avg=2.90\n",
            "[266 | 505.83] loss=2.81 avg=2.90\n",
            "[267 | 507.50] loss=3.38 avg=2.91\n",
            "[268 | 509.19] loss=3.50 avg=2.91\n",
            "[269 | 510.87] loss=2.63 avg=2.91\n",
            "[270 | 512.56] loss=2.53 avg=2.91\n",
            "[271 | 514.25] loss=3.24 avg=2.91\n",
            "[272 | 515.93] loss=3.00 avg=2.91\n",
            "[273 | 517.60] loss=2.88 avg=2.91\n",
            "[274 | 519.27] loss=2.97 avg=2.91\n",
            "[275 | 520.94] loss=3.39 avg=2.92\n",
            "[276 | 522.62] loss=2.96 avg=2.92\n",
            "[277 | 524.29] loss=2.70 avg=2.91\n",
            "[278 | 525.96] loss=2.85 avg=2.91\n",
            "[279 | 527.63] loss=2.97 avg=2.91\n",
            "[280 | 529.30] loss=0.47 avg=2.89\n",
            "[281 | 530.97] loss=2.74 avg=2.89\n",
            "[282 | 532.64] loss=3.54 avg=2.89\n",
            "[283 | 534.31] loss=2.82 avg=2.89\n",
            "[284 | 535.98] loss=3.31 avg=2.90\n",
            "[285 | 537.65] loss=2.55 avg=2.89\n",
            "[286 | 539.32] loss=2.53 avg=2.89\n",
            "[287 | 540.99] loss=3.04 avg=2.89\n",
            "[288 | 542.66] loss=2.85 avg=2.89\n",
            "[289 | 544.32] loss=2.68 avg=2.89\n",
            "[290 | 545.99] loss=2.40 avg=2.88\n",
            "[291 | 547.65] loss=2.99 avg=2.88\n",
            "[292 | 549.33] loss=2.73 avg=2.88\n",
            "[293 | 550.99] loss=3.00 avg=2.88\n",
            "[294 | 552.67] loss=3.30 avg=2.89\n",
            "[295 | 554.33] loss=2.86 avg=2.89\n",
            "[296 | 556.00] loss=2.99 avg=2.89\n",
            "[297 | 557.68] loss=2.66 avg=2.89\n",
            "[298 | 559.34] loss=2.77 avg=2.88\n",
            "[299 | 561.01] loss=2.47 avg=2.88\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " whim.\n",
            "She walked on and stood before the door. But she did not look back; she did not look for her father; nor did she look for her mother.\n",
            "\n",
            "She only turned, the motion of which was a sign that the woman did not yet live. She did not turn and wait any longer.\n",
            "\n",
            "Something in her, she remembered, seemed to move her; and it was that something which she had thought of as an old woman.\n",
            "\n",
            "For a moment it felt as if she held back her tears, but she turned back, and there appeared nothing further; and, again, everything was empty.\n",
            "\n",
            "She bent again towards the door and turned, and that which she thought was a long time since, as if she were in some dream, in the midst of nothing; was nothing now, merely that there was silence.\n",
            "\n",
            "She looked again, and saw nothing, though, upon the spot where that time when she began to be, all things had ceased, and she saw only these and nothing else.\n",
            "\n",
            "She walked on alone, without being able to look back, and when she did see her father, she was glad.\n",
            "\n",
            "When she sat down to write she did not think it was quite possible that she should be able to write that she should be alive.\n",
            "\n",
            "It was plain the child was alive and not dead. No doubt there was still a part still in her which it wanted to have, and she thought that it would not be possible, if she wrote this, for her to live.\n",
            "\n",
            "It would have been a difficult matter for her if she were to have written, if, now that she had written, she were to have known her daughter, and so she felt, that she was already born!\n",
            "\n",
            "That it would be impossible for her to know her mother at all, but it would be an impossibility, if she died.\n",
            "\n",
            "At last, after some years, after living an infinite time and not being able to think a morsel of her life, when she knew only that she was dead, she began to think this way:\n",
            "\n",
            "If she should have lived and been told, I should know her; and if she knew me, I should know her.\n",
            "\n",
            "That life was my mother, but I was my dying mother.\n",
            "\n",
            "What should be my name when I died? And what should be my surname?\n",
            "\n",
            "She now knew exactly what death meant, and that death, for one moment, seemed to come and to take me away.\n",
            "\n",
            "She wanted as surely to see her daughter's face, as a child does to see its mother. She had seen her daughter's face in dreams. She felt she should try to look at her, to see as she would as she knew, and yet not understand; to see as she must know she was, and yet understand her. She wanted to see her in spite of herself, and yet think she knew her, and that it was not so strange, since in some little way she was like her\n",
            "\n",
            "But she saw nothing, though she had no idea of what else in existence\n",
            "\n",
            "In the night she sat as she stood; a long time, a strange, strange time.\n",
            "\n",
            "She sat there for a long time, and at last when she was tired, and her mind had rested, she went in to her room.\n",
            "\n",
            "When she stepped in she saw that there was nothing in the room, for nothing was there.\n",
            "\n",
            "She ran up, as if on purpose to see the child in the darkness, but not there; and when she came into her room she was seized with a strong emotion, like that which one has when he knows that he has not come to be but feels that he knows\n",
            "What he has not yet understood; is he himself?\n",
            "\n",
            "The door opened, and, without speaking any word, without entering, she walked out; a cold, dark face before her, the eyes of a child quite lost in thought;\n",
            "dark eyes and a very terrible appearance. Something was very peculiar about her, and it was like her seeing something in the night when she walked in on the child. But she saw nothing in the darkness, and what she wanted, there was nothing, except that whatever she did hear or see, she could think what it might have been.\n",
            "\n",
            "She was very afraid to go out, for fear she should not know what she had been thinking of. But there was nothing farther in the darkness but what was not there, but only the darkness of night; and when she spoke of what she thought she heard the child's mother.\n",
            "\n",
            "When she heard that she saw it and could not believe it\n",
            "It was something that she had not been saying; she did not think that she heard or thought she heard or thought\n",
            "What was it?\n",
            "\n",
            "In spite of her own desire her voice, as it were, trembled to speak of it, and she trembled aloud. She was a little afraid to\n",
            "\n",
            "[300 | 586.20] loss=3.36 avg=2.89\n",
            "[301 | 587.87] loss=3.14 avg=2.89\n",
            "[302 | 589.54] loss=2.83 avg=2.89\n",
            "[303 | 591.20] loss=2.77 avg=2.89\n",
            "[304 | 592.88] loss=2.49 avg=2.88\n",
            "[305 | 594.53] loss=2.41 avg=2.88\n",
            "[306 | 596.20] loss=2.45 avg=2.87\n",
            "[307 | 597.87] loss=2.90 avg=2.87\n",
            "[308 | 599.54] loss=2.89 avg=2.87\n",
            "[309 | 601.21] loss=2.73 avg=2.87\n",
            "[310 | 602.88] loss=3.03 avg=2.87\n",
            "[311 | 604.54] loss=2.90 avg=2.87\n",
            "[312 | 606.20] loss=2.86 avg=2.87\n",
            "[313 | 607.87] loss=2.73 avg=2.87\n",
            "[314 | 609.52] loss=2.99 avg=2.87\n",
            "[315 | 611.19] loss=2.92 avg=2.87\n",
            "[316 | 612.85] loss=3.40 avg=2.88\n",
            "[317 | 614.51] loss=2.51 avg=2.88\n",
            "[318 | 616.18] loss=2.78 avg=2.87\n",
            "[319 | 617.85] loss=2.45 avg=2.87\n",
            "[320 | 619.52] loss=2.95 avg=2.87\n",
            "[321 | 621.19] loss=2.25 avg=2.86\n",
            "[322 | 622.86] loss=3.15 avg=2.87\n",
            "[323 | 624.53] loss=2.95 avg=2.87\n",
            "[324 | 626.20] loss=2.98 avg=2.87\n",
            "[325 | 627.87] loss=3.53 avg=2.88\n",
            "[326 | 629.54] loss=2.44 avg=2.87\n",
            "[327 | 631.20] loss=2.64 avg=2.87\n",
            "[328 | 632.88] loss=2.91 avg=2.87\n",
            "[329 | 634.56] loss=2.82 avg=2.87\n",
            "[330 | 636.24] loss=3.23 avg=2.87\n",
            "[331 | 637.91] loss=2.53 avg=2.87\n",
            "[332 | 639.59] loss=2.78 avg=2.87\n",
            "[333 | 641.27] loss=2.93 avg=2.87\n",
            "[334 | 642.94] loss=2.48 avg=2.87\n",
            "[335 | 644.62] loss=3.06 avg=2.87\n",
            "[336 | 646.29] loss=2.80 avg=2.87\n",
            "[337 | 647.96] loss=2.61 avg=2.86\n",
            "[338 | 649.65] loss=2.60 avg=2.86\n",
            "[339 | 651.33] loss=3.23 avg=2.86\n",
            "[340 | 653.02] loss=3.45 avg=2.87\n",
            "[341 | 654.71] loss=2.57 avg=2.87\n",
            "[342 | 656.39] loss=3.23 avg=2.87\n",
            "[343 | 658.07] loss=2.66 avg=2.87\n",
            "[344 | 659.76] loss=3.30 avg=2.87\n",
            "[345 | 661.45] loss=3.57 avg=2.88\n",
            "[346 | 663.13] loss=2.93 avg=2.88\n",
            "[347 | 664.81] loss=3.01 avg=2.88\n",
            "[348 | 666.50] loss=2.89 avg=2.88\n",
            "[349 | 668.17] loss=3.22 avg=2.89\n",
            "[350 | 669.84] loss=3.55 avg=2.89\n",
            "[351 | 671.51] loss=3.11 avg=2.90\n",
            "[352 | 673.19] loss=2.90 avg=2.90\n",
            "[353 | 674.88] loss=2.39 avg=2.89\n",
            "[354 | 676.55] loss=2.95 avg=2.89\n",
            "[355 | 678.24] loss=3.05 avg=2.89\n",
            "[356 | 679.91] loss=3.36 avg=2.90\n",
            "[357 | 681.58] loss=2.72 avg=2.90\n",
            "[358 | 683.25] loss=2.53 avg=2.89\n",
            "[359 | 684.94] loss=3.03 avg=2.89\n",
            "[360 | 686.61] loss=3.32 avg=2.90\n",
            "[361 | 688.28] loss=2.61 avg=2.89\n",
            "[362 | 689.95] loss=2.98 avg=2.90\n",
            "[363 | 691.62] loss=2.80 avg=2.89\n",
            "[364 | 693.29] loss=2.80 avg=2.89\n",
            "[365 | 694.96] loss=2.47 avg=2.89\n",
            "[366 | 696.62] loss=2.75 avg=2.89\n",
            "[367 | 698.29] loss=3.40 avg=2.89\n",
            "[368 | 699.96] loss=2.97 avg=2.89\n",
            "[369 | 701.63] loss=2.56 avg=2.89\n",
            "[370 | 703.31] loss=2.75 avg=2.89\n",
            "[371 | 704.98] loss=2.84 avg=2.89\n",
            "[372 | 706.64] loss=2.82 avg=2.89\n",
            "[373 | 708.32] loss=3.26 avg=2.89\n",
            "[374 | 709.99] loss=3.67 avg=2.90\n",
            "[375 | 711.66] loss=2.37 avg=2.89\n",
            "[376 | 713.32] loss=3.36 avg=2.90\n",
            "[377 | 714.99] loss=3.02 avg=2.90\n",
            "[378 | 716.66] loss=2.66 avg=2.90\n",
            "[379 | 718.33] loss=2.83 avg=2.90\n",
            "[380 | 719.99] loss=2.10 avg=2.89\n",
            "[381 | 721.66] loss=2.80 avg=2.89\n",
            "[382 | 723.34] loss=2.09 avg=2.88\n",
            "[383 | 725.01] loss=3.05 avg=2.88\n",
            "[384 | 726.69] loss=2.67 avg=2.88\n",
            "[385 | 728.36] loss=2.44 avg=2.87\n",
            "[386 | 730.03] loss=2.73 avg=2.87\n",
            "[387 | 731.71] loss=2.59 avg=2.87\n",
            "[388 | 733.37] loss=2.45 avg=2.87\n",
            "[389 | 735.05] loss=2.21 avg=2.86\n",
            "[390 | 736.73] loss=3.46 avg=2.87\n",
            "[391 | 738.40] loss=2.86 avg=2.87\n",
            "[392 | 740.07] loss=3.36 avg=2.87\n",
            "[393 | 741.76] loss=2.99 avg=2.87\n",
            "[394 | 743.44] loss=3.44 avg=2.88\n",
            "[395 | 745.11] loss=2.62 avg=2.87\n",
            "[396 | 746.80] loss=2.52 avg=2.87\n",
            "[397 | 748.47] loss=2.88 avg=2.87\n",
            "[398 | 750.16] loss=2.82 avg=2.87\n",
            "[399 | 751.85] loss=3.19 avg=2.87\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " me. I will tell you, I told you it had to be done.\"\n",
            "\n",
            "I began, \"The little boy did not want to see you for long. He seemed to see and love the little boy. He seemed to see and love your sister and I, and yet to love me. But you can never do it. You must go back to her, in the same way as your mother did, only without saying anything. My sister, what is your answer to that? . .  . .  . .  .  .  . .  .  .  .  .  .\"\n",
            "\n",
            "And I said, like a child in joy, \"Oh, do stop!\" and I ran for the door. I didn't know if I should be able to meet my sister's good-natured glance. I took the key which hung from the post of the door and I walked off. And without having told him, of course, what I was going to do that afternoon, who was his mother and I? My mother answered him. A moment or an hour after her answering him in the most natural and genuine way, she said, \"Mamma did not mean to have me.\"\n",
            "\n",
            "That had never come into my memory, or ever before.\n",
            "\n",
            "And that was the end of it—she had taken the key. I was the master again. She told me to go away as soon as I had finished that thing, and as soon as I had finished the letter she had written to me, I went back to bed, a little worried for the sake of my sister and for the sake of my mother with her.\n",
            "\n",
            "I had been up all night. I had not done anything. She was not looking at me, she was not talking to me, and there was nothing said she would say to me. She only said, in this way, that I should not be a good son, and that she would go away from me.\n",
            "\n",
            "One minute later I heard a knock at the door, and my mother went in and came back.\n",
            "\n",
            "\"How is he?\" she asked, and she began to count the letters written in it and said, \"You'll find he's my only son.\"\n",
            "\n",
            "When I had heard this, my mother opened the door: this door went up to the next bedroom. She was sitting with her arms out, she sat there a while.\n",
            "\n",
            "\"What does he want?\" she said, turning to me.\n",
            "\n",
            "\"What does he want?\" I tried to explain.\n",
            "\n",
            "\"For my mother; for you, for me.\"\n",
            "\n",
            "\"My mother?\"\n",
            "\n",
            "\"Yes.\"\n",
            "\n",
            "\"Good grief! what a miserable thing she is!\" she said, taking my arm, and turning her face to me.\n",
            "\n",
            "I had the misfortune of saying, in a tone that conveyed the truth, \"Yes, my mother.\" She had done that and that afternoon, that she thought I was an awful thing and that she would never know what it was to be free without being a bad slave. She did not let me say the words I desired, I did not dare to say anything—she wanted to hear and be heard.\n",
            "\n",
            "\"My mother, will you listen to me! Listen to me!\"\n",
            "\n",
            "And she spoke to her. She started when she heard what I said, and went out. When my mother came back, the door was half open, and she saw that I had gone out, and I felt some pity for her. So I asked whether she wanted to be free.\n",
            "\n",
            "\"Why would she?\" was the answer. I knew that she didn't want to be free, but I couldn't bear that she should take the step she did.\n",
            "\n",
            "\"Why not go away? He'll go home to my mother and she won't know! That, my dear, is what I wanted as soon as I heard you said you wouldn't tell him!\" she said to me in a voice that was full of tears. She was sobbing, and she cried with me, and I cried with her. The whole evening was gone out of my mind.\n",
            "\n",
            "The next day I was up all day making up what I had heard and saying to myself, \"What has she said to him, what has she said to him?\" I was afraid that I should make her love me in the same way that she loved my mother and my father and my sisters.\n",
            "\n",
            "The next morning she asked, \"Why wouldn't you go away?\" I could not answer. \"Why shouldn't you go away?\" said she. \"Why should she keep my brother in prison and in hell! Go away! . . .\"\n",
            "\n",
            "By midnight I was in bed. We were sleeping in that corner. I tried to lie down but I couldn't. She was looking at me, and with her eyes I said to myself, \"I don't know what to do, how to get him to go away\n",
            "\n",
            "[400 | 776.51] loss=3.17 avg=2.88\n",
            "[401 | 778.17] loss=3.42 avg=2.88\n",
            "[402 | 779.84] loss=2.66 avg=2.88\n",
            "[403 | 781.50] loss=2.40 avg=2.88\n",
            "[404 | 783.16] loss=2.24 avg=2.87\n",
            "[405 | 784.81] loss=2.79 avg=2.87\n",
            "[406 | 786.46] loss=3.00 avg=2.87\n",
            "[407 | 788.13] loss=2.61 avg=2.87\n",
            "[408 | 789.78] loss=2.81 avg=2.87\n",
            "[409 | 791.43] loss=2.41 avg=2.86\n",
            "[410 | 793.08] loss=2.97 avg=2.86\n",
            "[411 | 794.73] loss=2.90 avg=2.86\n",
            "[412 | 796.39] loss=3.24 avg=2.87\n",
            "[413 | 798.06] loss=2.88 avg=2.87\n",
            "[414 | 799.73] loss=3.00 avg=2.87\n",
            "[415 | 801.40] loss=2.49 avg=2.86\n",
            "[416 | 803.08] loss=3.08 avg=2.87\n",
            "[417 | 804.75] loss=3.20 avg=2.87\n",
            "[418 | 806.42] loss=2.62 avg=2.87\n",
            "[419 | 808.09] loss=2.86 avg=2.87\n",
            "[420 | 809.76] loss=3.06 avg=2.87\n",
            "[421 | 811.43] loss=3.23 avg=2.87\n",
            "[422 | 813.11] loss=2.84 avg=2.87\n",
            "[423 | 814.79] loss=2.71 avg=2.87\n",
            "[424 | 816.47] loss=2.35 avg=2.87\n",
            "[425 | 818.16] loss=0.44 avg=2.84\n",
            "[426 | 819.83] loss=2.86 avg=2.84\n",
            "[427 | 821.52] loss=3.40 avg=2.85\n",
            "[428 | 823.21] loss=2.68 avg=2.85\n",
            "[429 | 824.91] loss=3.20 avg=2.85\n",
            "[430 | 826.60] loss=3.20 avg=2.85\n",
            "[431 | 828.29] loss=2.60 avg=2.85\n",
            "[432 | 829.97] loss=2.70 avg=2.85\n",
            "[433 | 831.66] loss=2.95 avg=2.85\n",
            "[434 | 833.33] loss=2.31 avg=2.84\n",
            "[435 | 835.02] loss=3.18 avg=2.85\n",
            "[436 | 836.69] loss=2.46 avg=2.84\n",
            "[437 | 838.38] loss=3.29 avg=2.85\n",
            "[438 | 840.06] loss=3.29 avg=2.85\n",
            "[439 | 841.73] loss=2.73 avg=2.85\n",
            "[440 | 843.39] loss=2.67 avg=2.85\n",
            "[441 | 845.07] loss=2.82 avg=2.85\n",
            "[442 | 846.74] loss=2.73 avg=2.85\n",
            "[443 | 848.41] loss=2.70 avg=2.85\n",
            "[444 | 850.08] loss=3.01 avg=2.85\n",
            "[445 | 851.76] loss=2.87 avg=2.85\n",
            "[446 | 853.43] loss=3.13 avg=2.85\n",
            "[447 | 855.10] loss=3.50 avg=2.86\n",
            "[448 | 856.78] loss=3.03 avg=2.86\n",
            "[449 | 858.44] loss=3.31 avg=2.86\n",
            "[450 | 860.11] loss=2.99 avg=2.87\n",
            "[451 | 861.78] loss=2.57 avg=2.86\n",
            "[452 | 863.45] loss=3.14 avg=2.87\n",
            "[453 | 865.12] loss=2.94 avg=2.87\n",
            "[454 | 866.79] loss=2.92 avg=2.87\n",
            "[455 | 868.46] loss=3.21 avg=2.87\n",
            "[456 | 870.14] loss=2.66 avg=2.87\n",
            "[457 | 871.80] loss=2.57 avg=2.86\n",
            "[458 | 873.47] loss=3.50 avg=2.87\n",
            "[459 | 875.14] loss=3.21 avg=2.87\n",
            "[460 | 876.81] loss=2.70 avg=2.87\n",
            "[461 | 878.48] loss=3.60 avg=2.88\n",
            "[462 | 880.15] loss=3.08 avg=2.88\n",
            "[463 | 881.82] loss=2.66 avg=2.88\n",
            "[464 | 883.49] loss=2.89 avg=2.88\n",
            "[465 | 885.16] loss=2.66 avg=2.88\n",
            "[466 | 886.83] loss=2.72 avg=2.88\n",
            "[467 | 888.50] loss=3.25 avg=2.88\n",
            "[468 | 890.17] loss=2.87 avg=2.88\n",
            "[469 | 891.84] loss=2.87 avg=2.88\n",
            "[470 | 893.51] loss=3.24 avg=2.88\n",
            "[471 | 895.18] loss=2.49 avg=2.88\n",
            "[472 | 896.85] loss=3.03 avg=2.88\n",
            "[473 | 898.52] loss=2.46 avg=2.88\n",
            "[474 | 900.19] loss=2.78 avg=2.88\n",
            "[475 | 901.86] loss=2.24 avg=2.87\n",
            "[476 | 903.53] loss=3.37 avg=2.87\n",
            "[477 | 905.19] loss=3.24 avg=2.88\n",
            "[478 | 906.87] loss=2.51 avg=2.87\n",
            "[479 | 908.54] loss=2.90 avg=2.87\n",
            "[480 | 910.21] loss=2.85 avg=2.87\n",
            "[481 | 911.88] loss=2.28 avg=2.87\n",
            "[482 | 913.55] loss=2.93 avg=2.87\n",
            "[483 | 915.22] loss=2.91 avg=2.87\n",
            "[484 | 916.89] loss=2.25 avg=2.86\n",
            "[485 | 918.57] loss=2.83 avg=2.86\n",
            "[486 | 920.24] loss=3.36 avg=2.87\n",
            "[487 | 921.91] loss=3.62 avg=2.88\n",
            "[488 | 923.59] loss=2.36 avg=2.87\n",
            "[489 | 925.26] loss=2.66 avg=2.87\n",
            "[490 | 926.93] loss=3.12 avg=2.87\n",
            "[491 | 928.60] loss=2.33 avg=2.87\n",
            "[492 | 930.28] loss=2.71 avg=2.86\n",
            "[493 | 931.95] loss=2.46 avg=2.86\n",
            "[494 | 933.62] loss=2.99 avg=2.86\n",
            "[495 | 935.29] loss=2.26 avg=2.85\n",
            "[496 | 936.96] loss=2.83 avg=2.85\n",
            "[497 | 938.63] loss=2.63 avg=2.85\n",
            "[498 | 940.32] loss=2.77 avg=2.85\n",
            "[499 | 941.99] loss=2.33 avg=2.85\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "\n",
            "\n",
            "“Is she?” cried Mary, “I guess she is.”\n",
            "\n",
            "“Very good! It is a lady, you know, who\n",
            "said she had found you a great match for her, and would make you\n",
            "her good, and she shall tell you all about it—just as you told me\n",
            "about her husband.”\n",
            "\n",
            "“You must, I suppose, not be so far from you,” said Mary.\n",
            "\n",
            "“I would never doubt for a moment.” “I know her like the\n",
            "head.”\n",
            "\n",
            "“You saw her in the gardens, I suppose?”\n",
            "\n",
            "“No; but I heard her cry, as she had promised, when she\n",
            "made her choice. She did you no better than I ever could,\n",
            "except to say it seemed to her in her own mind to do her great\n",
            "justice, when she made her choice. You ought not to have doubted\n",
            "about it so much as I, for if you, my dear, should have been\n",
            "wondered at her choice, I should have answered you the same.\n",
            "\n",
            "“And that is, the fact, if ever I doubt, that she was right.”\n",
            "\n",
            "“She did tell me about her husband, and she did me the honour to bring\n",
            "me some food, though you must know that I have never been quite\n",
            "provided with one. I went to the window, and she kissed me,\n",
            "and she gave me some very dear tea for my breakfast. Yes,”\n",
            "says Miss Woodhouse; 'you must know how much she loves her husband. I\n",
            "never saw her so much.”\n",
            "\n",
            "“How so?”\n",
            "\n",
            "“I heard her say she loved him more than all the rest. She\n",
            "told me they both should die, and that they would marry one or\n",
            "other in some few years, and that they would have a son. I was\n",
            "very surprised; but, you know, I suppose Miss Woodhouse\n",
            "would not give her so much credit for the marriage as Miss\n",
            "Woodhouse did for her.”\n",
            "\n",
            "“Well, she did say some pretty good things about her husband; but I\n",
            "didn't hear her say anything so great about her husband.”\n",
            "\n",
            "“No: but it would be a great offence, perhaps, to go on with the\n",
            "story; or, rather, it must be done, in the first place.”\n",
            "\n",
            "“My dear Mary, if my father does go mad, or if he die, you ought not\n",
            "to say so much about his wife; they were great friends; but he said very\n",
            "well of no one; at all events.”\n",
            "\n",
            "“Yes, but his wife was very fond of him; which I will not, as much\n",
            "as I might.”\n",
            "\n",
            "“I will; and she is fond of him, but she said, of all persons in the world,\n",
            "Mary is the most charming; and, you know, I will say more than\n",
            "anything about her, if I can help it.”\n",
            "\n",
            "“Oh, if only I knew you could not make a little acquaintance with the\n",
            "Mrs. Woodhouse. I never heard her speak of her husband as much as you used\n",
            "to hear him speak of her.”\n",
            "\n",
            "“Ah! well, when I first came to London; and you were in her company; she\n",
            "was very kind; and you will recollect, that I am told she said one\n",
            "word to this effect to you; and she told me it was a beautiful thing to\n",
            "speak about her husband, when he told her all he knew, and\n",
            "she said, it seemed in a manner to give him pleasure. It is in your\n",
            "face, and in your face, I believe; a man might say it.”\n",
            "\n",
            "“You see, Miss Woodhouse, I think I might say it, but I am afraid it is\n",
            "an idle remark.”\n",
            "\n",
            "“What do you tell me I might tell you! I tell you, this is not the\n",
            "homesick talk of many of our women; but, no doubt, it is the\n",
            "commonly prevalent talk of the men, and of many of our women.”\n",
            "\n",
            "“Yes, that's one reason you should have been in Miss Woodhouse's\n",
            "company. You might have told her the matter, but you should\n",
            "have known I was not in it. I was in a sort of retirement, and\n",
            "I am very glad it was; and I was in a sort of retirement for some days,\n",
            "after a very long argument; but I am, as you have no doubt, glad\n",
            "I had come. And I was in an argument for two or three days with the\n",
            "Miss\n",
            "\n",
            "[500 | 966.74] loss=2.59 avg=2.84\n",
            "[501 | 968.41] loss=3.34 avg=2.85\n",
            "[502 | 970.07] loss=2.28 avg=2.84\n",
            "[503 | 971.72] loss=2.39 avg=2.84\n",
            "[504 | 973.38] loss=3.40 avg=2.84\n",
            "[505 | 975.05] loss=3.18 avg=2.85\n",
            "[506 | 976.71] loss=3.12 avg=2.85\n",
            "[507 | 978.37] loss=2.74 avg=2.85\n",
            "[508 | 980.03] loss=3.59 avg=2.86\n",
            "[509 | 981.68] loss=2.53 avg=2.85\n",
            "[510 | 983.34] loss=3.31 avg=2.86\n",
            "[511 | 985.01] loss=3.06 avg=2.86\n",
            "[512 | 986.67] loss=2.91 avg=2.86\n",
            "[513 | 988.34] loss=2.75 avg=2.86\n",
            "[514 | 990.01] loss=2.77 avg=2.86\n",
            "[515 | 991.68] loss=2.68 avg=2.86\n",
            "[516 | 993.35] loss=3.06 avg=2.86\n",
            "[517 | 995.03] loss=2.70 avg=2.86\n",
            "[518 | 996.71] loss=3.60 avg=2.86\n",
            "[519 | 998.38] loss=3.01 avg=2.87\n",
            "[520 | 1000.06] loss=2.60 avg=2.86\n",
            "[521 | 1001.73] loss=3.03 avg=2.86\n",
            "[522 | 1003.40] loss=3.18 avg=2.87\n",
            "[523 | 1005.09] loss=2.53 avg=2.86\n",
            "[524 | 1006.76] loss=3.02 avg=2.87\n",
            "[525 | 1008.43] loss=3.28 avg=2.87\n",
            "[526 | 1010.12] loss=2.57 avg=2.87\n",
            "[527 | 1011.79] loss=2.64 avg=2.87\n",
            "[528 | 1013.48] loss=3.13 avg=2.87\n",
            "[529 | 1015.17] loss=2.82 avg=2.87\n",
            "[530 | 1016.84] loss=2.34 avg=2.86\n",
            "[531 | 1018.53] loss=2.74 avg=2.86\n",
            "[532 | 1020.22] loss=3.02 avg=2.86\n",
            "[533 | 1021.91] loss=2.76 avg=2.86\n",
            "[534 | 1023.60] loss=2.58 avg=2.86\n",
            "[535 | 1025.27] loss=2.83 avg=2.86\n",
            "[536 | 1026.96] loss=3.03 avg=2.86\n",
            "[537 | 1028.65] loss=2.98 avg=2.86\n",
            "[538 | 1030.32] loss=3.00 avg=2.86\n",
            "[539 | 1031.99] loss=2.65 avg=2.86\n",
            "[540 | 1033.68] loss=2.94 avg=2.86\n",
            "[541 | 1035.36] loss=3.05 avg=2.86\n",
            "[542 | 1037.05] loss=2.41 avg=2.86\n",
            "[543 | 1038.72] loss=3.04 avg=2.86\n",
            "[544 | 1040.39] loss=2.66 avg=2.86\n",
            "[545 | 1042.07] loss=2.49 avg=2.85\n",
            "[546 | 1043.75] loss=2.48 avg=2.85\n",
            "[547 | 1045.43] loss=2.95 avg=2.85\n",
            "[548 | 1047.12] loss=3.23 avg=2.86\n",
            "[549 | 1048.79] loss=2.96 avg=2.86\n",
            "[550 | 1050.46] loss=2.69 avg=2.86\n",
            "[551 | 1052.13] loss=2.49 avg=2.85\n",
            "[552 | 1053.81] loss=2.93 avg=2.85\n",
            "[553 | 1055.48] loss=2.44 avg=2.85\n",
            "[554 | 1057.15] loss=2.75 avg=2.85\n",
            "[555 | 1058.82] loss=2.75 avg=2.85\n",
            "[556 | 1060.49] loss=2.71 avg=2.84\n",
            "[557 | 1062.16] loss=2.84 avg=2.84\n",
            "[558 | 1063.82] loss=2.65 avg=2.84\n",
            "[559 | 1065.49] loss=2.74 avg=2.84\n",
            "[560 | 1067.16] loss=3.08 avg=2.84\n",
            "[561 | 1068.83] loss=3.36 avg=2.85\n",
            "[562 | 1070.50] loss=2.81 avg=2.85\n",
            "[563 | 1072.17] loss=3.21 avg=2.85\n",
            "[564 | 1073.84] loss=2.74 avg=2.85\n",
            "[565 | 1075.51] loss=3.20 avg=2.85\n",
            "[566 | 1077.19] loss=2.94 avg=2.86\n",
            "[567 | 1078.85] loss=2.76 avg=2.85\n",
            "[568 | 1080.53] loss=3.12 avg=2.86\n",
            "[569 | 1082.20] loss=3.30 avg=2.86\n",
            "[570 | 1083.87] loss=2.75 avg=2.86\n",
            "[571 | 1085.54] loss=3.09 avg=2.86\n",
            "[572 | 1087.21] loss=2.72 avg=2.86\n",
            "[573 | 1088.88] loss=3.25 avg=2.87\n",
            "[574 | 1090.55] loss=2.41 avg=2.86\n",
            "[575 | 1092.22] loss=2.53 avg=2.86\n",
            "[576 | 1093.90] loss=2.93 avg=2.86\n",
            "[577 | 1095.58] loss=1.31 avg=2.84\n",
            "[578 | 1097.25] loss=3.14 avg=2.85\n",
            "[579 | 1098.94] loss=3.80 avg=2.86\n",
            "[580 | 1100.62] loss=3.42 avg=2.86\n",
            "[581 | 1102.30] loss=3.28 avg=2.87\n",
            "[582 | 1103.98] loss=3.16 avg=2.87\n",
            "[583 | 1105.67] loss=3.43 avg=2.87\n",
            "[584 | 1107.36] loss=2.76 avg=2.87\n",
            "[585 | 1109.03] loss=2.77 avg=2.87\n",
            "[586 | 1110.71] loss=2.72 avg=2.87\n",
            "[587 | 1112.39] loss=2.77 avg=2.87\n",
            "[588 | 1114.07] loss=2.60 avg=2.87\n",
            "[589 | 1115.74] loss=2.67 avg=2.86\n",
            "[590 | 1117.41] loss=3.14 avg=2.87\n",
            "[591 | 1119.09] loss=3.01 avg=2.87\n",
            "[592 | 1120.77] loss=2.57 avg=2.87\n",
            "[593 | 1122.45] loss=3.02 avg=2.87\n",
            "[594 | 1124.12] loss=0.57 avg=2.84\n",
            "[595 | 1125.80] loss=2.86 avg=2.84\n",
            "[596 | 1127.47] loss=2.61 avg=2.84\n",
            "[597 | 1129.15] loss=3.06 avg=2.84\n",
            "[598 | 1130.82] loss=3.13 avg=2.85\n",
            "[599 | 1132.49] loss=2.51 avg=2.84\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "ains in the streets of Tivoli, on the morrow, and, with the last of them, the last of all the children of those people,—when is the last of all the children of Tivoli?”\n",
            "\n",
            "The voice of the Count at last spoke with a loud whisper,\n",
            "--§§§§“§§§“\n",
            "\n",
            "“And with the same force in the name of the Father of all, and Mother of all.”\n",
            "\n",
            "“All together! All together!”\n",
            "\n",
            "“And with the same force in the name of the Son and Mother of all...”\n",
            "\n",
            "“So, so well!” said the Count. “So you like me!”\n",
            "\n",
            "At these words and those which followed, the Count’s face darkened; but, I should think, because he knew\n",
            "the children’s minds. The Count, too, felt that something was wrong. For he\n",
            "thought.\n",
            "\n",
            "“My child?” he ventured to ask.\n",
            "\n",
            "“Yes, my child.” said he. “So, so very well!”\n",
            "\n",
            "“And with the same force in the name of the Son and Mother of all, you are right!”\n",
            "\n",
            "“Are you?”\n",
            "\n",
            "“Do you hear me?\n",
            "\n",
            "“Yes. And with the same force in the name of the Son and Mother of all, you are right!”\n",
            "\n",
            "Again, that strange whisper of the Count’s, the whispering of the child’s mouth,\n",
            "was heard in the same echo, in the same echo.\n",
            "\n",
            "“So much is my mother wrong, I mean, my father,” said the child. 'My father’s\n",
            "mother was right.”\n",
            "\n",
            "The voice heard, like the other, was loud in the streets of Tivoli.\n",
            "\n",
            "“This is a poor man,” said the Count.\n",
            "\n",
            "“But all in all,” replied the child, “it is a very good child!”\n",
            "\n",
            "“And the child, that child, for whom you have been looking,” said the\n",
            "count, “has made good.”\n",
            "\n",
            "“But then your mother didn’t speak, she doesn’t speak.”\n",
            "\n",
            "“What have you seen, my boy?”\n",
            "\n",
            "“What has become of your mother? What have become of the children, my child?”\n",
            "\n",
            "“Is it that they all are dead?” said the child.\n",
            "\n",
            "“I am convinced, Mr. Marnette,” said the count, “that they are all dead, that is all of them.”\n",
            "\n",
            "That little voice, the voice which had been so loud for a long time, ceased and\n",
            "came to a halt in the streets. A cry could be heard in every\n",
            "gated community, and the voice with the strange little echo of the little girl,\n",
            "with the sudden change of its rhythm from fast, to slow, to soft, to loud and\n",
            "still, to the long faint and rapid sound of the children’s cries,\n",
            "from every gated house to the houses of the world. And when\n",
            "another cry could be heard, and that very very voice with that\n",
            "same faint, hurried echo, a certain fear might well have come of its\n",
            "being all broken by this sudden silence. That fear could be expressed\n",
            "with the child’s words, “They’ll be killed,” and with the\n",
            "voice of the child itself.\n",
            "\n",
            "“Mr. Marnette,” said the count, “the children, the children are dead. And\n",
            "all have been killed, for these are the old things. A little girl here\n",
            "was the last of all the children.”\n",
            "\n",
            "The cries were heard all round the houses of the citizens. People\n",
            "were rushing out of the houses with the child’s face toward them.\n",
            "All the houses, especially those of the people, and especially those of\n",
            "the people, in those days were occupied, as it were, by men, women,\n",
            "and children, or at least men and women and children in society. In the\n",
            "streets and the streets they ran and they crawled. The children in the\n",
            "streets, and there too, they ran and their little faces were on everyone\n",
            "who was passing. Men were passing and women were passing with their\n",
            "little faces on them, and young children walking about with the face\n",
            "of their little heads on everyone they passed. Children ran and their\n",
            "little faces were on others they passed, and mothers and fathers\n",
            "passed with their little faces still on ones of them that passed.\n",
            "\n",
            "[600 | 1157.20] loss=2.81 avg=2.84\n",
            "[601 | 1158.87] loss=2.65 avg=2.84\n",
            "[602 | 1160.54] loss=2.87 avg=2.84\n",
            "[603 | 1162.20] loss=2.74 avg=2.84\n",
            "[604 | 1163.87] loss=2.93 avg=2.84\n",
            "[605 | 1165.54] loss=2.53 avg=2.84\n",
            "[606 | 1167.21] loss=2.96 avg=2.84\n",
            "[607 | 1168.87] loss=2.83 avg=2.84\n",
            "[608 | 1170.54] loss=3.01 avg=2.84\n",
            "[609 | 1172.19] loss=2.85 avg=2.84\n",
            "[610 | 1173.86] loss=3.06 avg=2.84\n",
            "[611 | 1175.51] loss=2.87 avg=2.84\n",
            "[612 | 1177.18] loss=3.16 avg=2.85\n",
            "[613 | 1178.84] loss=2.16 avg=2.84\n",
            "[614 | 1180.51] loss=2.61 avg=2.84\n",
            "[615 | 1182.18] loss=2.94 avg=2.84\n",
            "[616 | 1183.84] loss=3.01 avg=2.84\n",
            "[617 | 1185.51] loss=2.27 avg=2.83\n",
            "[618 | 1187.18] loss=2.14 avg=2.83\n",
            "[619 | 1188.85] loss=2.74 avg=2.83\n",
            "[620 | 1190.51] loss=3.57 avg=2.83\n",
            "[621 | 1192.17] loss=3.02 avg=2.84\n",
            "[622 | 1193.84] loss=3.09 avg=2.84\n",
            "[623 | 1195.52] loss=3.14 avg=2.84\n",
            "[624 | 1197.19] loss=2.70 avg=2.84\n",
            "[625 | 1198.86] loss=2.41 avg=2.84\n",
            "[626 | 1200.53] loss=2.52 avg=2.83\n",
            "[627 | 1202.20] loss=3.41 avg=2.84\n",
            "[628 | 1203.87] loss=3.06 avg=2.84\n",
            "[629 | 1205.53] loss=2.87 avg=2.84\n",
            "[630 | 1207.20] loss=3.02 avg=2.84\n",
            "[631 | 1208.87] loss=2.96 avg=2.84\n",
            "[632 | 1210.54] loss=2.89 avg=2.84\n",
            "[633 | 1212.21] loss=2.96 avg=2.85\n",
            "[634 | 1213.88] loss=2.85 avg=2.85\n",
            "[635 | 1215.54] loss=2.70 avg=2.84\n",
            "[636 | 1217.22] loss=2.78 avg=2.84\n",
            "[637 | 1218.89] loss=2.43 avg=2.84\n",
            "[638 | 1220.56] loss=3.41 avg=2.85\n",
            "[639 | 1222.23] loss=2.46 avg=2.84\n",
            "[640 | 1223.90] loss=3.01 avg=2.84\n",
            "[641 | 1225.57] loss=2.66 avg=2.84\n",
            "[642 | 1227.24] loss=2.57 avg=2.84\n",
            "[643 | 1228.91] loss=2.71 avg=2.84\n",
            "[644 | 1230.58] loss=3.01 avg=2.84\n",
            "[645 | 1232.25] loss=3.08 avg=2.84\n",
            "[646 | 1233.92] loss=2.92 avg=2.84\n",
            "[647 | 1235.59] loss=2.57 avg=2.84\n",
            "[648 | 1237.27] loss=2.36 avg=2.83\n",
            "[649 | 1238.95] loss=2.73 avg=2.83\n",
            "[650 | 1240.64] loss=2.78 avg=2.83\n",
            "[651 | 1242.30] loss=3.18 avg=2.84\n",
            "[652 | 1243.98] loss=2.78 avg=2.84\n",
            "[653 | 1245.67] loss=3.19 avg=2.84\n",
            "[654 | 1247.34] loss=2.84 avg=2.84\n",
            "[655 | 1249.02] loss=2.79 avg=2.84\n",
            "[656 | 1250.69] loss=3.16 avg=2.84\n",
            "[657 | 1252.38] loss=2.68 avg=2.84\n",
            "[658 | 1254.05] loss=3.40 avg=2.85\n",
            "[659 | 1255.72] loss=2.79 avg=2.85\n",
            "[660 | 1257.41] loss=2.72 avg=2.84\n",
            "[661 | 1259.09] loss=2.95 avg=2.85\n",
            "[662 | 1260.78] loss=2.54 avg=2.84\n",
            "[663 | 1262.46] loss=1.65 avg=2.83\n",
            "[664 | 1264.15] loss=3.21 avg=2.83\n",
            "[665 | 1265.85] loss=2.61 avg=2.83\n",
            "[666 | 1267.53] loss=2.85 avg=2.83\n",
            "[667 | 1269.22] loss=2.44 avg=2.83\n",
            "[668 | 1270.91] loss=2.97 avg=2.83\n",
            "[669 | 1272.58] loss=2.69 avg=2.83\n",
            "[670 | 1274.25] loss=2.81 avg=2.83\n",
            "[671 | 1275.93] loss=3.28 avg=2.83\n",
            "[672 | 1277.61] loss=2.71 avg=2.83\n",
            "[673 | 1279.31] loss=3.44 avg=2.84\n",
            "[674 | 1280.98] loss=2.93 avg=2.84\n",
            "[675 | 1282.67] loss=2.70 avg=2.84\n",
            "[676 | 1284.36] loss=2.65 avg=2.84\n",
            "[677 | 1286.03] loss=2.90 avg=2.84\n",
            "[678 | 1287.72] loss=2.92 avg=2.84\n",
            "[679 | 1289.41] loss=2.77 avg=2.84\n",
            "[680 | 1291.08] loss=2.86 avg=2.84\n",
            "[681 | 1292.77] loss=2.68 avg=2.83\n",
            "[682 | 1294.44] loss=2.66 avg=2.83\n",
            "[683 | 1296.13] loss=2.95 avg=2.83\n",
            "[684 | 1297.80] loss=2.95 avg=2.84\n",
            "[685 | 1299.49] loss=2.35 avg=2.83\n",
            "[686 | 1301.18] loss=2.95 avg=2.83\n",
            "[687 | 1302.85] loss=3.49 avg=2.84\n",
            "[688 | 1304.54] loss=2.41 avg=2.83\n",
            "[689 | 1306.23] loss=3.13 avg=2.84\n",
            "[690 | 1307.92] loss=2.93 avg=2.84\n",
            "[691 | 1309.61] loss=2.75 avg=2.84\n",
            "[692 | 1311.29] loss=2.70 avg=2.84\n",
            "[693 | 1312.98] loss=2.69 avg=2.83\n",
            "[694 | 1314.67] loss=2.36 avg=2.83\n",
            "[695 | 1316.36] loss=3.09 avg=2.83\n",
            "[696 | 1318.06] loss=2.88 avg=2.83\n",
            "[697 | 1319.74] loss=2.51 avg=2.83\n",
            "[698 | 1321.43] loss=3.40 avg=2.84\n",
            "[699 | 1323.11] loss=2.56 avg=2.83\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "but then again they didn't, it was just that the one guy who was a bit more active in their lives was a bit too much in his mind.\n",
            "\n",
            "'I knew it from talking to him about it that he was not so much interested in the\n",
            "other boys,' he had said. 'I really thought that if he were there—when they thought he was\n",
            "not—he would have to let them have fun.'\n",
            "\n",
            "But what good was it to be fond of his fellows. He had a right to be free and have\n",
            "a view, and there was nothing wrong with that; but then there came the\n",
            "wholly unnecessary and altogether silly incident in his old life and the idea of\n",
            "having to put that aside.\n",
            "\n",
            "'You're not going to marry me?' he said. 'But how do they suppose\n",
            "you're going to go down to that country after you get married? Will all\n",
            "you do, I wonder, with all the ladies and a man like them?'\n",
            "\n",
            "'No, the girls!' said he with a smile. 'The sisters--I've no\n",
            "choice.'\n",
            "\n",
            "'She hasn't got any sisters.'\n",
            "\n",
            "'Oh!' said he. 'That's no matter.'\n",
            "\n",
            "'I never thought of it,' said she. 'Of course they'll have them--that's\n",
            "not a problem for me. That'll be all. When you come to the end of\n",
            "your life you'll get married, right? When you're dead!'\n",
            "\n",
            "'I'd have rather been unmarried than married. My aunt is to be my\n",
            "wife and I ain't no good at it.'\n",
            "\n",
            "'Well,' she said, 'if she's married, you don't see the thing.'\n",
            "\n",
            "He laughed, and said, 'Don't you understand?'\n",
            "\n",
            "'All right,' she said. 'And if she's a good fellow I should marry her.\n",
            "I might think of that. If she didn't have any sisters she'd be\n",
            "married to some good fellow, which in my opinion would be quite a\n",
            "lodge of a wife, you know, I might think of that.'\n",
            "\n",
            "'It's nothing!'\n",
            "\n",
            "'But you don't get married, either,' said he. 'What, you can't\n",
            "think of that.'\n",
            "\n",
            "'No, for ever,' said she. 'I'm going to get married.'\n",
            "\n",
            "'Yes, I am,' he said. 'And no trouble.'\n",
            "\n",
            "'Of course!' she said. 'And if you've got nothing better to do,\n",
            "you'd better get married. You'll do a good thing.'\n",
            "\n",
            "'Well,' he said, 'I'll do nothing.'\n",
            "\n",
            "'You will,' she said. 'I won't let you have a heart, for\n",
            "that's a good thing.'\n",
            "\n",
            "The subject was now going on with its own peculiar difficulty, which could\n",
            "and ought to be explained by various other considerations. Miss Wylie\n",
            "danced, however, as she pleased, and had an equally good time.\n",
            "\n",
            "'I've got a thing with the ladies,' said she. 'One of them is\n",
            "mad, and she'd been a bit off on one side.'\n",
            "\n",
            "'But you'll go?' said he.\n",
            "\n",
            "'I'm doing something about it,' she said. 'I've had some\n",
            "things in my mind and it's very bad for me. I think I'd rather\n",
            "be alone myself. You'll get married or I won't.'\n",
            "\n",
            "'I'm not going,' he said, with a little frown, and went on.\n",
            "\n",
            "'And I am sure there's not a poor wench in the world to get a\n",
            "marriage of that kind from,' she said.\n",
            "\n",
            "'No, no, no,' said he. 'No, no, no, no. Well, I haven't--my brother\n",
            "is married at last--and here I am. And I'm sorry for him. I thought I'd\n",
            "be better off just staying. If I'd known his will I'd tell him\n",
            "so now; just think of it--to be one step and one year away.\n",
            "I just can't do it. I'd have been married for a long time now.\n",
            "I've got a good deal to give him. I'd give them up.'\n",
            "\n",
            "'How much?' she asked.\n",
            "\n",
            "'A lot, you understand.'\n",
            "\n",
            "'Well,' she said. 'I want to get married. We need a man like\n",
            "you in our house, and it would be much easier.'\n",
            "\n",
            "'I think I might,' he said.\n",
            "\n",
            "'I think you might,' she said. 'Oh, well, I'd put a lot of\n",
            "work into it. It seems like it, doesn't it? I've got some money that\n",
            "might turn into something.'\n",
            "\n",
            "'And you mustn't tell anybody I've got\n",
            "\n",
            "[700 | 1347.60] loss=3.25 avg=2.84\n",
            "[701 | 1349.26] loss=3.24 avg=2.84\n",
            "[702 | 1350.91] loss=3.04 avg=2.84\n",
            "[703 | 1352.56] loss=2.79 avg=2.84\n",
            "[704 | 1354.21] loss=3.08 avg=2.84\n",
            "[705 | 1355.86] loss=3.00 avg=2.85\n",
            "[706 | 1357.52] loss=2.41 avg=2.84\n",
            "[707 | 1359.17] loss=3.01 avg=2.84\n",
            "[708 | 1360.82] loss=2.75 avg=2.84\n",
            "[709 | 1362.47] loss=2.63 avg=2.84\n",
            "[710 | 1364.11] loss=0.32 avg=2.81\n",
            "[711 | 1365.76] loss=2.66 avg=2.81\n",
            "[712 | 1367.42] loss=3.19 avg=2.82\n",
            "[713 | 1369.07] loss=2.74 avg=2.82\n",
            "[714 | 1370.73] loss=2.88 avg=2.82\n",
            "[715 | 1372.39] loss=2.50 avg=2.81\n",
            "[716 | 1374.04] loss=3.15 avg=2.82\n",
            "[717 | 1375.69] loss=2.81 avg=2.82\n",
            "[718 | 1377.36] loss=2.81 avg=2.82\n",
            "[719 | 1379.02] loss=2.71 avg=2.82\n",
            "[720 | 1380.69] loss=2.46 avg=2.81\n",
            "[721 | 1382.35] loss=3.02 avg=2.81\n",
            "[722 | 1384.03] loss=2.71 avg=2.81\n",
            "[723 | 1385.70] loss=2.25 avg=2.81\n",
            "[724 | 1387.37] loss=2.31 avg=2.80\n",
            "[725 | 1389.04] loss=2.78 avg=2.80\n",
            "[726 | 1390.72] loss=3.01 avg=2.80\n",
            "[727 | 1392.40] loss=2.82 avg=2.80\n",
            "[728 | 1394.09] loss=3.36 avg=2.81\n",
            "[729 | 1395.78] loss=2.47 avg=2.81\n",
            "[730 | 1397.48] loss=3.02 avg=2.81\n",
            "[731 | 1399.17] loss=3.46 avg=2.82\n",
            "[732 | 1400.87] loss=3.12 avg=2.82\n",
            "[733 | 1402.56] loss=3.24 avg=2.82\n",
            "[734 | 1404.25] loss=3.20 avg=2.83\n",
            "[735 | 1405.95] loss=3.00 avg=2.83\n",
            "[736 | 1407.64] loss=3.45 avg=2.83\n",
            "[737 | 1409.33] loss=2.39 avg=2.83\n",
            "[738 | 1411.01] loss=2.25 avg=2.82\n",
            "[739 | 1412.71] loss=2.94 avg=2.83\n",
            "[740 | 1414.40] loss=3.32 avg=2.83\n",
            "[741 | 1416.08] loss=2.57 avg=2.83\n",
            "[742 | 1417.76] loss=3.06 avg=2.83\n",
            "[743 | 1419.44] loss=2.79 avg=2.83\n",
            "[744 | 1421.12] loss=2.47 avg=2.83\n",
            "[745 | 1422.79] loss=2.73 avg=2.83\n",
            "[746 | 1424.46] loss=3.63 avg=2.83\n",
            "[747 | 1426.13] loss=2.77 avg=2.83\n",
            "[748 | 1427.79] loss=2.63 avg=2.83\n",
            "[749 | 1429.46] loss=3.19 avg=2.83\n",
            "[750 | 1431.13] loss=3.23 avg=2.84\n",
            "[751 | 1432.80] loss=2.73 avg=2.84\n",
            "[752 | 1434.47] loss=2.28 avg=2.83\n",
            "[753 | 1436.15] loss=2.80 avg=2.83\n",
            "[754 | 1437.82] loss=3.06 avg=2.83\n",
            "[755 | 1439.49] loss=2.84 avg=2.83\n",
            "[756 | 1441.16] loss=3.22 avg=2.84\n",
            "[757 | 1442.82] loss=2.94 avg=2.84\n",
            "[758 | 1444.49] loss=3.34 avg=2.84\n",
            "[759 | 1446.14] loss=2.95 avg=2.84\n",
            "[760 | 1447.80] loss=2.79 avg=2.84\n",
            "[761 | 1449.47] loss=2.79 avg=2.84\n",
            "[762 | 1451.14] loss=2.78 avg=2.84\n",
            "[763 | 1452.81] loss=2.98 avg=2.84\n",
            "[764 | 1454.48] loss=3.43 avg=2.85\n",
            "[765 | 1456.15] loss=2.79 avg=2.85\n",
            "[766 | 1457.82] loss=2.74 avg=2.85\n",
            "[767 | 1459.49] loss=2.36 avg=2.84\n",
            "[768 | 1461.16] loss=3.27 avg=2.85\n",
            "[769 | 1462.83] loss=2.89 avg=2.85\n",
            "[770 | 1464.51] loss=2.55 avg=2.84\n",
            "[771 | 1466.18] loss=2.88 avg=2.85\n",
            "[772 | 1467.86] loss=2.68 avg=2.84\n",
            "[773 | 1469.53] loss=3.22 avg=2.85\n",
            "[774 | 1471.20] loss=2.40 avg=2.84\n",
            "[775 | 1472.88] loss=2.44 avg=2.84\n",
            "[776 | 1474.55] loss=3.11 avg=2.84\n",
            "[777 | 1476.23] loss=2.97 avg=2.84\n",
            "[778 | 1477.91] loss=2.60 avg=2.84\n",
            "[779 | 1479.60] loss=2.19 avg=2.83\n",
            "[780 | 1481.28] loss=2.99 avg=2.84\n",
            "[781 | 1482.96] loss=2.66 avg=2.83\n",
            "[782 | 1484.64] loss=2.87 avg=2.83\n",
            "[783 | 1486.33] loss=3.24 avg=2.84\n",
            "[784 | 1488.00] loss=2.67 avg=2.84\n",
            "[785 | 1489.67] loss=3.21 avg=2.84\n",
            "[786 | 1491.34] loss=2.24 avg=2.83\n",
            "[787 | 1493.03] loss=2.61 avg=2.83\n",
            "[788 | 1494.71] loss=2.61 avg=2.83\n",
            "[789 | 1496.40] loss=2.73 avg=2.83\n",
            "[790 | 1498.07] loss=2.85 avg=2.83\n",
            "[791 | 1499.76] loss=2.97 avg=2.83\n",
            "[792 | 1501.44] loss=2.96 avg=2.83\n",
            "[793 | 1503.12] loss=3.04 avg=2.83\n",
            "[794 | 1504.81] loss=2.63 avg=2.83\n",
            "[795 | 1506.49] loss=2.50 avg=2.83\n",
            "[796 | 1508.17] loss=2.49 avg=2.83\n",
            "[797 | 1509.86] loss=2.72 avg=2.82\n",
            "[798 | 1511.53] loss=2.91 avg=2.83\n",
            "[799 | 1513.21] loss=2.85 avg=2.83\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " remarked\n",
            "in a tone that seemed rather timid, and very probably was not that\n",
            "which he would have been proud of.\n",
            "\n",
            "“Yes, of course! This was always a subject of great interest to you, and\n",
            "I think, perhaps, you thought my opinion of the matter,” said Mr. Wickham.\n",
            "\n",
            "“Then the old man and myself will see that no one, no one, “should\n",
            "understand why I should approve;”\n",
            "\n",
            "“Understand why I should,” replied Mr. Wickham, with a hint of\n",
            "trying to impress Mr. Moore.\n",
            "\n",
            "When Mr. Wickham had finished speaking, he said nothing.\n",
            "\n",
            "Mr. Moore, Mr. Moore, and Mr. Wickham continued a short time, as they\n",
            "did the night before. Then the three men, leaving the apartment, walked\n",
            "to their rooms. Mr. Wickham returned to his old friends, and said,\n",
            "“It is not a part of the business of this house to speak of any\n",
            "things that may appear to us to be a little odd. And,\n",
            "in case of any doubts, it is right to say so, and give the\n",
            "full information to the jury, if you will. Then we will leave\n",
            "you, sir, for the night, and have supper together.”\n",
            "\n",
            "This was done, and Mr. Moore and Mr. Wickham parted at the door. The third young\n",
            "friend went out on the stairs to the garden, where he met his\n",
            "older friend in the garden, and they embraced each other.\n",
            "\n",
            "“What! it is a pretty house, and the old man and I are very happy,—I\n",
            "am sure, of everything. I have never seen one that better, I have\n",
            "never known one that could be more agreeable to me, sir, and I\n",
            "love her.”\n",
            "\n",
            "“It would be more pleasant if she could write better than you have,” Mr.\n",
            "Wickham said, smiling, as he handed me his letters.\n",
            "\n",
            "“Yes, I am satisfied, Mr. Moore, as I am pleased; and I hope you will\n",
            "let me have it with you in due course.”\n",
            "\n",
            "Mr. Wickham glanced at me with the little smile and affection that\n",
            "used to be in his face, and I felt that he did not mean to express these\n",
            "pleasurees any less than it was clear that he felt them; which did\n",
            "measure his whole attitude.\n",
            "\n",
            "When he went to fetch his letter he spoke as he used to in the garden:\n",
            "\n",
            "“I was glad to see my boy, and have the pleasure of speaking with him.”\n",
            "\n",
            "They did not look at each other. But the silence, and the sense of\n",
            "stubborn horror that the young man and Mr. Moore felt, seemed to\n",
            "Mr. Wickham to be a reflection of this, and not a reflection of\n",
            "nothing, which was to him an overwhelming consideration. This\n",
            "was not to say that their feeling was mutual. It was plain to\n",
            "Mr. Wickham that he had to be exceedingly careful of what\n",
            "the old man saw—that he must not have a sense of his own\n",
            "soul against these young men, and must take himself very far from\n",
            "their views.\n",
            "\n",
            "“But,” said Mr. Wickham, putting his hand to his head, “I trust in\n",
            "your favor.”\n",
            "\n",
            "“That,” replied Mr. Wickham, in a very small voice, which he made\n",
            "himself heard over the other. “Very well! I shall take some steps to be\n",
            "sufficient to secure to you the best chance of being a good witness in\n",
            "the matter, but you must do the rest by me.”\n",
            "\n",
            "They did not rise by the door. As Mr. Wickham went upstairs he said,\n",
            "\n",
            "“It is not a very easy thing to answer the question—but, it is\n",
            "my opinion, if my boy is alive, he has nothing to fear from\n",
            "such old gentlemen,” and then proceeded, in a very low voice,\n",
            "“and that, I am sure, you have already examined, sir. There is\n",
            "nothing in the world but good; and if you will leave, my boy, I\n",
            "shall come down at midnight.”\n",
            "\n",
            "When he came away he looked at me in the face with the utmost solemnity\n",
            "and took my hand in his, as if to make a solemn promise.\n",
            "\n",
            "“I will come very soon,” he said, holding it out to me.\n",
            "\n",
            "He turned the letter, and it was signed by my dear mother, and by Mr.\n",
            "Wickham as a brother, that he must have the best of health.\n",
            "\n",
            "He had a great deal to be\n",
            "\n",
            "[800 | 1537.83] loss=2.85 avg=2.83\n",
            "[801 | 1539.49] loss=3.02 avg=2.83\n",
            "[802 | 1541.15] loss=2.50 avg=2.82\n",
            "[803 | 1542.82] loss=2.57 avg=2.82\n",
            "[804 | 1544.48] loss=2.97 avg=2.82\n",
            "[805 | 1546.14] loss=2.84 avg=2.82\n",
            "[806 | 1547.80] loss=3.18 avg=2.83\n",
            "[807 | 1549.46] loss=2.72 avg=2.83\n",
            "[808 | 1551.13] loss=2.80 avg=2.83\n",
            "[809 | 1552.79] loss=2.40 avg=2.82\n",
            "[810 | 1554.45] loss=3.48 avg=2.83\n",
            "[811 | 1556.11] loss=2.41 avg=2.82\n",
            "[812 | 1557.77] loss=2.40 avg=2.82\n",
            "[813 | 1559.44] loss=3.29 avg=2.82\n",
            "[814 | 1561.10] loss=2.93 avg=2.83\n",
            "[815 | 1562.77] loss=2.93 avg=2.83\n",
            "[816 | 1564.43] loss=2.93 avg=2.83\n",
            "[817 | 1566.10] loss=2.39 avg=2.82\n",
            "[818 | 1567.77] loss=2.92 avg=2.82\n",
            "[819 | 1569.44] loss=2.72 avg=2.82\n",
            "[820 | 1571.11] loss=2.91 avg=2.82\n",
            "[821 | 1572.78] loss=3.11 avg=2.83\n",
            "[822 | 1574.46] loss=3.74 avg=2.84\n",
            "[823 | 1576.13] loss=2.71 avg=2.83\n",
            "[824 | 1577.79] loss=3.19 avg=2.84\n",
            "[825 | 1579.46] loss=2.55 avg=2.84\n",
            "[826 | 1581.14] loss=3.12 avg=2.84\n",
            "[827 | 1582.81] loss=3.28 avg=2.84\n",
            "[828 | 1584.48] loss=2.65 avg=2.84\n",
            "[829 | 1586.16] loss=3.01 avg=2.84\n",
            "[830 | 1587.84] loss=2.85 avg=2.84\n",
            "[831 | 1589.52] loss=3.03 avg=2.84\n",
            "[832 | 1591.19] loss=2.22 avg=2.84\n",
            "[833 | 1592.87] loss=2.99 avg=2.84\n",
            "[834 | 1594.56] loss=3.27 avg=2.84\n",
            "[835 | 1596.24] loss=2.75 avg=2.84\n",
            "[836 | 1597.92] loss=2.81 avg=2.84\n",
            "[837 | 1599.60] loss=2.77 avg=2.84\n",
            "[838 | 1601.29] loss=3.09 avg=2.84\n",
            "[839 | 1602.97] loss=2.85 avg=2.84\n",
            "[840 | 1604.64] loss=2.58 avg=2.84\n",
            "[841 | 1606.31] loss=2.82 avg=2.84\n",
            "[842 | 1608.00] loss=3.31 avg=2.85\n",
            "[843 | 1609.68] loss=2.99 avg=2.85\n",
            "[844 | 1611.36] loss=2.67 avg=2.85\n",
            "[845 | 1613.05] loss=2.99 avg=2.85\n",
            "[846 | 1614.74] loss=2.38 avg=2.84\n",
            "[847 | 1616.42] loss=2.97 avg=2.84\n",
            "[848 | 1618.10] loss=2.56 avg=2.84\n",
            "[849 | 1619.78] loss=3.28 avg=2.85\n",
            "[850 | 1621.46] loss=3.00 avg=2.85\n",
            "[851 | 1623.13] loss=2.88 avg=2.85\n",
            "[852 | 1624.82] loss=2.95 avg=2.85\n",
            "[853 | 1626.50] loss=2.81 avg=2.85\n",
            "[854 | 1628.18] loss=2.82 avg=2.85\n",
            "[855 | 1629.87] loss=2.49 avg=2.84\n",
            "[856 | 1631.55] loss=2.68 avg=2.84\n",
            "[857 | 1633.23] loss=2.34 avg=2.84\n",
            "[858 | 1634.90] loss=3.03 avg=2.84\n",
            "[859 | 1636.58] loss=2.81 avg=2.84\n",
            "[860 | 1638.27] loss=2.79 avg=2.84\n",
            "[861 | 1639.95] loss=2.90 avg=2.84\n",
            "[862 | 1641.65] loss=2.44 avg=2.84\n",
            "[863 | 1643.32] loss=2.77 avg=2.83\n",
            "[864 | 1645.00] loss=2.91 avg=2.84\n",
            "[865 | 1646.70] loss=3.19 avg=2.84\n",
            "[866 | 1648.37] loss=3.15 avg=2.84\n",
            "[867 | 1650.05] loss=3.44 avg=2.85\n",
            "[868 | 1651.73] loss=3.01 avg=2.85\n",
            "[869 | 1653.42] loss=2.64 avg=2.85\n",
            "[870 | 1655.08] loss=2.75 avg=2.85\n",
            "[871 | 1656.75] loss=2.51 avg=2.84\n",
            "[872 | 1658.43] loss=2.88 avg=2.84\n",
            "[873 | 1660.12] loss=2.63 avg=2.84\n",
            "[874 | 1661.79] loss=2.64 avg=2.84\n",
            "[875 | 1663.47] loss=2.63 avg=2.84\n",
            "[876 | 1665.16] loss=2.33 avg=2.83\n",
            "[877 | 1666.83] loss=2.84 avg=2.83\n",
            "[878 | 1668.51] loss=2.44 avg=2.83\n",
            "[879 | 1670.18] loss=3.16 avg=2.83\n",
            "[880 | 1671.86] loss=2.87 avg=2.83\n",
            "[881 | 1673.55] loss=2.60 avg=2.83\n",
            "[882 | 1675.22] loss=2.71 avg=2.83\n",
            "[883 | 1676.90] loss=2.68 avg=2.83\n",
            "[884 | 1678.58] loss=3.08 avg=2.83\n",
            "[885 | 1680.26] loss=2.95 avg=2.83\n",
            "[886 | 1681.94] loss=2.92 avg=2.83\n",
            "[887 | 1683.62] loss=3.20 avg=2.84\n",
            "[888 | 1685.30] loss=2.13 avg=2.83\n",
            "[889 | 1686.98] loss=3.20 avg=2.83\n",
            "[890 | 1688.65] loss=2.25 avg=2.83\n",
            "[891 | 1690.34] loss=2.82 avg=2.83\n",
            "[892 | 1692.01] loss=3.47 avg=2.83\n",
            "[893 | 1693.69] loss=3.16 avg=2.84\n",
            "[894 | 1695.38] loss=3.18 avg=2.84\n",
            "[895 | 1697.07] loss=2.98 avg=2.84\n",
            "[896 | 1698.76] loss=2.91 avg=2.84\n",
            "[897 | 1700.43] loss=2.85 avg=2.84\n",
            "[898 | 1702.12] loss=3.02 avg=2.84\n",
            "[899 | 1703.79] loss=3.38 avg=2.85\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " around her neck\n",
            "and shoulder was a small yellow-lacquered knife.\n",
            "\n",
            "'I'd thought you would be a bit more careful,' thought Harriet; 'what do you\n",
            "mean by a little, I wonder?'\n",
            "\n",
            "'Well, I should say a hair to the left, a hair to the right; the blade is\n",
            "just under your wrist, ma'am!' said the count, nodding his head, and\n",
            "following its progress with a grin. 'My own little thing!'\n",
            "\n",
            "'And what shall we do with it, ma'am?' muttered Harriet.\n",
            "\n",
            "                     ○ (1820 -1828)                  [ edit ]\n",
            "\n",
            "'And now the good thing about it—'\n",
            "\n",
            "'If you are so fortunate, sir—if you can get one of these in a day—what\n",
            "wish to put it in, ma'am?' he called up.\n",
            "\n",
            "'The man with a great head of hair who goes by the name of 'Hercule'\n",
            "must be very fortunate, Miss,' said Harriet, smiling at him in the same way\n",
            "with which she had smiled on her mother when they first met. 'When\n",
            "I have it, I shall wear it; when I have it, I shall kill him, ma'am!'\n",
            "\n",
            "'Good! Miss, there's another one with a bit less hair—a good\n",
            "thing for you!' suggested the count.\n",
            "\n",
            "                 ○ (1828 -1880)                  [ edit ]\n",
            "\n",
            "I shall have one on myself, ma'am,' said Harriet, giving\n",
            "a finger to the thumb that lay on her elbow, and letting it drop\n",
            "down to where her knuckles touched her skin, where it rested on the\n",
            "shoulder of the man who had been looking at her.\n",
            "\n",
            "'Oh, what a dear sight!' cried the count, 'what an awful sight!'\n",
            "“And you're in love? You can't tell!' said\n",
            "Harriet; and she wiped down her cheeks and the inside of her\n",
            "neck, and again and again took up the knife and began cutting down to the\n",
            "head of her neck. 'I am determined to do exactly as I ought—I shall\n",
            "kill him!' she said.\n",
            "\n",
            "The man with his beard, which was not white as usual and very large,\n",
            "grew up quickly, and began to rise to the man next to her. The little\n",
            "girl with a little scar running down her right side, and growing larger as\n",
            "it went on, seemed to Harriet more like a lion than a sheep.\n",
            "\n",
            "“I must make haste, ma'am, for I won't get the first one. Miss\n",
            "Mailloux, I've got his head on here, ma'am,' cried the count. 'He may be\n",
            "a fawnd to me in a minute.'\n",
            "\n",
            "“That will do, ma'am!' was the answer; and Harriet brought down her\n",
            "bow and took this shot, which she felt as if she would never leave home. It\n",
            "fell just on his head, and with a glance at it, Harriet put it in the\n",
            "hand of a white-haired man, who was now coming up to her, and putting\n",
            "it into his pocket.\n",
            "\n",
            "\"Now come up,\" said she, 'there I will have this one! You don't take\n",
            "it!”\n",
            "\n",
            "The man with a small beard, a hair still hanging down from his chin, turned\n",
            "right round, looking at her, and holding the knife. “I never said\n",
            "I took the knife!” he said; “you'll shoot him!”\n",
            "\n",
            "“Then I'll kill him,' said Harriet; “if you like it my way, Miss\n",
            "Mailloux, and we'll be going down to Marseilles, ma'am!'\n",
            "\n",
            "“Ah, you'll make a great mistake, ma'am!' he said, smiling a smile. “The\n",
            "man who did it is not my master. It was the devil here—it was the\n",
            "blame we wanted to hang on the old witch, who lived in a cave on the\n",
            "side of the hill: and he could not be put out by another who would only\n",
            "put himself in harm's way as a means of procuring mercy for\n",
            "himself; for he couldn't have taken the pain if he had lived to see\n",
            "it: the man he killed was a man he thought himself safe in, in order to\n",
            "kill you, Miss. You have done this to the devil, ma'am: if you\n",
            "\n",
            "[900 | 1729.02] loss=2.82 avg=2.85\n",
            "[901 | 1730.68] loss=2.77 avg=2.85\n",
            "[902 | 1732.35] loss=2.25 avg=2.84\n",
            "[903 | 1734.01] loss=2.57 avg=2.84\n",
            "[904 | 1735.68] loss=2.89 avg=2.84\n",
            "[905 | 1737.34] loss=2.66 avg=2.84\n",
            "[906 | 1739.00] loss=2.88 avg=2.84\n",
            "[907 | 1740.66] loss=3.70 avg=2.85\n",
            "[908 | 1742.32] loss=2.88 avg=2.85\n",
            "[909 | 1743.97] loss=0.36 avg=2.82\n",
            "[910 | 1745.63] loss=3.32 avg=2.83\n",
            "[911 | 1747.29] loss=2.73 avg=2.83\n",
            "[912 | 1748.95] loss=2.70 avg=2.82\n",
            "[913 | 1750.62] loss=3.36 avg=2.83\n",
            "[914 | 1752.27] loss=3.29 avg=2.83\n",
            "[915 | 1753.94] loss=2.72 avg=2.83\n",
            "[916 | 1755.61] loss=2.25 avg=2.83\n",
            "[917 | 1757.28] loss=2.47 avg=2.82\n",
            "[918 | 1758.95] loss=2.76 avg=2.82\n",
            "[919 | 1760.63] loss=2.37 avg=2.82\n",
            "[920 | 1762.30] loss=2.31 avg=2.81\n",
            "[921 | 1763.97] loss=2.82 avg=2.81\n",
            "[922 | 1765.64] loss=2.84 avg=2.81\n",
            "[923 | 1767.31] loss=3.02 avg=2.82\n",
            "[924 | 1768.98] loss=2.72 avg=2.82\n",
            "[925 | 1770.65] loss=2.63 avg=2.81\n",
            "[926 | 1772.31] loss=2.67 avg=2.81\n",
            "[927 | 1773.99] loss=3.32 avg=2.82\n",
            "[928 | 1775.66] loss=2.73 avg=2.82\n",
            "[929 | 1777.33] loss=3.18 avg=2.82\n",
            "[930 | 1779.00] loss=2.86 avg=2.82\n",
            "[931 | 1780.69] loss=3.03 avg=2.82\n",
            "[932 | 1782.36] loss=3.55 avg=2.83\n",
            "[933 | 1784.05] loss=3.20 avg=2.83\n",
            "[934 | 1785.72] loss=3.10 avg=2.84\n",
            "[935 | 1787.41] loss=2.42 avg=2.83\n",
            "[936 | 1789.10] loss=3.15 avg=2.84\n",
            "[937 | 1790.77] loss=2.43 avg=2.83\n",
            "[938 | 1792.46] loss=3.72 avg=2.84\n",
            "[939 | 1794.14] loss=3.67 avg=2.85\n",
            "[940 | 1795.82] loss=2.91 avg=2.85\n",
            "[941 | 1797.50] loss=3.55 avg=2.86\n",
            "[942 | 1799.18] loss=2.66 avg=2.85\n",
            "[943 | 1800.87] loss=2.91 avg=2.85\n",
            "[944 | 1802.55] loss=2.88 avg=2.85\n",
            "[945 | 1804.24] loss=2.31 avg=2.85\n",
            "[946 | 1805.92] loss=0.37 avg=2.82\n",
            "[947 | 1807.62] loss=2.62 avg=2.82\n",
            "[948 | 1809.29] loss=3.03 avg=2.82\n",
            "[949 | 1810.98] loss=2.54 avg=2.82\n",
            "[950 | 1812.66] loss=3.59 avg=2.83\n",
            "[951 | 1814.33] loss=2.81 avg=2.83\n",
            "[952 | 1816.00] loss=2.62 avg=2.83\n",
            "[953 | 1817.69] loss=2.61 avg=2.82\n",
            "[954 | 1819.36] loss=2.97 avg=2.83\n",
            "[955 | 1821.04] loss=2.72 avg=2.83\n",
            "[956 | 1822.72] loss=2.81 avg=2.82\n",
            "[957 | 1824.41] loss=2.36 avg=2.82\n",
            "[958 | 1826.08] loss=2.94 avg=2.82\n",
            "[959 | 1827.77] loss=2.54 avg=2.82\n",
            "[960 | 1829.45] loss=3.07 avg=2.82\n",
            "[961 | 1831.11] loss=3.04 avg=2.82\n",
            "[962 | 1832.78] loss=3.35 avg=2.83\n",
            "[963 | 1834.45] loss=3.05 avg=2.83\n",
            "[964 | 1836.12] loss=3.23 avg=2.83\n",
            "[965 | 1837.81] loss=3.15 avg=2.84\n",
            "[966 | 1839.48] loss=2.61 avg=2.84\n",
            "[967 | 1841.15] loss=3.43 avg=2.84\n",
            "[968 | 1842.82] loss=2.70 avg=2.84\n",
            "[969 | 1844.49] loss=2.57 avg=2.84\n",
            "[970 | 1846.18] loss=2.69 avg=2.84\n",
            "[971 | 1847.85] loss=3.01 avg=2.84\n",
            "[972 | 1849.52] loss=2.19 avg=2.83\n",
            "[973 | 1851.19] loss=2.60 avg=2.83\n",
            "[974 | 1852.86] loss=3.31 avg=2.83\n",
            "[975 | 1854.52] loss=3.04 avg=2.84\n",
            "[976 | 1856.20] loss=2.91 avg=2.84\n",
            "[977 | 1857.87] loss=2.86 avg=2.84\n",
            "[978 | 1859.54] loss=2.05 avg=2.83\n",
            "[979 | 1861.22] loss=2.93 avg=2.83\n",
            "[980 | 1862.88] loss=2.67 avg=2.83\n",
            "[981 | 1864.54] loss=2.81 avg=2.83\n",
            "[982 | 1866.23] loss=3.50 avg=2.83\n",
            "[983 | 1867.90] loss=2.92 avg=2.84\n",
            "[984 | 1869.57] loss=2.71 avg=2.83\n",
            "[985 | 1871.24] loss=2.81 avg=2.83\n",
            "[986 | 1872.92] loss=2.48 avg=2.83\n",
            "[987 | 1874.59] loss=2.30 avg=2.83\n",
            "[988 | 1876.26] loss=3.07 avg=2.83\n",
            "[989 | 1877.94] loss=2.63 avg=2.83\n",
            "[990 | 1879.61] loss=2.89 avg=2.83\n",
            "[991 | 1881.28] loss=2.39 avg=2.82\n",
            "[992 | 1882.95] loss=2.95 avg=2.82\n",
            "[993 | 1884.62] loss=2.48 avg=2.82\n",
            "[994 | 1886.29] loss=3.31 avg=2.82\n",
            "[995 | 1887.96] loss=2.33 avg=2.82\n",
            "[996 | 1889.63] loss=2.45 avg=2.82\n",
            "[997 | 1891.30] loss=2.77 avg=2.82\n",
            "[998 | 1892.97] loss=2.98 avg=2.82\n",
            "[999 | 1894.66] loss=3.04 avg=2.82\n",
            "Ran 1010 steps.\n",
            "Saving checkpoint/gutenberg_345/model-1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZaYR8gkh8UDS",
        "colab_type": "code",
        "outputId": "ea987e9f-2afe-4562-f8f1-884efec2019d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "## all atlantic training - with 345\n",
        "!PYTHONPATH=src ./train.py --dataset /content/all_atlantic.txt --run_name 'all_atlantic_345' --model_name '345M'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0627 21:09:38.834139 140190179219328 deprecation_wrapper.py:119] From /content/gpt-2/src/model.py:147: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
            "\n",
            "W0627 21:09:38.844184 140190179219328 deprecation_wrapper.py:119] From /content/gpt-2/src/memory_saving_gradients.py:13: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n",
            "W0627 21:09:38.949961 140190179219328 deprecation_wrapper.py:119] From ./train.py:88: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "W0627 21:09:38.950379 140190179219328 deprecation_wrapper.py:119] From ./train.py:91: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "2019-06-27 21:09:38.957622: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-06-27 21:09:38.957924: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1dfd100 executing computations on platform Host. Devices:\n",
            "2019-06-27 21:09:38.957962: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2019-06-27 21:09:38.960665: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\n",
            "2019-06-27 21:09:39.126342: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-27 21:09:39.126947: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1dfc840 executing computations on platform CUDA. Devices:\n",
            "2019-06-27 21:09:39.126980: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2019-06-27 21:09:39.127249: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-27 21:09:39.127674: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-06-27 21:09:39.128092: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-06-27 21:09:39.129548: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-06-27 21:09:39.130952: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-06-27 21:09:39.131381: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-06-27 21:09:39.133249: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-06-27 21:09:39.136620: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-06-27 21:09:39.142736: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-06-27 21:09:39.142899: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-27 21:09:39.143430: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-27 21:09:39.143902: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
            "2019-06-27 21:09:39.143988: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-06-27 21:09:39.145387: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-06-27 21:09:39.145424: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n",
            "2019-06-27 21:09:39.145443: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n",
            "2019-06-27 21:09:39.145926: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-27 21:09:39.146657: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-27 21:09:39.147110: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14202 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "W0627 21:09:39.148058 140190179219328 deprecation_wrapper.py:119] From ./train.py:92: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0627 21:09:51.295527 140190179219328 deprecation.py:323] From /content/gpt-2/src/sample.py:65: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "W0627 21:09:51.310724 140190179219328 deprecation.py:323] From /content/gpt-2/src/sample.py:16: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0627 21:09:51.312508 140190179219328 deprecation.py:323] From /content/gpt-2/src/sample.py:70: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.random.categorical` instead.\n",
            "W0627 21:09:51.323500 140190179219328 deprecation_wrapper.py:119] From ./train.py:121: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "W0627 21:10:07.820118 140190179219328 deprecation_wrapper.py:119] From ./train.py:144: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n",
            "W0627 21:10:07.823529 140190179219328 deprecation_wrapper.py:119] From ./train.py:147: The name tf.summary.merge is deprecated. Please use tf.compat.v1.summary.merge instead.\n",
            "\n",
            "W0627 21:10:07.824432 140190179219328 deprecation_wrapper.py:119] From ./train.py:149: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
            "\n",
            "W0627 21:10:07.825375 140190179219328 deprecation_wrapper.py:119] From ./train.py:152: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "Loading checkpoint models/345M/model.ckpt\n",
            "W0627 21:10:22.403299 140190179219328 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "Loading dataset...\n",
            "100% 1/1 [00:24<00:00, 24.10s/it]\n",
            "dataset has 3824306 tokens\n",
            "Training...\n",
            "2019-06-27 21:11:00.076362: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n",
            "2019-06-27 21:11:00.851569: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "[1 | 13.69] loss=2.85 avg=2.85\n",
            "[2 | 15.30] loss=2.88 avg=2.87\n",
            "[3 | 16.95] loss=3.65 avg=3.13\n",
            "[4 | 18.58] loss=2.74 avg=3.03\n",
            "[5 | 20.23] loss=3.06 avg=3.04\n",
            "[6 | 21.90] loss=2.53 avg=2.95\n",
            "[7 | 23.58] loss=2.97 avg=2.95\n",
            "[8 | 25.26] loss=3.06 avg=2.97\n",
            "[9 | 26.94] loss=2.70 avg=2.94\n",
            "[10 | 28.62] loss=2.99 avg=2.94\n",
            "[11 | 30.32] loss=3.05 avg=2.95\n",
            "[12 | 32.04] loss=2.87 avg=2.94\n",
            "[13 | 33.75] loss=2.66 avg=2.92\n",
            "[14 | 35.46] loss=2.74 avg=2.91\n",
            "[15 | 37.20] loss=3.12 avg=2.92\n",
            "[16 | 38.94] loss=3.22 avg=2.94\n",
            "[17 | 40.68] loss=3.32 avg=2.97\n",
            "[18 | 42.42] loss=2.84 avg=2.96\n",
            "[19 | 44.17] loss=3.07 avg=2.96\n",
            "[20 | 45.90] loss=2.68 avg=2.95\n",
            "[21 | 47.63] loss=2.94 avg=2.95\n",
            "[22 | 49.36] loss=2.86 avg=2.94\n",
            "[23 | 51.09] loss=3.20 avg=2.96\n",
            "[24 | 52.80] loss=2.57 avg=2.94\n",
            "[25 | 54.50] loss=2.66 avg=2.93\n",
            "[26 | 56.19] loss=2.97 avg=2.93\n",
            "[27 | 57.89] loss=2.70 avg=2.92\n",
            "[28 | 59.56] loss=2.67 avg=2.91\n",
            "[29 | 61.24] loss=2.68 avg=2.90\n",
            "[30 | 62.91] loss=2.80 avg=2.90\n",
            "[31 | 64.58] loss=2.69 avg=2.89\n",
            "[32 | 66.25] loss=2.65 avg=2.88\n",
            "[33 | 67.90] loss=3.01 avg=2.88\n",
            "[34 | 69.57] loss=3.06 avg=2.89\n",
            "[35 | 71.22] loss=2.60 avg=2.88\n",
            "[36 | 72.87] loss=3.00 avg=2.88\n",
            "[37 | 74.51] loss=3.04 avg=2.89\n",
            "[38 | 76.16] loss=2.69 avg=2.88\n",
            "[39 | 77.80] loss=2.24 avg=2.86\n",
            "[40 | 79.45] loss=2.76 avg=2.86\n",
            "[41 | 81.10] loss=2.87 avg=2.86\n",
            "[42 | 82.73] loss=2.89 avg=2.86\n",
            "[43 | 84.37] loss=2.84 avg=2.86\n",
            "[44 | 86.00] loss=2.90 avg=2.86\n",
            "[45 | 87.64] loss=2.86 avg=2.86\n",
            "[46 | 89.27] loss=2.64 avg=2.86\n",
            "[47 | 90.90] loss=2.89 avg=2.86\n",
            "[48 | 92.54] loss=2.85 avg=2.86\n",
            "[49 | 94.17] loss=3.10 avg=2.86\n",
            "[50 | 95.83] loss=2.55 avg=2.86\n",
            "[51 | 97.47] loss=3.05 avg=2.86\n",
            "[52 | 99.12] loss=3.03 avg=2.86\n",
            "[53 | 100.76] loss=2.69 avg=2.86\n",
            "[54 | 102.42] loss=2.81 avg=2.86\n",
            "[55 | 104.07] loss=2.73 avg=2.86\n",
            "[56 | 105.73] loss=2.63 avg=2.85\n",
            "[57 | 107.38] loss=2.60 avg=2.84\n",
            "[58 | 109.03] loss=2.89 avg=2.85\n",
            "[59 | 110.68] loss=3.01 avg=2.85\n",
            "[60 | 112.33] loss=3.26 avg=2.86\n",
            "[61 | 113.98] loss=2.41 avg=2.85\n",
            "[62 | 115.64] loss=2.72 avg=2.85\n",
            "[63 | 117.30] loss=2.70 avg=2.84\n",
            "[64 | 118.97] loss=3.20 avg=2.85\n",
            "[65 | 120.64] loss=2.62 avg=2.85\n",
            "[66 | 122.30] loss=2.85 avg=2.85\n",
            "[67 | 123.97] loss=3.29 avg=2.86\n",
            "[68 | 125.63] loss=2.75 avg=2.85\n",
            "[69 | 127.31] loss=2.18 avg=2.84\n",
            "[70 | 128.97] loss=3.18 avg=2.85\n",
            "[71 | 130.65] loss=2.79 avg=2.85\n",
            "[72 | 132.32] loss=2.95 avg=2.85\n",
            "[73 | 133.99] loss=3.01 avg=2.85\n",
            "[74 | 135.65] loss=2.70 avg=2.85\n",
            "[75 | 137.32] loss=2.67 avg=2.84\n",
            "[76 | 138.99] loss=2.74 avg=2.84\n",
            "[77 | 140.66] loss=3.01 avg=2.85\n",
            "[78 | 142.33] loss=2.41 avg=2.84\n",
            "[79 | 144.00] loss=2.67 avg=2.83\n",
            "[80 | 145.67] loss=2.75 avg=2.83\n",
            "[81 | 147.34] loss=2.97 avg=2.83\n",
            "[82 | 149.01] loss=2.43 avg=2.83\n",
            "[83 | 150.67] loss=2.79 avg=2.83\n",
            "[84 | 152.33] loss=2.74 avg=2.83\n",
            "[85 | 154.00] loss=2.52 avg=2.82\n",
            "[86 | 155.66] loss=2.52 avg=2.82\n",
            "[87 | 157.32] loss=2.81 avg=2.82\n",
            "[88 | 158.98] loss=2.95 avg=2.82\n",
            "[89 | 160.64] loss=2.63 avg=2.81\n",
            "[90 | 162.31] loss=2.78 avg=2.81\n",
            "[91 | 163.97] loss=2.75 avg=2.81\n",
            "[92 | 165.64] loss=2.76 avg=2.81\n",
            "[93 | 167.31] loss=3.00 avg=2.81\n",
            "[94 | 168.98] loss=2.77 avg=2.81\n",
            "[95 | 170.63] loss=2.91 avg=2.82\n",
            "[96 | 172.29] loss=2.79 avg=2.82\n",
            "[97 | 173.95] loss=2.76 avg=2.81\n",
            "[98 | 175.61] loss=2.52 avg=2.81\n",
            "[99 | 177.27] loss=2.63 avg=2.81\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " the same exact words, but if you say it out loud—like, \"I want to eat sushi and drink beer and do all the things you do.\" No matter what, it takes you one step closer to those ends than talking directly.\n",
            "\n",
            "I don't really know whether it's about wanting to say something out loud or about wanting something out of someone else, but I also wish I could just say yes to everyone, so that's why I'd like to do that on a regular basis.\n",
            "\n",
            "Advertisement - Continue Reading Below\n",
            "\n",
            "The thing is not a new thing. It's been done in this country before. I think we have a little bit of tradition around it that has developed over the years. As you say, in the 1800s and early 1900s, women were less willing to do certain things when they were pregnant. But this country is much more supportive of different bodies than it was then. This is something that really has only come into the public discourse recently.\n",
            "\n",
            "We want to say, \"Come on, man. There's something really great about you. You really were great in the '60s, when you did this stuff. Come on, let's do this!\" You can't say, \"It's not a good thing to be black or female or lesbian. There are good and bad.\"\n",
            "\n",
            "So I think, I think it can be done because of history. I think it's possible, so I hope people can see the real people out there, like you and me, and that we have really good and bad days and good and bad times, not every day is good. There are many bad days in the life of a human being. And many of those days are really good days. We can say, \"Here is one of those times. I want to come out that way.\"\n",
            "\n",
            "\n",
            "In your new book, \"Fifty Shades of Grey: The Unholy Trinity of Hate Is Here,\" you write about how you're \"disappeared\" by that particular hate, and you say this has a lot to do with how you feel about what you call the \"Dark Carnival\" and \"the Dark Carnival of Evil\" for women.\n",
            "\n",
            "That phrase is one that I think has been used a lot by a lot of people in order to create this demonization of these women, as the ultimate evil. I think that makes sense. The question for me is—and this I have to do at some length, because some of you are getting bored—how do we take women as fully—as human beings in a society that sees them as inferior beings? And even if one comes in a good position, in some senses, they're still—they still have to be—the object in every negative situation they face. So how do we deal with them and figure out ways to be able to create spaces where they can have that kind of success and success?\n",
            "\n",
            "I believe that for me, what women had in their lives, or in their hearts for that matter, is not always to be found—even in their worst conditions, not always found in their worst situations. There's something for us to explore and find to be really beautiful. In the same way that we can see that the beauty of the ocean, the beauty of the wild is not something just discovered for us by chance or a chance encounter. It's something that we know and believe, and I think in some ways, that all of those things have come together and helped create the beautiful thing that is humanity and that is woman, in many ways.\n",
            "\n",
            "The other thing that I think was important—I don't think that it's a negative thing to say—I think it's a positive thing. I'm going to tell you it's not a negative thing to love a woman. I'm going to tell you, the positive thing that you want to do with people is love them for whatever reason that you can. I don't think it's a negative thing to love a woman because you have nothing else against her. I think it's a positive thing to love her just as much as you can.\n",
            "\n",
            "Advertisement - Continue Reading Below\n",
            "\n",
            "Advertisement - Continue Reading Below\n",
            "\n",
            "I've been told a lot of things by a lot of people, so I would never say what I'm really thinking. But I've said, \"This is not a negative thing to hate or to hate a woman for anything. This is not some kind of bad thing; this is a really beautiful, beautiful thing.\"\n",
            "\n",
            "I think what's really amazing about this book right now, though, is that I had no idea, ever, whether what was in the book would actually affect me. So if I had gotten a chance to see that it made a difference for me, I certainly couldn't have believed it. But I did, and I am still aghast and speechless, because every aspect of the book is about what I hate about women, from being invisible (for instance, in the book)\n",
            "\n",
            "[100 | 205.46] loss=2.93 avg=2.81\n",
            "[101 | 207.13] loss=2.77 avg=2.81\n",
            "[102 | 208.79] loss=2.67 avg=2.81\n",
            "[103 | 210.46] loss=2.90 avg=2.81\n",
            "[104 | 212.13] loss=2.79 avg=2.81\n",
            "[105 | 213.81] loss=2.93 avg=2.81\n",
            "[106 | 215.48] loss=2.52 avg=2.80\n",
            "[107 | 217.15] loss=3.10 avg=2.81\n",
            "[108 | 218.82] loss=2.99 avg=2.81\n",
            "[109 | 220.49] loss=2.96 avg=2.81\n",
            "[110 | 222.16] loss=3.03 avg=2.82\n",
            "[111 | 223.84] loss=2.55 avg=2.81\n",
            "[112 | 225.51] loss=2.50 avg=2.81\n",
            "[113 | 227.19] loss=2.64 avg=2.81\n",
            "[114 | 228.86] loss=3.10 avg=2.81\n",
            "[115 | 230.53] loss=2.88 avg=2.81\n",
            "[116 | 232.20] loss=3.06 avg=2.81\n",
            "[117 | 233.88] loss=2.78 avg=2.81\n",
            "[118 | 235.55] loss=2.77 avg=2.81\n",
            "[119 | 237.23] loss=2.50 avg=2.81\n",
            "[120 | 238.90] loss=2.87 avg=2.81\n",
            "[121 | 240.57] loss=2.90 avg=2.81\n",
            "[122 | 242.25] loss=3.13 avg=2.82\n",
            "[123 | 243.91] loss=2.87 avg=2.82\n",
            "[124 | 245.59] loss=2.49 avg=2.81\n",
            "[125 | 247.25] loss=2.94 avg=2.81\n",
            "[126 | 248.92] loss=2.79 avg=2.81\n",
            "[127 | 250.59] loss=2.75 avg=2.81\n",
            "[128 | 252.26] loss=2.73 avg=2.81\n",
            "[129 | 253.93] loss=2.66 avg=2.81\n",
            "[130 | 255.60] loss=3.18 avg=2.81\n",
            "[131 | 257.27] loss=2.77 avg=2.81\n",
            "[132 | 258.94] loss=2.80 avg=2.81\n",
            "[133 | 260.61] loss=2.44 avg=2.81\n",
            "[134 | 262.29] loss=2.62 avg=2.81\n",
            "[135 | 263.97] loss=2.84 avg=2.81\n",
            "[136 | 265.64] loss=2.66 avg=2.80\n",
            "[137 | 267.31] loss=2.73 avg=2.80\n",
            "[138 | 268.98] loss=2.38 avg=2.80\n",
            "[139 | 270.65] loss=2.94 avg=2.80\n",
            "[140 | 272.33] loss=2.57 avg=2.80\n",
            "[141 | 274.00] loss=2.60 avg=2.79\n",
            "[142 | 275.67] loss=2.52 avg=2.79\n",
            "[143 | 277.34] loss=2.76 avg=2.79\n",
            "[144 | 279.01] loss=2.80 avg=2.79\n",
            "[145 | 280.68] loss=3.00 avg=2.79\n",
            "[146 | 282.35] loss=2.73 avg=2.79\n",
            "[147 | 284.02] loss=2.74 avg=2.79\n",
            "[148 | 285.69] loss=3.19 avg=2.80\n",
            "[149 | 287.36] loss=2.62 avg=2.79\n",
            "[150 | 289.03] loss=2.83 avg=2.79\n",
            "[151 | 290.71] loss=2.87 avg=2.80\n",
            "[152 | 292.38] loss=2.66 avg=2.79\n",
            "[153 | 294.05] loss=2.57 avg=2.79\n",
            "[154 | 295.73] loss=2.53 avg=2.79\n",
            "[155 | 297.40] loss=3.35 avg=2.80\n",
            "[156 | 299.07] loss=2.77 avg=2.79\n",
            "[157 | 300.74] loss=2.70 avg=2.79\n",
            "[158 | 302.42] loss=2.88 avg=2.79\n",
            "[159 | 304.09] loss=2.53 avg=2.79\n",
            "[160 | 305.76] loss=2.76 avg=2.79\n",
            "[161 | 307.43] loss=2.64 avg=2.79\n",
            "[162 | 309.10] loss=2.56 avg=2.79\n",
            "[163 | 310.78] loss=3.03 avg=2.79\n",
            "[164 | 312.46] loss=2.94 avg=2.79\n",
            "[165 | 314.14] loss=2.75 avg=2.79\n",
            "[166 | 315.82] loss=2.60 avg=2.79\n",
            "[167 | 317.50] loss=2.71 avg=2.79\n",
            "[168 | 319.17] loss=2.75 avg=2.79\n",
            "[169 | 320.86] loss=2.82 avg=2.79\n",
            "[170 | 322.53] loss=2.83 avg=2.79\n",
            "[171 | 324.20] loss=2.83 avg=2.79\n",
            "[172 | 325.89] loss=2.46 avg=2.78\n",
            "[173 | 327.56] loss=2.91 avg=2.79\n",
            "[174 | 329.23] loss=2.71 avg=2.79\n",
            "[175 | 330.90] loss=3.02 avg=2.79\n",
            "[176 | 332.57] loss=2.72 avg=2.79\n",
            "[177 | 334.24] loss=2.57 avg=2.78\n",
            "[178 | 335.92] loss=3.01 avg=2.79\n",
            "[179 | 337.60] loss=3.01 avg=2.79\n",
            "[180 | 339.26] loss=3.05 avg=2.79\n",
            "[181 | 340.94] loss=3.06 avg=2.80\n",
            "[182 | 342.63] loss=3.30 avg=2.80\n",
            "[183 | 344.31] loss=2.63 avg=2.80\n",
            "[184 | 345.98] loss=3.26 avg=2.81\n",
            "[185 | 347.65] loss=2.73 avg=2.80\n",
            "[186 | 349.33] loss=2.66 avg=2.80\n",
            "[187 | 351.00] loss=2.83 avg=2.80\n",
            "[188 | 352.68] loss=2.63 avg=2.80\n",
            "[189 | 354.36] loss=2.51 avg=2.80\n",
            "[190 | 356.04] loss=2.71 avg=2.80\n",
            "[191 | 357.71] loss=2.86 avg=2.80\n",
            "[192 | 359.39] loss=2.39 avg=2.79\n",
            "[193 | 361.06] loss=2.75 avg=2.79\n",
            "[194 | 362.73] loss=2.68 avg=2.79\n",
            "[195 | 364.41] loss=2.96 avg=2.79\n",
            "[196 | 366.08] loss=3.03 avg=2.80\n",
            "[197 | 367.75] loss=2.48 avg=2.79\n",
            "[198 | 369.43] loss=3.22 avg=2.80\n",
            "[199 | 371.10] loss=2.60 avg=2.79\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " Homeland Health Care: Trump and the First 100 Days\n",
            "\n",
            "In addition to health care, Trump has also ordered a massive review of the Federal Reserve's monetary policies and, according to one report, called for a return to gold (although Trump has also ordered the Federal Aviation Administration to put a stop to the practice of flying over national parks). And in February, Trump announced that, once he took office, the U.S. would begin to use its own currency.\n",
            "\n",
            "But the most dramatic and enduring effect of Trump's presidency has been his decision to put his family through the personal and financial indignities of the presidency.\n",
            "\n",
            "On Saturday, Trump will step down as president. The Senate will choose Trump's replacement on Monday. On Monday evening, at 10:29 p.m., Trump will deliver a nationally televised address to the American people on the economy that will almost certainly be described by the media as just an \"economic address\" or a \"message\" or something else entirely—and the only thing that could be worse than an economic address would be a presidential one.\n",
            "\n",
            "Trump will talk about his vision for America that has so far consisted only of a promise to put America first (i.e. to \"make America great again\"), although there will be an unmistakable theme running through his speech: that the world is more divided than ever.\n",
            "\n",
            "Throughout his presidential campaign, Trump repeatedly promised to restore the bonds of traditional American family life. During the presidential debates, he repeatedly pledged to take our country back to the land of \"the families.\" The idea that the government should be focused exclusively on creating a better family life is the antithesis of Trump's campaign message. But this is exactly how Trump is proposing to replace the president: to cut the safety net for all Americans.\n",
            "\n",
            "Trump's administration has spent much of his time on the campaign trail and the transition team promising to make Americans great again, but his approach has been to turn around America's crumbling infrastructure and create vast new government programs which are not only not funded but could be dismantled without affecting job creation. On Friday, for example, Trump said during the campaign that he would not cut the National Institutes of Health (NIH) because they were \"doing amazing work.\" But as Politico reported, Trump was then asked during the campaign if he knew that the organization's funding was based on grants for research:\n",
            "\n",
            "Asked if there was a dollar amount of grant money that would not be affected by Trump not funding the NIH, the president of the National Institutes of Health, Francis Collins, responded: \"It would probably have to be, or it wouldn't get funded, because … that's the way that things work. But if you cut that funding, then other areas would not be getting access to it. So that's something that I'll have to look at.\"\n",
            "\n",
            "What does this mean for programs like Medicaid? It means that, as president, Trump will have to determine how much of the money the program will need each year. But, by not making a decision on the funding for all areas of the program, Trump has already set the stage for what would be a devastating cut in the funding for Medicaid in the future as well as for every program that benefits people across the country. So, even beyond that, what will be the effect on the entire federal budget? The Congressional Budget Office announced on Monday that a cut in Medicaid spending would cut $742 billion from the federal government, although the figure, like the overall budget, won't be finalized until October.\n",
            "\n",
            "In his speech tonight in Philadelphia, Trump also proposes to completely cut government spending for Social Security and Medicare. Trump would reduce Social Security by 20 percent in one year alone, with a cut of almost $150 billion in the following year. Trump would also eliminate the Social Security payroll tax cap in five years and phase out the payroll tax for people making more than $200,000. At $106,000, Trump would eliminate a tax credit worth almost $250 billion a year. At $75,000, he would cut more than $100 billion. And, at $40,000, Trump would end the deduction for personal exemptions (a credit worth more than $250 billion) and the charitable tax credit (about $47 billion).\n",
            "\n",
            "But Social Security is certainly the most important program that Trump plans to cut in the coming years. The programs make up roughly 13 percent of the federal budget and, unlike TrumpCare, that program won't grow in size over time. And Trump is proposing a 50 percent cut in that program over 10 years, which would mean about $10.7 trillion in cuts. Trump also wants to eliminate a program for making the Federal Credit Union Act, named for Social Security, more affordable, although the Congressional Budget Office reports that such a change to the law would result in $30 billion in cost cuts every year.\n",
            "\n",
            "The Trump administration also proposes to eliminate about 7 percent of the federal budget in 2019, while, as Trump puts it, \"the cost of protecting entitlements\n",
            "\n",
            "[200 | 395.59] loss=2.63 avg=2.79\n",
            "[201 | 397.26] loss=2.53 avg=2.79\n",
            "[202 | 398.92] loss=2.94 avg=2.79\n",
            "[203 | 400.58] loss=2.73 avg=2.79\n",
            "[204 | 402.24] loss=2.56 avg=2.79\n",
            "[205 | 403.90] loss=3.22 avg=2.79\n",
            "[206 | 405.55] loss=2.92 avg=2.79\n",
            "[207 | 407.21] loss=2.81 avg=2.79\n",
            "[208 | 408.87] loss=2.72 avg=2.79\n",
            "[209 | 410.52] loss=2.72 avg=2.79\n",
            "[210 | 412.17] loss=2.79 avg=2.79\n",
            "[211 | 413.84] loss=2.88 avg=2.79\n",
            "[212 | 415.50] loss=3.01 avg=2.80\n",
            "[213 | 417.16] loss=2.81 avg=2.80\n",
            "[214 | 418.82] loss=2.60 avg=2.79\n",
            "[215 | 420.48] loss=2.36 avg=2.79\n",
            "[216 | 422.14] loss=2.69 avg=2.79\n",
            "[217 | 423.81] loss=2.83 avg=2.79\n",
            "[218 | 425.46] loss=3.34 avg=2.79\n",
            "[219 | 427.13] loss=2.92 avg=2.80\n",
            "[220 | 428.79] loss=2.46 avg=2.79\n",
            "[221 | 430.45] loss=2.04 avg=2.78\n",
            "[222 | 432.12] loss=2.75 avg=2.78\n",
            "[223 | 433.79] loss=2.90 avg=2.78\n",
            "[224 | 435.46] loss=2.80 avg=2.79\n",
            "[225 | 437.13] loss=3.09 avg=2.79\n",
            "[226 | 438.79] loss=3.00 avg=2.79\n",
            "[227 | 440.47] loss=2.79 avg=2.79\n",
            "[228 | 442.14] loss=3.06 avg=2.79\n",
            "[229 | 443.81] loss=2.71 avg=2.79\n",
            "[230 | 445.48] loss=2.57 avg=2.79\n",
            "[231 | 447.15] loss=2.77 avg=2.79\n",
            "[232 | 448.84] loss=2.67 avg=2.79\n",
            "[233 | 450.50] loss=2.18 avg=2.78\n",
            "[234 | 452.18] loss=2.91 avg=2.78\n",
            "[235 | 453.86] loss=2.79 avg=2.78\n",
            "[236 | 455.54] loss=2.52 avg=2.78\n",
            "[237 | 457.21] loss=3.09 avg=2.78\n",
            "[238 | 458.90] loss=2.98 avg=2.79\n",
            "[239 | 460.58] loss=2.60 avg=2.78\n",
            "[240 | 462.26] loss=2.98 avg=2.79\n",
            "[241 | 463.93] loss=2.77 avg=2.79\n",
            "[242 | 465.60] loss=2.35 avg=2.78\n",
            "[243 | 467.29] loss=2.83 avg=2.78\n",
            "[244 | 468.96] loss=2.57 avg=2.78\n",
            "[245 | 470.63] loss=2.62 avg=2.78\n",
            "[246 | 472.31] loss=2.52 avg=2.78\n",
            "[247 | 473.99] loss=2.75 avg=2.78\n",
            "[248 | 475.66] loss=2.61 avg=2.77\n",
            "[249 | 477.33] loss=2.62 avg=2.77\n",
            "[250 | 479.01] loss=2.60 avg=2.77\n",
            "[251 | 480.69] loss=2.68 avg=2.77\n",
            "[252 | 482.36] loss=2.75 avg=2.77\n",
            "[253 | 484.03] loss=2.59 avg=2.77\n",
            "[254 | 485.70] loss=2.66 avg=2.77\n",
            "[255 | 487.37] loss=2.66 avg=2.76\n",
            "[256 | 489.04] loss=2.84 avg=2.77\n",
            "[257 | 490.72] loss=3.11 avg=2.77\n",
            "[258 | 492.39] loss=2.91 avg=2.77\n",
            "[259 | 494.06] loss=2.88 avg=2.77\n",
            "[260 | 495.74] loss=2.53 avg=2.77\n",
            "[261 | 497.41] loss=2.75 avg=2.77\n",
            "[262 | 499.09] loss=2.62 avg=2.77\n",
            "[263 | 500.76] loss=2.46 avg=2.76\n",
            "[264 | 502.43] loss=3.19 avg=2.77\n",
            "[265 | 504.10] loss=3.01 avg=2.77\n",
            "[266 | 505.78] loss=2.98 avg=2.77\n",
            "[267 | 507.45] loss=3.20 avg=2.78\n",
            "[268 | 509.12] loss=2.96 avg=2.78\n",
            "[269 | 510.79] loss=2.73 avg=2.78\n",
            "[270 | 512.47] loss=3.23 avg=2.78\n",
            "[271 | 514.14] loss=2.82 avg=2.78\n",
            "[272 | 515.81] loss=3.12 avg=2.79\n",
            "[273 | 517.48] loss=2.94 avg=2.79\n",
            "[274 | 519.15] loss=2.75 avg=2.79\n",
            "[275 | 520.82] loss=2.76 avg=2.79\n",
            "[276 | 522.48] loss=2.65 avg=2.79\n",
            "[277 | 524.15] loss=2.99 avg=2.79\n",
            "[278 | 525.82] loss=2.67 avg=2.79\n",
            "[279 | 527.49] loss=2.74 avg=2.79\n",
            "[280 | 529.16] loss=2.43 avg=2.78\n",
            "[281 | 530.83] loss=2.55 avg=2.78\n",
            "[282 | 532.50] loss=2.83 avg=2.78\n",
            "[283 | 534.17] loss=2.85 avg=2.78\n",
            "[284 | 535.84] loss=2.97 avg=2.78\n",
            "[285 | 537.51] loss=2.99 avg=2.79\n",
            "[286 | 539.18] loss=2.40 avg=2.78\n",
            "[287 | 540.84] loss=3.04 avg=2.79\n",
            "[288 | 542.52] loss=2.33 avg=2.78\n",
            "[289 | 544.18] loss=2.79 avg=2.78\n",
            "[290 | 545.85] loss=2.91 avg=2.78\n",
            "[291 | 547.52] loss=2.63 avg=2.78\n",
            "[292 | 549.20] loss=3.06 avg=2.78\n",
            "[293 | 550.87] loss=2.89 avg=2.78\n",
            "[294 | 552.54] loss=3.39 avg=2.79\n",
            "[295 | 554.21] loss=2.67 avg=2.79\n",
            "[296 | 555.88] loss=2.66 avg=2.79\n",
            "[297 | 557.55] loss=2.53 avg=2.79\n",
            "[298 | 559.22] loss=2.89 avg=2.79\n",
            "[299 | 560.89] loss=2.85 avg=2.79\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            ").\n",
            "\n",
            "\"You can use our app. There are so many apps today that are just designed to make you feel better, to keep you going through a period of pain, that you can just jump on Facebook and use it for relief,\" says Marcello Iannucci, a health policy expert who directs the University of Chicago's health inequality project. In the United States, this is the case with the likes of Viagra and the erectile dysfunction drug Viagra Flex.\n",
            "\n",
            "But Iannucci and other health reform advocates are starting to raise questions about that rationale: After all, Viagra is widely considered to be a drug whose primary use is to relieve erectile difficulties. And as such, it's widely expected that many men will be comfortable using it, whether through the app or not.\n",
            "\n",
            "Some are even taking the pill to improve their mental health, which is, after all, the biggest reason to get it in the first place. But some other Americans have reason to be cautious. According to the National Center for Health Statistics, one in four men ages 18 to 49 will live in poverty. That is alarming — especially when it happens to someone who is likely to be taking Viagra for the same reason.\n",
            "\n",
            "On Friday, the Food and Drug Administration announced that a drug called Abzasil will not be used to treat erectile dysfunction, at least by women. Abzasil is a generic antidepressant. It is available to men ages 18 to 49. That's not surprising—women can't afford the drug.\n",
            "\n",
            "Abzasil, and the generic versions of it, have had mixed success for patients, with some studies showing some users become more likely to commit suicide than the generic versions. But the drug's creator, a California-based company called Medtronic, insists it's not a suicide drug. And Iannucci disputes that.\n",
            "\n",
            "\"It's a pharmaceutical,\" he says. Iannucci is referring to the generic versions. (We're testing out his hypothesis, though, with the drug and several other drugs in a randomized trial of the generic version.) \"You're using medication or you're not using medication. We're not making a suicide drug when it comes to those two things.\"\n",
            "\n",
            "Some health reform advocates say they worry that if the generic versions are used to treat erectile dysfunction but are later withdrawn because of a side effect, men may take advantage of this change to make themselves feel worse. That could be dangerous if the people who are taking a drug to get relief from pain are doing something that could lead to an adverse side effect. \"If they're taking the generic version or they're doing some other thing that could cause some of the problems,\" says Iannucci, \"then that could be the end.\"\n",
            "\n",
            "Iannucci dismisses fears the drug could lead one to depression. \"Medtronic had that with Vicodin—there was concern about the side effects, and I think the medication is safe and it was not a suicide drug. I don't feel our concerns are legitimate. The generic version is not suicide.\" Iannucci says the generic version would be approved, approved and used by doctors, nurses and pharmacists.\n",
            "\n",
            "And if the drugs are withdrawn before the generic versions are approved, Iannucci says it might still be approved after the generic version is discontinued. So he says he's worried that he won't get the drug on the shelves, then the generic version will be used, and the medication will be on prescription. That could affect the use of his prescription for Viagra, a pill with both generic and generic version.\n",
            "\n",
            "In any case, Iannucci doubts that one person will decide to use the generic version of the drug. He says Viagra might appeal to one group, such as depressed people. But he says that Viagra may also appeal to an other: \"One of the problems is that there is a whole culture of depression right now. People are using the [generic] version to relieve themselves, if you will, but in some ways the generic version is very good. There could be a problem if somebody decides this is a way to end their pain.\"\n",
            "\n",
            "It's unclear how widespread the generic versions are right now. The company Medtronic tells me it is not saying how many people may be taking the drugs or how many will be approved or approved for generic use, both of which are important things for the FDA to include in its decisions.\n",
            "\n",
            "But there are concerns across the board about the way generic versions of Viagra and Abzasil are being used. The company tells me it has identified two studies that demonstrated there are concerns with the efficacy and the long-term safety of the generic versions of these drugs. One study in Europe, by the American Society for Clinical Pharmacology and Toxicology, evaluated the efficacy and safety of Abzasil compared with the generic version of the drug.\n",
            "\n",
            "When I visited Abzasil and Viagra in New York the other day,\n",
            "\n",
            "[300 | 585.33] loss=2.77 avg=2.79\n",
            "[301 | 587.00] loss=2.46 avg=2.78\n",
            "[302 | 588.67] loss=2.77 avg=2.78\n",
            "[303 | 590.35] loss=2.60 avg=2.78\n",
            "[304 | 592.02] loss=2.90 avg=2.78\n",
            "[305 | 593.69] loss=3.05 avg=2.79\n",
            "[306 | 595.36] loss=3.06 avg=2.79\n",
            "[307 | 597.02] loss=3.14 avg=2.79\n",
            "[308 | 598.69] loss=2.59 avg=2.79\n",
            "[309 | 600.34] loss=2.68 avg=2.79\n",
            "[310 | 602.01] loss=2.51 avg=2.79\n",
            "[311 | 603.68] loss=3.21 avg=2.79\n",
            "[312 | 605.34] loss=3.08 avg=2.79\n",
            "[313 | 607.00] loss=2.72 avg=2.79\n",
            "[314 | 608.66] loss=2.68 avg=2.79\n",
            "[315 | 610.31] loss=2.58 avg=2.79\n",
            "[316 | 611.98] loss=2.66 avg=2.79\n",
            "[317 | 613.65] loss=2.93 avg=2.79\n",
            "[318 | 615.32] loss=3.01 avg=2.79\n",
            "[319 | 616.97] loss=3.16 avg=2.80\n",
            "[320 | 618.63] loss=2.95 avg=2.80\n",
            "[321 | 620.30] loss=2.53 avg=2.79\n",
            "[322 | 621.96] loss=2.50 avg=2.79\n",
            "[323 | 623.62] loss=2.96 avg=2.79\n",
            "[324 | 625.29] loss=3.02 avg=2.80\n",
            "[325 | 626.95] loss=2.11 avg=2.79\n",
            "[326 | 628.61] loss=2.76 avg=2.79\n",
            "[327 | 630.27] loss=2.41 avg=2.78\n",
            "[328 | 631.94] loss=2.78 avg=2.78\n",
            "[329 | 633.60] loss=2.76 avg=2.78\n",
            "[330 | 635.26] loss=2.55 avg=2.78\n",
            "[331 | 636.93] loss=2.56 avg=2.78\n",
            "[332 | 638.59] loss=2.51 avg=2.78\n",
            "[333 | 640.26] loss=3.04 avg=2.78\n",
            "[334 | 641.92] loss=2.72 avg=2.78\n",
            "[335 | 643.59] loss=2.70 avg=2.78\n",
            "[336 | 645.26] loss=2.93 avg=2.78\n",
            "[337 | 646.92] loss=2.50 avg=2.78\n",
            "[338 | 648.59] loss=3.09 avg=2.78\n",
            "[339 | 650.25] loss=3.10 avg=2.78\n",
            "[340 | 651.92] loss=2.89 avg=2.78\n",
            "[341 | 653.58] loss=2.68 avg=2.78\n",
            "[342 | 655.25] loss=2.90 avg=2.78\n",
            "[343 | 656.91] loss=2.69 avg=2.78\n",
            "[344 | 658.57] loss=2.11 avg=2.78\n",
            "[345 | 660.24] loss=2.82 avg=2.78\n",
            "[346 | 661.91] loss=2.81 avg=2.78\n",
            "[347 | 663.58] loss=2.33 avg=2.77\n",
            "[348 | 665.24] loss=2.53 avg=2.77\n",
            "[349 | 666.89] loss=2.78 avg=2.77\n",
            "[350 | 668.55] loss=2.89 avg=2.77\n",
            "[351 | 670.22] loss=2.65 avg=2.77\n",
            "[352 | 671.89] loss=2.55 avg=2.77\n",
            "[353 | 673.56] loss=2.79 avg=2.77\n",
            "[354 | 675.23] loss=2.93 avg=2.77\n",
            "[355 | 676.88] loss=2.61 avg=2.77\n",
            "[356 | 678.55] loss=2.41 avg=2.76\n",
            "[357 | 680.22] loss=2.55 avg=2.76\n",
            "[358 | 681.89] loss=2.81 avg=2.76\n",
            "[359 | 683.56] loss=2.56 avg=2.76\n",
            "[360 | 685.23] loss=2.07 avg=2.75\n",
            "[361 | 686.90] loss=3.15 avg=2.76\n",
            "[362 | 688.57] loss=2.83 avg=2.76\n",
            "[363 | 690.25] loss=2.92 avg=2.76\n",
            "[364 | 691.92] loss=2.97 avg=2.76\n",
            "[365 | 693.59] loss=3.07 avg=2.77\n",
            "[366 | 695.26] loss=2.67 avg=2.76\n",
            "[367 | 696.93] loss=2.80 avg=2.76\n",
            "[368 | 698.60] loss=2.61 avg=2.76\n",
            "[369 | 700.27] loss=2.61 avg=2.76\n",
            "[370 | 701.93] loss=2.76 avg=2.76\n",
            "[371 | 703.60] loss=2.70 avg=2.76\n",
            "[372 | 705.27] loss=2.83 avg=2.76\n",
            "[373 | 706.95] loss=3.34 avg=2.77\n",
            "[374 | 708.62] loss=2.08 avg=2.76\n",
            "[375 | 710.29] loss=3.04 avg=2.76\n",
            "[376 | 711.96] loss=2.82 avg=2.76\n",
            "[377 | 713.63] loss=3.36 avg=2.77\n",
            "[378 | 715.30] loss=2.61 avg=2.77\n",
            "[379 | 716.97] loss=2.59 avg=2.77\n",
            "[380 | 718.64] loss=2.59 avg=2.76\n",
            "[381 | 720.31] loss=2.77 avg=2.76\n",
            "[382 | 721.98] loss=2.83 avg=2.77\n",
            "[383 | 723.65] loss=2.75 avg=2.77\n",
            "[384 | 725.32] loss=2.66 avg=2.76\n",
            "[385 | 726.99] loss=2.88 avg=2.77\n",
            "[386 | 728.66] loss=2.57 avg=2.76\n",
            "[387 | 730.33] loss=3.04 avg=2.77\n",
            "[388 | 732.00] loss=2.70 avg=2.77\n",
            "[389 | 733.66] loss=3.03 avg=2.77\n",
            "[390 | 735.33] loss=2.75 avg=2.77\n",
            "[391 | 737.00] loss=2.57 avg=2.77\n",
            "[392 | 738.67] loss=2.51 avg=2.76\n",
            "[393 | 740.34] loss=2.77 avg=2.76\n",
            "[394 | 742.01] loss=2.66 avg=2.76\n",
            "[395 | 743.68] loss=2.69 avg=2.76\n",
            "[396 | 745.35] loss=2.98 avg=2.76\n",
            "[397 | 747.02] loss=2.78 avg=2.76\n",
            "[398 | 748.69] loss=2.63 avg=2.76\n",
            "[399 | 750.36] loss=3.18 avg=2.77\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " said.\n",
            "\n",
            "\"These laws could put hundreds of thousands of people at risk. I think we need to take a very big step back on the war on drugs,\" the congressman said.\n",
            "\n",
            "The bill has bipartisan support, including from members of the White House. Last week, President Donald Trump signed a bill into law that will give states the power to stop people on the streets for minor offenses like selling drugs in public.\n",
            "\n",
            "Sen. Dianne Feinstein from California wrote a letter to Trump on Wednesday arguing that there had been no harm to the public and the laws protecting people from arrest would be redundant, and called it an attack on Americans' rights. Her bill, S. 838, is now getting the same treatment.\n",
            "\n",
            "\"The White House should immediately stop this unconstitutional assault on our constitutional rights,\" said Sen. Sheldon Whitehouse in a statement Thursday.\n",
            "\n",
            "The ACLU pointed out in its letter to the administration that, since the 1980s, the U.S. has had laws that criminalize possession of up to 10 grams of marijuana, possession of less than 1 gram of marijuana, possession of 5 grams of marijuana, possession of 8 grams of marijuana, possession of more than 8 grams of marijuana, and possession of an unregistered and ungifted marijuana plant.\n",
            "\n",
            "\"The White House should stop this assault on drug users' lives by passing a comprehensive bill restoring civil rights and protections for medical marijuana patients in our states,\" said Sarah Dacey, the director of the ACLU's Access to Food and Drug Enforcement Project who wrote the letter to the White House.\n",
            "\n",
            "The Trump administration did not respond to a request for comment.\n",
            "\n",
            "In a letter last week to the House Judiciary Committee, the U.S. Chamber of Commerce pointed out that in the 1990s, Congress banned marijuana possession, but that hasn't stopped people from carrying the drug.\n",
            "\n",
            "\"With thousands of arrests for possession of the most common illegal drug in the world and hundreds of thousands more people arrested for drug offenses in our country alone annually, marijuana possession is being used as a tool for enforcement — including with the U.S. Drug Enforcement Administration. It would be wrong, cruel, and un-American to use those laws to keep people in jail for possessing small quantities of marijuana,\" the letter read.\n",
            "\n",
            "While there is no current law that specifically bans the possession of pot, it is illegal in most of the 13 states with medical marijuana programs.\n",
            "\n",
            "The Trump administration said it will review the letter, but could face a lawsuit from the Marijuana Policy Project, the same group that wrote the original letter to the White House.\n",
            "\n",
            "In addition to pot legalization, the White House is planning to create the Department of Justice and the Secretary of the Justice Department to oversee federal enforcement of marijuana laws. The Obama administration, which tried to crack down on pot, was also considering closing down states that approved pot use for adults, or at least reducing the number of people in prison for selling in recreational pot stores.\n",
            "\n",
            "The White House recently introduced a budget proposal that would allow states to experiment with the use of medical marijuana as a means of treating serious psychiatric illnesses. It also calls for the creation of a new $1.7 billion fund to combat opioid and heroin abuse and addiction.\n",
            "\n",
            "Despite the new measures, Trump has expressed interest in pot legalization, according to the AP.<|endoftext|>New Brunswick is among several U.S. states that have passed medical marijuana legislation, and the country has taken its lead on the development and production of its plant. But there's a catch:\n",
            "\n",
            "With the medical marijuana legislation now moving through the House of Representative, New Brunswick is likely a long way from creating a national system of medical marijuana as well.\n",
            "\n",
            "It's an idea that some in Washington have already tried, and there are many ways the federal government could address the fact that federal law has blocked states and localities from setting up their own medical marijuana programs. The federal government already supports medical marijuana in the states where it has been approved.\n",
            "\n",
            "The feds could also simply impose criminal penalties (up to 15 years) for producing or manufacturing a regulated or regulated-for-research product. The federal government has already put a hold on many cannabis-related businesses, most notably a medical marijuana dispensary in Texas, but also marijuana producers in Washington, Colorado, and Oregon.\n",
            "\n",
            "The federal government could also make its own laws to control the production and distribution of cannabidiol, or CBD, a drug that many patients are now enjoying. The federal Bureau of Alcohol, Tobacco, Firearms and Explosives is also on the job working on the issue.\n",
            "\n",
            "Even those with more medical marijuana experience could face problems.\n",
            "\n",
            "It's worth noting that the federal government is not the only entity that could potentially punish the production of a regulated product, even if a more-experienced producer produces the product.\n",
            "\n",
            "There are numerous situations in the world where cannabis is legal under state or local law. The most notable example is medical marijuana. It is also legal to carry medical marijuana\n",
            "\n",
            "[400 | 774.99] loss=2.70 avg=2.77\n",
            "[401 | 776.65] loss=2.64 avg=2.77\n",
            "[402 | 778.31] loss=2.55 avg=2.76\n",
            "[403 | 779.97] loss=3.13 avg=2.77\n",
            "[404 | 781.64] loss=3.23 avg=2.77\n",
            "[405 | 783.30] loss=2.52 avg=2.77\n",
            "[406 | 784.95] loss=3.16 avg=2.77\n",
            "[407 | 786.61] loss=2.92 avg=2.77\n",
            "[408 | 788.28] loss=2.71 avg=2.77\n",
            "[409 | 789.93] loss=2.76 avg=2.77\n",
            "[410 | 791.60] loss=2.72 avg=2.77\n",
            "[411 | 793.25] loss=2.78 avg=2.77\n",
            "[412 | 794.92] loss=2.32 avg=2.77\n",
            "[413 | 796.58] loss=2.93 avg=2.77\n",
            "[414 | 798.24] loss=2.67 avg=2.77\n",
            "[415 | 799.91] loss=2.99 avg=2.77\n",
            "[416 | 801.57] loss=2.11 avg=2.76\n",
            "[417 | 803.22] loss=2.59 avg=2.76\n",
            "[418 | 804.89] loss=2.80 avg=2.76\n",
            "[419 | 806.56] loss=2.80 avg=2.76\n",
            "[420 | 808.22] loss=2.75 avg=2.76\n",
            "[421 | 809.88] loss=2.61 avg=2.76\n",
            "[422 | 811.55] loss=3.04 avg=2.76\n",
            "[423 | 813.22] loss=3.28 avg=2.77\n",
            "[424 | 814.88] loss=2.89 avg=2.77\n",
            "[425 | 816.55] loss=2.52 avg=2.77\n",
            "[426 | 818.21] loss=2.92 avg=2.77\n",
            "[427 | 819.88] loss=2.99 avg=2.77\n",
            "[428 | 821.54] loss=3.43 avg=2.78\n",
            "[429 | 823.21] loss=2.50 avg=2.78\n",
            "[430 | 824.88] loss=2.82 avg=2.78\n",
            "[431 | 826.55] loss=2.42 avg=2.77\n",
            "[432 | 828.20] loss=2.84 avg=2.77\n",
            "[433 | 829.87] loss=2.41 avg=2.77\n",
            "[434 | 831.53] loss=2.52 avg=2.77\n",
            "[435 | 833.20] loss=2.25 avg=2.76\n",
            "[436 | 834.85] loss=2.84 avg=2.76\n",
            "[437 | 836.52] loss=2.43 avg=2.76\n",
            "[438 | 838.19] loss=3.25 avg=2.76\n",
            "[439 | 839.86] loss=2.21 avg=2.76\n",
            "[440 | 841.53] loss=2.75 avg=2.76\n",
            "[441 | 843.20] loss=2.87 avg=2.76\n",
            "[442 | 844.87] loss=2.30 avg=2.76\n",
            "[443 | 846.53] loss=2.64 avg=2.75\n",
            "[444 | 848.20] loss=2.71 avg=2.75\n",
            "[445 | 849.87] loss=2.96 avg=2.76\n",
            "[446 | 851.54] loss=2.68 avg=2.75\n",
            "[447 | 853.21] loss=2.60 avg=2.75\n",
            "[448 | 854.88] loss=3.12 avg=2.76\n",
            "[449 | 856.53] loss=2.83 avg=2.76\n",
            "[450 | 858.21] loss=2.83 avg=2.76\n",
            "[451 | 859.88] loss=2.91 avg=2.76\n",
            "[452 | 861.54] loss=2.73 avg=2.76\n",
            "[453 | 863.21] loss=2.82 avg=2.76\n",
            "[454 | 864.88] loss=2.95 avg=2.76\n",
            "[455 | 866.55] loss=2.59 avg=2.76\n",
            "[456 | 868.21] loss=2.59 avg=2.76\n",
            "[457 | 869.87] loss=2.89 avg=2.76\n",
            "[458 | 871.54] loss=2.83 avg=2.76\n",
            "[459 | 873.21] loss=2.85 avg=2.76\n",
            "[460 | 874.88] loss=3.22 avg=2.77\n",
            "[461 | 876.55] loss=2.92 avg=2.77\n",
            "[462 | 878.22] loss=3.21 avg=2.77\n",
            "[463 | 879.88] loss=2.62 avg=2.77\n",
            "[464 | 881.56] loss=2.76 avg=2.77\n",
            "[465 | 883.23] loss=3.28 avg=2.78\n",
            "[466 | 884.90] loss=2.68 avg=2.77\n",
            "[467 | 886.57] loss=2.60 avg=2.77\n",
            "[468 | 888.24] loss=2.80 avg=2.77\n",
            "[469 | 889.91] loss=2.93 avg=2.77\n",
            "[470 | 891.58] loss=2.68 avg=2.77\n",
            "[471 | 893.24] loss=2.53 avg=2.77\n",
            "[472 | 894.92] loss=3.05 avg=2.77\n",
            "[473 | 896.58] loss=2.83 avg=2.77\n",
            "[474 | 898.26] loss=2.54 avg=2.77\n",
            "[475 | 899.93] loss=2.64 avg=2.77\n",
            "[476 | 901.59] loss=2.85 avg=2.77\n",
            "[477 | 903.26] loss=2.65 avg=2.77\n",
            "[478 | 904.93] loss=2.99 avg=2.77\n",
            "[479 | 906.61] loss=2.94 avg=2.77\n",
            "[480 | 908.28] loss=2.58 avg=2.77\n",
            "[481 | 909.95] loss=2.88 avg=2.77\n",
            "[482 | 911.62] loss=2.10 avg=2.77\n",
            "[483 | 913.30] loss=2.76 avg=2.77\n",
            "[484 | 914.97] loss=2.47 avg=2.76\n",
            "[485 | 916.63] loss=2.92 avg=2.77\n",
            "[486 | 918.32] loss=3.07 avg=2.77\n",
            "[487 | 919.99] loss=2.70 avg=2.77\n",
            "[488 | 921.66] loss=2.48 avg=2.76\n",
            "[489 | 923.33] loss=2.50 avg=2.76\n",
            "[490 | 925.00] loss=2.71 avg=2.76\n",
            "[491 | 926.67] loss=2.57 avg=2.76\n",
            "[492 | 928.34] loss=2.59 avg=2.76\n",
            "[493 | 930.01] loss=2.95 avg=2.76\n",
            "[494 | 931.68] loss=2.63 avg=2.76\n",
            "[495 | 933.35] loss=2.68 avg=2.76\n",
            "[496 | 935.02] loss=2.41 avg=2.75\n",
            "[497 | 936.69] loss=2.64 avg=2.75\n",
            "[498 | 938.35] loss=2.68 avg=2.75\n",
            "[499 | 940.03] loss=3.04 avg=2.76\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "�’t want him to win a Nobel Prize because they don’t believe they’ve gotten it yet.” The article quoted her as saying that it was the first time she had heard of a person who could defeat Hitler in a one-on-one fight.\n",
            "\n",
            "Her father, a professor who taught law at the University of Berlin in the 1960s and the '70s, was in office from 1981 to 1994 and was assassinated by terrorist attacks in Oslo, the city where he taught, after a period of intense political persecution.\n",
            "\n",
            "In her book, A Short History of Violence, a recent article in The New York Times included a chapter on the story of the daughter of a German diplomat who became a leader of the National Socialist Underground in 1989. The article quoted a former high-level official at the Ministry of Foreign Affairs of Germany (MOFS, the Ministry for Foreign Affairs in Berlin) as saying that he never imagined the N.Y. Times would publish an exposé on him, given how hard it was for him to talk about politics at the time. The article also noted that Germany’s Ministry of Justice was investigating the article’s publication, in a bid to get past any possible legal problems.\n",
            "\n",
            "A few hours after the interview, I called the MOFS office to see if we could hear from the family’s’ attorney. It didn’t matter, because she confirmed the publication dates.\n",
            "\n",
            "At that point, the author of the story, who identified herself only as Kristin and told me that it was a family source, said that, during a dinner with her mother, Kristin’s older sister, who was also working at the MoFS, Kristin was told that her father was going through terminal illnesses. Kristin’s mother said she felt that she should tell them about the suicide threat.\n",
            "\n",
            "“I remember her saying that she’d always wanted to tell the rest of the family about it,” she testified.\n",
            "\n",
            "This is the only instance in which I’d heard from a person who could have prevented my father being an inspiration for people, and it was a sad and regrettable one. It was an awful time for my entire family. The story of my father is one of the most incredible things I’ve ever known. He was a really special man. He was a great father and his family was extraordinary. He and I did not have a good relationship. We couldn’t work together as parents. But he was never in any way a coldhearted person. He was never malicious. He was always kind. And he was a patriot who fought for the country. And I would have loved to meet him.’\n",
            "\n",
            "As I drove away at that point, I was reminded of the story of the girl who would change history. The girl who changed my grandmother’s life and my father’s.\n",
            "\n",
            "\n",
            "\n",
            "It is, and should be, the national anthem for the United States. It does not mean for some people, however, and especially for many Americans, it will have no bearing on everyday interactions between citizens.\n",
            "\n",
            "A chorus of young people, singing the national anthem with gusto and with some sense of irony, are calling on their school boards to remove the word “American.”\n",
            "\n",
            "They’re demanding that the words American be removed from school placards, pamphlets, and classroom instruction about America’s shared history that includes a long and illustrious history of immigrants, refugees, and immigrants.\n",
            "\n",
            "Some of these students in schools such as my own are from small towns. These kids have little to no idea that they’ll be in for a rude awakening when they walk outside this week and see young Americans protesting their parents for abandoning America, for refusing to uphold their oath to make America great again.\n",
            "\n",
            "“Every country has its own songs,” Christopher J. Fagan, an assistant professor at the University of Tennessee School of Education and an associate of the University of Delaware’s Center for Research on Globalization, told me in an interview.\n",
            "\n",
            "Fagan is one of a growing number of analysts who believe that the national anthem, or any of the words in it, represents something not just that nation but more broadly—some concept of the nation of origin, some idea of what Americans mean by “free.”\n",
            "\n",
            "Americans are not free, according to this analysis, but they ought to be.\n",
            "\n",
            "At the same time they’re not free, they’ve got to be conscious that those who claim to be free should pay for their freedom, regardless of how it’s paid for.\n",
            "\n",
            "“It’s very important to understand that this whole idea of free speech in the United States is a construct of what was a form of slavery,” Fagan said. “It’s hard\n",
            "\n",
            "[500 | 964.23] loss=3.01 avg=2.76\n",
            "[501 | 965.89] loss=2.42 avg=2.75\n",
            "[502 | 967.56] loss=2.72 avg=2.75\n",
            "[503 | 969.22] loss=3.02 avg=2.76\n",
            "[504 | 970.89] loss=2.85 avg=2.76\n",
            "[505 | 972.55] loss=3.38 avg=2.76\n",
            "[506 | 974.21] loss=2.78 avg=2.76\n",
            "[507 | 975.87] loss=3.06 avg=2.77\n",
            "[508 | 977.53] loss=2.66 avg=2.77\n",
            "[509 | 979.20] loss=3.17 avg=2.77\n",
            "[510 | 980.85] loss=3.08 avg=2.77\n",
            "[511 | 982.51] loss=2.48 avg=2.77\n",
            "[512 | 984.17] loss=2.87 avg=2.77\n",
            "[513 | 985.83] loss=2.72 avg=2.77\n",
            "[514 | 987.49] loss=2.24 avg=2.77\n",
            "[515 | 989.15] loss=2.94 avg=2.77\n",
            "[516 | 990.82] loss=2.86 avg=2.77\n",
            "[517 | 992.49] loss=2.47 avg=2.77\n",
            "[518 | 994.14] loss=2.59 avg=2.76\n",
            "[519 | 995.82] loss=2.78 avg=2.76\n",
            "[520 | 997.49] loss=2.75 avg=2.76\n",
            "[521 | 999.15] loss=2.43 avg=2.76\n",
            "[522 | 1000.83] loss=2.73 avg=2.76\n",
            "[523 | 1002.51] loss=2.45 avg=2.76\n",
            "[524 | 1004.18] loss=2.34 avg=2.75\n",
            "[525 | 1005.85] loss=2.57 avg=2.75\n",
            "[526 | 1007.54] loss=2.66 avg=2.75\n",
            "[527 | 1009.21] loss=2.84 avg=2.75\n",
            "[528 | 1010.88] loss=3.01 avg=2.75\n",
            "[529 | 1012.55] loss=3.23 avg=2.76\n",
            "[530 | 1014.24] loss=2.65 avg=2.76\n",
            "[531 | 1015.91] loss=2.83 avg=2.76\n",
            "[532 | 1017.58] loss=2.54 avg=2.76\n",
            "[533 | 1019.25] loss=2.93 avg=2.76\n",
            "[534 | 1020.92] loss=2.46 avg=2.75\n",
            "[535 | 1022.59] loss=2.46 avg=2.75\n",
            "[536 | 1024.25] loss=2.93 avg=2.75\n",
            "[537 | 1025.94] loss=2.84 avg=2.75\n",
            "[538 | 1027.61] loss=2.54 avg=2.75\n",
            "[539 | 1029.29] loss=3.75 avg=2.76\n",
            "[540 | 1030.98] loss=2.60 avg=2.76\n",
            "[541 | 1032.65] loss=2.50 avg=2.76\n",
            "[542 | 1034.34] loss=2.73 avg=2.76\n",
            "[543 | 1036.03] loss=2.52 avg=2.76\n",
            "[544 | 1037.71] loss=2.95 avg=2.76\n",
            "[545 | 1039.38] loss=2.85 avg=2.76\n",
            "[546 | 1041.06] loss=2.59 avg=2.76\n",
            "[547 | 1042.75] loss=2.94 avg=2.76\n",
            "[548 | 1044.42] loss=2.69 avg=2.76\n",
            "[549 | 1046.09] loss=2.45 avg=2.75\n",
            "[550 | 1047.77] loss=2.78 avg=2.75\n",
            "[551 | 1049.44] loss=2.88 avg=2.76\n",
            "[552 | 1051.11] loss=3.18 avg=2.76\n",
            "[553 | 1052.78] loss=2.77 avg=2.76\n",
            "[554 | 1054.45] loss=3.10 avg=2.76\n",
            "[555 | 1056.13] loss=2.72 avg=2.76\n",
            "[556 | 1057.81] loss=2.28 avg=2.76\n",
            "[557 | 1059.50] loss=2.72 avg=2.76\n",
            "[558 | 1061.16] loss=2.37 avg=2.75\n",
            "[559 | 1062.83] loss=3.02 avg=2.76\n",
            "[560 | 1064.51] loss=2.44 avg=2.75\n",
            "[561 | 1066.18] loss=2.73 avg=2.75\n",
            "[562 | 1067.85] loss=2.93 avg=2.76\n",
            "[563 | 1069.52] loss=3.17 avg=2.76\n",
            "[564 | 1071.18] loss=2.65 avg=2.76\n",
            "[565 | 1072.86] loss=2.61 avg=2.76\n",
            "[566 | 1074.53] loss=2.39 avg=2.75\n",
            "[567 | 1076.21] loss=2.74 avg=2.75\n",
            "[568 | 1077.89] loss=3.04 avg=2.76\n",
            "[569 | 1079.56] loss=2.63 avg=2.75\n",
            "[570 | 1081.23] loss=2.34 avg=2.75\n",
            "[571 | 1082.90] loss=2.48 avg=2.75\n",
            "[572 | 1084.56] loss=2.79 avg=2.75\n",
            "[573 | 1086.23] loss=2.49 avg=2.75\n",
            "[574 | 1087.91] loss=2.69 avg=2.74\n",
            "[575 | 1089.58] loss=2.61 avg=2.74\n",
            "[576 | 1091.25] loss=2.39 avg=2.74\n",
            "[577 | 1092.91] loss=2.78 avg=2.74\n",
            "[578 | 1094.57] loss=3.38 avg=2.75\n",
            "[579 | 1096.24] loss=3.25 avg=2.75\n",
            "[580 | 1097.91] loss=3.22 avg=2.76\n",
            "[581 | 1099.58] loss=2.56 avg=2.75\n",
            "[582 | 1101.25] loss=2.70 avg=2.75\n",
            "[583 | 1102.92] loss=2.79 avg=2.75\n",
            "[584 | 1104.58] loss=2.60 avg=2.75\n",
            "[585 | 1106.25] loss=2.79 avg=2.75\n",
            "[586 | 1107.90] loss=2.88 avg=2.75\n",
            "[587 | 1109.58] loss=2.28 avg=2.75\n",
            "[588 | 1111.25] loss=2.88 avg=2.75\n",
            "[589 | 1112.90] loss=2.75 avg=2.75\n",
            "[590 | 1114.57] loss=2.80 avg=2.75\n",
            "[591 | 1116.23] loss=2.48 avg=2.75\n",
            "[592 | 1117.90] loss=2.90 avg=2.75\n",
            "[593 | 1119.55] loss=2.84 avg=2.75\n",
            "[594 | 1121.22] loss=2.36 avg=2.75\n",
            "[595 | 1122.88] loss=2.94 avg=2.75\n",
            "[596 | 1124.55] loss=2.33 avg=2.74\n",
            "[597 | 1126.21] loss=2.89 avg=2.75\n",
            "[598 | 1127.88] loss=2.66 avg=2.75\n",
            "[599 | 1129.55] loss=2.83 avg=2.75\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " two individuals are willing to do what they can for others because they know how much they care about others, it's really important to find their partners who are willing to help. And then if you can build that connection of friendship, that can really be enough to sustain that connection, and sometimes I think that's something that even if they've been dating a long time that when they start seeing each other again could be a way for them to really build that relationship. It can create a new bond for you.\n",
            "\n",
            "And just so there's no misunderstanding what it's like to go through the long period of commitment, or when someone gets a partner, you have to be comfortable that they will be a very stable partner.\n",
            "\n",
            "What do you think people should do? Is it okay to date men or women, or is it good to just date guys without any preconceived notions of who he's going to end up being for you?\n",
            "\n",
            "I think it's definitely not a bad idea to date men. A lot of guys just don't understand the impact it can have in terms of your future.\n",
            "\n",
            "So what should guys do if they want to try?\n",
            "\n",
            "\n",
            "I think guys who think about that are better guys than guys who have any preconceived notions about who they want to end up being with. It's really about finding those relationships that aren't necessarily monogamous that can sustain that. I mean, there have been couples before where both partners had this idea that if you did something together, you would love each other. And they were like, No! You don't even want your kids!\n",
            "\n",
            "There is a great TED talk by a guy named Mike Adams and someone named John Kagan that has a great video on sexual satisfaction in relationships. And this isn't necessarily a guy talking about the kind of relationships you shouldn't marry, but it's a guy talking about how couples who are open and honest can have a more satisfying relationship.\n",
            "\n",
            "When I first moved to a nice, clean apartment, I had a nice little, small-town apartment in the city, and I knew nothing about guys or women. When I moved out to my full-time apartment, I knew two guys who worked at the apartment building. And so I realized that, in my apartment, I didn't have that luxury. If I didn�t be able to see those guys and ask my guys to meet with them, or if there was no one there, I could be out of luck. Because I wasn�t open with them.\n",
            "\n",
            "I wanted my own apartment because I wanted to be free so I didn�t have to share the luxury of just being open with my friends. And so I wanted a whole other world. They knew me. They knew my family. They knew my coworkers. And so I really wanted their apartment.\n",
            "\n",
            "So maybe there is this expectation that if you don�t have friends, you don�t have any friends. I want to think that there is some value to dating. You know? I think it�s pretty important to find out guys out there who have guys they are going to hang out with, to meet with their buddies and maybe try and have a relationship and not have to deal with the baggage of how someone has met somebody else just because they don��t want to hang out, or because they don�t want to date someone.\n",
            "\n",
            "I really think it shows the value of being around people that you don�t already know. That you don�t have to deal with the burden of people you don�t know. That you don�t have to deal with the baggage of someone else.\n",
            "\n",
            "You are married. You have children. You have a stable job and they know you and they can say things like, �Hey, hey! Why don�t you guys go see the movies tonight, or have some fun. You guys have a really nice, old-world apartment in the city. Hey, look at the new kids. You think the kids are smart as hell? I might even let you borrow some clothes for them. I got two of them. But you know, Mom, you have no kids yet, so you may need a few more months to pick up the kids. Maybe you will pick up my kids. Maybe you will move them in with me because you and I have good old-world, old-town common sense about it.\n",
            "\n",
            "\n",
            "It was a rough time for me, right? What was going through your mind as a woman who had to go through so much in the early days, and now what you are going through is so much better.\n",
            "\n",
            "\n",
            "The truth was that I was completely alone in the beginning. A lot of women went through a lot of pain in the beginning because of the idea that you can just kind of have any sort of relationship where you pick up the kids or go out in the country and go on vacation and be together and just get married. And it can happen. And you can feel like it\n",
            "\n",
            "[600 | 1153.79] loss=2.64 avg=2.75\n",
            "[601 | 1155.45] loss=2.51 avg=2.74\n",
            "[602 | 1157.12] loss=2.60 avg=2.74\n",
            "[603 | 1158.79] loss=2.93 avg=2.74\n",
            "[604 | 1160.46] loss=2.42 avg=2.74\n",
            "[605 | 1162.13] loss=2.75 avg=2.74\n",
            "[606 | 1163.80] loss=2.65 avg=2.74\n",
            "[607 | 1165.45] loss=2.70 avg=2.74\n",
            "[608 | 1167.12] loss=3.00 avg=2.74\n",
            "[609 | 1168.79] loss=2.47 avg=2.74\n",
            "[610 | 1170.46] loss=2.91 avg=2.74\n",
            "[611 | 1172.13] loss=2.04 avg=2.73\n",
            "[612 | 1173.80] loss=2.73 avg=2.73\n",
            "[613 | 1175.47] loss=2.47 avg=2.73\n",
            "[614 | 1177.14] loss=2.60 avg=2.73\n",
            "[615 | 1178.81] loss=3.00 avg=2.73\n",
            "[616 | 1180.48] loss=2.67 avg=2.73\n",
            "[617 | 1182.15] loss=2.56 avg=2.73\n",
            "[618 | 1183.83] loss=2.68 avg=2.73\n",
            "[619 | 1185.50] loss=2.68 avg=2.73\n",
            "[620 | 1187.18] loss=2.79 avg=2.73\n",
            "[621 | 1188.85] loss=2.32 avg=2.73\n",
            "[622 | 1190.52] loss=2.88 avg=2.73\n",
            "[623 | 1192.19] loss=2.88 avg=2.73\n",
            "[624 | 1193.86] loss=2.30 avg=2.72\n",
            "[625 | 1195.52] loss=2.63 avg=2.72\n",
            "[626 | 1197.19] loss=3.30 avg=2.73\n",
            "[627 | 1198.85] loss=3.00 avg=2.73\n",
            "[628 | 1200.52] loss=2.96 avg=2.73\n",
            "[629 | 1202.19] loss=2.61 avg=2.73\n",
            "[630 | 1203.85] loss=2.74 avg=2.73\n",
            "[631 | 1205.52] loss=3.04 avg=2.74\n",
            "[632 | 1207.19] loss=2.58 avg=2.73\n",
            "[633 | 1208.86] loss=2.35 avg=2.73\n",
            "[634 | 1210.53] loss=2.90 avg=2.73\n",
            "[635 | 1212.20] loss=2.75 avg=2.73\n",
            "[636 | 1213.87] loss=2.06 avg=2.73\n",
            "[637 | 1215.54] loss=2.81 avg=2.73\n",
            "[638 | 1217.21] loss=3.30 avg=2.73\n",
            "[639 | 1218.88] loss=3.15 avg=2.74\n",
            "[640 | 1220.55] loss=2.47 avg=2.73\n",
            "[641 | 1222.22] loss=2.93 avg=2.74\n",
            "[642 | 1223.89] loss=2.80 avg=2.74\n",
            "[643 | 1225.56] loss=2.77 avg=2.74\n",
            "[644 | 1227.23] loss=2.52 avg=2.73\n",
            "[645 | 1228.90] loss=3.18 avg=2.74\n",
            "[646 | 1230.57] loss=2.89 avg=2.74\n",
            "[647 | 1232.24] loss=2.83 avg=2.74\n",
            "[648 | 1233.91] loss=2.65 avg=2.74\n",
            "[649 | 1235.59] loss=3.22 avg=2.75\n",
            "[650 | 1237.24] loss=2.70 avg=2.74\n",
            "[651 | 1238.91] loss=2.87 avg=2.75\n",
            "[652 | 1240.58] loss=2.64 avg=2.75\n",
            "[653 | 1242.26] loss=2.55 avg=2.74\n",
            "[654 | 1243.93] loss=3.09 avg=2.75\n",
            "[655 | 1245.60] loss=2.35 avg=2.74\n",
            "[656 | 1247.27] loss=2.80 avg=2.74\n",
            "[657 | 1248.95] loss=2.59 avg=2.74\n",
            "[658 | 1250.62] loss=3.03 avg=2.74\n",
            "[659 | 1252.28] loss=2.88 avg=2.75\n",
            "[660 | 1253.95] loss=3.46 avg=2.75\n",
            "[661 | 1255.62] loss=2.72 avg=2.75\n",
            "[662 | 1257.30] loss=2.59 avg=2.75\n",
            "[663 | 1258.97] loss=2.48 avg=2.75\n",
            "[664 | 1260.64] loss=2.70 avg=2.75\n",
            "[665 | 1262.31] loss=2.74 avg=2.75\n",
            "[666 | 1263.97] loss=2.78 avg=2.75\n",
            "[667 | 1265.65] loss=3.01 avg=2.75\n",
            "[668 | 1267.32] loss=2.48 avg=2.75\n",
            "[669 | 1268.99] loss=2.57 avg=2.75\n",
            "[670 | 1270.66] loss=2.42 avg=2.74\n",
            "[671 | 1272.33] loss=2.71 avg=2.74\n",
            "[672 | 1273.99] loss=2.62 avg=2.74\n",
            "[673 | 1275.65] loss=2.39 avg=2.74\n",
            "[674 | 1277.32] loss=2.97 avg=2.74\n",
            "[675 | 1278.99] loss=3.03 avg=2.74\n",
            "[676 | 1280.65] loss=2.50 avg=2.74\n",
            "[677 | 1282.31] loss=2.89 avg=2.74\n",
            "[678 | 1283.99] loss=2.46 avg=2.74\n",
            "[679 | 1285.65] loss=2.92 avg=2.74\n",
            "[680 | 1287.31] loss=2.72 avg=2.74\n",
            "[681 | 1288.97] loss=3.06 avg=2.74\n",
            "[682 | 1290.64] loss=2.57 avg=2.74\n",
            "[683 | 1292.30] loss=2.90 avg=2.74\n",
            "[684 | 1293.98] loss=2.71 avg=2.74\n",
            "[685 | 1295.64] loss=2.75 avg=2.74\n",
            "[686 | 1297.32] loss=2.68 avg=2.74\n",
            "[687 | 1298.99] loss=2.60 avg=2.74\n",
            "[688 | 1300.66] loss=2.81 avg=2.74\n",
            "[689 | 1302.33] loss=2.31 avg=2.74\n",
            "[690 | 1304.01] loss=2.61 avg=2.74\n",
            "[691 | 1305.68] loss=2.81 avg=2.74\n",
            "[692 | 1307.35] loss=2.33 avg=2.73\n",
            "[693 | 1309.02] loss=3.14 avg=2.74\n",
            "[694 | 1310.69] loss=2.62 avg=2.74\n",
            "[695 | 1312.36] loss=2.89 avg=2.74\n",
            "[696 | 1314.03] loss=3.01 avg=2.74\n",
            "[697 | 1315.68] loss=2.41 avg=2.74\n",
            "[698 | 1317.35] loss=2.68 avg=2.74\n",
            "[699 | 1319.01] loss=2.81 avg=2.74\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " States, including the first time since 2004.\n",
            "\n",
            "But it is important to keep in mind that the number of Americans who call themselves Christians has grown over the last 20 years, largely because fewer people have embraced the label. According to Pew Research, the share of Americans who identify themselves as Protestant dropped to 19 percent in the wake of the Sept. 11 terrorist attacks, which brought with it the adoption of the phrase \"secular humanism,\" rather than \"secular humanism\" – another name for \"Christian humanism.\" A separate survey by the Pew Center, for example, found that 42 percent of Americans say they have no religious affiliation.\n",
            "\n",
            "Yet even among people identifying themselves as Protestants, the rate of secular humanism doesn't always hold up against its more extreme enemies. It seems that secular humanism is best described as the position that the world should not impose its faith on people.\n",
            "\n",
            "The fact that Christians do not dominate American public life and public life is not lost on critics of the term. When it comes to defining secular humanism, they argue, atheists, agnostics, and other religious people are simply not seen as important enough to be considered part of the public conscience. They argue that it is the most politically expedient term for the term to take on. Many secular humanists, they say, simply need to get a bit more \"religious\" to win over the public.\n",
            "\n",
            "The trouble is, religion is important and powerful. Atheists and agnostics have more influence over conservative politics than religious figures do, and they account for the vast majority of the so-called evangelicals who are active in politics. Religious voters tend to be Republicans, and evangelicals have long been a strong voting bloc for Republicans and their party platforms. As a result, conservatives and evangelicals may be less susceptible to secular humanism's appeal among their own constituents. While it may not be a bad thing, it may just be the case that secular humanism takes on the same meaning, both now and in the future, that religious people do.\n",
            "\n",
            "According to the American Civil Liberties Union, between 20,000 to 30,000 children now grow up without a parent in the home and face a greater rate of suicide than their Catholic- and Protestant-identified peers. Most of those cases include atheists or agnostics who self-identify as secular humans, or secular beings with no religion, who feel they don't fit into society's traditional definitions of decency.\n",
            "\n",
            "Some atheists and agnostics think secular humanism is dead, and a movement to redefine itself is needed. After all, how else to describe such a group of people who, according to their own definition of decency, must be in the closet, wearing religious head coverings, making religious claims at every opportunity, and making a fuss to make it? They may not be the same in person, but they aren't the same in name either. They don't speak the same language, but that does not mean they don't feel the same needs.\n",
            "\n",
            "Some secular humanists feel that it is only appropriate to define secular humanism as a secular movement, not as a movement of individuals, churches, or denominations, or even a movement of believers. That is, secular humanism doesn't have to be just anyone taking a stand. That is, secular humanism doesn't have to be secular humanism.\n",
            "\n",
            "Perhaps, given that the term secular humanism has become synonymous with religious people, the only real choice is to leave the religion in secular humanism to its own devices and define something new. But it is only because the secular humanism movement exists, it doesn't necessarily even have to make it a standard set of political stances, it just has to have an existence.\n",
            "\n",
            "But then, perhaps the only other choice are to let the label die and try something new, a different word, something more inclusive. Or, at least, a more inclusive political stance.\n",
            "\n",
            "But I won't. It is still secular humanism. It must have that name.\n",
            "\n",
            "\n",
            "\n",
            "We have many other issues here at CNN, but most of them have to do with our culture and human beings.\n",
            "\n",
            "The question is always: Does the world want us here? Or does the world want us dead?\n",
            "\n",
            "So, this week, when Donald Trump promised that he would be a better negotiator and a better negotiator than Barack Obama, he made some pretty bold promises. His foreign policy, which has already been pretty radical by American standards, will likely be worse than Obama hoped. One of his promises was to move away from NATO, a policy that seems to have been largely successful.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Donald Trump said Saturday he would not change the U.S. approach toward Iran. He would make them pay. He said he was willing to be a lot tougher in the Middle East than Obama was. He said the United States had a nuclear arsenal.\n",
            "\n",
            "It may well be true that Trump can be a bad negotiator, and a bad negotiator can be a\n",
            "\n",
            "[700 | 1343.29] loss=2.53 avg=2.74\n",
            "[701 | 1344.94] loss=2.70 avg=2.73\n",
            "[702 | 1346.61] loss=2.66 avg=2.73\n",
            "[703 | 1348.26] loss=2.96 avg=2.74\n",
            "[704 | 1349.92] loss=2.51 avg=2.73\n",
            "[705 | 1351.59] loss=2.46 avg=2.73\n",
            "[706 | 1353.26] loss=2.91 avg=2.73\n",
            "[707 | 1354.93] loss=2.79 avg=2.73\n",
            "[708 | 1356.60] loss=2.57 avg=2.73\n",
            "[709 | 1358.28] loss=2.83 avg=2.73\n",
            "[710 | 1359.95] loss=2.97 avg=2.74\n",
            "[711 | 1361.61] loss=2.57 avg=2.73\n",
            "[712 | 1363.28] loss=2.49 avg=2.73\n",
            "[713 | 1364.94] loss=3.43 avg=2.74\n",
            "[714 | 1366.62] loss=2.83 avg=2.74\n",
            "[715 | 1368.29] loss=2.92 avg=2.74\n",
            "[716 | 1369.94] loss=2.98 avg=2.74\n",
            "[717 | 1371.61] loss=2.50 avg=2.74\n",
            "[718 | 1373.28] loss=3.02 avg=2.74\n",
            "[719 | 1374.94] loss=2.28 avg=2.74\n",
            "[720 | 1376.59] loss=3.16 avg=2.74\n",
            "[721 | 1378.27] loss=2.44 avg=2.74\n",
            "[722 | 1379.94] loss=2.76 avg=2.74\n",
            "[723 | 1381.62] loss=3.09 avg=2.74\n",
            "[724 | 1383.29] loss=3.04 avg=2.75\n",
            "[725 | 1384.96] loss=2.63 avg=2.75\n",
            "[726 | 1386.63] loss=2.75 avg=2.75\n",
            "[727 | 1388.29] loss=2.67 avg=2.75\n",
            "[728 | 1389.96] loss=2.58 avg=2.74\n",
            "[729 | 1391.63] loss=2.48 avg=2.74\n",
            "[730 | 1393.30] loss=2.44 avg=2.74\n",
            "[731 | 1394.96] loss=2.45 avg=2.74\n",
            "[732 | 1396.63] loss=3.05 avg=2.74\n",
            "[733 | 1398.30] loss=3.03 avg=2.74\n",
            "[734 | 1399.97] loss=2.94 avg=2.74\n",
            "[735 | 1401.64] loss=2.73 avg=2.74\n",
            "[736 | 1403.30] loss=2.53 avg=2.74\n",
            "[737 | 1404.97] loss=2.64 avg=2.74\n",
            "[738 | 1406.64] loss=2.57 avg=2.74\n",
            "[739 | 1408.31] loss=2.38 avg=2.73\n",
            "[740 | 1409.99] loss=2.50 avg=2.73\n",
            "[741 | 1411.65] loss=2.52 avg=2.73\n",
            "[742 | 1413.32] loss=2.31 avg=2.73\n",
            "[743 | 1415.00] loss=2.91 avg=2.73\n",
            "[744 | 1416.67] loss=2.51 avg=2.73\n",
            "[745 | 1418.34] loss=2.59 avg=2.72\n",
            "[746 | 1420.01] loss=2.55 avg=2.72\n",
            "[747 | 1421.68] loss=2.81 avg=2.72\n",
            "[748 | 1423.35] loss=2.71 avg=2.72\n",
            "[749 | 1425.01] loss=2.38 avg=2.72\n",
            "[750 | 1426.68] loss=2.59 avg=2.72\n",
            "[751 | 1428.36] loss=2.68 avg=2.72\n",
            "[752 | 1430.02] loss=2.31 avg=2.71\n",
            "[753 | 1431.70] loss=2.97 avg=2.72\n",
            "[754 | 1433.37] loss=2.78 avg=2.72\n",
            "[755 | 1435.04] loss=2.59 avg=2.72\n",
            "[756 | 1436.71] loss=2.80 avg=2.72\n",
            "[757 | 1438.37] loss=3.32 avg=2.72\n",
            "[758 | 1440.04] loss=2.65 avg=2.72\n",
            "[759 | 1441.69] loss=3.24 avg=2.73\n",
            "[760 | 1443.36] loss=3.13 avg=2.73\n",
            "[761 | 1445.02] loss=1.83 avg=2.72\n",
            "[762 | 1446.69] loss=2.72 avg=2.72\n",
            "[763 | 1448.36] loss=2.92 avg=2.72\n",
            "[764 | 1450.03] loss=2.61 avg=2.72\n",
            "[765 | 1451.71] loss=2.38 avg=2.72\n",
            "[766 | 1453.37] loss=2.81 avg=2.72\n",
            "[767 | 1455.04] loss=2.29 avg=2.72\n",
            "[768 | 1456.72] loss=3.02 avg=2.72\n",
            "[769 | 1458.39] loss=2.91 avg=2.72\n",
            "[770 | 1460.06] loss=2.66 avg=2.72\n",
            "[771 | 1461.73] loss=2.49 avg=2.72\n",
            "[772 | 1463.40] loss=3.07 avg=2.72\n",
            "[773 | 1465.07] loss=2.75 avg=2.72\n",
            "[774 | 1466.74] loss=2.64 avg=2.72\n",
            "[775 | 1468.41] loss=3.00 avg=2.72\n",
            "[776 | 1470.08] loss=2.78 avg=2.72\n",
            "[777 | 1471.75] loss=2.35 avg=2.72\n",
            "[778 | 1473.42] loss=2.79 avg=2.72\n",
            "[779 | 1475.09] loss=2.91 avg=2.72\n",
            "[780 | 1476.76] loss=2.42 avg=2.72\n",
            "[781 | 1478.43] loss=2.84 avg=2.72\n",
            "[782 | 1480.10] loss=2.67 avg=2.72\n",
            "[783 | 1481.77] loss=2.65 avg=2.72\n",
            "[784 | 1483.44] loss=2.59 avg=2.72\n",
            "[785 | 1485.11] loss=2.66 avg=2.72\n",
            "[786 | 1486.78] loss=2.51 avg=2.72\n",
            "[787 | 1488.44] loss=2.91 avg=2.72\n",
            "[788 | 1490.10] loss=2.90 avg=2.72\n",
            "[789 | 1491.77] loss=2.86 avg=2.72\n",
            "[790 | 1493.44] loss=2.36 avg=2.72\n",
            "[791 | 1495.11] loss=3.09 avg=2.72\n",
            "[792 | 1496.78] loss=2.48 avg=2.72\n",
            "[793 | 1498.45] loss=2.69 avg=2.72\n",
            "[794 | 1500.12] loss=2.49 avg=2.72\n",
            "[795 | 1501.80] loss=2.57 avg=2.72\n",
            "[796 | 1503.46] loss=3.03 avg=2.72\n",
            "[797 | 1505.13] loss=2.50 avg=2.72\n",
            "[798 | 1506.80] loss=2.51 avg=2.71\n",
            "[799 | 1508.48] loss=3.12 avg=2.72\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " his parents, but they are now back in her home, where she lives with a father and mother and is living in a tent with a small garden.\n",
            "\n",
            "Now she is working with the United Kingdom for a charity called Women on Waves, which is raising money for victims of sexual trafficking, mostly in Bangladesh, to help them rebuild their lives as refugees.\n",
            "\n",
            "But her life, she said, is not any better there.\n",
            "\n",
            "\"I don't know if I will ever find another place like this, where the people who work are good, where the people are happy, where it feels like your life is yours,\" she said. \"My life is gone. All the things I loved are gone. I want the same thing for myself.\"\n",
            "\n",
            "In the absence of support, she feels alone, isolated and hopeless. She wants to return home. And she doesn't know where.\n",
            "\n",
            "Lina Khair, an adviser to the U.S. Embassy in Dhaka, said the International Organization for Migration has received multiple complaints from women who tried to return home before finding refuge in Bangladesh. Women who are rescued from being trafficked have reported that traffickers have sexually abused them before being able to return.\n",
            "\n",
            "The plight of women facing the reality that their lives are lost in Bangladesh is not unusual. The situation is the same as the one faced by many others who are lured into work as a sex worker, or forced into domestic labor, and then abused and trafficked to other countries. According to United Nations data, women make up one-fourth of all women trafficked in Bangladesh, and a majority of those who are forced to sell their bodies for sex are women.\n",
            "\n",
            "The Bangladesh migrant worker crisis has been underreported because of government and NGO policies that prevent them from seeing the outside world. Many of the Bangladesh migrant workers are in fear of their safety and are afraid to speak up. Those who have come forward as trafficking victims have been denied shelter and forced to work with their backs to the wall and at work places where they are physically and sexually assaulted. Many of the women are fearful to speak out because they fear of their safety. They are trapped inside themselves.\n",
            "\n",
            "There is no national government solution because the United Nations and World Health Organization do not recognize or support the Bangladesh government.\n",
            "\n",
            "For a long time, governments and NGOs have paid for women and girls to return to the camps, in order to be able to feed and house them. But, due to the inability of governments to pay, aid agencies have stepped in to pay for the women to leave the camps. But, the money is not there, because in Bangladesh, the NGOs don’t have the money to pay. The women cannot leave with a government-provided plane ticket or accommodation. When the money is available, it’s given to them, but they are still forced to work in unsafe conditions.\n",
            "\n",
            "There is only so much food you can produce in the camps. Bangladesh has no basic food supply. The food production is controlled by the traffickers. When the money dries up, you know you can’t make anything. That is what happened to Khandur. There is no money. So, that is what this humanitarian crisis is all about.\n",
            "\n",
            "Humanitarians like Khandur need to get in touch with these people, because if they can get in touch with the women to the rescue camps in Bangladesh, who will have access to them?\n",
            "\n",
            "\n",
            "\n",
            "The first thing a student needs to realize when she begins college is that her life will be different from any other student she could have been if she wasn’t working.\n",
            "\n",
            "This fact is true for any student who doesn’t attend a high-performing school in the country, because it means that she has the option of remaining there through graduation, though she’s unlikely to get a job.\n",
            "\n",
            "But for a student at the same public or charter school who was not in need of extra help—who had no financial aid to spare—and was given the same opportunity to get an education as others, she can move on and make her own life. This is what the parents of many students who attend for-profit universities do to find work in their fields after graduating. The choice is clear: stay home and pay for tuition; or move to a lower-paid job if possible.\n",
            "\n",
            "Some students pursue this path because it’s the best option and they can find a job quickly after their graduation, because it allows them to pay off their loans before their debt becomes unmanageable.\n",
            "\n",
            "What parents at for-profit universities do not realize is that they have many of these students—students who want a better education and better lives, and they also have an obligation to find a job to pay off their loans and get their educations. They have to work when they can, not when we give them the opportunity to stay at home and take care of their parents or relatives.\n",
            "\n",
            "One of my former college professors\n",
            "\n",
            "[800 | 1532.92] loss=2.43 avg=2.72\n",
            "[801 | 1534.59] loss=2.77 avg=2.72\n",
            "[802 | 1536.25] loss=2.55 avg=2.71\n",
            "[803 | 1537.90] loss=2.76 avg=2.71\n",
            "[804 | 1539.54] loss=2.71 avg=2.71\n",
            "[805 | 1541.20] loss=2.64 avg=2.71\n",
            "[806 | 1542.85] loss=3.02 avg=2.72\n",
            "[807 | 1544.51] loss=2.66 avg=2.72\n",
            "[808 | 1546.17] loss=3.08 avg=2.72\n",
            "[809 | 1547.82] loss=2.32 avg=2.72\n",
            "[810 | 1549.49] loss=2.93 avg=2.72\n",
            "[811 | 1551.14] loss=2.78 avg=2.72\n",
            "[812 | 1552.79] loss=2.84 avg=2.72\n",
            "[813 | 1554.45] loss=2.65 avg=2.72\n",
            "[814 | 1556.11] loss=2.72 avg=2.72\n",
            "[815 | 1557.76] loss=2.65 avg=2.72\n",
            "[816 | 1559.42] loss=2.77 avg=2.72\n",
            "[817 | 1561.07] loss=2.37 avg=2.72\n",
            "[818 | 1562.74] loss=3.26 avg=2.72\n",
            "[819 | 1564.40] loss=2.71 avg=2.72\n",
            "[820 | 1566.07] loss=2.90 avg=2.72\n",
            "[821 | 1567.74] loss=2.47 avg=2.72\n",
            "[822 | 1569.41] loss=2.66 avg=2.72\n",
            "[823 | 1571.08] loss=2.56 avg=2.72\n",
            "[824 | 1572.75] loss=2.60 avg=2.72\n",
            "[825 | 1574.42] loss=2.44 avg=2.71\n",
            "[826 | 1576.09] loss=2.65 avg=2.71\n",
            "[827 | 1577.77] loss=2.63 avg=2.71\n",
            "[828 | 1579.44] loss=3.03 avg=2.72\n",
            "[829 | 1581.11] loss=2.57 avg=2.71\n",
            "[830 | 1582.78] loss=2.82 avg=2.72\n",
            "[831 | 1584.44] loss=2.82 avg=2.72\n",
            "[832 | 1586.11] loss=3.13 avg=2.72\n",
            "[833 | 1587.78] loss=2.64 avg=2.72\n",
            "[834 | 1589.46] loss=2.92 avg=2.72\n",
            "[835 | 1591.13] loss=2.89 avg=2.72\n",
            "[836 | 1592.80] loss=2.70 avg=2.72\n",
            "[837 | 1594.47] loss=2.34 avg=2.72\n",
            "[838 | 1596.15] loss=2.79 avg=2.72\n",
            "[839 | 1597.82] loss=2.67 avg=2.72\n",
            "[840 | 1599.49] loss=2.71 avg=2.72\n",
            "[841 | 1601.16] loss=2.50 avg=2.72\n",
            "[842 | 1602.84] loss=3.06 avg=2.72\n",
            "[843 | 1604.52] loss=2.66 avg=2.72\n",
            "[844 | 1606.19] loss=2.99 avg=2.72\n",
            "[845 | 1607.86] loss=2.62 avg=2.72\n",
            "[846 | 1609.53] loss=2.74 avg=2.72\n",
            "[847 | 1611.20] loss=2.62 avg=2.72\n",
            "[848 | 1612.88] loss=3.52 avg=2.73\n",
            "[849 | 1614.56] loss=2.38 avg=2.73\n",
            "[850 | 1616.24] loss=3.04 avg=2.73\n",
            "[851 | 1617.92] loss=2.62 avg=2.73\n",
            "[852 | 1619.59] loss=2.46 avg=2.72\n",
            "[853 | 1621.27] loss=2.83 avg=2.73\n",
            "[854 | 1622.96] loss=2.51 avg=2.72\n",
            "[855 | 1624.63] loss=2.78 avg=2.72\n",
            "[856 | 1626.31] loss=2.46 avg=2.72\n",
            "[857 | 1628.00] loss=2.83 avg=2.72\n",
            "[858 | 1629.67] loss=3.00 avg=2.73\n",
            "[859 | 1631.36] loss=2.49 avg=2.72\n",
            "[860 | 1633.03] loss=3.06 avg=2.73\n",
            "[861 | 1634.70] loss=3.31 avg=2.73\n",
            "[862 | 1636.37] loss=2.25 avg=2.73\n",
            "[863 | 1638.06] loss=2.73 avg=2.73\n",
            "[864 | 1639.73] loss=2.48 avg=2.73\n",
            "[865 | 1641.40] loss=2.53 avg=2.72\n",
            "[866 | 1643.07] loss=2.39 avg=2.72\n",
            "[867 | 1644.74] loss=2.75 avg=2.72\n",
            "[868 | 1646.42] loss=2.82 avg=2.72\n",
            "[869 | 1648.09] loss=2.63 avg=2.72\n",
            "[870 | 1649.76] loss=2.61 avg=2.72\n",
            "[871 | 1651.43] loss=2.67 avg=2.72\n",
            "[872 | 1653.11] loss=2.75 avg=2.72\n",
            "[873 | 1654.79] loss=3.09 avg=2.72\n",
            "[874 | 1656.46] loss=2.99 avg=2.73\n",
            "[875 | 1658.13] loss=2.83 avg=2.73\n",
            "[876 | 1659.80] loss=2.83 avg=2.73\n",
            "[877 | 1661.47] loss=2.76 avg=2.73\n",
            "[878 | 1663.15] loss=2.84 avg=2.73\n",
            "[879 | 1664.82] loss=2.75 avg=2.73\n",
            "[880 | 1666.48] loss=2.57 avg=2.73\n",
            "[881 | 1668.14] loss=2.67 avg=2.73\n",
            "[882 | 1669.82] loss=2.94 avg=2.73\n",
            "[883 | 1671.48] loss=2.70 avg=2.73\n",
            "[884 | 1673.15] loss=2.44 avg=2.73\n",
            "[885 | 1674.82] loss=2.89 avg=2.73\n",
            "[886 | 1676.49] loss=2.77 avg=2.73\n",
            "[887 | 1678.16] loss=3.01 avg=2.73\n",
            "[888 | 1679.83] loss=2.57 avg=2.73\n",
            "[889 | 1681.50] loss=2.73 avg=2.73\n",
            "[890 | 1683.16] loss=2.53 avg=2.73\n",
            "[891 | 1684.83] loss=2.51 avg=2.73\n",
            "[892 | 1686.50] loss=2.73 avg=2.73\n",
            "[893 | 1688.17] loss=2.52 avg=2.72\n",
            "[894 | 1689.82] loss=2.40 avg=2.72\n",
            "[895 | 1691.48] loss=3.04 avg=2.72\n",
            "[896 | 1693.15] loss=2.74 avg=2.72\n",
            "[897 | 1694.82] loss=2.51 avg=2.72\n",
            "[898 | 1696.48] loss=2.60 avg=2.72\n",
            "[899 | 1698.15] loss=2.64 avg=2.72\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " unraveling. What would it take for those who have been on these flights to go home?\n",
            "\n",
            "The answer is far from simple. As the New York Times pointed out this week, this decision could be an indication of how much of the debate in the aftermath of the terrorist attacks would have to change. Or it could signal that officials on the ground have learned from past mistakes, and that more is happening to ensure that those who fall ill here are well cared for. (A spokesman for the Centers for Disease Control and Prevention said that he cannot confirm the number of Americans who have returned from the United States since last year.)\n",
            "\n",
            "It would also indicate that it is possible to do more to prevent such tragedies from happening in the future. In the weeks between the deadly attacks in Westminster and the March 16 attack in New York, the public quickly rallied around the victims and their families. As the Times and the Guardian reported, the response was immediate and overwhelming, with the U.K. Prime Minister Theresa May describing the attacks as a 'tragic reminder of the dangers facing the next generation of British Muslims. ‎The incident demonstrates how the actions of a small group can have an outsized impact on a community in its infancy.'\n",
            "\n",
            "It's too early to draw conclusions. The United States has no specific policies for returning home sick citizens. But the Trump administration has already made some steps: the White House has directed federal agencies to create guidelines around returning sick children to their parents. And President Trump has instructed agencies such as the Centers for Disease Control and Prevention to prepare the sickness statistics they report to Congress.\n",
            "\n",
            "So, how would a Trump administration address sick travelers in the future? In response to the March 16 attack in Westminster, the Trump administration has issued guidance that would direct agencies to release more information to Congress, but also to make it harder for the public to find that information publicly; it has made the public aware of the number of sick travelers on American soil that has not appeared in the United States since December of 2017. Last week, the Trump administration issued a statement that outlined the new requirements, and suggested that the next administration would require that DHS establish an open source database that includes information from the data that was created by the DHS as of March 16.\n",
            "\n",
            "I think it would be a mistake for the Trump administration to be too harsh with returning U.K.'s, and we have a huge number of Americans who would still be on the plane, but I also think we also need to talk about the fact that it would be unfair that the sickest passengers would be excluded from knowing the number of U.S. citizens that are sick. You had this week an interview with the Guardian in which their reporter, Emily O'Brien, asked how many Americans were still alive. ‎She asked if the government knew about people still alive, and she was told 'only the sickest people.' That kind of question suggests a certain kind of entitlement—not because it is unfair, but because it is arbitrary and, to paraphrase Shakespeare, unfair.\n",
            "\n",
            "And that kind of behavior also can leave an opening for terrorism. It wouldn’t surprise me if an additional terrorist attack was launched on the ground, on the ground in a U.K. town center, to cause a few hundred American citizens to go home. Just take this example: An English teacher and an American who did a workshop together and they went out for dinner. Then, there wasn’t much to do. “It’s hard for me to imagine why someone who has to take a plane to Paris and back wouldn’t want to go home. But, of course, that’s not what I’ve heard. One of our citizens was planning to leave for the United States on the same flight.” There. One of our citizens. Why don’t we address this now? I wonder what you’d make of that.\n",
            "\n",
            "In any event, when I met with my colleagues on Capitol Hill this week, I said that we should work together and have a conversation—a conversation with everyone in the United States. I’m just so glad this is finally happening. We’ve been looking at this from the outside for so many years. That’s why we’ve not gotten this off the ground in a lot of countries. It’s only going to get stronger because it is in the heart of the U.S.\n",
            "\n",
            "AMY GOODMAN: Theresa May, the prime minister of the United Kingdom, was speaking at the U.K. Parliament and then later attending a ceremony at Battersea Power Station—the U.K. electricity station.\n",
            "\n",
            "THERESA MAY: And, of course, Brexit is the right thing to do. To say that you’re a U.K. citizen and a British citizen living in Great Britain—and, as long as they do meet the criteria, they will be allowed to be British citizens. And, in\n",
            "\n",
            "[900 | 1722.75] loss=2.37 avg=2.72\n",
            "[901 | 1724.42] loss=2.44 avg=2.71\n",
            "[902 | 1726.08] loss=3.26 avg=2.72\n",
            "[903 | 1727.75] loss=2.55 avg=2.72\n",
            "[904 | 1729.41] loss=2.84 avg=2.72\n",
            "[905 | 1731.08] loss=2.40 avg=2.72\n",
            "[906 | 1732.75] loss=1.85 avg=2.71\n",
            "[907 | 1734.41] loss=2.72 avg=2.71\n",
            "[908 | 1736.08] loss=2.80 avg=2.71\n",
            "[909 | 1737.73] loss=2.55 avg=2.71\n",
            "[910 | 1739.39] loss=2.65 avg=2.71\n",
            "[911 | 1741.07] loss=2.92 avg=2.71\n",
            "[912 | 1742.73] loss=2.80 avg=2.71\n",
            "[913 | 1744.40] loss=2.82 avg=2.71\n",
            "[914 | 1746.06] loss=2.80 avg=2.71\n",
            "[915 | 1747.73] loss=2.73 avg=2.71\n",
            "[916 | 1749.40] loss=2.68 avg=2.71\n",
            "[917 | 1751.06] loss=2.99 avg=2.71\n",
            "[918 | 1752.72] loss=2.68 avg=2.71\n",
            "[919 | 1754.39] loss=2.56 avg=2.71\n",
            "[920 | 1756.05] loss=2.48 avg=2.71\n",
            "[921 | 1757.72] loss=1.85 avg=2.70\n",
            "[922 | 1759.38] loss=2.52 avg=2.70\n",
            "[923 | 1761.04] loss=2.70 avg=2.70\n",
            "[924 | 1762.71] loss=2.85 avg=2.70\n",
            "[925 | 1764.38] loss=2.83 avg=2.70\n",
            "[926 | 1766.04] loss=2.48 avg=2.70\n",
            "[927 | 1767.70] loss=2.73 avg=2.70\n",
            "[928 | 1769.36] loss=2.92 avg=2.70\n",
            "[929 | 1771.01] loss=2.72 avg=2.70\n",
            "[930 | 1772.67] loss=2.65 avg=2.70\n",
            "[931 | 1774.33] loss=2.09 avg=2.70\n",
            "[932 | 1775.98] loss=2.51 avg=2.69\n",
            "[933 | 1777.64] loss=2.44 avg=2.69\n",
            "[934 | 1779.30] loss=3.06 avg=2.69\n",
            "[935 | 1780.97] loss=2.49 avg=2.69\n",
            "[936 | 1782.63] loss=3.06 avg=2.70\n",
            "[937 | 1784.29] loss=2.49 avg=2.69\n",
            "[938 | 1785.95] loss=1.96 avg=2.69\n",
            "[939 | 1787.62] loss=2.70 avg=2.69\n",
            "[940 | 1789.29] loss=2.85 avg=2.69\n",
            "[941 | 1790.95] loss=2.97 avg=2.69\n",
            "[942 | 1792.62] loss=2.50 avg=2.69\n",
            "[943 | 1794.28] loss=2.55 avg=2.69\n",
            "[944 | 1795.95] loss=2.77 avg=2.69\n",
            "[945 | 1797.60] loss=2.58 avg=2.69\n",
            "[946 | 1799.27] loss=2.82 avg=2.69\n",
            "[947 | 1800.94] loss=2.86 avg=2.69\n",
            "[948 | 1802.60] loss=2.48 avg=2.69\n",
            "[949 | 1804.27] loss=2.18 avg=2.68\n",
            "[950 | 1805.94] loss=1.75 avg=2.67\n",
            "[951 | 1807.60] loss=2.75 avg=2.67\n",
            "[952 | 1809.28] loss=2.68 avg=2.67\n",
            "[953 | 1810.94] loss=2.74 avg=2.68\n",
            "[954 | 1812.62] loss=2.81 avg=2.68\n",
            "[955 | 1814.29] loss=2.71 avg=2.68\n",
            "[956 | 1815.96] loss=2.56 avg=2.68\n",
            "[957 | 1817.63] loss=1.68 avg=2.67\n",
            "[958 | 1819.30] loss=2.78 avg=2.67\n",
            "[959 | 1820.96] loss=2.38 avg=2.66\n",
            "[960 | 1822.63] loss=2.69 avg=2.66\n",
            "[961 | 1824.31] loss=2.37 avg=2.66\n",
            "[962 | 1825.97] loss=2.51 avg=2.66\n",
            "[963 | 1827.64] loss=3.18 avg=2.67\n",
            "[964 | 1829.31] loss=2.62 avg=2.66\n",
            "[965 | 1830.98] loss=2.72 avg=2.67\n",
            "[966 | 1832.65] loss=2.59 avg=2.66\n",
            "[967 | 1834.33] loss=2.62 avg=2.66\n",
            "[968 | 1835.99] loss=2.93 avg=2.67\n",
            "[969 | 1837.66] loss=2.74 avg=2.67\n",
            "[970 | 1839.35] loss=3.19 avg=2.67\n",
            "[971 | 1841.03] loss=2.61 avg=2.67\n",
            "[972 | 1842.70] loss=2.81 avg=2.67\n",
            "[973 | 1844.37] loss=2.72 avg=2.67\n",
            "[974 | 1846.04] loss=2.56 avg=2.67\n",
            "[975 | 1847.72] loss=2.70 avg=2.67\n",
            "[976 | 1849.39] loss=3.28 avg=2.68\n",
            "[977 | 1851.05] loss=2.90 avg=2.68\n",
            "[978 | 1852.72] loss=2.59 avg=2.68\n",
            "[979 | 1854.39] loss=2.78 avg=2.68\n",
            "[980 | 1856.07] loss=2.73 avg=2.68\n",
            "[981 | 1857.74] loss=3.57 avg=2.69\n",
            "[982 | 1859.42] loss=2.67 avg=2.69\n",
            "[983 | 1861.09] loss=2.62 avg=2.69\n",
            "[984 | 1862.76] loss=2.43 avg=2.69\n",
            "[985 | 1864.43] loss=2.53 avg=2.69\n",
            "[986 | 1866.10] loss=2.84 avg=2.69\n",
            "[987 | 1867.78] loss=2.98 avg=2.69\n",
            "[988 | 1869.45] loss=2.66 avg=2.69\n",
            "[989 | 1871.12] loss=1.99 avg=2.68\n",
            "[990 | 1872.79] loss=2.68 avg=2.68\n",
            "[991 | 1874.47] loss=2.72 avg=2.68\n",
            "[992 | 1876.15] loss=2.57 avg=2.68\n",
            "[993 | 1877.82] loss=2.75 avg=2.68\n",
            "[994 | 1879.49] loss=2.99 avg=2.69\n",
            "[995 | 1881.16] loss=2.58 avg=2.68\n",
            "[996 | 1882.83] loss=2.74 avg=2.69\n",
            "[997 | 1884.50] loss=2.72 avg=2.69\n",
            "[998 | 1886.17] loss=2.67 avg=2.69\n",
            "[999 | 1887.85] loss=1.73 avg=2.68\n",
            "Ran 1010 steps.\n",
            "Saving checkpoint/all_atlantic_345/model-1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHT5eGey8UKC",
        "colab_type": "code",
        "outputId": "f8cb830a-4b79-4c67-89de-7dbb222c6b2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "## personal essays training - with 345\n",
        "!PYTHONPATH=src ./train.py --dataset /content/all_my_journals.txt --run_name 'personal_essays_345' --model_name '345M'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0627 21:42:35.603564 140100349421440 deprecation_wrapper.py:119] From /content/gpt-2/src/model.py:147: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
            "\n",
            "W0627 21:42:35.612083 140100349421440 deprecation_wrapper.py:119] From /content/gpt-2/src/memory_saving_gradients.py:13: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n",
            "W0627 21:42:35.710467 140100349421440 deprecation_wrapper.py:119] From ./train.py:88: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "W0627 21:42:35.710817 140100349421440 deprecation_wrapper.py:119] From ./train.py:91: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "2019-06-27 21:42:35.717078: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-06-27 21:42:35.717354: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1f6d100 executing computations on platform Host. Devices:\n",
            "2019-06-27 21:42:35.717388: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2019-06-27 21:42:35.719677: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\n",
            "2019-06-27 21:42:35.872234: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-27 21:42:35.872802: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1f6c840 executing computations on platform CUDA. Devices:\n",
            "2019-06-27 21:42:35.872832: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2019-06-27 21:42:35.873093: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-27 21:42:35.873462: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-06-27 21:42:35.873849: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-06-27 21:42:35.875104: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-06-27 21:42:35.876426: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-06-27 21:42:35.876793: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-06-27 21:42:35.878206: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-06-27 21:42:35.879366: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-06-27 21:42:35.882578: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-06-27 21:42:35.882715: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-27 21:42:35.883115: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-27 21:42:35.883468: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
            "2019-06-27 21:42:35.883534: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-06-27 21:42:35.884503: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-06-27 21:42:35.884528: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n",
            "2019-06-27 21:42:35.884539: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n",
            "2019-06-27 21:42:35.884836: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-27 21:42:35.885231: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-27 21:42:35.885611: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14202 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "W0627 21:42:35.886469 140100349421440 deprecation_wrapper.py:119] From ./train.py:92: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0627 21:42:46.730016 140100349421440 deprecation.py:323] From /content/gpt-2/src/sample.py:65: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "W0627 21:42:46.744240 140100349421440 deprecation.py:323] From /content/gpt-2/src/sample.py:16: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0627 21:42:46.745896 140100349421440 deprecation.py:323] From /content/gpt-2/src/sample.py:70: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.random.categorical` instead.\n",
            "W0627 21:42:46.755747 140100349421440 deprecation_wrapper.py:119] From ./train.py:121: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "W0627 21:43:02.062875 140100349421440 deprecation_wrapper.py:119] From ./train.py:144: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n",
            "W0627 21:43:02.066201 140100349421440 deprecation_wrapper.py:119] From ./train.py:147: The name tf.summary.merge is deprecated. Please use tf.compat.v1.summary.merge instead.\n",
            "\n",
            "W0627 21:43:02.067227 140100349421440 deprecation_wrapper.py:119] From ./train.py:149: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
            "\n",
            "W0627 21:43:02.068093 140100349421440 deprecation_wrapper.py:119] From ./train.py:152: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "Loading checkpoint models/345M/model.ckpt\n",
            "W0627 21:43:15.452598 140100349421440 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "Loading dataset...\n",
            "100% 1/1 [00:04<00:00,  4.27s/it]\n",
            "dataset has 608277 tokens\n",
            "Training...\n",
            "2019-06-27 21:43:33.025266: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n",
            "2019-06-27 21:43:33.777816: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "[1 | 13.46] loss=3.38 avg=3.38\n",
            "[2 | 15.08] loss=3.46 avg=3.42\n",
            "[3 | 16.71] loss=3.09 avg=3.31\n",
            "[4 | 18.34] loss=2.71 avg=3.16\n",
            "[5 | 19.97] loss=3.32 avg=3.19\n",
            "[6 | 21.62] loss=2.57 avg=3.08\n",
            "[7 | 23.28] loss=3.37 avg=3.13\n",
            "[8 | 24.96] loss=3.11 avg=3.12\n",
            "[9 | 26.64] loss=3.01 avg=3.11\n",
            "[10 | 28.31] loss=3.18 avg=3.12\n",
            "[11 | 30.01] loss=2.66 avg=3.07\n",
            "[12 | 31.71] loss=3.13 avg=3.08\n",
            "[13 | 33.42] loss=3.24 avg=3.09\n",
            "[14 | 35.14] loss=3.29 avg=3.11\n",
            "[15 | 36.85] loss=3.41 avg=3.13\n",
            "[16 | 38.59] loss=3.12 avg=3.13\n",
            "[17 | 40.33] loss=3.14 avg=3.13\n",
            "[18 | 42.06] loss=2.83 avg=3.11\n",
            "[19 | 43.80] loss=2.95 avg=3.10\n",
            "[20 | 45.53] loss=3.30 avg=3.11\n",
            "[21 | 47.27] loss=3.21 avg=3.12\n",
            "[22 | 49.00] loss=3.28 avg=3.13\n",
            "[23 | 50.70] loss=3.01 avg=3.12\n",
            "[24 | 52.40] loss=2.99 avg=3.11\n",
            "[25 | 54.09] loss=3.23 avg=3.12\n",
            "[26 | 55.77] loss=3.19 avg=3.12\n",
            "[27 | 57.46] loss=3.01 avg=3.12\n",
            "[28 | 59.13] loss=2.77 avg=3.10\n",
            "[29 | 60.80] loss=3.07 avg=3.10\n",
            "[30 | 62.47] loss=3.07 avg=3.10\n",
            "[31 | 64.13] loss=3.26 avg=3.11\n",
            "[32 | 65.79] loss=2.69 avg=3.09\n",
            "[33 | 67.44] loss=3.12 avg=3.09\n",
            "[34 | 69.10] loss=3.05 avg=3.09\n",
            "[35 | 70.75] loss=3.12 avg=3.09\n",
            "[36 | 72.39] loss=3.16 avg=3.09\n",
            "[37 | 74.04] loss=3.22 avg=3.10\n",
            "[38 | 75.68] loss=3.23 avg=3.10\n",
            "[39 | 77.32] loss=2.86 avg=3.09\n",
            "[40 | 78.95] loss=3.00 avg=3.09\n",
            "[41 | 80.59] loss=3.42 avg=3.10\n",
            "[42 | 82.22] loss=3.03 avg=3.10\n",
            "[43 | 83.85] loss=3.01 avg=3.10\n",
            "[44 | 85.50] loss=2.94 avg=3.09\n",
            "[45 | 87.15] loss=3.04 avg=3.09\n",
            "[46 | 88.78] loss=2.80 avg=3.08\n",
            "[47 | 90.41] loss=2.99 avg=3.08\n",
            "[48 | 92.04] loss=2.92 avg=3.08\n",
            "[49 | 93.68] loss=3.09 avg=3.08\n",
            "[50 | 95.33] loss=3.36 avg=3.08\n",
            "[51 | 96.96] loss=3.21 avg=3.09\n",
            "[52 | 98.60] loss=2.71 avg=3.08\n",
            "[53 | 100.23] loss=3.22 avg=3.08\n",
            "[54 | 101.87] loss=3.15 avg=3.08\n",
            "[55 | 103.52] loss=3.42 avg=3.09\n",
            "[56 | 105.16] loss=3.18 avg=3.09\n",
            "[57 | 106.82] loss=3.23 avg=3.10\n",
            "[58 | 108.46] loss=3.22 avg=3.10\n",
            "[59 | 110.11] loss=3.12 avg=3.10\n",
            "[60 | 111.77] loss=2.92 avg=3.10\n",
            "[61 | 113.42] loss=3.77 avg=3.11\n",
            "[62 | 115.07] loss=3.06 avg=3.11\n",
            "[63 | 116.73] loss=3.33 avg=3.11\n",
            "[64 | 118.39] loss=3.08 avg=3.11\n",
            "[65 | 120.05] loss=2.84 avg=3.11\n",
            "[66 | 121.71] loss=3.27 avg=3.11\n",
            "[67 | 123.37] loss=3.14 avg=3.11\n",
            "[68 | 125.04] loss=3.24 avg=3.11\n",
            "[69 | 126.71] loss=3.09 avg=3.11\n",
            "[70 | 128.38] loss=3.27 avg=3.12\n",
            "[71 | 130.04] loss=3.11 avg=3.12\n",
            "[72 | 131.70] loss=2.68 avg=3.11\n",
            "[73 | 133.37] loss=2.97 avg=3.10\n",
            "[74 | 135.04] loss=2.97 avg=3.10\n",
            "[75 | 136.70] loss=2.89 avg=3.10\n",
            "[76 | 138.37] loss=2.97 avg=3.10\n",
            "[77 | 140.04] loss=3.17 avg=3.10\n",
            "[78 | 141.70] loss=3.14 avg=3.10\n",
            "[79 | 143.36] loss=3.06 avg=3.10\n",
            "[80 | 145.02] loss=2.97 avg=3.09\n",
            "[81 | 146.68] loss=3.28 avg=3.10\n",
            "[82 | 148.34] loss=2.66 avg=3.09\n",
            "[83 | 150.00] loss=3.10 avg=3.09\n",
            "[84 | 151.66] loss=3.13 avg=3.09\n",
            "[85 | 153.31] loss=3.12 avg=3.09\n",
            "[86 | 154.97] loss=3.32 avg=3.10\n",
            "[87 | 156.62] loss=3.41 avg=3.10\n",
            "[88 | 158.28] loss=2.98 avg=3.10\n",
            "[89 | 159.94] loss=2.86 avg=3.10\n",
            "[90 | 161.59] loss=3.12 avg=3.10\n",
            "[91 | 163.27] loss=2.82 avg=3.09\n",
            "[92 | 164.92] loss=3.12 avg=3.09\n",
            "[93 | 166.58] loss=3.53 avg=3.10\n",
            "[94 | 168.24] loss=2.92 avg=3.10\n",
            "[95 | 169.90] loss=2.93 avg=3.09\n",
            "[96 | 171.58] loss=2.96 avg=3.09\n",
            "[97 | 173.23] loss=3.13 avg=3.09\n",
            "[98 | 174.88] loss=3.19 avg=3.09\n",
            "[99 | 176.54] loss=3.42 avg=3.10\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " him or her out of their comfort zone, into it.\n",
            "\n",
            "For example, I have a friend who once told me something very painful to this day. She told me about her husband's death. She wasn't sure what he wanted, she didn't know whether it was okay or not, but as a lifelong believer in monogamy (the idea that we should always be sexually active until marriage), she understood what my friend meant. She had never believed that, until recently (I can't remember what year, but I remember it was a recent month), but she then saw the words, and realized that this was the correct, rational response.\n",
            "\n",
            "It should be the first thing out of his mouth, and if it is, it should take away the pain. But of course, his mouth doesn't say so.\n",
            "\n",
            "I think the reason is simple. And I'm glad I don't have to go into details. The first reason is that if we live in such a world we are forced to think in terms of relationships. In the world I have lived in, our relationships were based on love and affection, not sex, which is a concept far removed from the ones we find in our society today. But even if we were to live in such a world again, then the fact is that so little has changed with regard to our society. So while sex is now regarded by some people as the most natural and intimate experience, it is the fact of the relationship that is viewed as the most important part of life and the thing most important to be shared. We would be well served to be honest with ourselves about the reason.\n",
            "\n",
            "The second reason is a bit more specific. In our romantic conception, a relationship is more than just a contract or a promise to share something and so we expect it to last, meaning that once a relationship is established, there are always things we would like to do differently or change. But in reality, it isn't our desires that change, it is our relationships. How much we desire a thing is also what it means to be in the right relationship. When you spend all your life doing things you hate doing or you resent doing, your desire will be the driving force behind your actions all your life. But once you've made the transition, your desire will not be strong enough to lead you to things you hate doing. Your desire will disappear once your body allows it, and that is what really makes you human, because once you want something as much as a person, your desires are always strong enough to make up for it.\n",
            "\n",
            "That brings me to the third and fourth reasons. To the extent that relationships are the only thing which matter, then the desire for them is what is important. But when you can choose to live your life, to live your life with the possibility of making a difference, if your desire for something does not trump your need for others, then you are not living your life at all. Or at least to live your life you have to find ways to accommodate what is lacking: what you want is only something that you find pleasurable; what you don't want is anything that you find dangerous or dangerous to you. In this sense you are living your life within your constraints on what you want instead of being free to do so any time you desire.\n",
            "\n",
            "One way to think about this is to consider how your desires have the power to set you apart from all other people. What if I want someone with whom I can have a romantic relationship? I don't necessarily want them. But it is always a possibility and I don't know when I will find one. I don't know how long it will take to find one; it is the expectation I have of it which makes me accept that it will not come. But what if something else comes? You have already started looking for a partner; what if you are wrong about it and are rejected or you find a new partner? What if you end up with a new partner again but only with someone who also likes you? Or maybe you find a new partner who is not the same person you love? Or if someone comes along, what happens? How do you adjust a life you have become accustomed to accepting as normal?\n",
            "\n",
            "I am not saying the future is not so bright for someone who chooses to live a life of what they want instead of what someone expects to happen. But I am saying that a life of what they want does not necessarily bring with it the same possibilities for you. Someone in whom you want a long life will tend to focus on things you hate that you can make up—things you are often not good at getting away from. These problems often lead people to look for ways to make this life work even when they know it is not going to work. You cannot be able to be an adult who does not want to be an adult.\n",
            "\n",
            "This is not to say that everything can be made to work, nor should we expect that life-hating things cannot be made to work. But it\n",
            "\n",
            "[100 | 204.07] loss=3.02 avg=3.10\n",
            "[101 | 205.72] loss=2.75 avg=3.09\n",
            "[102 | 207.39] loss=3.06 avg=3.09\n",
            "[103 | 209.06] loss=2.97 avg=3.09\n",
            "[104 | 210.73] loss=3.11 avg=3.09\n",
            "[105 | 212.40] loss=2.84 avg=3.09\n",
            "[106 | 214.07] loss=3.48 avg=3.09\n",
            "[107 | 215.74] loss=3.12 avg=3.09\n",
            "[108 | 217.41] loss=3.11 avg=3.09\n",
            "[109 | 219.08] loss=3.10 avg=3.09\n",
            "[110 | 220.75] loss=3.12 avg=3.09\n",
            "[111 | 222.43] loss=3.22 avg=3.10\n",
            "[112 | 224.10] loss=3.11 avg=3.10\n",
            "[113 | 225.77] loss=3.09 avg=3.10\n",
            "[114 | 227.44] loss=2.49 avg=3.09\n",
            "[115 | 229.10] loss=3.02 avg=3.09\n",
            "[116 | 230.78] loss=2.88 avg=3.08\n",
            "[117 | 232.45] loss=2.98 avg=3.08\n",
            "[118 | 234.14] loss=3.23 avg=3.08\n",
            "[119 | 235.81] loss=3.20 avg=3.08\n",
            "[120 | 237.49] loss=2.74 avg=3.08\n",
            "[121 | 239.16] loss=3.04 avg=3.08\n",
            "[122 | 240.83] loss=3.01 avg=3.08\n",
            "[123 | 242.51] loss=3.15 avg=3.08\n",
            "[124 | 244.18] loss=3.03 avg=3.08\n",
            "[125 | 245.85] loss=3.10 avg=3.08\n",
            "[126 | 247.54] loss=3.11 avg=3.08\n",
            "[127 | 249.22] loss=3.15 avg=3.08\n",
            "[128 | 250.89] loss=2.69 avg=3.08\n",
            "[129 | 252.56] loss=3.24 avg=3.08\n",
            "[130 | 254.25] loss=3.20 avg=3.08\n",
            "[131 | 255.93] loss=3.26 avg=3.08\n",
            "[132 | 257.60] loss=3.14 avg=3.08\n",
            "[133 | 259.27] loss=3.09 avg=3.08\n",
            "[134 | 260.95] loss=3.21 avg=3.08\n",
            "[135 | 262.63] loss=2.60 avg=3.08\n",
            "[136 | 264.30] loss=3.13 avg=3.08\n",
            "[137 | 265.97] loss=2.96 avg=3.08\n",
            "[138 | 267.63] loss=2.27 avg=3.07\n",
            "[139 | 269.31] loss=2.74 avg=3.06\n",
            "[140 | 270.98] loss=2.97 avg=3.06\n",
            "[141 | 272.65] loss=2.93 avg=3.06\n",
            "[142 | 274.32] loss=3.12 avg=3.06\n",
            "[143 | 275.99] loss=2.99 avg=3.06\n",
            "[144 | 277.67] loss=2.92 avg=3.06\n",
            "[145 | 279.35] loss=3.18 avg=3.06\n",
            "[146 | 281.02] loss=2.98 avg=3.06\n",
            "[147 | 282.69] loss=2.71 avg=3.05\n",
            "[148 | 284.36] loss=2.96 avg=3.05\n",
            "[149 | 286.03] loss=2.95 avg=3.05\n",
            "[150 | 287.70] loss=3.28 avg=3.05\n",
            "[151 | 289.35] loss=3.25 avg=3.06\n",
            "[152 | 291.01] loss=3.43 avg=3.06\n",
            "[153 | 292.69] loss=3.22 avg=3.06\n",
            "[154 | 294.35] loss=2.94 avg=3.06\n",
            "[155 | 296.03] loss=3.08 avg=3.06\n",
            "[156 | 297.70] loss=2.84 avg=3.06\n",
            "[157 | 299.37] loss=2.63 avg=3.05\n",
            "[158 | 301.04] loss=3.04 avg=3.05\n",
            "[159 | 302.71] loss=2.75 avg=3.05\n",
            "[160 | 304.38] loss=3.78 avg=3.06\n",
            "[161 | 306.05] loss=3.03 avg=3.06\n",
            "[162 | 307.70] loss=3.19 avg=3.06\n",
            "[163 | 309.38] loss=2.99 avg=3.06\n",
            "[164 | 311.05] loss=3.02 avg=3.06\n",
            "[165 | 312.71] loss=2.99 avg=3.06\n",
            "[166 | 314.38] loss=2.98 avg=3.06\n",
            "[167 | 316.05] loss=3.01 avg=3.06\n",
            "[168 | 317.71] loss=2.97 avg=3.05\n",
            "[169 | 319.38] loss=2.30 avg=3.05\n",
            "[170 | 321.05] loss=2.90 avg=3.04\n",
            "[171 | 322.72] loss=2.98 avg=3.04\n",
            "[172 | 324.39] loss=2.93 avg=3.04\n",
            "[173 | 326.06] loss=3.00 avg=3.04\n",
            "[174 | 327.73] loss=3.36 avg=3.04\n",
            "[175 | 329.39] loss=2.87 avg=3.04\n",
            "[176 | 331.05] loss=3.23 avg=3.05\n",
            "[177 | 332.72] loss=2.95 avg=3.04\n",
            "[178 | 334.39] loss=3.46 avg=3.05\n",
            "[179 | 336.06] loss=3.09 avg=3.05\n",
            "[180 | 337.73] loss=3.17 avg=3.05\n",
            "[181 | 339.40] loss=2.90 avg=3.05\n",
            "[182 | 341.07] loss=2.97 avg=3.05\n",
            "[183 | 342.74] loss=3.14 avg=3.05\n",
            "[184 | 344.39] loss=2.91 avg=3.05\n",
            "[185 | 346.06] loss=3.03 avg=3.05\n",
            "[186 | 347.72] loss=3.00 avg=3.05\n",
            "[187 | 349.40] loss=2.54 avg=3.04\n",
            "[188 | 351.07] loss=3.10 avg=3.04\n",
            "[189 | 352.74] loss=2.91 avg=3.04\n",
            "[190 | 354.40] loss=3.28 avg=3.04\n",
            "[191 | 356.06] loss=2.92 avg=3.04\n",
            "[192 | 357.73] loss=2.98 avg=3.04\n",
            "[193 | 359.40] loss=3.08 avg=3.04\n",
            "[194 | 361.05] loss=2.98 avg=3.04\n",
            "[195 | 362.71] loss=2.84 avg=3.04\n",
            "[196 | 364.38] loss=3.08 avg=3.04\n",
            "[197 | 366.04] loss=2.93 avg=3.04\n",
            "[198 | 367.71] loss=2.99 avg=3.04\n",
            "[199 | 369.38] loss=2.99 avg=3.04\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " not a very good use of time and that it can only be learned from experience, so he is better off trying to learn as much as he can. I'm not saying he should start learning any more than is necessary. I'm saying it matters less to me for him to learn that much more. I might even say that it does not matter at all.\n",
            "#\n",
            "I think this is also why I don't find much of any sense to this whole discussion: I'm not sure it's worth it, because I believe this is so much a way of talking about the human spirit that a more honest engagement would be more useful. (As far as I can tell, I'm actually more comfortable talking about this through the prism of my previous discussions with my sister and other acquaintances.)\n",
            "This is obviously more of a way of saying, though, that I am not entirely sure, and probably a little less certain, how the discussion here will turn out. \n",
            "- - - - - - - - -- \n",
            "I guess I should probably explain this. At its heart it is a question of what you define, or at least I would say defines what I define as human. My definition of human is quite different from your definition of human as much as that may be. \n",
            "If you define human as:\n",
            "a man who is not an idealized embodiment of the male spirit; a man who has not, through his actions, defined in a particular way the boundaries of his own identity \n",
            "if that definition were to be replaced by a different definition you would probably change it  \n",
            "So when you take this definition and apply it to my definition of human, what is the outcome? I should say this: in that definition human would be exactly what I would define human in the first place: an idealized version of men, men who have seen the world and come to accept themselves and want to be seen \n",
            "So in that definition I would have to say I am human and in many ways my being is not at all that of a man (this is certainly the point of the discussion!) But the thing that seems to bother me about your definition is the way it says that \"there is a human soul.\" You say that you define it as a human entity but you do not define the human soul that is. Or maybe the definition of soul is wrong. Or if it is wrong, maybe the definition is wrong in a way that makes it not only wrong but perhaps even immoral as well. \n",
            "\n",
            "I am not sure if I can make that clear here. Maybe it is a way of saying what you define a human soul as. Or maybe you are trying to say some other kind of answer. \n",
            "\n",
            "But it seems to me I should say something more concrete. I think that when we define a human soul we have no idea what sort of human soul that we define. And the people who defined man in the first place, many of whom were not men (including me), thought those definitions were better than ours to define a human soul. They define it as a human entity but they do not define the human soul that is. So when you say you define human soul, you are implicitly defining the human soul as it truly is and you are saying the human soul is a bad definition. \n",
            "\n",
            "- - - - - - - - - - --\n",
            "So what can be done? First, I hope what I am trying to suggest is, with my sister, that we not try to define this definition by our own standards but rather look to your definition as a good starting point. We should then come back to our idea that the human soul is \n",
            "\n",
            "a man who accepts himself and wants to be seen as a man, an entity that lives in being who wants recognition and validation\n",
            "\n",
            "- - - - - - - - - --\n",
            "<|endoftext|>Monsanto, in one of its more audacious strategies of the century, has made an argument that the genetic engineering of corn would have never occurred in the world and not be able to explain much of the genetic variation that is characteristic of crops that can be grown in many climates at any one time.\n",
            "\n",
            "Monsanto claims it has a statistical probability of 1 in 20 billion. This is a statistic which would mean that if 100 generations of corn were created today, each generation would not be able to produce corn in any particular environment (or perhaps, the corn could in fact be considered unique).\n",
            "\n",
            "I agree with Monsanto that it would likely have never occurred had this theory come true. But I am even less sure that this conclusion cannot be true: the amount of genetic variations that may have been created by industrial farming in the past seems to be greater now than in the past.\n",
            "\n",
            "I will begin by drawing a simple chart, one I think might be a good starting point for answering this question.\n",
            "\n",
            "I'll start with these two numbers:\n",
            "\n",
            "Monsanto's Number (or, the statistical likelihood that they would have been generated)\n",
            "\n",
            "\n",
            "\n",
            "[200 | 393.80] loss=2.91 avg=3.03\n",
            "[201 | 395.47] loss=3.00 avg=3.03\n",
            "[202 | 397.12] loss=3.08 avg=3.03\n",
            "[203 | 398.78] loss=2.99 avg=3.03\n",
            "[204 | 400.44] loss=3.05 avg=3.03\n",
            "[205 | 402.11] loss=3.07 avg=3.03\n",
            "[206 | 403.78] loss=2.80 avg=3.03\n",
            "[207 | 405.44] loss=3.01 avg=3.03\n",
            "[208 | 407.11] loss=3.18 avg=3.03\n",
            "[209 | 408.77] loss=2.81 avg=3.03\n",
            "[210 | 410.44] loss=3.12 avg=3.03\n",
            "[211 | 412.11] loss=2.84 avg=3.03\n",
            "[212 | 413.77] loss=2.98 avg=3.03\n",
            "[213 | 415.44] loss=3.16 avg=3.03\n",
            "[214 | 417.11] loss=2.95 avg=3.03\n",
            "[215 | 418.78] loss=3.12 avg=3.03\n",
            "[216 | 420.45] loss=3.10 avg=3.03\n",
            "[217 | 422.12] loss=2.95 avg=3.03\n",
            "[218 | 423.79] loss=3.18 avg=3.03\n",
            "[219 | 425.45] loss=3.04 avg=3.03\n",
            "[220 | 427.11] loss=2.83 avg=3.03\n",
            "[221 | 428.78] loss=2.84 avg=3.03\n",
            "[222 | 430.45] loss=3.01 avg=3.03\n",
            "[223 | 432.12] loss=2.59 avg=3.02\n",
            "[224 | 433.79] loss=2.90 avg=3.02\n",
            "[225 | 435.46] loss=2.99 avg=3.02\n",
            "[226 | 437.13] loss=2.87 avg=3.02\n",
            "[227 | 438.80] loss=3.27 avg=3.02\n",
            "[228 | 440.46] loss=3.08 avg=3.02\n",
            "[229 | 442.13] loss=2.94 avg=3.02\n",
            "[230 | 443.80] loss=2.82 avg=3.02\n",
            "[231 | 445.46] loss=3.13 avg=3.02\n",
            "[232 | 447.13] loss=3.17 avg=3.02\n",
            "[233 | 448.81] loss=2.94 avg=3.02\n",
            "[234 | 450.48] loss=3.21 avg=3.02\n",
            "[235 | 452.15] loss=3.12 avg=3.03\n",
            "[236 | 453.81] loss=3.15 avg=3.03\n",
            "[237 | 455.48] loss=3.12 avg=3.03\n",
            "[238 | 457.14] loss=2.79 avg=3.02\n",
            "[239 | 458.81] loss=3.37 avg=3.03\n",
            "[240 | 460.48] loss=3.14 avg=3.03\n",
            "[241 | 462.14] loss=3.01 avg=3.03\n",
            "[242 | 463.81] loss=3.01 avg=3.03\n",
            "[243 | 465.48] loss=2.71 avg=3.03\n",
            "[244 | 467.15] loss=3.29 avg=3.03\n",
            "[245 | 468.82] loss=2.54 avg=3.02\n",
            "[246 | 470.49] loss=3.21 avg=3.03\n",
            "[247 | 472.15] loss=3.02 avg=3.03\n",
            "[248 | 473.82] loss=3.14 avg=3.03\n",
            "[249 | 475.48] loss=3.19 avg=3.03\n",
            "[250 | 477.15] loss=3.09 avg=3.03\n",
            "[251 | 478.82] loss=3.09 avg=3.03\n",
            "[252 | 480.49] loss=3.40 avg=3.03\n",
            "[253 | 482.16] loss=3.04 avg=3.03\n",
            "[254 | 483.81] loss=2.97 avg=3.03\n",
            "[255 | 485.49] loss=2.95 avg=3.03\n",
            "[256 | 487.16] loss=3.23 avg=3.03\n",
            "[257 | 488.83] loss=3.05 avg=3.03\n",
            "[258 | 490.50] loss=3.02 avg=3.03\n",
            "[259 | 492.17] loss=2.86 avg=3.03\n",
            "[260 | 493.84] loss=2.95 avg=3.03\n",
            "[261 | 495.50] loss=2.91 avg=3.03\n",
            "[262 | 497.17] loss=3.08 avg=3.03\n",
            "[263 | 498.84] loss=3.00 avg=3.03\n",
            "[264 | 500.51] loss=2.97 avg=3.03\n",
            "[265 | 502.18] loss=2.95 avg=3.03\n",
            "[266 | 503.85] loss=2.71 avg=3.03\n",
            "[267 | 505.52] loss=2.98 avg=3.03\n",
            "[268 | 507.19] loss=2.89 avg=3.02\n",
            "[269 | 508.86] loss=2.93 avg=3.02\n",
            "[270 | 510.52] loss=2.91 avg=3.02\n",
            "[271 | 512.20] loss=3.39 avg=3.03\n",
            "[272 | 513.86] loss=3.00 avg=3.03\n",
            "[273 | 515.52] loss=3.56 avg=3.03\n",
            "[274 | 517.19] loss=3.26 avg=3.03\n",
            "[275 | 518.84] loss=2.95 avg=3.03\n",
            "[276 | 520.52] loss=2.97 avg=3.03\n",
            "[277 | 522.19] loss=2.90 avg=3.03\n",
            "[278 | 523.86] loss=3.28 avg=3.03\n",
            "[279 | 525.53] loss=2.86 avg=3.03\n",
            "[280 | 527.20] loss=2.82 avg=3.03\n",
            "[281 | 528.88] loss=3.08 avg=3.03\n",
            "[282 | 530.55] loss=3.05 avg=3.03\n",
            "[283 | 532.21] loss=3.06 avg=3.03\n",
            "[284 | 533.88] loss=3.00 avg=3.03\n",
            "[285 | 535.56] loss=3.32 avg=3.03\n",
            "[286 | 537.23] loss=2.91 avg=3.03\n",
            "[287 | 538.90] loss=2.72 avg=3.03\n",
            "[288 | 540.58] loss=3.02 avg=3.03\n",
            "[289 | 542.26] loss=3.40 avg=3.03\n",
            "[290 | 543.93] loss=3.12 avg=3.03\n",
            "[291 | 545.60] loss=3.02 avg=3.03\n",
            "[292 | 547.27] loss=2.96 avg=3.03\n",
            "[293 | 548.95] loss=3.11 avg=3.03\n",
            "[294 | 550.62] loss=3.14 avg=3.03\n",
            "[295 | 552.29] loss=3.05 avg=3.03\n",
            "[296 | 553.96] loss=2.91 avg=3.03\n",
            "[297 | 555.63] loss=2.99 avg=3.03\n",
            "[298 | 557.31] loss=2.99 avg=3.03\n",
            "[299 | 558.98] loss=2.86 avg=3.03\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " was at the time.\"\n",
            "\n",
            "He was the one who was pushing to make a film on the subject that did not directly address the religious aspects of it. He said he would not be doing it because the film was \"too religious,\" which seemed unfair given the fact these films would probably be much less popular now. But he said he thought that the film could be made at least partly within the boundaries of traditional religious views. I did not like the way he phrased this, partly because as an atheist and a man working in a small, conservative church, I knew I had to be careful about how I described my concerns. But it wasn't so much that I did not see why he should not see the importance of the film, it was that, at the time, he did not see it.\n",
            "\n",
            "In fact, he was not even sure that it was possible. He said he had made a couple of requests to the school and he thought they would have made him reconsider, and he did not really want to do the film. But he said he was hoping that when the school made the film, he could find someone who was in a different situation. I think if he wrote back to me and said \"I hope it's not too religious,\" I would probably be more receptive to doing it, but he was not sure it would happen. But when I told him the same thing, he said he might want to do it because he was now aware that he could make money from it. He said this, and I have to admit the details were not very reassuring for me. He said he was not going to put the money toward a movie, and this was kind of a weird thing to say. I know, I said, I told him, and I mean really he said, I didn't say no to a movie. But at the time, I believed him.\n",
            "\n",
            "\n",
            "There are things you should be sure you are in touch with, and they have specific meaning. For years I was very open about how often I felt I could no longer be friends with people, or about having friends I could never talk to and that I could never see again, and this fact had a deep psychological implication. As I moved through college, I felt like the problem grew further out, and I began to feel that there needed to be some sort of solution. And so the only way I could do this (and I still believe this) was for me to write a self-critical piece about being unable to form relationships and to explore my sexuality as a means of figuring out what was important to me. I never did write it. I never wrote it because for the most part, I did not want to write it. It felt like the logical last step in a process that was otherwise in some sense inevitable.\n",
            "\n",
            "At any rate, we talked a lot about the film, and while I never wrote it myself there are a few things I can say that are still accurate. I wrote up the plot, some things I think could be added, or even modified. My opinion is that if I had written the piece myself I would have been more precise about my intentions, but I guess I wouldn't have written at all. But I also think for some of the reasons above that even if I did write the piece, the resulting manuscript would probably not have been as good as I expected because it was written, in part, in response that I never had a chance to do what I wanted to do with the piece. (Also, it is very difficult to write self-critical pieces, they are often filled with all kinds of thoughts and emotions which one is hard pressed to let out.)\n",
            "\n",
            "I guess you could say it is too early to say whether the film was ever produced, but I believe I think I had at least a tentative idea of what it was about. And what I also believe, and what I have no reason to believe is a fact, is that, at that time, neither I nor anyone else in our community would have seen The End of Faith as an essential film in the conversation of religion. But the question remains whether these conversations can ever get to a real conversation in the first place (not that that is necessarily a bad thing).\n",
            "\n",
            "One thing I do know is that this story should be understood in that the question being asked is not whether or not the film was never made, but whether one could have made it. So although the question is clearly something I am asking, I hope it is still relevant to anyone writing about a religious film.<|endoftext|>The following chart shows how many times the average player on the Lions team has appeared on the field each season (in descending order of total career games seen in the NFL)\n",
            "\n",
            "Year Team Team Season Position Plays in 2016 2014 49ers Rams WR Jeremy Kerley 3 times 2013 Seahawks Lions TE Taylor Mays 2 times 2012 49ers Cardinals WR Amari Cooper 1 time 2011 Vikings Vikings TE Matthew Stafford 1 time 2010 Giants Packers WR Roddy White 1 times 2009 Eagles Eagles Redskins WR De\n",
            "\n",
            "[300 | 583.49] loss=2.96 avg=3.03\n",
            "[301 | 585.15] loss=2.98 avg=3.03\n",
            "[302 | 586.81] loss=3.30 avg=3.03\n",
            "[303 | 588.47] loss=3.21 avg=3.03\n",
            "[304 | 590.14] loss=2.94 avg=3.03\n",
            "[305 | 591.79] loss=2.91 avg=3.03\n",
            "[306 | 593.45] loss=2.93 avg=3.03\n",
            "[307 | 595.10] loss=2.92 avg=3.03\n",
            "[308 | 596.76] loss=2.98 avg=3.03\n",
            "[309 | 598.43] loss=2.83 avg=3.03\n",
            "[310 | 600.09] loss=2.75 avg=3.02\n",
            "[311 | 601.75] loss=3.23 avg=3.03\n",
            "[312 | 603.41] loss=2.39 avg=3.02\n",
            "[313 | 605.08] loss=2.69 avg=3.02\n",
            "[314 | 606.74] loss=3.13 avg=3.02\n",
            "[315 | 608.41] loss=2.86 avg=3.02\n",
            "[316 | 610.08] loss=2.75 avg=3.01\n",
            "[317 | 611.74] loss=3.22 avg=3.01\n",
            "[318 | 613.41] loss=3.00 avg=3.01\n",
            "[319 | 615.08] loss=2.98 avg=3.01\n",
            "[320 | 616.75] loss=2.93 avg=3.01\n",
            "[321 | 618.41] loss=2.66 avg=3.01\n",
            "[322 | 620.08] loss=2.92 avg=3.01\n",
            "[323 | 621.75] loss=3.15 avg=3.01\n",
            "[324 | 623.42] loss=2.81 avg=3.01\n",
            "[325 | 625.09] loss=2.91 avg=3.01\n",
            "[326 | 626.76] loss=2.51 avg=3.00\n",
            "[327 | 628.43] loss=2.29 avg=2.99\n",
            "[328 | 630.10] loss=2.96 avg=2.99\n",
            "[329 | 631.77] loss=2.53 avg=2.99\n",
            "[330 | 633.43] loss=3.15 avg=2.99\n",
            "[331 | 635.12] loss=3.12 avg=2.99\n",
            "[332 | 636.80] loss=2.34 avg=2.99\n",
            "[333 | 638.47] loss=2.85 avg=2.98\n",
            "[334 | 640.15] loss=2.98 avg=2.98\n",
            "[335 | 641.83] loss=2.82 avg=2.98\n",
            "[336 | 643.52] loss=3.15 avg=2.98\n",
            "[337 | 645.19] loss=3.21 avg=2.99\n",
            "[338 | 646.86] loss=3.16 avg=2.99\n",
            "[339 | 648.54] loss=2.72 avg=2.99\n",
            "[340 | 650.23] loss=3.49 avg=2.99\n",
            "[341 | 651.92] loss=3.13 avg=2.99\n",
            "[342 | 653.59] loss=2.96 avg=2.99\n",
            "[343 | 655.27] loss=2.71 avg=2.99\n",
            "[344 | 656.94] loss=3.12 avg=2.99\n",
            "[345 | 658.62] loss=2.63 avg=2.99\n",
            "[346 | 660.28] loss=3.13 avg=2.99\n",
            "[347 | 661.96] loss=3.18 avg=2.99\n",
            "[348 | 663.65] loss=3.01 avg=2.99\n",
            "[349 | 665.32] loss=3.16 avg=2.99\n",
            "[350 | 666.98] loss=3.17 avg=2.99\n",
            "[351 | 668.66] loss=3.16 avg=3.00\n",
            "[352 | 670.33] loss=3.05 avg=3.00\n",
            "[353 | 672.00] loss=2.74 avg=2.99\n",
            "[354 | 673.67] loss=2.69 avg=2.99\n",
            "[355 | 675.34] loss=3.03 avg=2.99\n",
            "[356 | 677.02] loss=2.93 avg=2.99\n",
            "[357 | 678.69] loss=3.02 avg=2.99\n",
            "[358 | 680.37] loss=2.98 avg=2.99\n",
            "[359 | 682.04] loss=3.18 avg=2.99\n",
            "[360 | 683.72] loss=2.77 avg=2.99\n",
            "[361 | 685.39] loss=3.06 avg=2.99\n",
            "[362 | 687.06] loss=2.94 avg=2.99\n",
            "[363 | 688.73] loss=2.86 avg=2.99\n",
            "[364 | 690.40] loss=2.73 avg=2.99\n",
            "[365 | 692.07] loss=2.77 avg=2.98\n",
            "[366 | 693.73] loss=2.68 avg=2.98\n",
            "[367 | 695.41] loss=2.99 avg=2.98\n",
            "[368 | 697.07] loss=2.57 avg=2.98\n",
            "[369 | 698.74] loss=2.83 avg=2.98\n",
            "[370 | 700.41] loss=2.46 avg=2.97\n",
            "[371 | 702.07] loss=2.92 avg=2.97\n",
            "[372 | 703.75] loss=3.07 avg=2.97\n",
            "[373 | 705.42] loss=2.04 avg=2.96\n",
            "[374 | 707.09] loss=2.85 avg=2.96\n",
            "[375 | 708.76] loss=3.09 avg=2.96\n",
            "[376 | 710.43] loss=2.84 avg=2.96\n",
            "[377 | 712.11] loss=3.07 avg=2.96\n",
            "[378 | 713.78] loss=2.97 avg=2.96\n",
            "[379 | 715.45] loss=2.67 avg=2.96\n",
            "[380 | 717.13] loss=3.01 avg=2.96\n",
            "[381 | 718.80] loss=3.31 avg=2.96\n",
            "[382 | 720.46] loss=2.88 avg=2.96\n",
            "[383 | 722.13] loss=2.51 avg=2.96\n",
            "[384 | 723.80] loss=2.89 avg=2.96\n",
            "[385 | 725.46] loss=2.88 avg=2.96\n",
            "[386 | 727.12] loss=2.85 avg=2.95\n",
            "[387 | 728.79] loss=2.98 avg=2.95\n",
            "[388 | 730.47] loss=3.03 avg=2.96\n",
            "[389 | 732.13] loss=3.14 avg=2.96\n",
            "[390 | 733.80] loss=2.68 avg=2.95\n",
            "[391 | 735.47] loss=3.01 avg=2.95\n",
            "[392 | 737.14] loss=3.22 avg=2.96\n",
            "[393 | 738.81] loss=3.08 avg=2.96\n",
            "[394 | 740.48] loss=3.03 avg=2.96\n",
            "[395 | 742.15] loss=3.13 avg=2.96\n",
            "[396 | 743.82] loss=2.84 avg=2.96\n",
            "[397 | 745.49] loss=2.54 avg=2.96\n",
            "[398 | 747.16] loss=3.17 avg=2.96\n",
            "[399 | 748.82] loss=2.56 avg=2.95\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "iwis is really just telling you that when you eat a food that you enjoy, it can be made to taste terrible.   It isn't true. The more I think about the idea that there is something wrong with what one eats, and that being able to taste things is the real reason it is horrible, the more I want to eat the food again, but I cannot.\n",
            "[//]\n",
            "\n",
            "Wednesday February 14, 2018 — 10:59 AM\n",
            "It was the first time since I was 16 that I felt I truly wanted someone or something to like me and to be part of me again, and that I could live this new life as a woman. \n",
            "\n",
            "It was the end of my sophomore term at Rutgers. As my parents had recently moved back to America from England, I had a boyfriend then. While my mother had been a nurse in a nursing home in Houston, she and I moved to Annapolis, Maryland where I was now finishing my BA and began law school. \n",
            "\n",
            "When I arrived in Annapolis, my family was working at the old J&J supermarket. My parents and I went to a local bar and then to the local tavern, and it was only at the bar where the conversation was mostly about a book I had on the list of course and how good of a date it was to meet the man with whom I was not yet involved. I also had my first date with this guy but of course after dinner, we went to my room and slept on the floor. \n",
            "\n",
            "I do not remember how I met someone to be with. Of course, we both had friends before the breakup, and I never felt that the experience was in any way connected to the breakup. But I did experience the feelings of loss at the end of the night and an intense desire to go back to our previous relationship, but of course, I did not go to her room to look up a note before doing so. I am not sure how I came to take this second date, but I do remember me walking towards the door, and asking my boyfriend to walk with me. \n",
            "\n",
            "I am trying to be as candid as I can, I don't know where all my pain is coming from, and if I were to give my partner any hints, it would be the fact that I can't find the same level of happiness in these three weeks I have been together. I guess there is something deeper that is at work here, and I don't want to find out what that is yet.\n",
            "\n",
            "I think when I was sixteen, I felt my body finally accepting me as female. So I had an idea about what a female body should feel like. I thought a female body should feel good but should not feel shameful due to my body being new from birth or because they were unfamiliar to me. \n",
            "\n",
            "When one of us was young a part of my body was a shame and was used by me to keep us safe and separate from each other. However, as one grows old, this part of my body is a part of you and you no longer need it. You used it for years to guard and protect you and now as you grow older, it no longer holds you and it does not protect you. \n",
            "\n",
            "If I was sixteen now how would I feel, confident and in control over my life? I was nineteen and I did not want to grow old. I tried, but was not able to achieve the same amount of security. So as I got older it was still not my part of my body but it was no longer my safe zone, hence the need to take this second date which was now nothing like the first date. I wanted to be with him, but I was not sure where to begin, and the idea of coming home from that trip with two empty suitcases, and being unable to feel secure enough to trust anything I ate or drank, was not appealing at all. When we were on the date, although I was with him, I felt no closeness, as if my self and mine were two separate people. I felt comfortable, like I had taken care of myself and that it was a matter of my own accord who cared what happened. \n",
            "\n",
            "In the days that followed all my fears about the first dating trip vanished. The idea of coming home for a night at a bar was never as unthinkable as it was the day after. But I can not say I was entirely reassured by this. Instead, it left me feeling like something else had occurred. The lack of a safe place in which the pair could live. The discomfort of being in a relationship with a body I did not want to see. I can recall a time when I was on a date and I would talk about a movie I wanted to see and would have dinner with him before we left the bar. During this meal (I believe the dinner was about six days after the first date) we were eating dinner at his place in Annapolis, but the conversation was not about what I was reading and\n",
            "\n",
            "[400 | 773.23] loss=2.82 avg=2.95\n",
            "[401 | 774.90] loss=2.67 avg=2.95\n",
            "[402 | 776.56] loss=3.08 avg=2.95\n",
            "[403 | 778.21] loss=2.92 avg=2.95\n",
            "[404 | 779.88] loss=2.87 avg=2.95\n",
            "[405 | 781.55] loss=2.70 avg=2.95\n",
            "[406 | 783.22] loss=3.02 avg=2.95\n",
            "[407 | 784.88] loss=2.86 avg=2.95\n",
            "[408 | 786.55] loss=2.88 avg=2.95\n",
            "[409 | 788.22] loss=2.76 avg=2.94\n",
            "[410 | 789.88] loss=2.82 avg=2.94\n",
            "[411 | 791.56] loss=3.09 avg=2.94\n",
            "[412 | 793.23] loss=3.01 avg=2.95\n",
            "[413 | 794.89] loss=3.11 avg=2.95\n",
            "[414 | 796.56] loss=2.86 avg=2.95\n",
            "[415 | 798.23] loss=2.71 avg=2.94\n",
            "[416 | 799.89] loss=3.04 avg=2.94\n",
            "[417 | 801.56] loss=2.55 avg=2.94\n",
            "[418 | 803.22] loss=2.85 avg=2.94\n",
            "[419 | 804.89] loss=2.85 avg=2.94\n",
            "[420 | 806.56] loss=2.94 avg=2.94\n",
            "[421 | 808.23] loss=2.68 avg=2.94\n",
            "[422 | 809.88] loss=2.77 avg=2.93\n",
            "[423 | 811.55] loss=3.00 avg=2.94\n",
            "[424 | 813.22] loss=2.76 avg=2.93\n",
            "[425 | 814.88] loss=2.99 avg=2.93\n",
            "[426 | 816.55] loss=2.91 avg=2.93\n",
            "[427 | 818.22] loss=2.77 avg=2.93\n",
            "[428 | 819.88] loss=2.63 avg=2.93\n",
            "[429 | 821.53] loss=3.06 avg=2.93\n",
            "[430 | 823.19] loss=2.71 avg=2.93\n",
            "[431 | 824.86] loss=2.91 avg=2.93\n",
            "[432 | 826.52] loss=2.93 avg=2.93\n",
            "[433 | 828.19] loss=2.81 avg=2.93\n",
            "[434 | 829.87] loss=2.65 avg=2.92\n",
            "[435 | 831.53] loss=2.93 avg=2.92\n",
            "[436 | 833.19] loss=2.85 avg=2.92\n",
            "[437 | 834.86] loss=2.98 avg=2.92\n",
            "[438 | 836.53] loss=3.30 avg=2.93\n",
            "[439 | 838.20] loss=2.85 avg=2.93\n",
            "[440 | 839.87] loss=2.91 avg=2.93\n",
            "[441 | 841.54] loss=2.95 avg=2.93\n",
            "[442 | 843.21] loss=2.57 avg=2.92\n",
            "[443 | 844.88] loss=2.51 avg=2.92\n",
            "[444 | 846.55] loss=2.98 avg=2.92\n",
            "[445 | 848.22] loss=3.19 avg=2.92\n",
            "[446 | 849.89] loss=3.52 avg=2.93\n",
            "[447 | 851.56] loss=2.99 avg=2.93\n",
            "[448 | 853.22] loss=2.78 avg=2.93\n",
            "[449 | 854.89] loss=2.63 avg=2.92\n",
            "[450 | 856.56] loss=2.75 avg=2.92\n",
            "[451 | 858.24] loss=3.22 avg=2.93\n",
            "[452 | 859.90] loss=3.38 avg=2.93\n",
            "[453 | 861.57] loss=2.82 avg=2.93\n",
            "[454 | 863.24] loss=2.48 avg=2.92\n",
            "[455 | 864.91] loss=2.89 avg=2.92\n",
            "[456 | 866.58] loss=3.15 avg=2.93\n",
            "[457 | 868.25] loss=2.87 avg=2.93\n",
            "[458 | 869.92] loss=2.71 avg=2.92\n",
            "[459 | 871.59] loss=3.38 avg=2.93\n",
            "[460 | 873.26] loss=2.96 avg=2.93\n",
            "[461 | 874.93] loss=2.93 avg=2.93\n",
            "[462 | 876.60] loss=3.09 avg=2.93\n",
            "[463 | 878.27] loss=3.18 avg=2.93\n",
            "[464 | 879.94] loss=3.06 avg=2.93\n",
            "[465 | 881.59] loss=2.97 avg=2.93\n",
            "[466 | 883.27] loss=2.93 avg=2.93\n",
            "[467 | 884.93] loss=2.55 avg=2.93\n",
            "[468 | 886.59] loss=2.90 avg=2.93\n",
            "[469 | 888.27] loss=2.98 avg=2.93\n",
            "[470 | 889.94] loss=2.45 avg=2.93\n",
            "[471 | 891.60] loss=2.87 avg=2.93\n",
            "[472 | 893.26] loss=2.92 avg=2.93\n",
            "[473 | 894.94] loss=3.02 avg=2.93\n",
            "[474 | 896.62] loss=3.01 avg=2.93\n",
            "[475 | 898.28] loss=2.87 avg=2.93\n",
            "[476 | 899.96] loss=2.52 avg=2.92\n",
            "[477 | 901.64] loss=3.21 avg=2.93\n",
            "[478 | 903.31] loss=2.96 avg=2.93\n",
            "[479 | 904.98] loss=3.31 avg=2.93\n",
            "[480 | 906.65] loss=2.57 avg=2.93\n",
            "[481 | 908.32] loss=3.31 avg=2.93\n",
            "[482 | 909.99] loss=3.05 avg=2.93\n",
            "[483 | 911.66] loss=3.16 avg=2.93\n",
            "[484 | 913.33] loss=3.06 avg=2.93\n",
            "[485 | 915.01] loss=2.70 avg=2.93\n",
            "[486 | 916.69] loss=2.94 avg=2.93\n",
            "[487 | 918.37] loss=2.96 avg=2.93\n",
            "[488 | 920.04] loss=2.69 avg=2.93\n",
            "[489 | 921.71] loss=3.01 avg=2.93\n",
            "[490 | 923.39] loss=3.04 avg=2.93\n",
            "[491 | 925.06] loss=2.82 avg=2.93\n",
            "[492 | 926.74] loss=2.82 avg=2.93\n",
            "[493 | 928.41] loss=2.68 avg=2.93\n",
            "[494 | 930.09] loss=2.87 avg=2.93\n",
            "[495 | 931.75] loss=2.76 avg=2.93\n",
            "[496 | 933.43] loss=2.99 avg=2.93\n",
            "[497 | 935.10] loss=2.83 avg=2.92\n",
            "[498 | 936.77] loss=2.92 avg=2.92\n",
            "[499 | 938.44] loss=2.86 avg=2.92\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "Pro’s (or rather “his”) ideas and ideas of the rest of the students in terms of the truth claims that they could make. He said these “truth claims” were in the nature of questions to be answered in an attempt to understand how the world was constructed.  \n",
            "\n",
            "In retrospect, the two most telling things about what I’m remembering at this time is the events in “The Fall”. At the events that took place this semester I was trying to understand the idea that the world could be said to be constructed from a set of statements. In the course that I spent in “The Fall” it was fairly clear to me that if one could establish a structure of the world (a set of principles for that structure) in one’s head and then try to make this structure work in reality one would find that when people constructed a building they could not figure out where the foundation came from, how a room was lit, how a table was set up, and that the most common problem people have with such structure is how it is maintained. \n",
            "\n",
            "That is what took place on my first day of teaching. For the first time in years, I felt out of place among an upper-level class of students. The classes I went to were usually small, not the kind that would allow the students I worked with to have a comfortable and enjoyable learning environment. A lot of the younger students didn’t want to work with me, even though they felt I didn’t need to do more than what I did, and sometimes when I was doing something I was often unable to do it. \n",
            "\n",
            "On that last point about the students in the classes I was trying to learn and not working with, I would often say to myself there was something very wrong about all the students of these classes. Because for a few hours a day, I kept working on the work I was already in and even after a week of this, and days of the same work, I never was able to finish it. \n",
            "\n",
            "The work that I was doing during that time, had to do with the basic concepts (in this case, rules, methods, and concepts) of mathematics. I always started with a theory of an idea. I would set out a problem in the form of an example of the problem (which had to be an interesting one) and after I developed this example or a theory of the problem and some of the tools for making an interesting example, then I would take the framework of the problem (such as a diagram (which should be clear enough to understand the concept) or some sort of logical proof of the problem) and then develop it in a way that made it interesting, perhaps even surprising and interesting but not so good that I had to completely revisit the problem or try to answer it in some way else. \n",
            "\n",
            "In a way, I had to start with a framework in order to develop a solution. When you get really in the weeds, the task becomes to develop a problem from scratch. In a way, this was what I’m working towards today. I have to start with a basic understanding of what the basic problem is and I want to develop one or more tools to help me understand this. \n",
            "\n",
            "The work was not entirely finished on the first day since a lot of the first problems were so bad that it took multiple days to complete and that I was not sure I needed to try harder. The first thing I had to do is try to find some new problems to work on. \n",
            "\n",
            "This was something that I did on the first day in a new environment, with different problems that I had never worked through before which made me feel strange at first but I then realized was really important. \n",
            "\n",
            "When I started working through all the problems that I was trying to find, in retrospect the first task was to find the basic problem that was the first step of getting started. This problem seemed like an easy one, with some logical statements and a simple structure. But it proved to be too complicated for my interests (which I will be discussing momentarily) so I ended up doing all of the problems in order instead. This work was done on two different days; in one day I was working on the first problem from the main work, and in the other day I was working on a second (and unrelated) problem that had to do with the \"rules to do things\" for building a building and the way in which the building needed to be lit. \n",
            "\n",
            "\n",
            "The day after I began doing the work, I had to get in some more work. \n",
            "\n",
            "\n",
            "\n",
            "The first work I had to complete was the construction of the building. I had a few things I wanted to work on and I wanted to start early in this task because I could do it later if I wanted to. A day after starting work on building, I made a few attempts to make some ideas about what the building should look\n",
            "\n",
            "[500 | 962.84] loss=2.84 avg=2.92\n",
            "[501 | 964.51] loss=3.11 avg=2.93\n",
            "[502 | 966.16] loss=2.94 avg=2.93\n",
            "[503 | 967.83] loss=2.99 avg=2.93\n",
            "[504 | 969.49] loss=3.00 avg=2.93\n",
            "[505 | 971.16] loss=2.75 avg=2.93\n",
            "[506 | 972.82] loss=3.01 avg=2.93\n",
            "[507 | 974.49] loss=3.04 avg=2.93\n",
            "[508 | 976.15] loss=2.83 avg=2.93\n",
            "[509 | 977.81] loss=2.68 avg=2.92\n",
            "[510 | 979.48] loss=3.09 avg=2.93\n",
            "[511 | 981.14] loss=2.81 avg=2.92\n",
            "[512 | 982.80] loss=3.01 avg=2.92\n",
            "[513 | 984.46] loss=3.12 avg=2.93\n",
            "[514 | 986.13] loss=2.81 avg=2.93\n",
            "[515 | 987.78] loss=2.67 avg=2.92\n",
            "[516 | 989.43] loss=2.54 avg=2.92\n",
            "[517 | 991.10] loss=3.00 avg=2.92\n",
            "[518 | 992.77] loss=2.88 avg=2.92\n",
            "[519 | 994.43] loss=2.76 avg=2.92\n",
            "[520 | 996.10] loss=3.05 avg=2.92\n",
            "[521 | 997.76] loss=3.00 avg=2.92\n",
            "[522 | 999.42] loss=2.98 avg=2.92\n",
            "[523 | 1001.09] loss=3.15 avg=2.92\n",
            "[524 | 1002.75] loss=2.36 avg=2.92\n",
            "[525 | 1004.40] loss=2.38 avg=2.91\n",
            "[526 | 1006.07] loss=2.61 avg=2.91\n",
            "[527 | 1007.73] loss=2.76 avg=2.91\n",
            "[528 | 1009.40] loss=2.94 avg=2.91\n",
            "[529 | 1011.07] loss=2.64 avg=2.91\n",
            "[530 | 1012.74] loss=2.75 avg=2.90\n",
            "[531 | 1014.41] loss=3.16 avg=2.91\n",
            "[532 | 1016.09] loss=2.97 avg=2.91\n",
            "[533 | 1017.75] loss=3.11 avg=2.91\n",
            "[534 | 1019.42] loss=2.50 avg=2.90\n",
            "[535 | 1021.09] loss=3.06 avg=2.91\n",
            "[536 | 1022.76] loss=3.02 avg=2.91\n",
            "[537 | 1024.43] loss=3.03 avg=2.91\n",
            "[538 | 1026.10] loss=2.58 avg=2.91\n",
            "[539 | 1027.77] loss=3.27 avg=2.91\n",
            "[540 | 1029.44] loss=2.72 avg=2.91\n",
            "[541 | 1031.11] loss=2.19 avg=2.90\n",
            "[542 | 1032.79] loss=3.06 avg=2.90\n",
            "[543 | 1034.47] loss=2.80 avg=2.90\n",
            "[544 | 1036.14] loss=2.67 avg=2.90\n",
            "[545 | 1037.82] loss=2.72 avg=2.90\n",
            "[546 | 1039.51] loss=3.15 avg=2.90\n",
            "[547 | 1041.18] loss=2.90 avg=2.90\n",
            "[548 | 1042.86] loss=3.19 avg=2.90\n",
            "[549 | 1044.53] loss=2.59 avg=2.90\n",
            "[550 | 1046.21] loss=2.97 avg=2.90\n",
            "[551 | 1047.88] loss=3.05 avg=2.90\n",
            "[552 | 1049.57] loss=3.01 avg=2.90\n",
            "[553 | 1051.25] loss=2.75 avg=2.90\n",
            "[554 | 1052.94] loss=3.19 avg=2.90\n",
            "[555 | 1054.61] loss=2.69 avg=2.90\n",
            "[556 | 1056.29] loss=3.15 avg=2.90\n",
            "[557 | 1057.97] loss=3.23 avg=2.91\n",
            "[558 | 1059.65] loss=2.53 avg=2.90\n",
            "[559 | 1061.33] loss=3.28 avg=2.91\n",
            "[560 | 1063.01] loss=2.33 avg=2.90\n",
            "[561 | 1064.69] loss=2.66 avg=2.90\n",
            "[562 | 1066.37] loss=3.09 avg=2.90\n",
            "[563 | 1068.05] loss=2.55 avg=2.90\n",
            "[564 | 1069.74] loss=2.88 avg=2.90\n",
            "[565 | 1071.43] loss=2.47 avg=2.89\n",
            "[566 | 1073.10] loss=2.89 avg=2.89\n",
            "[567 | 1074.78] loss=2.31 avg=2.89\n",
            "[568 | 1076.46] loss=3.07 avg=2.89\n",
            "[569 | 1078.15] loss=2.73 avg=2.89\n",
            "[570 | 1079.81] loss=2.46 avg=2.88\n",
            "[571 | 1081.50] loss=3.24 avg=2.89\n",
            "[572 | 1083.17] loss=2.44 avg=2.88\n",
            "[573 | 1084.85] loss=2.75 avg=2.88\n",
            "[574 | 1086.53] loss=2.48 avg=2.88\n",
            "[575 | 1088.20] loss=2.76 avg=2.88\n",
            "[576 | 1089.89] loss=2.71 avg=2.87\n",
            "[577 | 1091.57] loss=2.72 avg=2.87\n",
            "[578 | 1093.25] loss=2.53 avg=2.87\n",
            "[579 | 1094.93] loss=2.42 avg=2.86\n",
            "[580 | 1096.60] loss=2.71 avg=2.86\n",
            "[581 | 1098.28] loss=2.89 avg=2.86\n",
            "[582 | 1099.96] loss=3.58 avg=2.87\n",
            "[583 | 1101.63] loss=3.13 avg=2.87\n",
            "[584 | 1103.30] loss=2.49 avg=2.87\n",
            "[585 | 1104.97] loss=3.18 avg=2.87\n",
            "[586 | 1106.64] loss=2.73 avg=2.87\n",
            "[587 | 1108.31] loss=2.73 avg=2.87\n",
            "[588 | 1109.98] loss=2.78 avg=2.87\n",
            "[589 | 1111.66] loss=2.79 avg=2.87\n",
            "[590 | 1113.34] loss=2.82 avg=2.87\n",
            "[591 | 1115.01] loss=2.97 avg=2.87\n",
            "[592 | 1116.69] loss=2.99 avg=2.87\n",
            "[593 | 1118.36] loss=2.82 avg=2.87\n",
            "[594 | 1120.03] loss=2.67 avg=2.87\n",
            "[595 | 1121.70] loss=2.89 avg=2.87\n",
            "[596 | 1123.37] loss=2.88 avg=2.87\n",
            "[597 | 1125.05] loss=2.52 avg=2.86\n",
            "[598 | 1126.71] loss=2.60 avg=2.86\n",
            "[599 | 1128.38] loss=2.90 avg=2.86\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " additional properties. One of these properties is the inverse square law. \n",
            "\n",
            "Theorem 6\n",
            "If the square of the square of the square of the square of the square of the square can be written as the result of finding the e square of the square of the square of the square of the square of the square of the square of the square of the square of the square and using the above equation\n",
            "\n",
            "Theorem 7\n",
            "Given E=2\\circ 3 , E=sqrt(2\\sin(2\\pi^2) 2)\n",
            "\n",
            "E=4,\n",
            "\n",
            "E=7,\n",
            "E=14,\n",
            "E=21,\n",
            "E=35,\n",
            "E=51, \\end{equation}\n",
            "\n",
            "\n",
            "We can see that this answer is correct and that this is in fact the solution. The answer must be in fact correct, either because of something in the context or because it was chosen by a priori. And yet I cannot for the life of me understand why this solution is taken as valid. What exactly is this priori reasoning? Is there some mathematical proof which proves I've found this priori correct? Does a priori validating this answer also prove that I've found this answer? No, of course not; for it is logically incorrect to say as a priori validates what we cannot prove. However, it does at least appear to be an accepted pattern in mathematics. So what is the problem?\n",
            "\n",
            "I am looking to understand the properties that lead to finding the e square of a square and then looking through the square and seeing the e square of the square and seeing the square of the square, then looking through the square a further f^(1) and seeing the e square of the square of the square, finally finding that the Square of the square is the square of the square of the square of the square of the square, and then looking through the square and seeing the square of the square, then the e square of the square of the square, and finally finding a solution to the square of the square. \n",
            "\n",
            "What is the problem?\n",
            "Supposing it is indeed correct by a priori, the problem is that it leads to the square of the square, and not to the final solution. The Square of the square is the square of the square, and it is not the final solution for the square. In fact, what we get is a different solution. So the problem is that the result itself is incorrect and yet the resulting square of the square is incorrect from a \"pre-computation\" point of view, where the initial solution and subsequent solution were correct. \n",
            "\n",
            "And this seems like an impossible task because we know we are taking a priori values. And this is what the problem is really about. In this post, I will attempt to make the following statements:\n",
            "\n",
            "We cannot generalizé the e square of the square of the square, so e^n is incorrect\n",
            "\n",
            "We cannot find the e square of the square of the square, so we are in a contradiction (by way of contradiction in e^n)\n",
            "\n",
            "Finding the e-square of the e-square we would have in fact concluded that e^n was incorrect and thus this would have been the ending to the whole question. \n",
            "\n",
            "\n",
            "The Problem\n",
            "As I said , this problem is a little difficult because we know we can solve the q e of a square, but what is q and why is it such an important question? Since it is not necessarily the first expression used in the equation, it is the simplest one we can use. The original problem was that I looked online and found the problem and had to learn how to solve it. I realized it was a bit difficult, and I then attempted to make the problem easier and I learned that it was quite interesting and did indeed have a satisfying answer. \n",
            "\n",
            "I would like to address something for a moment before answering this question. In order to solve the e e square for e^n = (1-2.7)n*4+2.75*p*x we need to choose the value of the e^-square of the e-square for that value. \n",
            "\n",
            "I am not sure if you are familiar with the problem of solving the square of the square? It is quite simple (and it requires some understanding or an awareness of the solution to be complete), but since there is no way we can solve it, we have to work through the solution. As far as I am aware, the solution is given in the following\n",
            "\n",
            "where the \"e^-square\" represents e^n and the number 4*p are the e^-squared (the e^n itself)\n",
            "\n",
            "and the number p is the e^-squared in equation (2). \n",
            "\n",
            "What is called the \"eqn\" in this equation is a mathematical operator which gives you the solution of a question and you can check the answer if\n",
            "\n",
            "[600 | 1152.80] loss=3.05 avg=2.86\n",
            "[601 | 1154.46] loss=2.58 avg=2.86\n",
            "[602 | 1156.11] loss=2.51 avg=2.86\n",
            "[603 | 1157.77] loss=2.73 avg=2.86\n",
            "[604 | 1159.43] loss=2.94 avg=2.86\n",
            "[605 | 1161.09] loss=2.73 avg=2.86\n",
            "[606 | 1162.73] loss=3.08 avg=2.86\n",
            "[607 | 1164.38] loss=3.32 avg=2.86\n",
            "[608 | 1166.04] loss=2.76 avg=2.86\n",
            "[609 | 1167.69] loss=2.68 avg=2.86\n",
            "[610 | 1169.33] loss=3.27 avg=2.86\n",
            "[611 | 1170.98] loss=2.96 avg=2.86\n",
            "[612 | 1172.64] loss=2.66 avg=2.86\n",
            "[613 | 1174.30] loss=3.11 avg=2.86\n",
            "[614 | 1175.97] loss=2.78 avg=2.86\n",
            "[615 | 1177.63] loss=2.60 avg=2.86\n",
            "[616 | 1179.30] loss=2.65 avg=2.86\n",
            "[617 | 1180.97] loss=3.00 avg=2.86\n",
            "[618 | 1182.63] loss=2.13 avg=2.85\n",
            "[619 | 1184.30] loss=3.16 avg=2.86\n",
            "[620 | 1185.97] loss=2.74 avg=2.86\n",
            "[621 | 1187.64] loss=2.79 avg=2.85\n",
            "[622 | 1189.31] loss=2.84 avg=2.85\n",
            "[623 | 1190.97] loss=3.02 avg=2.86\n",
            "[624 | 1192.64] loss=2.72 avg=2.85\n",
            "[625 | 1194.31] loss=2.77 avg=2.85\n",
            "[626 | 1195.97] loss=3.18 avg=2.86\n",
            "[627 | 1197.64] loss=3.07 avg=2.86\n",
            "[628 | 1199.31] loss=2.70 avg=2.86\n",
            "[629 | 1200.99] loss=2.77 avg=2.86\n",
            "[630 | 1202.66] loss=2.73 avg=2.86\n",
            "[631 | 1204.33] loss=2.81 avg=2.86\n",
            "[632 | 1206.01] loss=3.17 avg=2.86\n",
            "[633 | 1207.69] loss=2.91 avg=2.86\n",
            "[634 | 1209.37] loss=2.84 avg=2.86\n",
            "[635 | 1211.03] loss=3.14 avg=2.86\n",
            "[636 | 1212.70] loss=2.29 avg=2.86\n",
            "[637 | 1214.37] loss=2.98 avg=2.86\n",
            "[638 | 1216.05] loss=2.96 avg=2.86\n",
            "[639 | 1217.74] loss=2.34 avg=2.85\n",
            "[640 | 1219.43] loss=2.50 avg=2.85\n",
            "[641 | 1221.10] loss=2.72 avg=2.85\n",
            "[642 | 1222.79] loss=3.02 avg=2.85\n",
            "[643 | 1224.47] loss=2.77 avg=2.85\n",
            "[644 | 1226.15] loss=2.89 avg=2.85\n",
            "[645 | 1227.83] loss=3.21 avg=2.85\n",
            "[646 | 1229.50] loss=3.01 avg=2.85\n",
            "[647 | 1231.19] loss=3.14 avg=2.86\n",
            "[648 | 1232.88] loss=2.81 avg=2.86\n",
            "[649 | 1234.56] loss=2.72 avg=2.86\n",
            "[650 | 1236.24] loss=3.34 avg=2.86\n",
            "[651 | 1237.93] loss=3.08 avg=2.86\n",
            "[652 | 1239.60] loss=2.97 avg=2.86\n",
            "[653 | 1241.28] loss=2.72 avg=2.86\n",
            "[654 | 1242.96] loss=3.21 avg=2.87\n",
            "[655 | 1244.64] loss=3.08 avg=2.87\n",
            "[656 | 1246.32] loss=2.73 avg=2.87\n",
            "[657 | 1248.00] loss=2.48 avg=2.86\n",
            "[658 | 1249.68] loss=2.71 avg=2.86\n",
            "[659 | 1251.36] loss=3.11 avg=2.86\n",
            "[660 | 1253.03] loss=3.07 avg=2.87\n",
            "[661 | 1254.70] loss=2.60 avg=2.86\n",
            "[662 | 1256.38] loss=2.79 avg=2.86\n",
            "[663 | 1258.07] loss=3.02 avg=2.86\n",
            "[664 | 1259.75] loss=3.01 avg=2.87\n",
            "[665 | 1261.43] loss=2.81 avg=2.86\n",
            "[666 | 1263.10] loss=3.06 avg=2.87\n",
            "[667 | 1264.78] loss=2.96 avg=2.87\n",
            "[668 | 1266.46] loss=2.77 avg=2.87\n",
            "[669 | 1268.14] loss=3.10 avg=2.87\n",
            "[670 | 1269.82] loss=2.85 avg=2.87\n",
            "[671 | 1271.49] loss=2.83 avg=2.87\n",
            "[672 | 1273.18] loss=2.93 avg=2.87\n",
            "[673 | 1274.86] loss=3.13 avg=2.87\n",
            "[674 | 1276.55] loss=2.90 avg=2.87\n",
            "[675 | 1278.22] loss=2.92 avg=2.87\n",
            "[676 | 1279.89] loss=2.70 avg=2.87\n",
            "[677 | 1281.58] loss=2.85 avg=2.87\n",
            "[678 | 1283.24] loss=2.80 avg=2.87\n",
            "[679 | 1284.91] loss=2.71 avg=2.87\n",
            "[680 | 1286.58] loss=2.92 avg=2.87\n",
            "[681 | 1288.25] loss=3.04 avg=2.87\n",
            "[682 | 1289.94] loss=2.93 avg=2.87\n",
            "[683 | 1291.61] loss=2.98 avg=2.87\n",
            "[684 | 1293.28] loss=3.05 avg=2.87\n",
            "[685 | 1294.95] loss=2.88 avg=2.87\n",
            "[686 | 1296.62] loss=2.46 avg=2.87\n",
            "[687 | 1298.29] loss=2.88 avg=2.87\n",
            "[688 | 1299.98] loss=2.80 avg=2.87\n",
            "[689 | 1301.66] loss=2.77 avg=2.87\n",
            "[690 | 1303.34] loss=2.61 avg=2.87\n",
            "[691 | 1305.01] loss=2.76 avg=2.86\n",
            "[692 | 1306.69] loss=3.02 avg=2.87\n",
            "[693 | 1308.36] loss=2.63 avg=2.86\n",
            "[694 | 1310.03] loss=2.77 avg=2.86\n",
            "[695 | 1311.70] loss=3.02 avg=2.86\n",
            "[696 | 1313.37] loss=2.90 avg=2.86\n",
            "[697 | 1315.04] loss=2.79 avg=2.86\n",
            "[698 | 1316.71] loss=2.65 avg=2.86\n",
            "[699 | 1318.38] loss=2.68 avg=2.86\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " now to the extent of an argument which says that for every action there is an equal and opposite reaction” I would argue it is false. In order for our minds to take some actions, we must have some initial state that is perceived as the state in which those actions must take place. There would be a difference between that initial state and the state in which those actions are taking place and it is this difference which explains why we cannot conceive of the world objectively or even qualitatively as a logical whole. \n",
            "\n",
            "So it would appear that we have a paradox in epistemology which can be resolved only by taking our minds back to a simpler mode of cognition. This paradox is like the paradox about the universe, but the contradiction is that instead of one being true, two are false, and so for all practical purposes you can not see that each of the two are false. Rather, it is a truth in which all are true: A world exists in which all of the various individuals who exist in it have some sort of relationship to one another or to the larger world. Now if the world of which this world is a manifestation were true, and a large enough portion of it came into being as a result of a process other than the one which created the world, and if the world in which the world is a manifestation were the world, then in a world in which all these individuals do not exist there would be no world, no people, and no individuals. \n",
            "\n",
            "The solution I believe is to reinterpret the paradox in terms of two different things: If the universe is a mathematical representation of a world, and if the world does not exist in the sense which we have in mind, then what is it that leads us to believe that the universe is not a mathematical representation of the world?  Why is the world so small and finite, and why should we be willing to accept that a small universe is no more a world than a large one? \n",
            "\n",
            "[//]\n",
            "\n",
            "Tuesday April 24, 2018 — 2:21PM\n",
            "Questions for the Future\n",
            "When I discuss epistemology, my most frequent theme is the possibility that I may be losing vision of the future. The future has clearly been made manifest to me by experience. As a result of the experience, the future seems ever more manifest, and I see many things which I did not notice before, I see things which I never took the time to see existed, and things which I had always expected to exist. However, since I have no way a person could know what they will see in the future I cannot see that such things are possible. \n",
            "\n",
            "Is this the same logic where I should work on the task I am currently working on more carefully so that in the future when I do work I can be more certain of what will come from the work in the present. \n",
            "\n",
            "These same questions are now asking me when I am able to work on work which requires knowledge about the future. \n",
            "\n",
            "Can the future really be foreseen ? A world in which the individuals who exist in the world have a relationship to one another or to the larger world, but one is in which none of these individuals exist, implies that the world is finite in space. This is why we can no longer conceive of the world as a logical whole. But now one cannot distinguish whether the world is a finite world (because there are infinite numbers of persons) or a world which does not even exist (because the world includes no such persons). \n",
            "\n",
            "Are we on the same path as the people I have been with and thus we are on the same path when we attempt to see the future? Or do we fall further away from where we think we are? \n",
            "\n",
            "Are we still on the path we began but have we fallen off of it? Are we still in the realm of imagination where one sees what may be before we are there? \n",
            "\n",
            "So as I continue to work on the world we may be seeing things which we had intended would not exist and therefore we need to look into further the world which does not exist. \n",
            "\n",
            "Wednesday April 25, 2018 — 1:27AM\n",
            "“Questions for the Future”\n",
            "By “Questions for the Future” I mean questions which I can answer in the future. I might answer, “Is there a good answer to the question or is the question too complex for a future solution?”, or “Can my solution be improved and make it a more complete answer to the question?”.” Questions for the future are questions which I can answer in the future, \n",
            "“What are important problems in philosophy and other scientific sciences that philosophers could study to better answer questions in the future?”\n",
            "\n",
            "My list is a bit of a work in progress. It might be expanded over time based on feedback on the questions. \n",
            "\n",
            "(1) What are the problems in philosophy and other scientific sciences that philosophers could study to be better answer the questions\n",
            "\n",
            "[700 | 1342.99] loss=2.57 avg=2.86\n",
            "[701 | 1344.65] loss=2.52 avg=2.85\n",
            "[702 | 1346.32] loss=3.02 avg=2.86\n",
            "[703 | 1347.98] loss=3.26 avg=2.86\n",
            "[704 | 1349.65] loss=2.96 avg=2.86\n",
            "[705 | 1351.31] loss=2.74 avg=2.86\n",
            "[706 | 1352.97] loss=2.81 avg=2.86\n",
            "[707 | 1354.63] loss=2.72 avg=2.86\n",
            "[708 | 1356.29] loss=3.04 avg=2.86\n",
            "[709 | 1357.96] loss=2.82 avg=2.86\n",
            "[710 | 1359.62] loss=2.98 avg=2.86\n",
            "[711 | 1361.28] loss=2.59 avg=2.86\n",
            "[712 | 1362.95] loss=3.07 avg=2.86\n",
            "[713 | 1364.61] loss=2.84 avg=2.86\n",
            "[714 | 1366.28] loss=2.84 avg=2.86\n",
            "[715 | 1367.94] loss=2.62 avg=2.86\n",
            "[716 | 1369.61] loss=2.69 avg=2.86\n",
            "[717 | 1371.27] loss=2.95 avg=2.86\n",
            "[718 | 1372.94] loss=2.39 avg=2.85\n",
            "[719 | 1374.61] loss=2.61 avg=2.85\n",
            "[720 | 1376.28] loss=2.67 avg=2.85\n",
            "[721 | 1377.95] loss=2.77 avg=2.85\n",
            "[722 | 1379.62] loss=2.43 avg=2.84\n",
            "[723 | 1381.29] loss=2.96 avg=2.84\n",
            "[724 | 1382.96] loss=2.97 avg=2.84\n",
            "[725 | 1384.63] loss=2.80 avg=2.84\n",
            "[726 | 1386.31] loss=2.80 avg=2.84\n",
            "[727 | 1387.98] loss=2.83 avg=2.84\n",
            "[728 | 1389.65] loss=3.09 avg=2.85\n",
            "[729 | 1391.32] loss=2.68 avg=2.84\n",
            "[730 | 1393.00] loss=3.50 avg=2.85\n",
            "[731 | 1394.67] loss=2.94 avg=2.85\n",
            "[732 | 1396.34] loss=3.09 avg=2.85\n",
            "[733 | 1398.02] loss=2.62 avg=2.85\n",
            "[734 | 1399.69] loss=2.92 avg=2.85\n",
            "[735 | 1401.37] loss=2.79 avg=2.85\n",
            "[736 | 1403.04] loss=2.84 avg=2.85\n",
            "[737 | 1404.71] loss=2.57 avg=2.85\n",
            "[738 | 1406.40] loss=2.40 avg=2.84\n",
            "[739 | 1408.08] loss=2.89 avg=2.84\n",
            "[740 | 1409.77] loss=2.73 avg=2.84\n",
            "[741 | 1411.44] loss=2.39 avg=2.84\n",
            "[742 | 1413.11] loss=2.98 avg=2.84\n",
            "[743 | 1414.80] loss=3.01 avg=2.84\n",
            "[744 | 1416.48] loss=2.73 avg=2.84\n",
            "[745 | 1418.17] loss=2.66 avg=2.84\n",
            "[746 | 1419.85] loss=2.89 avg=2.84\n",
            "[747 | 1421.53] loss=2.96 avg=2.84\n",
            "[748 | 1423.22] loss=3.09 avg=2.84\n",
            "[749 | 1424.90] loss=2.92 avg=2.84\n",
            "[750 | 1426.59] loss=3.04 avg=2.85\n",
            "[751 | 1428.26] loss=3.06 avg=2.85\n",
            "[752 | 1429.95] loss=2.37 avg=2.84\n",
            "[753 | 1431.63] loss=2.67 avg=2.84\n",
            "[754 | 1433.30] loss=2.83 avg=2.84\n",
            "[755 | 1434.98] loss=2.71 avg=2.84\n",
            "[756 | 1436.66] loss=2.66 avg=2.84\n",
            "[757 | 1438.35] loss=2.75 avg=2.84\n",
            "[758 | 1440.02] loss=2.20 avg=2.83\n",
            "[759 | 1441.69] loss=2.87 avg=2.83\n",
            "[760 | 1443.38] loss=2.65 avg=2.83\n",
            "[761 | 1445.05] loss=2.68 avg=2.83\n",
            "[762 | 1446.72] loss=2.99 avg=2.83\n",
            "[763 | 1448.39] loss=2.95 avg=2.83\n",
            "[764 | 1450.06] loss=3.25 avg=2.84\n",
            "[765 | 1451.73] loss=2.32 avg=2.83\n",
            "[766 | 1453.42] loss=2.68 avg=2.83\n",
            "[767 | 1455.09] loss=2.58 avg=2.83\n",
            "[768 | 1456.76] loss=2.62 avg=2.82\n",
            "[769 | 1458.43] loss=2.20 avg=2.82\n",
            "[770 | 1460.10] loss=2.90 avg=2.82\n",
            "[771 | 1461.78] loss=2.77 avg=2.82\n",
            "[772 | 1463.45] loss=2.66 avg=2.82\n",
            "[773 | 1465.12] loss=2.62 avg=2.81\n",
            "[774 | 1466.79] loss=2.93 avg=2.82\n",
            "[775 | 1468.47] loss=2.55 avg=2.81\n",
            "[776 | 1470.14] loss=3.24 avg=2.82\n",
            "[777 | 1471.81] loss=2.66 avg=2.82\n",
            "[778 | 1473.48] loss=2.56 avg=2.81\n",
            "[779 | 1475.16] loss=2.77 avg=2.81\n",
            "[780 | 1476.83] loss=2.78 avg=2.81\n",
            "[781 | 1478.50] loss=2.31 avg=2.81\n",
            "[782 | 1480.17] loss=3.14 avg=2.81\n",
            "[783 | 1481.84] loss=3.01 avg=2.81\n",
            "[784 | 1483.51] loss=2.95 avg=2.81\n",
            "[785 | 1485.18] loss=3.15 avg=2.82\n",
            "[786 | 1486.85] loss=3.15 avg=2.82\n",
            "[787 | 1488.52] loss=2.49 avg=2.82\n",
            "[788 | 1490.19] loss=2.88 avg=2.82\n",
            "[789 | 1491.86] loss=3.07 avg=2.82\n",
            "[790 | 1493.53] loss=2.81 avg=2.82\n",
            "[791 | 1495.20] loss=2.91 avg=2.82\n",
            "[792 | 1496.87] loss=2.75 avg=2.82\n",
            "[793 | 1498.54] loss=2.87 avg=2.82\n",
            "[794 | 1500.21] loss=2.52 avg=2.82\n",
            "[795 | 1501.88] loss=2.80 avg=2.82\n",
            "[796 | 1503.55] loss=2.87 avg=2.82\n",
            "[797 | 1505.22] loss=2.71 avg=2.82\n",
            "[798 | 1506.89] loss=2.41 avg=2.81\n",
            "[799 | 1508.56] loss=2.52 avg=2.81\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            ". There are also things I've learned about myself and about the universe — something that I would never have believed myself or the planet could know. Perhaps, the reason I was allowed to live in this place was because of my experience. \n",
            "\n",
            "I wonder if anything, anything, could change this. I wonder if it's just as simple as the idea that a person can change their mind, not because they were given some epiphany but because they finally feel true to their beliefs. It's always easier in the moment—I can do it almost as easily as I can get into a car—but I must admit it's not always so simple afterward. \n",
            "\n",
            "And what would you do if you were in a position to do something again? Perhaps you will simply accept it, and just live with the consequences of your behavior and the way the world chooses to treat you for doing something you never intended to do. But if the world chose to treat you this way, what would it do to you? Would you get over the fact that the only thing you ever believed you deserved was the love you showed to others, and maybe, you would also stop judging yourself because it's better to think before acting. You don't have to live with the consequences of past behavior you never intended to live, because you already know the consequences are not as dire or dangerous as they might seem. \n",
            "\n",
            "[//]\n",
            "\n",
            "January 25, 2016 — 6:13 PM Thursday\n",
            "A Tale of Two Cities: The New York Post and the \"Gutted\" Times\n",
            "\n",
            "By Matthew Mollenkopf\n",
            "\n",
            "\n",
            "On Dec. 18, the New York Post's Mark Landler wrote about the story of a gay New Yorker being denied admission into the Stonewall Inn because he was wearing a t-shirt which said \"Don't Mess With Boston\". Two days later, in an article titled \"Hate Crime Is Worse Than You Think,\" John Aravosis wrote about the \"gutted\" New York Times writing that gay men had the same rights as any other group of people. A day after this article appeared, John Aravosis wrote an article on whether he should change his column. On May 22, the article he wrote was republished in The New York Times. \n",
            "\n",
            "The article begins with a statement like the ones we make whenever we are arguing about the merits of a policy: \"A gay person can do nothing wrong, because he can't have thoughts about women or about other people,\" or something like this. Aravosis's assertion is supported by a number of statements. A reader writes in an article that the article \"really is true.\" A writer claims it \"is not about gays.\" Many people, including members of The New York Times, write that the article is \"not about gays, and it's certainly not a story about gays (or anyone)\". This last sentence is an interesting one because it is not true that the article is \"not about gays\" (in any sense of the word) for it is very clear—as it was so far, anyway—that the article exists in opposition to that definition. For reasons which seem to go beyond its use, the article's authors seem to view gays as people and as such the article does not have a story about gays. \n",
            "\n",
            "These questions about the \"true\" nature of the article, how the authors view and describe it, are all questions which might have been asked years ago. For one, it is quite interesting that the writers seem to have been able to formulate a definition and language which seemed to be consistent with the history of the American gay rights movement. But even more interesting is the fact that the writers and editors seem quite willing to see no problem with an article that describes things however they wish to see them described. For example, they seem to see no problem with the article describing the \"gay community\" as \"a community of people with beliefs about their humanity\" as well as people who do \"things about the law and the system, and about how their community should be run\" as being a good thing. For those who are already familiar with these terms, this description is not entirely out of place and even the authors of the article seem to recognize it to be \"a good thing,\" but perhaps the best way for the readers is perhaps to go to a dictionary and search for a term such as \"gay,\" \"community,\" \"homosexual,\" \"homophobes\" , \"homophobia,\" or \"homosexists.\" Maybe even just Google \"gay community\" and you will find such articles.\n",
            "\n",
            "Now before I go on, I have to tell you the reason for my writing this. I wanted to understand what these people who are writing about these things feel about them. It was not my intention to become emotionally attached to the people behind the article or so I thought, but rather merely an observer to see how my observations would contribute to the community, to see how I could contribute to its work. I was merely following the\n",
            "\n",
            "[800 | 1532.96] loss=2.61 avg=2.81\n",
            "[801 | 1534.63] loss=3.19 avg=2.81\n",
            "[802 | 1536.30] loss=3.00 avg=2.81\n",
            "[803 | 1537.96] loss=3.04 avg=2.82\n",
            "[804 | 1539.63] loss=2.62 avg=2.81\n",
            "[805 | 1541.29] loss=2.60 avg=2.81\n",
            "[806 | 1542.96] loss=2.73 avg=2.81\n",
            "[807 | 1544.63] loss=2.75 avg=2.81\n",
            "[808 | 1546.29] loss=2.63 avg=2.81\n",
            "[809 | 1547.95] loss=2.61 avg=2.81\n",
            "[810 | 1549.60] loss=2.66 avg=2.81\n",
            "[811 | 1551.24] loss=3.05 avg=2.81\n",
            "[812 | 1552.90] loss=3.01 avg=2.81\n",
            "[813 | 1554.57] loss=3.06 avg=2.81\n",
            "[814 | 1556.23] loss=1.98 avg=2.80\n",
            "[815 | 1557.89] loss=2.69 avg=2.80\n",
            "[816 | 1559.56] loss=2.75 avg=2.80\n",
            "[817 | 1561.22] loss=2.71 avg=2.80\n",
            "[818 | 1562.88] loss=3.13 avg=2.80\n",
            "[819 | 1564.55] loss=2.33 avg=2.80\n",
            "[820 | 1566.21] loss=2.96 avg=2.80\n",
            "[821 | 1567.88] loss=2.85 avg=2.80\n",
            "[822 | 1569.55] loss=3.09 avg=2.81\n",
            "[823 | 1571.22] loss=3.04 avg=2.81\n",
            "[824 | 1572.89] loss=2.20 avg=2.80\n",
            "[825 | 1574.56] loss=3.07 avg=2.80\n",
            "[826 | 1576.23] loss=2.51 avg=2.80\n",
            "[827 | 1577.91] loss=2.99 avg=2.80\n",
            "[828 | 1579.58] loss=2.46 avg=2.80\n",
            "[829 | 1581.24] loss=2.95 avg=2.80\n",
            "[830 | 1582.91] loss=2.23 avg=2.80\n",
            "[831 | 1584.59] loss=2.70 avg=2.79\n",
            "[832 | 1586.26] loss=2.64 avg=2.79\n",
            "[833 | 1587.94] loss=2.34 avg=2.79\n",
            "[834 | 1589.61] loss=2.74 avg=2.79\n",
            "[835 | 1591.28] loss=2.47 avg=2.78\n",
            "[836 | 1592.95] loss=2.91 avg=2.79\n",
            "[837 | 1594.62] loss=2.96 avg=2.79\n",
            "[838 | 1596.30] loss=2.82 avg=2.79\n",
            "[839 | 1597.97] loss=2.43 avg=2.78\n",
            "[840 | 1599.65] loss=2.80 avg=2.78\n",
            "[841 | 1601.32] loss=2.77 avg=2.78\n",
            "[842 | 1603.00] loss=2.57 avg=2.78\n",
            "[843 | 1604.69] loss=2.81 avg=2.78\n",
            "[844 | 1606.36] loss=2.64 avg=2.78\n",
            "[845 | 1608.03] loss=2.62 avg=2.78\n",
            "[846 | 1609.71] loss=2.87 avg=2.78\n",
            "[847 | 1611.38] loss=2.83 avg=2.78\n",
            "[848 | 1613.07] loss=2.32 avg=2.78\n",
            "[849 | 1614.75] loss=2.62 avg=2.77\n",
            "[850 | 1616.44] loss=3.18 avg=2.78\n",
            "[851 | 1618.12] loss=2.72 avg=2.78\n",
            "[852 | 1619.79] loss=2.77 avg=2.78\n",
            "[853 | 1621.47] loss=2.69 avg=2.78\n",
            "[854 | 1623.15] loss=2.28 avg=2.77\n",
            "[855 | 1624.83] loss=2.43 avg=2.77\n",
            "[856 | 1626.51] loss=3.05 avg=2.77\n",
            "[857 | 1628.17] loss=2.52 avg=2.77\n",
            "[858 | 1629.84] loss=3.20 avg=2.77\n",
            "[859 | 1631.53] loss=2.61 avg=2.77\n",
            "[860 | 1633.20] loss=2.96 avg=2.77\n",
            "[861 | 1634.89] loss=3.06 avg=2.78\n",
            "[862 | 1636.56] loss=2.96 avg=2.78\n",
            "[863 | 1638.23] loss=2.46 avg=2.78\n",
            "[864 | 1639.91] loss=2.62 avg=2.77\n",
            "[865 | 1641.59] loss=2.88 avg=2.77\n",
            "[866 | 1643.27] loss=2.99 avg=2.78\n",
            "[867 | 1644.96] loss=2.99 avg=2.78\n",
            "[868 | 1646.64] loss=3.17 avg=2.78\n",
            "[869 | 1648.31] loss=2.55 avg=2.78\n",
            "[870 | 1649.99] loss=2.96 avg=2.78\n",
            "[871 | 1651.65] loss=2.54 avg=2.78\n",
            "[872 | 1653.32] loss=2.10 avg=2.77\n",
            "[873 | 1655.00] loss=3.13 avg=2.78\n",
            "[874 | 1656.68] loss=2.57 avg=2.77\n",
            "[875 | 1658.36] loss=3.36 avg=2.78\n",
            "[876 | 1660.03] loss=2.42 avg=2.78\n",
            "[877 | 1661.70] loss=2.94 avg=2.78\n",
            "[878 | 1663.38] loss=2.85 avg=2.78\n",
            "[879 | 1665.05] loss=2.53 avg=2.78\n",
            "[880 | 1666.71] loss=3.31 avg=2.78\n",
            "[881 | 1668.38] loss=3.23 avg=2.79\n",
            "[882 | 1670.05] loss=2.75 avg=2.79\n",
            "[883 | 1671.72] loss=2.88 avg=2.79\n",
            "[884 | 1673.40] loss=3.00 avg=2.79\n",
            "[885 | 1675.07] loss=2.81 avg=2.79\n",
            "[886 | 1676.75] loss=2.57 avg=2.79\n",
            "[887 | 1678.42] loss=2.90 avg=2.79\n",
            "[888 | 1680.09] loss=2.51 avg=2.79\n",
            "[889 | 1681.77] loss=2.56 avg=2.78\n",
            "[890 | 1683.45] loss=2.10 avg=2.78\n",
            "[891 | 1685.12] loss=2.72 avg=2.78\n",
            "[892 | 1686.79] loss=3.07 avg=2.78\n",
            "[893 | 1688.46] loss=2.92 avg=2.78\n",
            "[894 | 1690.13] loss=3.13 avg=2.78\n",
            "[895 | 1691.82] loss=2.86 avg=2.78\n",
            "[896 | 1693.50] loss=2.79 avg=2.78\n",
            "[897 | 1695.18] loss=2.39 avg=2.78\n",
            "[898 | 1696.87] loss=2.96 avg=2.78\n",
            "[899 | 1698.56] loss=3.08 avg=2.79\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "Another one of the authors of that paper, I believe, said something similar before, and he was on to something. He had observed that women and girls are as likely in China to be in graduate school as in law. To me this suggests a question. If you're in physics for a couple of years…and you graduate from physics, but then you take a year off from physics, and then you take a year from biology, and then you take a year from math, and then you take a year from social work, and then…And if this happens, is there a relationship between your graduate school years and your later post-physics career? If so, the relationship is more likely to be negative for girls than affirmative for boys, but the relationship is more negative for girls than positive for boys.\n",
            "\n",
            "[…]\n",
            "\"It is not at all uncommon or unusual for girls to be in the lowest socio-economic levels in physics. The fact is that women who do not graduate from physics at the most advanced level are also in a class where they can expect to lose out on an education. For some of these women, the dropout rate is 50 percent.\"\n",
            "… \"The fact is that at such levels the female population is so large that they must be able to absorb the negative feedback that comes with a loss of a great educator.\"\n",
            "I have no way of knowing if what I've just read was actually correct. I have also never bothered to check it out. I am going to keep putting off it to come to MIT for my first summer. But on reflection I realize that one of the ways professors and administrators can keep their jobs is if they keep professors from entering the field. This is particularly true for these men who hold positions of power and who presumably hold positions of wealth and respect. So the main problem I have with this research is not that the work is contradictory to most of the other research in the field; the problem is that, in some ways, the work itself is contradictory to everything else I've seen in the field. The study states women and girls in STEM fields tend to have lower GPAs than the general population (or at least the most \"advanced\" women-only STEM fields). But, the authors speculate that this could be because women who graduate graduate from physics do so at the lower end of the GPAs of the general population, and therefore there is some bias against the women of this sub-population since the GPAs they graduate at, will likely be more consistent with those of the general population. But here you have a question that I don't really think I understand, but the results of the study would suggest that this is not truly a question of science. The question is whether some of these female graduate students come back to the world of academia when they are old enough to be of use.\n",
            "—\n",
            "Wednesday: The Other Side\n",
            "\n",
            "The story of the black girls in math, the stories of Black girls who got dropped out of the University of Texas who did well in college, and those who never went to college. In the 1970s, all of the black males and females attending these four universities—Texas, Boston College, California State University, and Yale University—were students of color. But these universities also had a white majority. And all of these students were coming on a wave of affirmative action policies that made college more and more an opportunity to obtain employment with a pay that far exceeded that of the lowest-paid jobs. Many of these students were getting jobs that they had never had, having jobs and jobs they had barely dreamed about in the past, but were a necessity for them and their families. And the students at these universities were still trying to figure out who their oppressors would be in society as it looked like. They were also attempting to figure out how to live independently of that white system. I think the students were mostly black. The women and the students of color who attended these universities were mostly white. \n",
            "\n",
            "\n",
            "I think of the students as characters in a story I have always known was true: The stories of people who never really experienced the world because for them, what they were experiencing was something else. The story is a story of a black community living their lives as if that world consisted of two different versions: one where they were constantly watched and policed and policed and watched and policed and watched, and one which was in the dark. And because the reality they were experiencing was so different from the \"normal\" world, the students were able to step outside that world, to see for themselves the world of their oppressors and find that the story that was presented to them was a story they would never have heard about had it not been for the affirmative action policies which made college a legitimate route to a more economically sustainable existence. The students were never supposed to be able to go to Harvard or MIT. The students had always been told that these institutions belonged to them—and I think they had always known this—but they were never supposed to be able to do\n",
            "\n",
            "[900 | 1722.87] loss=3.31 avg=2.79\n",
            "[901 | 1724.52] loss=2.89 avg=2.79\n",
            "[902 | 1726.17] loss=2.74 avg=2.79\n",
            "[903 | 1727.84] loss=2.55 avg=2.79\n",
            "[904 | 1729.50] loss=2.66 avg=2.79\n",
            "[905 | 1731.16] loss=2.92 avg=2.79\n",
            "[906 | 1732.82] loss=2.64 avg=2.79\n",
            "[907 | 1734.48] loss=2.81 avg=2.79\n",
            "[908 | 1736.14] loss=2.57 avg=2.79\n",
            "[909 | 1737.80] loss=2.69 avg=2.78\n",
            "[910 | 1739.46] loss=3.01 avg=2.79\n",
            "[911 | 1741.12] loss=2.62 avg=2.79\n",
            "[912 | 1742.78] loss=3.00 avg=2.79\n",
            "[913 | 1744.45] loss=2.84 avg=2.79\n",
            "[914 | 1746.11] loss=2.79 avg=2.79\n",
            "[915 | 1747.78] loss=2.60 avg=2.79\n",
            "[916 | 1749.45] loss=2.34 avg=2.78\n",
            "[917 | 1751.11] loss=3.01 avg=2.78\n",
            "[918 | 1752.78] loss=2.35 avg=2.78\n",
            "[919 | 1754.45] loss=2.75 avg=2.78\n",
            "[920 | 1756.12] loss=2.84 avg=2.78\n",
            "[921 | 1757.79] loss=3.36 avg=2.79\n",
            "[922 | 1759.47] loss=2.80 avg=2.79\n",
            "[923 | 1761.13] loss=2.76 avg=2.79\n",
            "[924 | 1762.81] loss=2.42 avg=2.78\n",
            "[925 | 1764.48] loss=3.04 avg=2.78\n",
            "[926 | 1766.15] loss=2.90 avg=2.79\n",
            "[927 | 1767.82] loss=2.84 avg=2.79\n",
            "[928 | 1769.49] loss=2.66 avg=2.78\n",
            "[929 | 1771.16] loss=2.30 avg=2.78\n",
            "[930 | 1772.83] loss=3.00 avg=2.78\n",
            "[931 | 1774.50] loss=2.17 avg=2.78\n",
            "[932 | 1776.17] loss=2.68 avg=2.77\n",
            "[933 | 1777.83] loss=2.79 avg=2.78\n",
            "[934 | 1779.51] loss=3.25 avg=2.78\n",
            "[935 | 1781.20] loss=2.54 avg=2.78\n",
            "[936 | 1782.87] loss=2.44 avg=2.77\n",
            "[937 | 1784.54] loss=2.76 avg=2.77\n",
            "[938 | 1786.22] loss=2.64 avg=2.77\n",
            "[939 | 1787.90] loss=2.94 avg=2.77\n",
            "[940 | 1789.59] loss=2.63 avg=2.77\n",
            "[941 | 1791.26] loss=2.61 avg=2.77\n",
            "[942 | 1792.94] loss=2.36 avg=2.77\n",
            "[943 | 1794.62] loss=2.71 avg=2.77\n",
            "[944 | 1796.30] loss=2.75 avg=2.77\n",
            "[945 | 1797.99] loss=2.82 avg=2.77\n",
            "[946 | 1799.67] loss=2.54 avg=2.76\n",
            "[947 | 1801.35] loss=2.70 avg=2.76\n",
            "[948 | 1803.02] loss=2.24 avg=2.76\n",
            "[949 | 1804.71] loss=2.84 avg=2.76\n",
            "[950 | 1806.39] loss=2.95 avg=2.76\n",
            "[951 | 1808.06] loss=2.70 avg=2.76\n",
            "[952 | 1809.74] loss=2.38 avg=2.76\n",
            "[953 | 1811.41] loss=2.96 avg=2.76\n",
            "[954 | 1813.10] loss=3.02 avg=2.76\n",
            "[955 | 1814.78] loss=2.85 avg=2.76\n",
            "[956 | 1816.46] loss=2.96 avg=2.76\n",
            "[957 | 1818.14] loss=3.17 avg=2.77\n",
            "[958 | 1819.82] loss=2.50 avg=2.77\n",
            "[959 | 1821.49] loss=2.13 avg=2.76\n",
            "[960 | 1823.17] loss=2.61 avg=2.76\n",
            "[961 | 1824.85] loss=2.96 avg=2.76\n",
            "[962 | 1826.53] loss=2.60 avg=2.76\n",
            "[963 | 1828.20] loss=2.69 avg=2.76\n",
            "[964 | 1829.88] loss=3.04 avg=2.76\n",
            "[965 | 1831.56] loss=2.49 avg=2.76\n",
            "[966 | 1833.24] loss=2.82 avg=2.76\n",
            "[967 | 1834.92] loss=2.31 avg=2.75\n",
            "[968 | 1836.59] loss=2.50 avg=2.75\n",
            "[969 | 1838.26] loss=2.85 avg=2.75\n",
            "[970 | 1839.94] loss=2.75 avg=2.75\n",
            "[971 | 1841.62] loss=2.36 avg=2.75\n",
            "[972 | 1843.29] loss=2.31 avg=2.74\n",
            "[973 | 1844.96] loss=2.39 avg=2.74\n",
            "[974 | 1846.64] loss=2.83 avg=2.74\n",
            "[975 | 1848.31] loss=2.83 avg=2.74\n",
            "[976 | 1849.99] loss=2.62 avg=2.74\n",
            "[977 | 1851.66] loss=2.12 avg=2.73\n",
            "[978 | 1853.34] loss=3.04 avg=2.74\n",
            "[979 | 1855.02] loss=2.36 avg=2.73\n",
            "[980 | 1856.69] loss=2.88 avg=2.74\n",
            "[981 | 1858.36] loss=2.59 avg=2.73\n",
            "[982 | 1860.04] loss=2.51 avg=2.73\n",
            "[983 | 1861.71] loss=2.75 avg=2.73\n",
            "[984 | 1863.40] loss=2.52 avg=2.73\n",
            "[985 | 1865.07] loss=2.14 avg=2.72\n",
            "[986 | 1866.74] loss=2.64 avg=2.72\n",
            "[987 | 1868.42] loss=2.42 avg=2.72\n",
            "[988 | 1870.09] loss=2.42 avg=2.72\n",
            "[989 | 1871.78] loss=2.50 avg=2.71\n",
            "[990 | 1873.45] loss=2.76 avg=2.72\n",
            "[991 | 1875.13] loss=2.46 avg=2.71\n",
            "[992 | 1876.80] loss=2.27 avg=2.71\n",
            "[993 | 1878.47] loss=2.56 avg=2.71\n",
            "[994 | 1880.14] loss=2.79 avg=2.71\n",
            "[995 | 1881.81] loss=2.39 avg=2.70\n",
            "[996 | 1883.49] loss=2.55 avg=2.70\n",
            "[997 | 1885.16] loss=2.67 avg=2.70\n",
            "[998 | 1886.83] loss=2.34 avg=2.70\n",
            "[999 | 1888.51] loss=2.37 avg=2.70\n",
            "Ran 1010 steps.\n",
            "Saving checkpoint/personal_essays_345/model-1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYNl_LduYGLQ",
        "colab_type": "code",
        "outputId": "e73ff065-5789-4303-ea53-88b02b9c2946",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "## slate essays training - with 345\n",
        "!PYTHONPATH=src ./train.py --dataset /content/all_slate.txt --run_name 'all_slate_345' --model_name '345M'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0627 22:15:08.971491 139862786262912 deprecation_wrapper.py:119] From /content/gpt-2/src/model.py:147: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
            "\n",
            "W0627 22:15:08.980223 139862786262912 deprecation_wrapper.py:119] From /content/gpt-2/src/memory_saving_gradients.py:13: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n",
            "W0627 22:15:09.071049 139862786262912 deprecation_wrapper.py:119] From ./train.py:88: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "W0627 22:15:09.071394 139862786262912 deprecation_wrapper.py:119] From ./train.py:91: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "2019-06-27 22:15:09.077538: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-06-27 22:15:09.077791: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2d91100 executing computations on platform Host. Devices:\n",
            "2019-06-27 22:15:09.077832: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2019-06-27 22:15:09.080087: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\n",
            "2019-06-27 22:15:09.233953: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-27 22:15:09.236117: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2d90840 executing computations on platform CUDA. Devices:\n",
            "2019-06-27 22:15:09.236155: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2019-06-27 22:15:09.236493: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-27 22:15:09.237035: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-06-27 22:15:09.237560: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-06-27 22:15:09.240033: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-06-27 22:15:09.241710: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-06-27 22:15:09.242085: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-06-27 22:15:09.243889: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-06-27 22:15:09.245159: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-06-27 22:15:09.249265: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-06-27 22:15:09.249448: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-27 22:15:09.249854: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-27 22:15:09.250174: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
            "2019-06-27 22:15:09.250236: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-06-27 22:15:09.251188: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-06-27 22:15:09.251214: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n",
            "2019-06-27 22:15:09.251225: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n",
            "2019-06-27 22:15:09.251608: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-27 22:15:09.252023: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-27 22:15:09.252409: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14202 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "W0627 22:15:09.253237 139862786262912 deprecation_wrapper.py:119] From ./train.py:92: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0627 22:15:20.355380 139862786262912 deprecation.py:323] From /content/gpt-2/src/sample.py:65: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "W0627 22:15:20.370202 139862786262912 deprecation.py:323] From /content/gpt-2/src/sample.py:16: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0627 22:15:20.372049 139862786262912 deprecation.py:323] From /content/gpt-2/src/sample.py:70: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.random.categorical` instead.\n",
            "W0627 22:15:20.382189 139862786262912 deprecation_wrapper.py:119] From ./train.py:121: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "W0627 22:15:35.937473 139862786262912 deprecation_wrapper.py:119] From ./train.py:144: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n",
            "W0627 22:15:35.940295 139862786262912 deprecation_wrapper.py:119] From ./train.py:147: The name tf.summary.merge is deprecated. Please use tf.compat.v1.summary.merge instead.\n",
            "\n",
            "W0627 22:15:35.941250 139862786262912 deprecation_wrapper.py:119] From ./train.py:149: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
            "\n",
            "W0627 22:15:35.942094 139862786262912 deprecation_wrapper.py:119] From ./train.py:152: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "Loading checkpoint models/345M/model.ckpt\n",
            "W0627 22:15:48.921473 139862786262912 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "Loading dataset...\n",
            "100% 1/1 [00:01<00:00,  1.30s/it]\n",
            "dataset has 138764 tokens\n",
            "Training...\n",
            "2019-06-27 22:16:02.765109: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n",
            "2019-06-27 22:16:03.462041: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "[1 | 12.68] loss=3.23 avg=3.23\n",
            "[2 | 14.30] loss=2.68 avg=2.96\n",
            "[3 | 15.93] loss=2.94 avg=2.95\n",
            "[4 | 17.58] loss=2.47 avg=2.83\n",
            "[5 | 19.22] loss=3.31 avg=2.93\n",
            "[6 | 20.87] loss=2.84 avg=2.91\n",
            "[7 | 22.54] loss=2.92 avg=2.91\n",
            "[8 | 24.21] loss=2.87 avg=2.91\n",
            "[9 | 25.88] loss=2.81 avg=2.90\n",
            "[10 | 27.57] loss=3.19 avg=2.93\n",
            "[11 | 29.25] loss=3.04 avg=2.94\n",
            "[12 | 30.96] loss=3.19 avg=2.96\n",
            "[13 | 32.68] loss=2.68 avg=2.94\n",
            "[14 | 34.39] loss=2.99 avg=2.94\n",
            "[15 | 36.13] loss=2.91 avg=2.94\n",
            "[16 | 37.86] loss=2.59 avg=2.92\n",
            "[17 | 39.60] loss=3.02 avg=2.92\n",
            "[18 | 41.33] loss=2.49 avg=2.90\n",
            "[19 | 43.06] loss=2.82 avg=2.89\n",
            "[20 | 44.79] loss=2.99 avg=2.90\n",
            "[21 | 46.52] loss=2.91 avg=2.90\n",
            "[22 | 48.23] loss=2.63 avg=2.88\n",
            "[23 | 49.94] loss=3.25 avg=2.90\n",
            "[24 | 51.65] loss=3.32 avg=2.92\n",
            "[25 | 53.34] loss=2.92 avg=2.92\n",
            "[26 | 55.04] loss=2.74 avg=2.91\n",
            "[27 | 56.73] loss=2.58 avg=2.90\n",
            "[28 | 58.40] loss=2.59 avg=2.89\n",
            "[29 | 60.07] loss=2.84 avg=2.89\n",
            "[30 | 61.74] loss=2.91 avg=2.89\n",
            "[31 | 63.41] loss=2.63 avg=2.88\n",
            "[32 | 65.07] loss=2.67 avg=2.87\n",
            "[33 | 66.73] loss=3.08 avg=2.88\n",
            "[34 | 68.38] loss=3.11 avg=2.89\n",
            "[35 | 70.03] loss=2.61 avg=2.88\n",
            "[36 | 71.68] loss=3.13 avg=2.88\n",
            "[37 | 73.33] loss=2.93 avg=2.89\n",
            "[38 | 74.98] loss=2.67 avg=2.88\n",
            "[39 | 76.62] loss=2.93 avg=2.88\n",
            "[40 | 78.26] loss=2.87 avg=2.88\n",
            "[41 | 79.91] loss=2.53 avg=2.87\n",
            "[42 | 81.55] loss=2.95 avg=2.87\n",
            "[43 | 83.18] loss=2.47 avg=2.86\n",
            "[44 | 84.82] loss=2.68 avg=2.86\n",
            "[45 | 86.45] loss=2.71 avg=2.85\n",
            "[46 | 88.08] loss=2.88 avg=2.85\n",
            "[47 | 89.72] loss=2.73 avg=2.85\n",
            "[48 | 91.36] loss=2.69 avg=2.85\n",
            "[49 | 92.99] loss=2.96 avg=2.85\n",
            "[50 | 94.63] loss=2.43 avg=2.84\n",
            "[51 | 96.27] loss=3.34 avg=2.85\n",
            "[52 | 97.92] loss=2.91 avg=2.85\n",
            "[53 | 99.55] loss=2.72 avg=2.85\n",
            "[54 | 101.19] loss=2.91 avg=2.85\n",
            "[55 | 102.84] loss=2.80 avg=2.85\n",
            "[56 | 104.49] loss=3.36 avg=2.86\n",
            "[57 | 106.13] loss=2.69 avg=2.86\n",
            "[58 | 107.78] loss=3.03 avg=2.86\n",
            "[59 | 109.43] loss=2.60 avg=2.85\n",
            "[60 | 111.08] loss=2.95 avg=2.86\n",
            "[61 | 112.73] loss=2.61 avg=2.85\n",
            "[62 | 114.39] loss=3.27 avg=2.86\n",
            "[63 | 116.05] loss=2.99 avg=2.86\n",
            "[64 | 117.71] loss=2.66 avg=2.86\n",
            "[65 | 119.37] loss=3.00 avg=2.86\n",
            "[66 | 121.03] loss=3.37 avg=2.87\n",
            "[67 | 122.69] loss=2.52 avg=2.87\n",
            "[68 | 124.36] loss=2.68 avg=2.86\n",
            "[69 | 126.03] loss=2.57 avg=2.86\n",
            "[70 | 127.72] loss=2.92 avg=2.86\n",
            "[71 | 129.39] loss=3.01 avg=2.86\n",
            "[72 | 131.06] loss=2.63 avg=2.86\n",
            "[73 | 132.73] loss=2.27 avg=2.84\n",
            "[74 | 134.40] loss=3.00 avg=2.85\n",
            "[75 | 136.08] loss=2.67 avg=2.84\n",
            "[76 | 137.75] loss=2.96 avg=2.85\n",
            "[77 | 139.42] loss=2.79 avg=2.85\n",
            "[78 | 141.09] loss=3.20 avg=2.85\n",
            "[79 | 142.76] loss=3.23 avg=2.86\n",
            "[80 | 144.42] loss=2.79 avg=2.86\n",
            "[81 | 146.09] loss=2.83 avg=2.86\n",
            "[82 | 147.75] loss=2.69 avg=2.85\n",
            "[83 | 149.41] loss=3.02 avg=2.86\n",
            "[84 | 151.08] loss=2.95 avg=2.86\n",
            "[85 | 152.74] loss=3.23 avg=2.87\n",
            "[86 | 154.40] loss=3.10 avg=2.87\n",
            "[87 | 156.06] loss=3.13 avg=2.87\n",
            "[88 | 157.72] loss=2.84 avg=2.87\n",
            "[89 | 159.39] loss=2.94 avg=2.87\n",
            "[90 | 161.05] loss=2.81 avg=2.87\n",
            "[91 | 162.71] loss=2.78 avg=2.87\n",
            "[92 | 164.37] loss=3.22 avg=2.88\n",
            "[93 | 166.03] loss=2.44 avg=2.87\n",
            "[94 | 167.69] loss=2.44 avg=2.86\n",
            "[95 | 169.34] loss=2.88 avg=2.86\n",
            "[96 | 171.00] loss=2.36 avg=2.86\n",
            "[97 | 172.66] loss=3.19 avg=2.86\n",
            "[98 | 174.32] loss=2.66 avg=2.86\n",
            "[99 | 175.98] loss=2.66 avg=2.85\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " women like this.\n",
            "\n",
            "One is the woman who goes to the bathroom, and the next is her friend who's in the shower with her. The first two women are in very much different social standing, so why do we feel like two different people in your case? It can make all the difference for who you're attracted to at the end of the day.\n",
            "\n",
            "I've always told people not to judge myself on how I look, particularly my looks. My favorite quote by Maya Angelou in her book, I think, was, that you can look at your face but you won't be able to look at my face. If you think I look fat, then I look fat. My boyfriend is bigger than me. I look at him and think, Oh yes, my ass.\n",
            "\n",
            "People say, That's hot, but I can't handle that and so I'm not into that. I don't have my own personal version of it. It's cool to be yourself, just not everybody else's. I don't think anybody has their own version of it for a lot of people.\n",
            "\n",
            "I had a friend who grew up in New York City very wealthy. And when we grew up, every time he used the word \"vulture,\" my friend would just go, \"You know what? I'm not a vulture!\" He knew that, but he kept doing it, like I think that we all do. You can be very rich without making things seem like they're not about you.\n",
            "\n",
            "This is the weirdest story: On TV, you're like a lot of people who were already successful; it's a huge part of the fun of this show. It's a weird show about you.\n",
            "\n",
            "Yeah, not in any way that a lot of people would find fun. Like, it's an art show, but it's also a show about our lives.\n",
            "\n",
            "I like doing show reviews as a way of saying, \"This is a really, really good show.\"\n",
            "\n",
            "It does feel a little weird being a TV critic.\n",
            "\n",
            "I'm a little concerned that it gives you a license to say anything you please about your character. When I was growing up watching TV, it was totally all about my parents and what they thought about me and did what they thought about me. If something was not working, I was the voice of reason. It was really hard to let go of that.\n",
            "\n",
            "On the other hand, that said, there is something about having a show like this that helps you see the world through the eyes of a girl who might not know about the show. It's very rare to do such things on a show, and for some girls it means that things will get a little real, especially with what could be a really weird experience in a public place like a crowded place.\n",
            "\n",
            "It's definitely a weird experience, but it also makes for really cute and silly moments for me, both as a regular human being. The second it ends — I don't know! [Laughs] — I'll be on all the best parts, and the weird part of it is, I don't have to say any more. [Laughs]\n",
            "\n",
            "Your mom, as an actress, was a huge influence. Did you read a lot of memoirs and TV essays by real people with real lives?\n",
            "\n",
            "I love memoirs. [Stops] It's kind of weird — I think I probably should have said all about myself.\n",
            "\n",
            "Have you been paying too much attention to TV?\n",
            "\n",
            "Not very much. It seems to just be what I watched on the weekends. I have a weird memory of going into my mom's dressing room and seeing the one thing on the dress: All the covers from the movies I loved were all on the dress. Then my mom says, \"What about that dress?\" and I say it's not on the dress that I really wear. I didn't know that meant something but then the thing starts to look like that and all these great dresses were not on it, I remember that.\n",
            "\n",
            "You mentioned your mom. How much of who your mother is and why do you think about her so much now?\n",
            "\n",
            "I've met a lot of different kinds of people in my life. I met a lot of different people that I didn't know existed. I met a lot of different people who lived this life that I don't get to experience. Like, in college and my friends. I met a lot of interesting people, like [the author] Caitlin Moran, who's a super cool person, but a lot of the time I just have this sense that she's not really who she looks like in that way. I know. I would like to find somebody who looks like her, but I do miss her more than anything.\n",
            "\n",
            "One of the things you're not known for these days, to me, is how you get across the lines. So I ask you, Why do you do What?\n",
            "\n",
            "\n",
            "[100 | 203.19] loss=2.62 avg=2.85\n",
            "[101 | 204.85] loss=3.03 avg=2.85\n",
            "[102 | 206.53] loss=2.58 avg=2.85\n",
            "[103 | 208.20] loss=2.42 avg=2.84\n",
            "[104 | 209.87] loss=2.57 avg=2.84\n",
            "[105 | 211.54] loss=2.94 avg=2.84\n",
            "[106 | 213.21] loss=2.52 avg=2.84\n",
            "[107 | 214.88] loss=2.39 avg=2.83\n",
            "[108 | 216.55] loss=2.86 avg=2.83\n",
            "[109 | 218.22] loss=2.39 avg=2.82\n",
            "[110 | 219.89] loss=2.82 avg=2.82\n",
            "[111 | 221.56] loss=2.83 avg=2.82\n",
            "[112 | 223.23] loss=2.36 avg=2.82\n",
            "[113 | 224.90] loss=2.70 avg=2.81\n",
            "[114 | 226.57] loss=2.61 avg=2.81\n",
            "[115 | 228.24] loss=2.40 avg=2.80\n",
            "[116 | 229.91] loss=3.07 avg=2.81\n",
            "[117 | 231.58] loss=2.64 avg=2.81\n",
            "[118 | 233.25] loss=2.60 avg=2.80\n",
            "[119 | 234.92] loss=2.45 avg=2.80\n",
            "[120 | 236.60] loss=3.17 avg=2.80\n",
            "[121 | 238.28] loss=3.22 avg=2.81\n",
            "[122 | 239.95] loss=2.90 avg=2.81\n",
            "[123 | 241.63] loss=2.27 avg=2.80\n",
            "[124 | 243.31] loss=2.35 avg=2.80\n",
            "[125 | 244.99] loss=2.83 avg=2.80\n",
            "[126 | 246.67] loss=2.68 avg=2.80\n",
            "[127 | 248.35] loss=3.01 avg=2.80\n",
            "[128 | 250.04] loss=2.76 avg=2.80\n",
            "[129 | 251.71] loss=2.70 avg=2.80\n",
            "[130 | 253.38] loss=2.72 avg=2.80\n",
            "[131 | 255.05] loss=2.76 avg=2.80\n",
            "[132 | 256.72] loss=3.25 avg=2.80\n",
            "[133 | 258.40] loss=3.36 avg=2.81\n",
            "[134 | 260.07] loss=2.59 avg=2.81\n",
            "[135 | 261.74] loss=2.61 avg=2.80\n",
            "[136 | 263.42] loss=2.35 avg=2.80\n",
            "[137 | 265.09] loss=2.71 avg=2.80\n",
            "[138 | 266.76] loss=2.99 avg=2.80\n",
            "[139 | 268.44] loss=3.06 avg=2.80\n",
            "[140 | 270.11] loss=2.42 avg=2.80\n",
            "[141 | 271.78] loss=2.71 avg=2.80\n",
            "[142 | 273.46] loss=2.26 avg=2.79\n",
            "[143 | 275.13] loss=2.31 avg=2.78\n",
            "[144 | 276.81] loss=2.96 avg=2.78\n",
            "[145 | 278.48] loss=2.04 avg=2.78\n",
            "[146 | 280.16] loss=3.02 avg=2.78\n",
            "[147 | 281.82] loss=2.99 avg=2.78\n",
            "[148 | 283.49] loss=2.31 avg=2.77\n",
            "[149 | 285.16] loss=2.66 avg=2.77\n",
            "[150 | 286.83] loss=2.51 avg=2.77\n",
            "[151 | 288.50] loss=2.25 avg=2.76\n",
            "[152 | 290.17] loss=2.94 avg=2.77\n",
            "[153 | 291.84] loss=2.82 avg=2.77\n",
            "[154 | 293.51] loss=2.42 avg=2.76\n",
            "[155 | 295.18] loss=3.10 avg=2.77\n",
            "[156 | 296.84] loss=3.29 avg=2.77\n",
            "[157 | 298.52] loss=2.23 avg=2.77\n",
            "[158 | 300.19] loss=2.61 avg=2.76\n",
            "[159 | 301.86] loss=2.83 avg=2.76\n",
            "[160 | 303.53] loss=2.49 avg=2.76\n",
            "[161 | 305.20] loss=2.41 avg=2.76\n",
            "[162 | 306.87] loss=2.44 avg=2.75\n",
            "[163 | 308.54] loss=3.17 avg=2.76\n",
            "[164 | 310.21] loss=3.10 avg=2.76\n",
            "[165 | 311.88] loss=3.03 avg=2.77\n",
            "[166 | 313.55] loss=3.23 avg=2.77\n",
            "[167 | 315.22] loss=2.90 avg=2.77\n",
            "[168 | 316.89] loss=2.10 avg=2.77\n",
            "[169 | 318.56] loss=2.34 avg=2.76\n",
            "[170 | 320.23] loss=2.83 avg=2.76\n",
            "[171 | 321.89] loss=3.03 avg=2.76\n",
            "[172 | 323.57] loss=2.10 avg=2.76\n",
            "[173 | 325.25] loss=2.66 avg=2.75\n",
            "[174 | 326.92] loss=2.95 avg=2.76\n",
            "[175 | 328.59] loss=3.15 avg=2.76\n",
            "[176 | 330.26] loss=3.09 avg=2.77\n",
            "[177 | 331.93] loss=2.82 avg=2.77\n",
            "[178 | 333.60] loss=2.69 avg=2.77\n",
            "[179 | 335.28] loss=2.98 avg=2.77\n",
            "[180 | 336.95] loss=2.40 avg=2.76\n",
            "[181 | 338.62] loss=2.55 avg=2.76\n",
            "[182 | 340.30] loss=3.27 avg=2.77\n",
            "[183 | 341.97] loss=2.69 avg=2.77\n",
            "[184 | 343.64] loss=3.05 avg=2.77\n",
            "[185 | 345.31] loss=2.25 avg=2.76\n",
            "[186 | 346.98] loss=2.64 avg=2.76\n",
            "[187 | 348.64] loss=2.50 avg=2.76\n",
            "[188 | 350.31] loss=2.66 avg=2.76\n",
            "[189 | 351.97] loss=2.16 avg=2.75\n",
            "[190 | 353.64] loss=2.70 avg=2.75\n",
            "[191 | 355.31] loss=1.96 avg=2.74\n",
            "[192 | 356.98] loss=3.06 avg=2.74\n",
            "[193 | 358.65] loss=2.44 avg=2.74\n",
            "[194 | 360.32] loss=3.11 avg=2.75\n",
            "[195 | 361.99] loss=2.01 avg=2.74\n",
            "[196 | 363.66] loss=2.39 avg=2.73\n",
            "[197 | 365.33] loss=2.83 avg=2.73\n",
            "[198 | 367.00] loss=2.14 avg=2.73\n",
            "[199 | 368.67] loss=2.42 avg=2.72\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " the current crisis.\n",
            "\n",
            "The report includes a few other notable findings. First, in 2014, women were more likely to be charged for the doctor services of a primary care physician than a psychiatrist, psychologist or speech pathologist. In fact, women spent an average of $813 more each time a physician visited them and $1,091 more each time a speech pathologist or psychiatrist visited them than a doctor.\n",
            "\n",
            "Second, there are also some startling disparities in the number of medical services provided per capita. The report includes figures for 2010, but it shows that in 2012, women paid more out of pocket for a doctor visit than a man. In 2013, the gap was even wider. In 2014, women paid an average of $1,070 more out of pocket for a primary care visit, compared to $1,073 for a psychiatric visit and $1,081 for a psychologist visit.\n",
            "\n",
            "Overall, the authors say, the gap between men and women in obtaining medical care is widening, \"and the gap is growing ever wider.\" The authors also note that in a survey of doctors nationwide by the organization Medscape, more than 60 percent of respondents said they would be forced to close their practices if the 2016 census is accurate.\n",
            "\n",
            "The report is an update to a 2009 study that used the same census data to estimate that the U.S. had more than 8 million women without primary care physicians and 7 million men without mental health services. The authors of the new report say that the difference between the two data sets is likely due to the fact that men are treated by their doctors more frequently than do women, and the same medical treatment can be given to both men and women, even if treatment is not necessarily more expensive.\n",
            "\n",
            "\"The findings suggest that despite having the same physician population, these services are being delivered to more men and women at a cost with which there may not be an appreciable financial difference,\" the authors write. \"With such an appreciable cost difference, it may be worthwhile to consider expanding the access of primary care in order to address this disparity.\"\n",
            "\n",
            "Not so fast, say the authors. \"The results do not justify efforts to change access patterns among physicians or to increase access for individuals seeking such services,\" they write. The authors suggest that the authors' calculations should be considered only as a starting point and not as a substitute for scientific and clinical research into the true scope or reasons for disparities between the U.S. and other industrialized nations. That research is, unfortunately, ongoing and not yet definitive.\n",
            "\n",
            "The biggest challenge the authors describe in the section on how to close the gap has to do with access to medical care. While the US accounts for about one-third of the world's population and one-quarter of world GDP, according to the report, women face an estimated 15 percent gap in access to routine health care services. That gender wage gap could be down to health care as a whole, and more specifically, the number of providers needed to treat various health conditions.\n",
            "\n",
            "\"As U.S. society becomes increasingly mobile over the next 50 years, we will become more unequal in our treatment of individuals with a wide variety of health conditions. For example, in the U.S. population with a medical problem, access to care will be reduced because providers that can treat the individual can tend to others, but when someone with a disability is treated, there may not be a similar level of care,\" Dr. Tumlin wrote. \"These issues require policymakers to identify and address the causes of this gender wage gap in the US.\"<|endoftext|>This is part one of my series on why your health care provider could not have told you this: you are a carrier.\n",
            "\n",
            "You might be thinking \"how does it matter if I had breast cancer or not?! It didn't stop me from caring for myself, I AM a carrier! I deserve to be treated for my cancer, in the way that every other human being should be taken advantage of every day! This is a basic biological truth, people! Not the crazy kind!\"\n",
            "\n",
            "There's no such thing as the crazy kind. Everyone is made up of an organ system consisting of a nervous system, an immune system (both adaptive and malignant), a muscular system, a circulatory system, and organs called blood vessels and bone marrow. Every organ system in our bodies is made of cells that will do what they do because they are made of living, functioning cells, including our own bodies. All of these cells need our energy to function, for survival.\n",
            "\n",
            "And what we need to maintain life is a healthy diet. We need to have enough food to deal with the normal fluctuations of our bodies which in turn requires adequate energy intake. I'll break food down into components and tell you what we have on a daily basis (there are a lot of things people can't usually tell you in a short space of time—like what's on their calendar or when we're celebrating a birthday!).\n",
            "\n",
            "In the short term,\n",
            "\n",
            "[200 | 393.00] loss=2.74 avg=2.72\n",
            "[201 | 394.66] loss=2.46 avg=2.72\n",
            "[202 | 396.31] loss=1.95 avg=2.71\n",
            "[203 | 397.98] loss=2.25 avg=2.71\n",
            "[204 | 399.65] loss=2.40 avg=2.70\n",
            "[205 | 401.31] loss=2.46 avg=2.70\n",
            "[206 | 402.98] loss=2.75 avg=2.70\n",
            "[207 | 404.65] loss=2.66 avg=2.70\n",
            "[208 | 406.33] loss=2.94 avg=2.70\n",
            "[209 | 407.99] loss=2.66 avg=2.70\n",
            "[210 | 409.65] loss=2.95 avg=2.71\n",
            "[211 | 411.32] loss=2.55 avg=2.70\n",
            "[212 | 412.98] loss=2.49 avg=2.70\n",
            "[213 | 414.65] loss=2.66 avg=2.70\n",
            "[214 | 416.32] loss=1.80 avg=2.69\n",
            "[215 | 417.98] loss=2.38 avg=2.69\n",
            "[216 | 419.65] loss=2.29 avg=2.68\n",
            "[217 | 421.31] loss=2.91 avg=2.69\n",
            "[218 | 422.97] loss=2.78 avg=2.69\n",
            "[219 | 424.64] loss=2.89 avg=2.69\n",
            "[220 | 426.31] loss=2.91 avg=2.69\n",
            "[221 | 427.98] loss=2.99 avg=2.69\n",
            "[222 | 429.65] loss=2.23 avg=2.69\n",
            "[223 | 431.32] loss=2.55 avg=2.69\n",
            "[224 | 432.98] loss=2.14 avg=2.68\n",
            "[225 | 434.65] loss=2.75 avg=2.68\n",
            "[226 | 436.31] loss=2.15 avg=2.68\n",
            "[227 | 437.98] loss=2.64 avg=2.68\n",
            "[228 | 439.65] loss=2.18 avg=2.67\n",
            "[229 | 441.31] loss=2.35 avg=2.67\n",
            "[230 | 442.97] loss=2.08 avg=2.66\n",
            "[231 | 444.63] loss=3.09 avg=2.67\n",
            "[232 | 446.30] loss=2.37 avg=2.66\n",
            "[233 | 447.96] loss=2.72 avg=2.66\n",
            "[234 | 449.63] loss=2.77 avg=2.66\n",
            "[235 | 451.29] loss=1.96 avg=2.66\n",
            "[236 | 452.95] loss=3.05 avg=2.66\n",
            "[237 | 454.61] loss=2.80 avg=2.66\n",
            "[238 | 456.28] loss=2.65 avg=2.66\n",
            "[239 | 457.94] loss=2.27 avg=2.66\n",
            "[240 | 459.60] loss=1.81 avg=2.65\n",
            "[241 | 461.27] loss=2.35 avg=2.64\n",
            "[242 | 462.94] loss=2.25 avg=2.64\n",
            "[243 | 464.60] loss=2.70 avg=2.64\n",
            "[244 | 466.27] loss=2.67 avg=2.64\n",
            "[245 | 467.94] loss=2.02 avg=2.63\n",
            "[246 | 469.61] loss=2.69 avg=2.64\n",
            "[247 | 471.27] loss=2.77 avg=2.64\n",
            "[248 | 472.93] loss=2.72 avg=2.64\n",
            "[249 | 474.61] loss=2.85 avg=2.64\n",
            "[250 | 476.27] loss=2.67 avg=2.64\n",
            "[251 | 477.93] loss=2.71 avg=2.64\n",
            "[252 | 479.60] loss=2.46 avg=2.64\n",
            "[253 | 481.27] loss=2.10 avg=2.63\n",
            "[254 | 482.94] loss=2.33 avg=2.63\n",
            "[255 | 484.61] loss=2.53 avg=2.63\n",
            "[256 | 486.28] loss=2.95 avg=2.63\n",
            "[257 | 487.95] loss=2.59 avg=2.63\n",
            "[258 | 489.62] loss=2.74 avg=2.63\n",
            "[259 | 491.29] loss=3.17 avg=2.64\n",
            "[260 | 492.96] loss=1.92 avg=2.63\n",
            "[261 | 494.63] loss=2.31 avg=2.63\n",
            "[262 | 496.30] loss=2.78 avg=2.63\n",
            "[263 | 497.97] loss=2.24 avg=2.63\n",
            "[264 | 499.64] loss=2.72 avg=2.63\n",
            "[265 | 501.31] loss=2.33 avg=2.62\n",
            "[266 | 502.98] loss=2.38 avg=2.62\n",
            "[267 | 504.65] loss=1.99 avg=2.61\n",
            "[268 | 506.32] loss=2.18 avg=2.61\n",
            "[269 | 507.99] loss=2.53 avg=2.61\n",
            "[270 | 509.66] loss=2.15 avg=2.60\n",
            "[271 | 511.33] loss=2.42 avg=2.60\n",
            "[272 | 513.00] loss=3.17 avg=2.61\n",
            "[273 | 514.65] loss=2.24 avg=2.60\n",
            "[274 | 516.32] loss=2.33 avg=2.60\n",
            "[275 | 517.98] loss=2.55 avg=2.60\n",
            "[276 | 519.65] loss=2.96 avg=2.60\n",
            "[277 | 521.32] loss=2.51 avg=2.60\n",
            "[278 | 523.00] loss=3.07 avg=2.61\n",
            "[279 | 524.66] loss=3.00 avg=2.61\n",
            "[280 | 526.34] loss=3.25 avg=2.62\n",
            "[281 | 528.01] loss=2.89 avg=2.62\n",
            "[282 | 529.68] loss=2.37 avg=2.62\n",
            "[283 | 531.36] loss=2.69 avg=2.62\n",
            "[284 | 533.03] loss=2.40 avg=2.62\n",
            "[285 | 534.71] loss=2.58 avg=2.62\n",
            "[286 | 536.37] loss=2.75 avg=2.62\n",
            "[287 | 538.04] loss=2.62 avg=2.62\n",
            "[288 | 539.71] loss=2.64 avg=2.62\n",
            "[289 | 541.39] loss=2.82 avg=2.62\n",
            "[290 | 543.06] loss=2.79 avg=2.62\n",
            "[291 | 544.74] loss=3.09 avg=2.63\n",
            "[292 | 546.41] loss=2.37 avg=2.62\n",
            "[293 | 548.07] loss=1.86 avg=2.62\n",
            "[294 | 549.74] loss=2.51 avg=2.62\n",
            "[295 | 551.41] loss=2.56 avg=2.62\n",
            "[296 | 553.07] loss=2.73 avg=2.62\n",
            "[297 | 554.75] loss=2.66 avg=2.62\n",
            "[298 | 556.42] loss=1.97 avg=2.61\n",
            "[299 | 558.09] loss=2.18 avg=2.61\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " from all of them to be a part of the new group? What could they have been, and from what point did they change their identities? What had they experienced that made them want to leave?\n",
            "\n",
            "The result was a fascinating tour of the very fabric of human relationships—the way you and your partner will get along, what kind of language we speak, what kind of ideas about sex we share—that, surprisingly, was quite rare. That the new research is so thoroughly researched may make this more valuable than it is. But I also wonder what we might miss if we ignore it. The researchers wanted to find out if the new data would help us to predict, for instance, how much a sexual partner would pay for sex, especially if you know the partner personally. Would we just assume it would be cheap?\n",
            "\n",
            "I spoke to Sarah, a 35-year-old college student from Boston and the only person in the study to ask the right questions to get the most definitive answer possible: Is there a level of intimacy you consider appropriate for sexual partners? The idea that sexual intimacy needs to be higher than an average person might seem like a pretty extreme position to take to find out if you and your partner really share the same sort of bond. But Sarah believes that there's a place for intimacy on the spectrum of a good partner (a higher level would involve more sharing), but that we shouldn't put too much importance on where sex goes. She also notes that in her experience, many women choose a slightly higher level of intimacy than others, which might be partly because we do our best to ensure our partners have partners who are high in emotional intimacy.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "You can still read about the findings in Science but here're a few questions that may be getting left on your Internet calendar:\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "It turns out that when you look at the face of someone who's already had sex or just had sex with someone who's been using a condom (the face of someone not having any problems with STIs has been removed), more time is spent talking about each person than the face of a condom and less time is spent on the person who has engaged in all the different things you consider to be normal things to talk about after getting a high. This is in an experiment that was done in three different countries and was described as a \"queer condom study.\" The study was funded by the US Centers for Disease Control and Prevention, but even so, it's a bit surprising that this study at all might have an impact on how we talk about the differences between a high and a low.\n",
            "\n",
            "The study came about because the Australian government decided that it wanted to know if people were talking about condoms as a sex talk show did and whether there were differences about who was talking about the difference between condom use and using a condom. While this experiment was a success, it also showed that there were a few ways that people talk about the differences between a low and a high—you talk about the differences between a low and a high, for instance—yet you don't have the same conversation about the differences between a high and an average. One reason that this conversation was so difficult was that people were also talking about the differences between a high and a low for a different sex act than someone engaging in a normal behavior. So you might be talking about the difference between having sex and not having sex because you've been tested positive for chlamydia, but not the differences between having a high and a low because your partner is high on an antihistamine or you're just not into sex.\n",
            "\n",
            "In one of the experiments, one researcher recruited a bunch of people who worked in latex condoms, then asked them to get together to talk about their experiences. The participants reported that the differences between a low and a high were:\n",
            "\n",
            "Most participants engaged in all the activities that would be considered a normal activity to talk about like reading about it, not getting pregnant, or not being a burden. But then there were some that had the distinction between being a low and being a low, with the participants saying that they would discuss other things as well. Here's an example:\n",
            "\n",
            "There were some participants who said they had a high level of intimacy compared to others whose average was actually the lowest. They also had some participants who said that their average was lower than people they have sex with regularly, but then they said that they would get along better with different people—like people they've never met or have worked with as part of the research—which in some instances might mean that the person is higher in emotional intimacy. And, of course, there were some participants who said that it was just a matter of \"getting back to normal\" with all the other things you need to talk about when engaging in sex. But if you look at the faces of people engaged in one of the activities where you're discussing each person individually, you would not see the faces of any high or low people.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "This story is the second in a series in which\n",
            "\n",
            "[300 | 582.28] loss=2.27 avg=2.60\n",
            "[301 | 583.95] loss=2.52 avg=2.60\n",
            "[302 | 585.62] loss=3.05 avg=2.61\n",
            "[303 | 587.29] loss=2.71 avg=2.61\n",
            "[304 | 588.96] loss=2.27 avg=2.60\n",
            "[305 | 590.60] loss=2.76 avg=2.60\n",
            "[306 | 592.26] loss=3.13 avg=2.61\n",
            "[307 | 593.92] loss=2.34 avg=2.61\n",
            "[308 | 595.59] loss=2.48 avg=2.61\n",
            "[309 | 597.25] loss=2.37 avg=2.60\n",
            "[310 | 598.91] loss=2.83 avg=2.61\n",
            "[311 | 600.57] loss=2.21 avg=2.60\n",
            "[312 | 602.23] loss=1.92 avg=2.59\n",
            "[313 | 603.89] loss=2.00 avg=2.59\n",
            "[314 | 605.54] loss=3.10 avg=2.59\n",
            "[315 | 607.20] loss=2.89 avg=2.60\n",
            "[316 | 608.86] loss=1.88 avg=2.59\n",
            "[317 | 610.51] loss=2.29 avg=2.59\n",
            "[318 | 612.17] loss=2.35 avg=2.58\n",
            "[319 | 613.83] loss=2.52 avg=2.58\n",
            "[320 | 615.50] loss=2.64 avg=2.58\n",
            "[321 | 617.16] loss=2.13 avg=2.58\n",
            "[322 | 618.83] loss=2.45 avg=2.58\n",
            "[323 | 620.50] loss=2.20 avg=2.57\n",
            "[324 | 622.16] loss=2.35 avg=2.57\n",
            "[325 | 623.83] loss=2.72 avg=2.57\n",
            "[326 | 625.50] loss=2.28 avg=2.57\n",
            "[327 | 627.18] loss=2.67 avg=2.57\n",
            "[328 | 628.84] loss=2.01 avg=2.57\n",
            "[329 | 630.51] loss=3.17 avg=2.57\n",
            "[330 | 632.19] loss=2.24 avg=2.57\n",
            "[331 | 633.86] loss=2.96 avg=2.57\n",
            "[332 | 635.53] loss=2.20 avg=2.57\n",
            "[333 | 637.20] loss=2.21 avg=2.56\n",
            "[334 | 638.87] loss=2.53 avg=2.56\n",
            "[335 | 640.54] loss=2.91 avg=2.57\n",
            "[336 | 642.21] loss=2.42 avg=2.57\n",
            "[337 | 643.88] loss=2.35 avg=2.56\n",
            "[338 | 645.55] loss=2.54 avg=2.56\n",
            "[339 | 647.22] loss=2.41 avg=2.56\n",
            "[340 | 648.89] loss=2.34 avg=2.56\n",
            "[341 | 650.57] loss=3.35 avg=2.57\n",
            "[342 | 652.25] loss=2.31 avg=2.57\n",
            "[343 | 653.91] loss=2.41 avg=2.56\n",
            "[344 | 655.59] loss=2.21 avg=2.56\n",
            "[345 | 657.26] loss=2.20 avg=2.56\n",
            "[346 | 658.93] loss=2.46 avg=2.56\n",
            "[347 | 660.60] loss=2.60 avg=2.56\n",
            "[348 | 662.27] loss=2.38 avg=2.55\n",
            "[349 | 663.95] loss=2.11 avg=2.55\n",
            "[350 | 665.62] loss=2.38 avg=2.55\n",
            "[351 | 667.30] loss=2.33 avg=2.55\n",
            "[352 | 668.97] loss=1.60 avg=2.54\n",
            "[353 | 670.65] loss=2.68 avg=2.54\n",
            "[354 | 672.32] loss=3.07 avg=2.54\n",
            "[355 | 674.00] loss=2.67 avg=2.54\n",
            "[356 | 675.67] loss=3.15 avg=2.55\n",
            "[357 | 677.34] loss=2.72 avg=2.55\n",
            "[358 | 679.01] loss=2.28 avg=2.55\n",
            "[359 | 680.69] loss=2.29 avg=2.55\n",
            "[360 | 682.38] loss=2.08 avg=2.54\n",
            "[361 | 684.05] loss=2.12 avg=2.54\n",
            "[362 | 685.72] loss=1.83 avg=2.53\n",
            "[363 | 687.39] loss=2.04 avg=2.53\n",
            "[364 | 689.07] loss=2.42 avg=2.52\n",
            "[365 | 690.75] loss=2.64 avg=2.53\n",
            "[366 | 692.42] loss=2.02 avg=2.52\n",
            "[367 | 694.09] loss=2.87 avg=2.52\n",
            "[368 | 695.78] loss=2.33 avg=2.52\n",
            "[369 | 697.45] loss=2.53 avg=2.52\n",
            "[370 | 699.13] loss=1.98 avg=2.52\n",
            "[371 | 700.81] loss=1.89 avg=2.51\n",
            "[372 | 702.48] loss=2.00 avg=2.50\n",
            "[373 | 704.17] loss=2.38 avg=2.50\n",
            "[374 | 705.84] loss=1.92 avg=2.50\n",
            "[375 | 707.51] loss=1.68 avg=2.49\n",
            "[376 | 709.19] loss=1.97 avg=2.48\n",
            "[377 | 710.86] loss=2.17 avg=2.48\n",
            "[378 | 712.55] loss=2.51 avg=2.48\n",
            "[379 | 714.22] loss=1.73 avg=2.47\n",
            "[380 | 715.89] loss=2.64 avg=2.48\n",
            "[381 | 717.56] loss=2.12 avg=2.47\n",
            "[382 | 719.22] loss=1.63 avg=2.46\n",
            "[383 | 720.90] loss=2.18 avg=2.46\n",
            "[384 | 722.57] loss=2.94 avg=2.46\n",
            "[385 | 724.24] loss=2.02 avg=2.46\n",
            "[386 | 725.91] loss=2.49 avg=2.46\n",
            "[387 | 727.58] loss=2.88 avg=2.46\n",
            "[388 | 729.25] loss=2.46 avg=2.46\n",
            "[389 | 730.92] loss=2.52 avg=2.47\n",
            "[390 | 732.59] loss=1.95 avg=2.46\n",
            "[391 | 734.26] loss=1.99 avg=2.46\n",
            "[392 | 735.93] loss=2.16 avg=2.45\n",
            "[393 | 737.60] loss=2.30 avg=2.45\n",
            "[394 | 739.28] loss=2.14 avg=2.45\n",
            "[395 | 740.95] loss=2.19 avg=2.44\n",
            "[396 | 742.62] loss=2.28 avg=2.44\n",
            "[397 | 744.29] loss=2.58 avg=2.44\n",
            "[398 | 745.96] loss=2.35 avg=2.44\n",
            "[399 | 747.64] loss=2.30 avg=2.44\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " from a few points, then I could add on a few degrees, and then maybe I could change the formula. But if you want this to work, you need to start with a really small number. And this is what we are left with, except instead of just some number that can be changed, perhaps some number that isn't the perfect number, but still quite small but very important, that can make a difference…\n",
            "\n",
            "As I said, I know this might be really weird, and it could just be me doing this kind of math-y fantasy. But I wonder if there are people out there who are experiencing, for better or worse, a sort of existential crisis related to what I just wrote. The thoughts are so similar to my own, it is hard not to wonder if there could also be an underlying neurological reason. Or maybe it is just me, but just a hint of melancholy over a bad life?\n",
            "\n",
            "Advertisement\n",
            "\n",
            "\n",
            "I have to give credit where credit is due. What I am getting at here is that, maybe, a little bit of research might actually turn a little bit of a profit. Maybe my lack of research was in the right place at the right time, giving me the advantage of understanding the basic concept. Maybe, because I was having such similar thoughts, if I knew exactly what to look for, a whole new set of questions and ideas would be possible.\n",
            "\n",
            "My brain is not particularly big on math or science. I love my family, but I am neither very mathematical nor very scientific (I never go to school for either, for example). My thoughts always revolve around what I am reading or watching, which often involves a great big question mark after a space. I might even be the weirdest person on earth for being so obsessed with math and science. I try not to think about it as a handicap, but I might be one of a few people in the entire world who are having these problems as a result of the research we are doing in AI. Perhaps that has something to do with the fact that in a field where so much focus is on the algorithms, the algorithms are the problem, the AI is the solution, and the AI companies are all around us, I might simply be the weird one.\n",
            "\n",
            "But maybe these sorts of things are not all that new. When I was a kid, we used to say that if a girl had short hair, she was probably not a girl. If she had dark skin, she probably was not black. If she had brown hair, she probably was not a woman. So I was kind of aware, maybe even a little excited, of these things.\n",
            "\n",
            "These things have always fascinated me. It seems weird, and a little strange, to me, to know myself as a bit of an anomaly. But the more I learn about the inner workings of the brain, and the ways in which human beings think, feel, and communicate, the more I realize that the very thing that gives me this weird feeling of being stuck in a bad life is precisely a tiny bit of progress in the field of mental health that would allow us to be really smart.\n",
            "\n",
            "At its heart, mental health is about our expectations—in the form of medical theories that we have to live by, or the kind of therapies we decide to try, or the care we give to our loved ones—of how well people can function as humans after a major trauma or illness, or to live without much or all of a limb or organ. As I have written before, mental health is in part about changing that expectation, or even abolishing it.\n",
            "\n",
            "In the past couple of decades, we have learned a lot about how to treat trauma and trauma-exposed people. We have learned how to change the expectations of what it means to be \"trauma-free.\" And we have learned how to change the way we think about a \"normal life.\" The good news, for all of us, is that we just need new, hard data to prove any of it—that, if we just keep going, one day, no one will have to live in some kind of horrible, self-pitying self-doubting, self-disingardic life.\n",
            "\n",
            "I am not suggesting this is the end of mental health research. Not yet. But it is possible that, as I wrote on Sunday, if we stop worrying about how much we know about trauma and trauma-exposed people, and start thinking about what we do know, we may be on course to more profound changes happening as a direct result of those efforts.\n",
            "\n",
            "I know this because—just as a kid, back when I remember this stuff, because it seemed like the subject all the time—I spent a lot of time at our family's local mental hospital. We weren't at the hospital because we had any mental health problems; we just happened to be staying at the hospital for the summer, which meant that everyone who wasn't in the hospital was off\n",
            "\n",
            "[400 | 771.90] loss=2.09 avg=2.44\n",
            "[401 | 773.56] loss=2.27 avg=2.44\n",
            "[402 | 775.23] loss=2.17 avg=2.43\n",
            "[403 | 776.89] loss=1.75 avg=2.43\n",
            "[404 | 778.55] loss=1.99 avg=2.42\n",
            "[405 | 780.21] loss=2.73 avg=2.43\n",
            "[406 | 781.87] loss=2.09 avg=2.42\n",
            "[407 | 783.54] loss=2.50 avg=2.42\n",
            "[408 | 785.20] loss=2.83 avg=2.43\n",
            "[409 | 786.86] loss=2.64 avg=2.43\n",
            "[410 | 788.50] loss=1.77 avg=2.42\n",
            "[411 | 790.16] loss=2.79 avg=2.43\n",
            "[412 | 791.82] loss=3.09 avg=2.43\n",
            "[413 | 793.48] loss=2.22 avg=2.43\n",
            "[414 | 795.15] loss=2.23 avg=2.43\n",
            "[415 | 796.81] loss=1.98 avg=2.42\n",
            "[416 | 798.47] loss=2.21 avg=2.42\n",
            "[417 | 800.14] loss=2.92 avg=2.43\n",
            "[418 | 801.80] loss=2.22 avg=2.43\n",
            "[419 | 803.46] loss=3.10 avg=2.43\n",
            "[420 | 805.12] loss=2.04 avg=2.43\n",
            "[421 | 806.79] loss=2.00 avg=2.42\n",
            "[422 | 808.47] loss=2.33 avg=2.42\n",
            "[423 | 810.13] loss=1.94 avg=2.42\n",
            "[424 | 811.79] loss=2.08 avg=2.41\n",
            "[425 | 813.46] loss=2.96 avg=2.42\n",
            "[426 | 815.13] loss=1.69 avg=2.41\n",
            "[427 | 816.80] loss=2.00 avg=2.41\n",
            "[428 | 818.47] loss=2.58 avg=2.41\n",
            "[429 | 820.13] loss=2.49 avg=2.41\n",
            "[430 | 821.80] loss=1.57 avg=2.40\n",
            "[431 | 823.47] loss=1.47 avg=2.39\n",
            "[432 | 825.14] loss=1.94 avg=2.39\n",
            "[433 | 826.81] loss=2.88 avg=2.39\n",
            "[434 | 828.48] loss=2.21 avg=2.39\n",
            "[435 | 830.15] loss=2.39 avg=2.39\n",
            "[436 | 831.82] loss=2.57 avg=2.39\n",
            "[437 | 833.49] loss=2.36 avg=2.39\n",
            "[438 | 835.16] loss=1.99 avg=2.39\n",
            "[439 | 836.83] loss=2.41 avg=2.39\n",
            "[440 | 838.50] loss=1.47 avg=2.38\n",
            "[441 | 840.17] loss=1.56 avg=2.37\n",
            "[442 | 841.85] loss=1.55 avg=2.36\n",
            "[443 | 843.52] loss=1.86 avg=2.36\n",
            "[444 | 845.20] loss=2.01 avg=2.35\n",
            "[445 | 846.87] loss=1.74 avg=2.35\n",
            "[446 | 848.55] loss=1.77 avg=2.34\n",
            "[447 | 850.22] loss=2.17 avg=2.34\n",
            "[448 | 851.89] loss=2.08 avg=2.34\n",
            "[449 | 853.57] loss=2.40 avg=2.34\n",
            "[450 | 855.25] loss=2.19 avg=2.34\n",
            "[451 | 856.93] loss=3.09 avg=2.35\n",
            "[452 | 858.61] loss=2.30 avg=2.34\n",
            "[453 | 860.29] loss=2.13 avg=2.34\n",
            "[454 | 861.97] loss=2.25 avg=2.34\n",
            "[455 | 863.64] loss=3.07 avg=2.35\n",
            "[456 | 865.32] loss=2.43 avg=2.35\n",
            "[457 | 866.99] loss=2.23 avg=2.35\n",
            "[458 | 868.66] loss=2.48 avg=2.35\n",
            "[459 | 870.33] loss=2.02 avg=2.35\n",
            "[460 | 872.00] loss=1.86 avg=2.34\n",
            "[461 | 873.68] loss=2.63 avg=2.34\n",
            "[462 | 875.35] loss=1.90 avg=2.34\n",
            "[463 | 877.04] loss=2.42 avg=2.34\n",
            "[464 | 878.71] loss=2.11 avg=2.34\n",
            "[465 | 880.39] loss=2.21 avg=2.34\n",
            "[466 | 882.06] loss=1.97 avg=2.33\n",
            "[467 | 883.75] loss=1.78 avg=2.33\n",
            "[468 | 885.43] loss=1.31 avg=2.32\n",
            "[469 | 887.11] loss=3.00 avg=2.32\n",
            "[470 | 888.80] loss=2.58 avg=2.33\n",
            "[471 | 890.47] loss=2.18 avg=2.33\n",
            "[472 | 892.14] loss=1.61 avg=2.32\n",
            "[473 | 893.82] loss=2.38 avg=2.32\n",
            "[474 | 895.50] loss=1.79 avg=2.31\n",
            "[475 | 897.18] loss=2.71 avg=2.32\n",
            "[476 | 898.85] loss=2.81 avg=2.32\n",
            "[477 | 900.51] loss=2.72 avg=2.33\n",
            "[478 | 902.18] loss=2.51 avg=2.33\n",
            "[479 | 903.86] loss=2.60 avg=2.33\n",
            "[480 | 905.52] loss=2.77 avg=2.34\n",
            "[481 | 907.19] loss=2.30 avg=2.34\n",
            "[482 | 908.87] loss=2.20 avg=2.33\n",
            "[483 | 910.55] loss=2.57 avg=2.34\n",
            "[484 | 912.23] loss=1.93 avg=2.33\n",
            "[485 | 913.91] loss=2.56 avg=2.33\n",
            "[486 | 915.58] loss=1.57 avg=2.33\n",
            "[487 | 917.25] loss=2.61 avg=2.33\n",
            "[488 | 918.93] loss=1.67 avg=2.32\n",
            "[489 | 920.61] loss=2.01 avg=2.32\n",
            "[490 | 922.28] loss=2.15 avg=2.32\n",
            "[491 | 923.95] loss=2.30 avg=2.32\n",
            "[492 | 925.62] loss=2.29 avg=2.32\n",
            "[493 | 927.29] loss=2.30 avg=2.32\n",
            "[494 | 928.97] loss=2.31 avg=2.32\n",
            "[495 | 930.64] loss=2.35 avg=2.32\n",
            "[496 | 932.32] loss=2.50 avg=2.32\n",
            "[497 | 934.00] loss=2.23 avg=2.32\n",
            "[498 | 935.68] loss=2.10 avg=2.32\n",
            "[499 | 937.36] loss=2.01 avg=2.31\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "rible things. But with our eyes, it might be like having an extra eye for small details.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "<|endoftext|>The UESPWiki – Your source for The Elder Scrolls since 1995\n",
            "\n",
            "This page is currently being rewritten as part of the Morrowind Character build Home Page.\n",
            "\n",
            "The page is being rewritten and checked in several stages. If you make an addition to this page, please update this template accordingly, but make sure you have observed the project guidelines. Detail Quests: written by Necroduck, not checked in\n",
            "\n",
            "The page is being rewritten and checked in a Random Events - Cleanup manner. If you make an addition to this page, please update this template accordingly, but make sure you have observed the project guidelines.\n",
            "\n",
            "Detail Walkthrough: not written\n",
            "\n",
            "Objectives [ edit ]\n",
            "\n",
            "Speak to Lady Salome, a Redguard noblewoman, at her residence, the Grand Duchess' Chapel, to request the location of the remains of a knight killed during the War of the Vindication.\n",
            "\n",
            "Detailed Walkthrough [ edit ]\n",
            "\n",
            "The scene plays out in a house at a remote location, and then the Redguard lady explains to you the reasons for bringing up the knight's name. It isn't that she cares – it isn't exactly clear that she does. She then gives you a detailed report on her conversation with Lady Salome, in which she states that she didn't find the information she was given to be particularly trustworthy (it appears to have been based on rumors from rumors rather than any proof of a grisly event). In passing, she also offers the chance of seeing some of the knight's remains, though she never tells you where they will be found.\n",
            "\n",
            "Upon reaching Lady Salome's residence, you have a choice of four options. The first option is to go through the house and ask the lady herself, Lady Salome of the House of the Dead, who will inform the player of the results of some investigation. However, there are a few problems with this approach. First, there is the fact that as of yet, Salome of the Dead has been absent when it comes to the subject of a knight missing for almost six years. Even assuming that a connection does exist, it would require the kind of high level access to royal records and military archives that you generally wouldn't have to seek, much less pay for. Second, you can't actually meet the girl involved because she is dead, along with a third knight in the line, though you certainly could, of course, and that would be more rewarding.\n",
            "\n",
            "The next option is to actually meet with Salome herself. Once again, she will insist that you talk to her at your own risk, but even if you decide to go through with the meeting, there are a few serious and likely time-consuming issues with the way in which the encounter is actually supposed to take place. First, and most obviously, Salome is dead for all of this time. I mean, what more does you want? Then there's the fact that this is Lady Salome of the House of Dead, who was supposed to be dead for several years after she was created but was brought back to life with magic by you in a different, more suitable form, making it a different character. This is not a coincidence if you want to see Salome doing anything significant, because she is the focus of this quest. It's also not a coincidence, and this would actually be more fun if Salome weren't the focus of all of this.\n",
            "\n",
            "You can certainly, however, go through with the meeting. You will then receive reports on the investigation, though your actual goal, in my opinion, is still to see the knights' remains. I have my doubts that will actually happen, though I have no doubt that they will still make a good impression. There probably is something here that needs to be verified, but I have no reason to be suspicious at this point. The only real disappointment comes in the form of having to travel, as you will have to meet up with the Lady Salome in a small village south of Solstheim where you can then meet your friends and visit a cemetery. It's actually a small one, of course, but you could still make it.<|endoftext|>A U.S. military base in Puerto Rico is currently experiencing a humanitarian crisis due to the lack of power and water. As Hurricane Maria barreled toward the island, the local military was forced to switch its primary generator – a move that will force the base to move over a week.\n",
            "\n",
            "In a statement, the U.S. Navy confirmed that the U.S. Naval Station in San Juan has switched to a backup backup generator, and that it will not be able to handle the increased number of generators due to Hurricane Maria. The Navy says it is working \"very closely\" with the Federal Emergency Management Agency and the Department of Energy to ensure that emergency personnel can continue responding to the base.\n",
            "\n",
            "On Sunday,\n",
            "\n",
            "[500 | 961.84] loss=1.77 avg=2.31\n",
            "[501 | 963.49] loss=1.97 avg=2.30\n",
            "[502 | 965.16] loss=2.48 avg=2.31\n",
            "[503 | 966.81] loss=2.46 avg=2.31\n",
            "[504 | 968.47] loss=2.57 avg=2.31\n",
            "[505 | 970.14] loss=2.40 avg=2.31\n",
            "[506 | 971.80] loss=2.75 avg=2.32\n",
            "[507 | 973.45] loss=2.76 avg=2.32\n",
            "[508 | 975.11] loss=2.59 avg=2.32\n",
            "[509 | 976.77] loss=2.49 avg=2.32\n",
            "[510 | 978.43] loss=2.49 avg=2.33\n",
            "[511 | 980.09] loss=1.76 avg=2.32\n",
            "[512 | 981.75] loss=2.09 avg=2.32\n",
            "[513 | 983.41] loss=1.94 avg=2.31\n",
            "[514 | 985.07] loss=2.04 avg=2.31\n",
            "[515 | 986.74] loss=1.87 avg=2.31\n",
            "[516 | 988.40] loss=3.05 avg=2.31\n",
            "[517 | 990.06] loss=2.23 avg=2.31\n",
            "[518 | 991.74] loss=2.26 avg=2.31\n",
            "[519 | 993.41] loss=2.58 avg=2.32\n",
            "[520 | 995.08] loss=1.82 avg=2.31\n",
            "[521 | 996.73] loss=1.70 avg=2.30\n",
            "[522 | 998.40] loss=2.40 avg=2.31\n",
            "[523 | 1000.07] loss=2.91 avg=2.31\n",
            "[524 | 1001.74] loss=1.53 avg=2.30\n",
            "[525 | 1003.42] loss=2.37 avg=2.30\n",
            "[526 | 1005.09] loss=2.07 avg=2.30\n",
            "[527 | 1006.76] loss=1.19 avg=2.29\n",
            "[528 | 1008.43] loss=1.46 avg=2.28\n",
            "[529 | 1010.11] loss=1.99 avg=2.28\n",
            "[530 | 1011.78] loss=2.37 avg=2.28\n",
            "[531 | 1013.47] loss=2.19 avg=2.28\n",
            "[532 | 1015.15] loss=3.24 avg=2.29\n",
            "[533 | 1016.84] loss=1.55 avg=2.28\n",
            "[534 | 1018.52] loss=2.49 avg=2.28\n",
            "[535 | 1020.20] loss=2.04 avg=2.28\n",
            "[536 | 1021.89] loss=2.54 avg=2.28\n",
            "[537 | 1023.57] loss=1.72 avg=2.28\n",
            "[538 | 1025.26] loss=2.34 avg=2.28\n",
            "[539 | 1026.95] loss=1.99 avg=2.28\n",
            "[540 | 1028.62] loss=2.18 avg=2.28\n",
            "[541 | 1030.31] loss=2.54 avg=2.28\n",
            "[542 | 1031.98] loss=1.73 avg=2.27\n",
            "[543 | 1033.66] loss=1.19 avg=2.26\n",
            "[544 | 1035.35] loss=2.24 avg=2.26\n",
            "[545 | 1037.02] loss=2.51 avg=2.26\n",
            "[546 | 1038.70] loss=2.24 avg=2.26\n",
            "[547 | 1040.38] loss=2.01 avg=2.26\n",
            "[548 | 1042.06] loss=1.42 avg=2.25\n",
            "[549 | 1043.73] loss=2.07 avg=2.25\n",
            "[550 | 1045.40] loss=1.99 avg=2.25\n",
            "[551 | 1047.06] loss=2.06 avg=2.25\n",
            "[552 | 1048.73] loss=2.34 avg=2.25\n",
            "[553 | 1050.40] loss=1.85 avg=2.24\n",
            "[554 | 1052.07] loss=2.26 avg=2.24\n",
            "[555 | 1053.74] loss=1.57 avg=2.24\n",
            "[556 | 1055.41] loss=1.35 avg=2.23\n",
            "[557 | 1057.08] loss=2.78 avg=2.23\n",
            "[558 | 1058.75] loss=1.77 avg=2.23\n",
            "[559 | 1060.42] loss=1.91 avg=2.23\n",
            "[560 | 1062.09] loss=2.50 avg=2.23\n",
            "[561 | 1063.76] loss=1.89 avg=2.22\n",
            "[562 | 1065.43] loss=2.49 avg=2.23\n",
            "[563 | 1067.10] loss=1.45 avg=2.22\n",
            "[564 | 1068.78] loss=1.80 avg=2.22\n",
            "[565 | 1070.44] loss=2.52 avg=2.22\n",
            "[566 | 1072.11] loss=2.17 avg=2.22\n",
            "[567 | 1073.79] loss=2.96 avg=2.23\n",
            "[568 | 1075.46] loss=1.42 avg=2.22\n",
            "[569 | 1077.13] loss=1.97 avg=2.21\n",
            "[570 | 1078.80] loss=1.36 avg=2.21\n",
            "[571 | 1080.46] loss=1.59 avg=2.20\n",
            "[572 | 1082.13] loss=1.42 avg=2.19\n",
            "[573 | 1083.80] loss=3.09 avg=2.20\n",
            "[574 | 1085.48] loss=1.80 avg=2.20\n",
            "[575 | 1087.14] loss=1.63 avg=2.19\n",
            "[576 | 1088.81] loss=2.11 avg=2.19\n",
            "[577 | 1090.47] loss=1.60 avg=2.18\n",
            "[578 | 1092.14] loss=1.75 avg=2.18\n",
            "[579 | 1093.81] loss=2.03 avg=2.18\n",
            "[580 | 1095.48] loss=1.89 avg=2.18\n",
            "[581 | 1097.15] loss=1.85 avg=2.17\n",
            "[582 | 1098.82] loss=3.19 avg=2.18\n",
            "[583 | 1100.49] loss=2.35 avg=2.18\n",
            "[584 | 1102.16] loss=2.40 avg=2.19\n",
            "[585 | 1103.83] loss=1.19 avg=2.18\n",
            "[586 | 1105.50] loss=1.76 avg=2.17\n",
            "[587 | 1107.17] loss=1.81 avg=2.17\n",
            "[588 | 1108.84] loss=1.77 avg=2.16\n",
            "[589 | 1110.51] loss=2.29 avg=2.17\n",
            "[590 | 1112.18] loss=1.44 avg=2.16\n",
            "[591 | 1113.85] loss=1.77 avg=2.15\n",
            "[592 | 1115.52] loss=2.27 avg=2.16\n",
            "[593 | 1117.19] loss=1.85 avg=2.15\n",
            "[594 | 1118.86] loss=1.48 avg=2.15\n",
            "[595 | 1120.53] loss=1.76 avg=2.14\n",
            "[596 | 1122.20] loss=1.92 avg=2.14\n",
            "[597 | 1123.87] loss=1.79 avg=2.14\n",
            "[598 | 1125.54] loss=2.10 avg=2.14\n",
            "[599 | 1127.21] loss=1.48 avg=2.13\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " could be considered just a little less than kosher, especially under the current circumstances.\n",
            "\n",
            "But what about the whole religious freedom thing? Can companies demand that people pray or celebrate a certain way just because they happen to be Jewish, or are of a certain race?\n",
            "\n",
            "Generally, yes. In a few cases, the government has intervened to protect people from discrimination. Most famously, in 1987, the Civil Rights Act banned employment discrimination on the basis of race and color, but also included sex and national origin. The DOJ is not involved, but it seems pretty clear that the federal government is comfortable stepping in when people of a certain religion face discrimination. The New York Times reports that the DOJ has \"made it a policy policy\" to intervene in pending religion-related lawsuits. This can include asking for waivers for certain actions, like prayer at official memorial services, which are normally granted on a case-by-case basis.\n",
            "\n",
            "But that might not always be enough. It's important to remember that federal courts are just looking at the law, not weighing whether particular religious practices violate it. As an example of how this might work, consider a case involving a mosque in Rhode Island. In that case, the wife of the imam is suing the imam's family for forcing him to watch pornographic movies and to not allow her to see the movies, because his wife is Muslim. The local imam is also a member of the mosque, and sued him as well. The wife claimed that her husband is forced to pray five different prayers a day because of his religion, and that it is a violation of their home to allow him that privilege. In a case from earlier this year, the judge sided with the wife, saying that her husband could not be forced to participate in the prayers. But that case went the rest of the way to a ruling that, while giving her an unusual victory, was mixed nonetheless.\n",
            "\n",
            "It could also be up to states, rather than the feds, to decide whether particular religious practices are actually a problem. Right now, there are no rules explicitly forbidding government officials from stepping in when people of a certain religious belief face discrimination, but there may be time limits on when that would be appropriate. It seems like a little extreme of a scenario to want to avoid at all costs, but it could also be worth it to protect people who are facing discrimination today. One solution could be a patchwork patchwork of state and federal laws that simply don't always line up—in the future, if someone from a community decides to file a federally protected religious discrimination case, for instance, that person would likely need to keep switching states and changing lawyers, in addition to occasionally having to deal with local politicians.\n",
            "\n",
            "If you or someone you know is experiencing discrimination because of your religion, what should you or your lawyer do next? If you or someone you know is experiencing a type of discrimination that would amount to a violation of your protected characteristics, you, or a lawyer you employ, should probably push for changes that keep that type of discrimination from happening. If that doesn’t work, though, you or a friend or family member, or a local, state, or federal prosecutor, should definitely take your case. The legal system can be incredibly powerful, making it harder to change the world if only because you don’t have unlimited resources to fight it.<|endoftext|>From Bulbapedia, the community-driven Pokémon encyclopedia.\n",
            "\n",
            "This article is under construction.\n",
            "\n",
            "Please feel free to edit this article to include new information, or expand on the article so that it does not conflict with other information in this article.\n",
            "\n",
            "Reasoning a Pokémon may have a higher chance of encountering a wild Pokémon is sometimes referred to as the reasonability effect, since that reasonability effect could be attributed to the reasonability of the reasonability of Pokémon.\n",
            "\n",
            "Reasonability Effect\n",
            "\n",
            "Reasonability effects have been observed in a number of Pokémon, including Delibird (with 0.099 chance of encountering a Caterpie), Caterpie (0.100%), Ekans (0.099%), Houndour (0.100%), Krabby (0.13%), Magnemite (0.13%), Marill (0.14%), Nidoran♂ (0.20%), Nidoran♂ (0.21%), Persian (0.3%) and Ponyta (0.33%)\n",
            "\n",
            "Reasonableness Effect\n",
            "\n",
            "Reasonableness effects arise in other places: for instance, in the evolution of Marill, its defense value increases as its defense rises to protect its partner, because its gender makes Marill the aggressor. However, in a reasons-based breeding program, if the partner is too weak, the trainer may choose to raise the partner at the expense of the weaker one to counter this change in strategy. In the Pokémon FireRed / LeafGreen games, if the trainer gives a Charizard to Turtwig so it can evolve into a Charizard X and teach it Double Team, its reasonability\n",
            "\n",
            "[600 | 1151.59] loss=1.24 avg=2.12\n",
            "[601 | 1153.26] loss=2.83 avg=2.13\n",
            "[602 | 1154.93] loss=1.47 avg=2.12\n",
            "[603 | 1156.60] loss=1.86 avg=2.12\n",
            "[604 | 1158.27] loss=2.67 avg=2.12\n",
            "[605 | 1159.94] loss=1.51 avg=2.12\n",
            "[606 | 1161.60] loss=1.59 avg=2.11\n",
            "[607 | 1163.27] loss=1.88 avg=2.11\n",
            "[608 | 1164.92] loss=1.80 avg=2.11\n",
            "[609 | 1166.58] loss=2.53 avg=2.11\n",
            "[610 | 1168.25] loss=0.99 avg=2.10\n",
            "[611 | 1169.92] loss=1.71 avg=2.10\n",
            "[612 | 1171.58] loss=1.96 avg=2.09\n",
            "[613 | 1173.25] loss=1.99 avg=2.09\n",
            "[614 | 1174.91] loss=1.91 avg=2.09\n",
            "[615 | 1176.57] loss=2.03 avg=2.09\n",
            "[616 | 1178.24] loss=1.47 avg=2.09\n",
            "[617 | 1179.91] loss=2.20 avg=2.09\n",
            "[618 | 1181.58] loss=2.02 avg=2.09\n",
            "[619 | 1183.24] loss=2.41 avg=2.09\n",
            "[620 | 1184.90] loss=2.62 avg=2.09\n",
            "[621 | 1186.57] loss=1.87 avg=2.09\n",
            "[622 | 1188.24] loss=1.59 avg=2.09\n",
            "[623 | 1189.90] loss=2.07 avg=2.09\n",
            "[624 | 1191.57] loss=2.23 avg=2.09\n",
            "[625 | 1193.24] loss=2.20 avg=2.09\n",
            "[626 | 1194.91] loss=1.79 avg=2.09\n",
            "[627 | 1196.58] loss=2.38 avg=2.09\n",
            "[628 | 1198.24] loss=1.58 avg=2.08\n",
            "[629 | 1199.91] loss=1.76 avg=2.08\n",
            "[630 | 1201.58] loss=2.14 avg=2.08\n",
            "[631 | 1203.25] loss=1.95 avg=2.08\n",
            "[632 | 1204.92] loss=1.75 avg=2.08\n",
            "[633 | 1206.60] loss=1.82 avg=2.07\n",
            "[634 | 1208.27] loss=1.66 avg=2.07\n",
            "[635 | 1209.93] loss=1.86 avg=2.07\n",
            "[636 | 1211.60] loss=0.78 avg=2.06\n",
            "[637 | 1213.27] loss=1.72 avg=2.05\n",
            "[638 | 1214.95] loss=1.68 avg=2.05\n",
            "[639 | 1216.62] loss=1.94 avg=2.05\n",
            "[640 | 1218.29] loss=1.20 avg=2.04\n",
            "[641 | 1219.96] loss=1.75 avg=2.04\n",
            "[642 | 1221.64] loss=1.43 avg=2.03\n",
            "[643 | 1223.31] loss=1.54 avg=2.02\n",
            "[644 | 1224.98] loss=1.69 avg=2.02\n",
            "[645 | 1226.65] loss=1.87 avg=2.02\n",
            "[646 | 1228.32] loss=1.46 avg=2.01\n",
            "[647 | 1230.00] loss=1.59 avg=2.01\n",
            "[648 | 1231.67] loss=1.77 avg=2.01\n",
            "[649 | 1233.33] loss=1.42 avg=2.00\n",
            "[650 | 1235.00] loss=2.11 avg=2.00\n",
            "[651 | 1236.67] loss=2.70 avg=2.01\n",
            "[652 | 1238.34] loss=1.18 avg=2.00\n",
            "[653 | 1240.01] loss=1.47 avg=2.00\n",
            "[654 | 1241.68] loss=2.35 avg=2.00\n",
            "[655 | 1243.35] loss=2.30 avg=2.00\n",
            "[656 | 1245.02] loss=1.86 avg=2.00\n",
            "[657 | 1246.69] loss=1.70 avg=2.00\n",
            "[658 | 1248.37] loss=2.08 avg=2.00\n",
            "[659 | 1250.05] loss=1.32 avg=1.99\n",
            "[660 | 1251.72] loss=1.24 avg=1.98\n",
            "[661 | 1253.39] loss=1.29 avg=1.98\n",
            "[662 | 1255.06] loss=1.83 avg=1.98\n",
            "[663 | 1256.73] loss=2.60 avg=1.98\n",
            "[664 | 1258.40] loss=1.23 avg=1.98\n",
            "[665 | 1260.08] loss=2.74 avg=1.98\n",
            "[666 | 1261.75] loss=1.94 avg=1.98\n",
            "[667 | 1263.42] loss=2.46 avg=1.99\n",
            "[668 | 1265.11] loss=1.85 avg=1.99\n",
            "[669 | 1266.77] loss=1.73 avg=1.98\n",
            "[670 | 1268.45] loss=1.96 avg=1.98\n",
            "[671 | 1270.12] loss=1.22 avg=1.98\n",
            "[672 | 1271.79] loss=1.94 avg=1.97\n",
            "[673 | 1273.46] loss=2.31 avg=1.98\n",
            "[674 | 1275.14] loss=2.39 avg=1.98\n",
            "[675 | 1276.81] loss=1.69 avg=1.98\n",
            "[676 | 1278.50] loss=2.12 avg=1.98\n",
            "[677 | 1280.17] loss=1.32 avg=1.97\n",
            "[678 | 1281.85] loss=1.52 avg=1.97\n",
            "[679 | 1283.52] loss=1.33 avg=1.96\n",
            "[680 | 1285.20] loss=2.07 avg=1.96\n",
            "[681 | 1286.87] loss=1.21 avg=1.96\n",
            "[682 | 1288.55] loss=1.53 avg=1.95\n",
            "[683 | 1290.22] loss=1.27 avg=1.95\n",
            "[684 | 1291.89] loss=2.22 avg=1.95\n",
            "[685 | 1293.57] loss=1.76 avg=1.95\n",
            "[686 | 1295.25] loss=1.97 avg=1.95\n",
            "[687 | 1296.93] loss=1.47 avg=1.94\n",
            "[688 | 1298.61] loss=1.96 avg=1.94\n",
            "[689 | 1300.27] loss=0.91 avg=1.93\n",
            "[690 | 1301.95] loss=1.34 avg=1.93\n",
            "[691 | 1303.61] loss=2.36 avg=1.93\n",
            "[692 | 1305.29] loss=1.21 avg=1.92\n",
            "[693 | 1306.96] loss=1.67 avg=1.92\n",
            "[694 | 1308.63] loss=1.57 avg=1.92\n",
            "[695 | 1310.30] loss=1.80 avg=1.92\n",
            "[696 | 1311.99] loss=1.67 avg=1.91\n",
            "[697 | 1313.66] loss=1.22 avg=1.91\n",
            "[698 | 1315.35] loss=1.21 avg=1.90\n",
            "[699 | 1317.02] loss=1.62 avg=1.90\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " occur in a population that weighs about half as much, at about 700 grams today, we would be doing ourselves a great disservice to expect our bodies to carry so much extra weight. But even as we strive to keep our bodies under 600 pounds, the human form isn’t static. In other words, the shape our ancestors took later in life played a role in how we look to others and our physical well-being for the rest of our lives.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "I grew up in a home with a rotating cast of characters. One was my mother. The other was my father. The third person to watch all involved watched me, too. I could see every twitch of their faces, every mumble they uttered. I could almost hear their grunts as each made a clicking motion with their doorbell. My mother was always the one answering, answering, answering, until I stopped having to listen altogether at age 65. She was dead.\n",
            "\n",
            "My father was barely around to see how things turned out. He died at the ripe old age of 82. He was an alcoholic, a cheat, an illegal alien, a car nut, and—probably my all-time best and saddest trait—a regular grump. Once upon a time, my dad was an awesome dad. I still miss him so much. But these days, I spend a great deal of time with my mother, who has lost a great deal of weight and is now considerably less of an alcoholic, cheat, illegal alien and car nut.\n",
            "\n",
            "We’ve always understood each other’s quirks and grudges. We’ve only gotten to know one other personier. But as we’re trying to make sense of the changes that have hit Texas—and there’s growing recognition that, in and of itself, the weight-loss craze is wonderful—we also have to reconcile how much of our lives we used to be able to separate, how much we used to have in common as children to bring little ones into the world, how much more important it is than ever to give one another the love—the time, the memories, the whole package.\n",
            "\n",
            "Maybe life can be a kinder, gentler version of that. I know the feeling.\n",
            "\n",
            "This piece first appeared on the American Prospect.\n",
            "\n",
            "*This piece appears in the spring 2016 collection, The Best American Essays. Click here to subscribe.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "When the dust settled Monday night on the House Intelligence Committee hearing on Russian election interference, the committee’s chairman, Rep. Devin Nunes (R-Calif.), had the committee classified as the same as all previous versions—a sign that the probe might be moving too slowly. It is, and its members, especially the Hispanic one, are sure that the committee doesn’t fully understand the scale of the cyberattack, and that some of its answers could damage the administration’s case.\n",
            "\n",
            "What makes Tuesday’s hearing so strange is that the committee had already classified some information as if nothing had changed and that the committee was trying to make up for that in writing. Chairman Devin Nunes said that committee members questioned a former national security official who had briefed senior Trump officials on the matter, but the official provided multiple misleading answers, including that there was no evidence of coordination between the Trump campaign and Moscow—an assertion that is contradicted elsewhere in the former official’s statement. There are also questions about the committee’s ability to conduct its business—it seems to have little ability to produce a full accounting of what it found and what it did not find—and, ultimately, the credibility of the full committee.\n",
            "\n",
            "For some, Wednesday’s hearing provided further confirmation of the problems with the way the Russia probe is being conducted.\n",
            "\n",
            "It was good to know a lot of things. But we still don’t know the full extent to which the Trump campaign colluded with the Russians—and it is hard to know what the full extent is because the committee has refused to produce reports that would detail exactly what it found. (The New York Times reported over the weekend that the committee might not even have the personnel to do that.) The committee has been working without a chair or a vice chair for months, and yet it is still struggling to come up with a chair and a vice chair.\n",
            "\n",
            "The second hearing was even worse. The chair of the House Intelligence Committee, Rep. Adam Schiff (D-Calif.), was under intense pressure from the left to appear on the record with the administration and to say that the committee had concluded that the Russian government was behind the election-year interference. He refused to do so. On Monday, Schiff insisted that he was “concerned that the administration was pushing this narrative that we have concluded was orchestrated by the Russians,” suggesting that he may leave his committee no better off than he found it.\n",
            "\n",
            "Schiff’s comments came after the committee confirmed that there is indeed\n",
            "\n",
            "[700 | 1341.30] loss=1.06 avg=1.89\n",
            "[701 | 1342.96] loss=1.49 avg=1.88\n",
            "[702 | 1344.63] loss=0.99 avg=1.88\n",
            "[703 | 1346.30] loss=1.54 avg=1.87\n",
            "[704 | 1347.96] loss=1.84 avg=1.87\n",
            "[705 | 1349.61] loss=1.40 avg=1.87\n",
            "[706 | 1351.27] loss=1.90 avg=1.87\n",
            "[707 | 1352.92] loss=2.36 avg=1.87\n",
            "[708 | 1354.58] loss=1.04 avg=1.86\n",
            "[709 | 1356.24] loss=1.02 avg=1.86\n",
            "[710 | 1357.90] loss=1.91 avg=1.86\n",
            "[711 | 1359.56] loss=1.64 avg=1.85\n",
            "[712 | 1361.21] loss=0.98 avg=1.85\n",
            "[713 | 1362.87] loss=1.54 avg=1.84\n",
            "[714 | 1364.53] loss=2.57 avg=1.85\n",
            "[715 | 1366.19] loss=1.39 avg=1.85\n",
            "[716 | 1367.86] loss=1.46 avg=1.84\n",
            "[717 | 1369.52] loss=1.76 avg=1.84\n",
            "[718 | 1371.19] loss=1.88 avg=1.84\n",
            "[719 | 1372.85] loss=1.53 avg=1.84\n",
            "[720 | 1374.52] loss=1.52 avg=1.83\n",
            "[721 | 1376.19] loss=1.62 avg=1.83\n",
            "[722 | 1377.86] loss=2.06 avg=1.83\n",
            "[723 | 1379.53] loss=1.41 avg=1.83\n",
            "[724 | 1381.20] loss=2.42 avg=1.84\n",
            "[725 | 1382.87] loss=2.06 avg=1.84\n",
            "[726 | 1384.54] loss=1.22 avg=1.83\n",
            "[727 | 1386.21] loss=1.61 avg=1.83\n",
            "[728 | 1387.89] loss=1.19 avg=1.82\n",
            "[729 | 1389.56] loss=1.12 avg=1.82\n",
            "[730 | 1391.22] loss=1.22 avg=1.81\n",
            "[731 | 1392.90] loss=1.10 avg=1.80\n",
            "[732 | 1394.58] loss=1.05 avg=1.80\n",
            "[733 | 1396.26] loss=1.25 avg=1.79\n",
            "[734 | 1397.93] loss=0.82 avg=1.78\n",
            "[735 | 1399.60] loss=1.35 avg=1.78\n",
            "[736 | 1401.28] loss=0.92 avg=1.77\n",
            "[737 | 1402.97] loss=1.38 avg=1.76\n",
            "[738 | 1404.66] loss=2.06 avg=1.77\n",
            "[739 | 1406.33] loss=1.71 avg=1.77\n",
            "[740 | 1408.01] loss=2.05 avg=1.77\n",
            "[741 | 1409.70] loss=1.84 avg=1.77\n",
            "[742 | 1411.37] loss=1.05 avg=1.76\n",
            "[743 | 1413.05] loss=1.10 avg=1.76\n",
            "[744 | 1414.74] loss=2.13 avg=1.76\n",
            "[745 | 1416.41] loss=1.97 avg=1.76\n",
            "[746 | 1418.08] loss=1.93 avg=1.76\n",
            "[747 | 1419.75] loss=2.23 avg=1.77\n",
            "[748 | 1421.43] loss=2.30 avg=1.77\n",
            "[749 | 1423.09] loss=1.13 avg=1.77\n",
            "[750 | 1424.77] loss=1.70 avg=1.77\n",
            "[751 | 1426.45] loss=1.31 avg=1.76\n",
            "[752 | 1428.11] loss=1.19 avg=1.76\n",
            "[753 | 1429.79] loss=1.65 avg=1.76\n",
            "[754 | 1431.46] loss=0.73 avg=1.74\n",
            "[755 | 1433.13] loss=2.12 avg=1.75\n",
            "[756 | 1434.80] loss=1.45 avg=1.75\n",
            "[757 | 1436.47] loss=1.48 avg=1.74\n",
            "[758 | 1438.14] loss=1.53 avg=1.74\n",
            "[759 | 1439.82] loss=1.25 avg=1.74\n",
            "[760 | 1441.48] loss=0.86 avg=1.73\n",
            "[761 | 1443.15] loss=2.27 avg=1.73\n",
            "[762 | 1444.82] loss=1.45 avg=1.73\n",
            "[763 | 1446.50] loss=0.79 avg=1.72\n",
            "[764 | 1448.18] loss=1.85 avg=1.72\n",
            "[765 | 1449.85] loss=1.54 avg=1.72\n",
            "[766 | 1451.52] loss=1.32 avg=1.72\n",
            "[767 | 1453.19] loss=1.64 avg=1.72\n",
            "[768 | 1454.88] loss=0.56 avg=1.70\n",
            "[769 | 1456.55] loss=0.50 avg=1.69\n",
            "[770 | 1458.22] loss=2.26 avg=1.70\n",
            "[771 | 1459.89] loss=1.23 avg=1.69\n",
            "[772 | 1461.56] loss=1.00 avg=1.69\n",
            "[773 | 1463.23] loss=0.86 avg=1.68\n",
            "[774 | 1464.90] loss=1.41 avg=1.67\n",
            "[775 | 1466.57] loss=1.85 avg=1.68\n",
            "[776 | 1468.24] loss=2.89 avg=1.69\n",
            "[777 | 1469.91] loss=1.48 avg=1.69\n",
            "[778 | 1471.58] loss=2.33 avg=1.69\n",
            "[779 | 1473.25] loss=2.07 avg=1.70\n",
            "[780 | 1474.92] loss=1.60 avg=1.70\n",
            "[781 | 1476.60] loss=1.10 avg=1.69\n",
            "[782 | 1478.27] loss=2.09 avg=1.69\n",
            "[783 | 1479.94] loss=0.74 avg=1.68\n",
            "[784 | 1481.61] loss=1.80 avg=1.69\n",
            "[785 | 1483.29] loss=1.53 avg=1.68\n",
            "[786 | 1484.96] loss=0.81 avg=1.68\n",
            "[787 | 1486.64] loss=1.15 avg=1.67\n",
            "[788 | 1488.31] loss=1.28 avg=1.67\n",
            "[789 | 1489.98] loss=1.80 avg=1.67\n",
            "[790 | 1491.66] loss=1.52 avg=1.67\n",
            "[791 | 1493.32] loss=0.56 avg=1.65\n",
            "[792 | 1494.98] loss=1.00 avg=1.65\n",
            "[793 | 1496.65] loss=1.95 avg=1.65\n",
            "[794 | 1498.32] loss=1.18 avg=1.65\n",
            "[795 | 1499.99] loss=0.85 avg=1.64\n",
            "[796 | 1501.66] loss=1.84 avg=1.64\n",
            "[797 | 1503.33] loss=1.84 avg=1.64\n",
            "[798 | 1505.00] loss=1.77 avg=1.64\n",
            "[799 | 1506.67] loss=1.61 avg=1.64\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " American and European governments that have poured billions of dollars into the project. In December 2016, SpaceX CEO Elon Musk said that he was \"appalled\" by reports that the space station construction had reached an early-termination stage and that he was \"looking into other options.\" This is not altogether surprising. Space station construction has been a mess ever since the International Space Station was awarded to Russia and is likely to reach its early-termination-stage phase again—it only takes one country to wreck it, and if they try again in five years, the U.S. might be left holding the bag. Still, there was something refreshing about Musk’s words, a sentiment that seems to have filtered down to Musk himself in the days after the news broke. “I’m glad everybody is getting along fine,” he wrote to employees recently.\n",
            "\n",
            "But one hopes for a bit less gluttony among major contractors when it comes to sourcing local contractors. While it’s true that taxpayers probably won’t foot the bill for the rocket launches that bring the station up and running again, it’s also true that it’s possible to minimize the cost of doing business by partnering with third parties. And as private companies have taken to taking on the mission, they have started offering options for getting around the logjams of contract administration. A startup is called Orbital ATK Privateers, and anyone can buy launch rights for them to any airport in the world. These launch rights cost money, but once purchased, they give airline passengers the ability to launch satellites into orbit for a set fee.\n",
            "\n",
            "Other options for getting around the logjams of contract administration include making sure your requests for launch rights are documented in Air Force orders, or posting your request on the Air Force’s website so engineers can go through your application and pick one that’s the most competitive. (NASA has done a great job of mapping out the physical and operational details for applications at https://www.nasa.gov/spacecommissions.) One option that isn’t yet implemented but is being used widely is called the Global Launch Adoption Network, or GoLNAS, an industry consortium of commercial launch providers that is trying to standardize procedures for getting around launch allocation delays. (In an ideal world, these launch adoption networks would also coordinate launch requests on a case-by-case basis via e-mail, something that’s been sorely lacking in the U.S. Internet launch market.)\n",
            "\n",
            "GoLNAS president and CEO Ryan O'Neal told me that for a launch to be designated as “adopted” in a launch adoption network, two things have to happen: (1) the launch needs to be launched by a commercial launch provider, and (2) the launch needs to take place within the contiguous 48 states. (The U.S. does have two space launch capacity constraints, though: There are just not that many launch sites along the Gulf Coast where launch trucks can be brought to life and launch the satellite, and there are technical hurdles involved with using solid rocket boosters to power satellites into geosynchronous transfer orbit.) The problem with using providers for launch adoption—and there is no simple fix for it—is that as many of these launches occur overseas, the U.S. doesn’t have the same level of control over the process that it does over taking photos for space photos. O'Neal said that GoLNAS was “planning to have launch adoption procedures in place by early 2019 that will ensure that the United States remains the only country with regulatory authority over launch providers at all times.”\n",
            "\n",
            "I was hopeful that satellite launch adoption networks would be more consistent in what they offered launch providers than the current mishmash of options offered by NASA and the FAA. “The current [launch adoption] guidelines are confusing and unwieldy,” says O'Neal. Launchers could choose to be represented on the networks by one of two entities: One could be a governmental entity, like the U.S. Air Force, or an aerospace company, like SpaceX. The other could be a private entity, like a startup. “You could have a launch originating out of Cape Canaveral or Wallops Island, and then have it originating out of a launch platform owned and operated by the U.S. Air Force.”\n",
            "\n",
            "What is not yet clear, however, is whether a provider like SpaceX would be permitted to be represented on a GoLNAS network, or whether it would have to register with the U.S. government to be represented. And one hopes that this would be clearer in the future — in the interim, GoLNAS could offer guidance to providers on which federal agencies they should be representing and what responsibilities they should bear.\n",
            "\n",
            "With SpaceX, there haven’t been any reports about the launch network structure yet, but I imagine there would likely be some guidelines in place in the future. When I asked\n",
            "\n",
            "[800 | 1531.67] loss=2.95 avg=1.66\n",
            "[801 | 1533.34] loss=1.61 avg=1.66\n",
            "[802 | 1535.01] loss=2.17 avg=1.66\n",
            "[803 | 1536.67] loss=1.88 avg=1.66\n",
            "[804 | 1538.34] loss=1.39 avg=1.66\n",
            "[805 | 1540.01] loss=2.92 avg=1.67\n",
            "[806 | 1541.67] loss=1.99 avg=1.68\n",
            "[807 | 1543.34] loss=1.13 avg=1.67\n",
            "[808 | 1545.00] loss=0.98 avg=1.66\n",
            "[809 | 1546.67] loss=1.89 avg=1.67\n",
            "[810 | 1548.34] loss=1.31 avg=1.66\n",
            "[811 | 1550.01] loss=1.48 avg=1.66\n",
            "[812 | 1551.66] loss=1.66 avg=1.66\n",
            "[813 | 1553.32] loss=1.97 avg=1.66\n",
            "[814 | 1554.98] loss=1.34 avg=1.66\n",
            "[815 | 1556.64] loss=1.22 avg=1.66\n",
            "[816 | 1558.30] loss=1.06 avg=1.65\n",
            "[817 | 1559.97] loss=1.49 avg=1.65\n",
            "[818 | 1561.62] loss=1.16 avg=1.64\n",
            "[819 | 1563.29] loss=0.50 avg=1.63\n",
            "[820 | 1564.96] loss=1.40 avg=1.63\n",
            "[821 | 1566.63] loss=0.96 avg=1.62\n",
            "[822 | 1568.28] loss=1.83 avg=1.63\n",
            "[823 | 1569.95] loss=1.26 avg=1.62\n",
            "[824 | 1571.62] loss=0.77 avg=1.61\n",
            "[825 | 1573.28] loss=1.31 avg=1.61\n",
            "[826 | 1574.95] loss=1.55 avg=1.61\n",
            "[827 | 1576.62] loss=1.45 avg=1.61\n",
            "[828 | 1578.29] loss=0.61 avg=1.60\n",
            "[829 | 1579.96] loss=1.88 avg=1.60\n",
            "[830 | 1581.63] loss=1.29 avg=1.60\n",
            "[831 | 1583.30] loss=2.05 avg=1.60\n",
            "[832 | 1584.97] loss=1.45 avg=1.60\n",
            "[833 | 1586.64] loss=1.98 avg=1.60\n",
            "[834 | 1588.31] loss=2.00 avg=1.61\n",
            "[835 | 1589.97] loss=1.19 avg=1.60\n",
            "[836 | 1591.65] loss=1.68 avg=1.61\n",
            "[837 | 1593.32] loss=1.79 avg=1.61\n",
            "[838 | 1594.99] loss=1.69 avg=1.61\n",
            "[839 | 1596.66] loss=1.53 avg=1.61\n",
            "[840 | 1598.33] loss=0.68 avg=1.60\n",
            "[841 | 1600.00] loss=1.10 avg=1.59\n",
            "[842 | 1601.67] loss=1.14 avg=1.59\n",
            "[843 | 1603.34] loss=1.21 avg=1.58\n",
            "[844 | 1605.01] loss=2.84 avg=1.60\n",
            "[845 | 1606.68] loss=0.92 avg=1.59\n",
            "[846 | 1608.35] loss=1.20 avg=1.59\n",
            "[847 | 1610.02] loss=1.91 avg=1.59\n",
            "[848 | 1611.69] loss=2.22 avg=1.60\n",
            "[849 | 1613.37] loss=1.25 avg=1.59\n",
            "[850 | 1615.04] loss=1.67 avg=1.59\n",
            "[851 | 1616.71] loss=1.61 avg=1.59\n",
            "[852 | 1618.39] loss=1.86 avg=1.60\n",
            "[853 | 1620.06] loss=1.25 avg=1.59\n",
            "[854 | 1621.74] loss=0.49 avg=1.58\n",
            "[855 | 1623.41] loss=0.90 avg=1.57\n",
            "[856 | 1625.09] loss=0.50 avg=1.56\n",
            "[857 | 1626.76] loss=1.88 avg=1.57\n",
            "[858 | 1628.44] loss=1.18 avg=1.56\n",
            "[859 | 1630.11] loss=0.94 avg=1.56\n",
            "[860 | 1631.78] loss=2.96 avg=1.57\n",
            "[861 | 1633.47] loss=1.25 avg=1.57\n",
            "[862 | 1635.14] loss=2.07 avg=1.57\n",
            "[863 | 1636.81] loss=1.24 avg=1.57\n",
            "[864 | 1638.48] loss=1.05 avg=1.56\n",
            "[865 | 1640.15] loss=2.18 avg=1.57\n",
            "[866 | 1641.82] loss=1.29 avg=1.57\n",
            "[867 | 1643.50] loss=1.71 avg=1.57\n",
            "[868 | 1645.17] loss=1.69 avg=1.57\n",
            "[869 | 1646.84] loss=0.91 avg=1.56\n",
            "[870 | 1648.52] loss=1.78 avg=1.57\n",
            "[871 | 1650.20] loss=0.57 avg=1.56\n",
            "[872 | 1651.88] loss=1.16 avg=1.55\n",
            "[873 | 1653.55] loss=1.50 avg=1.55\n",
            "[874 | 1655.23] loss=0.70 avg=1.54\n",
            "[875 | 1656.90] loss=0.72 avg=1.53\n",
            "[876 | 1658.58] loss=0.90 avg=1.53\n",
            "[877 | 1660.25] loss=1.46 avg=1.53\n",
            "[878 | 1661.94] loss=0.95 avg=1.52\n",
            "[879 | 1663.61] loss=1.61 avg=1.52\n",
            "[880 | 1665.28] loss=0.72 avg=1.51\n",
            "[881 | 1666.95] loss=1.31 avg=1.51\n",
            "[882 | 1668.62] loss=1.20 avg=1.51\n",
            "[883 | 1670.29] loss=1.86 avg=1.51\n",
            "[884 | 1671.96] loss=1.33 avg=1.51\n",
            "[885 | 1673.63] loss=1.37 avg=1.51\n",
            "[886 | 1675.30] loss=1.68 avg=1.51\n",
            "[887 | 1676.98] loss=2.79 avg=1.52\n",
            "[888 | 1678.65] loss=2.56 avg=1.53\n",
            "[889 | 1680.31] loss=0.68 avg=1.53\n",
            "[890 | 1681.99] loss=1.70 avg=1.53\n",
            "[891 | 1683.66] loss=1.47 avg=1.53\n",
            "[892 | 1685.33] loss=1.20 avg=1.52\n",
            "[893 | 1687.01] loss=1.46 avg=1.52\n",
            "[894 | 1688.68] loss=1.71 avg=1.53\n",
            "[895 | 1690.35] loss=1.55 avg=1.53\n",
            "[896 | 1692.02] loss=0.72 avg=1.52\n",
            "[897 | 1693.70] loss=1.27 avg=1.52\n",
            "[898 | 1695.37] loss=1.62 avg=1.52\n",
            "[899 | 1697.07] loss=1.65 avg=1.52\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " it is difficult to say how much the bill would accomplish, with supporters claiming it would not only expand the program but also increase it. \"You can go up to 80 percent in a city where it has been languishing in the 30s and 40s,\" said Sen. Joan Huffman, a Los Angeles Democrat.\n",
            "\n",
            "Advertisement\n",
            "\n",
            "\n",
            "Other analysts have offered similar critiques of the bill, with some saying the increase might not really be that much. Ilan Pappe, an economics professor at Princeton and author of the new book Weimar Conspiracies, called the bill's increase \"a conservative estimate\" of its potential benefit. And a report from the Congressional Research Service called the CBO score \"inadequate\" and said the Senate bill would increase the deficit by up to $55 billion a year.\n",
            "\n",
            "But to be fair, the increase in the minimum wage is a relatively small bump. From $7.25 an hour in 1968 to $7.55 by 2022, federal minimum wages grew at more than seven times the rate of inflation, peaking at $2.15 in 1992. State and local minimums, by comparison, grew at around three times the pace of inflation. To my mind, the increase from around $3.75 to $7.55 an hour at the national level seems fairly modest. (The federal minimum wage now stands at $7.25 an hour.)\n",
            "\n",
            "Another criticism of the bill is that it gives states flexibility to raise the minimum wage higher than $7.55 an hour (the federal minimum is $7.25). But research shows that even if every state raised its minimum to $7.55 an hour (which hasn’t happened, and may never happen), it would still take hundreds of new businesses and workers to make up for the lost income from a $7.55-an-hour minimum wage—which means that, at best, if-then-else language makes it easier for states to raise the floor. The goal of a strong, well-designed federal minimum wage is to give states the means to provide a living wage without burdening them with too much government paperwork, which is not what the Clinton administration was doing with its per capita federal minimum. In the case of the Clinton minimum, it appeared to be well-intentioned but dumb policy.\n",
            "\n",
            "The problem with the CBO analysis is that it doesn’t provide a breakdown of how much of an increase in the federal minimum wage would result in job growth. But the CBO found in 2008 and 2009 that if the federal minimum wage rose to $4.05 an hour, job growth would increase by only about 1.3 percent. At $7.55 an hour, that growth would be 0.7 percent, almost all due to growth in incomes for the lowest-paid workers. As the Working Poor Project put it to me, “this is just the beginning for the $7.55 an hour minimum wage, and it’s not a bad start.”\n",
            "\n",
            "There are other studies that find different employment effects from a $7.55-an-hour minimum wage. That’s because the research on whether a $7.55 minimum boosts employment growth is varied. There aren’t many studies that find a minimum wage increase leads to increased spending by businesses. There could also be tradeoffs workers must make. One study looking at the impact of San Francisco’s $15 minimum wage found that if the wage were raised to $15 an hour (which is where the city’s existing $7.55 minimum wage was set at at the time), businesses would see less of a need to expand, but also would have to lay off some people in the process.\n",
            "\n",
            "If, as some politicians are pushing for, Congress raises the minimum wage to $10.10 or even $10.15 an hour, we might get some research on this question off the ground. After all, a higher minimum wages helps workers make ends meet; companies tend to move production to workers who can afford a higher pay increase. In a study published in 2011, economists Jeffrey Miron and Jia You estimated that raising the minimum wage from $3.25 to $3.75 an hour led to around $3 billion in annual wage growth for low-wage workers and that they predicted the effect would be sustained.\n",
            "\n",
            "However, even if the effect of a $7.55-an-hour minimum was hard to study, the bigger question is whether increasing the federal minimum wage raises the share of income that goes to wages for all adults. In a 2012 paper, economists Brian DePratto and Kenneth Rogoff looked at the relationship between the minimum wage in 21 states in the early 1990s, when the federal minimum wage was set at $2.13, and growth in the minimum wage and inflation over the next few years. They wrote that the '90s minimum wage hike 'did not appear to have a discernible, statistically significant, statistically significant, or\n",
            "\n",
            "[900 | 1721.32] loss=0.57 avg=1.51\n",
            "[901 | 1722.99] loss=1.63 avg=1.51\n",
            "[902 | 1724.65] loss=1.28 avg=1.51\n",
            "[903 | 1726.31] loss=1.29 avg=1.51\n",
            "[904 | 1727.97] loss=1.20 avg=1.50\n",
            "[905 | 1729.63] loss=1.05 avg=1.50\n",
            "[906 | 1731.29] loss=2.40 avg=1.51\n",
            "[907 | 1732.95] loss=1.75 avg=1.51\n",
            "[908 | 1734.61] loss=0.79 avg=1.50\n",
            "[909 | 1736.27] loss=1.20 avg=1.50\n",
            "[910 | 1737.92] loss=2.27 avg=1.51\n",
            "[911 | 1739.58] loss=1.75 avg=1.51\n",
            "[912 | 1741.24] loss=1.98 avg=1.51\n",
            "[913 | 1742.90] loss=1.20 avg=1.51\n",
            "[914 | 1744.57] loss=1.19 avg=1.51\n",
            "[915 | 1746.23] loss=1.92 avg=1.51\n",
            "[916 | 1747.89] loss=1.18 avg=1.51\n",
            "[917 | 1749.57] loss=1.18 avg=1.50\n",
            "[918 | 1751.24] loss=0.75 avg=1.50\n",
            "[919 | 1752.91] loss=1.72 avg=1.50\n",
            "[920 | 1754.58] loss=0.37 avg=1.49\n",
            "[921 | 1756.24] loss=1.04 avg=1.48\n",
            "[922 | 1757.92] loss=2.16 avg=1.49\n",
            "[923 | 1759.59] loss=0.97 avg=1.49\n",
            "[924 | 1761.26] loss=1.53 avg=1.49\n",
            "[925 | 1762.93] loss=1.35 avg=1.48\n",
            "[926 | 1764.61] loss=0.93 avg=1.48\n",
            "[927 | 1766.28] loss=1.77 avg=1.48\n",
            "[928 | 1767.95] loss=1.77 avg=1.48\n",
            "[929 | 1769.63] loss=0.87 avg=1.48\n",
            "[930 | 1771.30] loss=0.98 avg=1.47\n",
            "[931 | 1772.98] loss=1.71 avg=1.48\n",
            "[932 | 1774.65] loss=0.54 avg=1.47\n",
            "[933 | 1776.34] loss=1.60 avg=1.47\n",
            "[934 | 1778.01] loss=0.88 avg=1.46\n",
            "[935 | 1779.70] loss=1.58 avg=1.46\n",
            "[936 | 1781.37] loss=0.80 avg=1.46\n",
            "[937 | 1783.06] loss=2.61 avg=1.47\n",
            "[938 | 1784.75] loss=1.75 avg=1.47\n",
            "[939 | 1786.44] loss=2.11 avg=1.48\n",
            "[940 | 1788.13] loss=1.21 avg=1.47\n",
            "[941 | 1789.82] loss=1.17 avg=1.47\n",
            "[942 | 1791.51] loss=1.54 avg=1.47\n",
            "[943 | 1793.19] loss=1.24 avg=1.47\n",
            "[944 | 1794.87] loss=1.11 avg=1.47\n",
            "[945 | 1796.56] loss=1.60 avg=1.47\n",
            "[946 | 1798.24] loss=0.85 avg=1.46\n",
            "[947 | 1799.91] loss=1.15 avg=1.46\n",
            "[948 | 1801.60] loss=1.37 avg=1.46\n",
            "[949 | 1803.28] loss=1.08 avg=1.45\n",
            "[950 | 1804.96] loss=1.45 avg=1.45\n",
            "[951 | 1806.63] loss=2.02 avg=1.46\n",
            "[952 | 1808.30] loss=1.44 avg=1.46\n",
            "[953 | 1809.99] loss=1.06 avg=1.46\n",
            "[954 | 1811.66] loss=1.75 avg=1.46\n",
            "[955 | 1813.34] loss=1.56 avg=1.46\n",
            "[956 | 1815.01] loss=0.91 avg=1.45\n",
            "[957 | 1816.68] loss=1.07 avg=1.45\n",
            "[958 | 1818.35] loss=1.13 avg=1.45\n",
            "[959 | 1820.02] loss=2.20 avg=1.45\n",
            "[960 | 1821.69] loss=1.89 avg=1.46\n",
            "[961 | 1823.37] loss=2.01 avg=1.46\n",
            "[962 | 1825.04] loss=1.19 avg=1.46\n",
            "[963 | 1826.71] loss=1.45 avg=1.46\n",
            "[964 | 1828.38] loss=1.83 avg=1.46\n",
            "[965 | 1830.05] loss=1.65 avg=1.47\n",
            "[966 | 1831.73] loss=1.21 avg=1.46\n",
            "[967 | 1833.40] loss=1.89 avg=1.47\n",
            "[968 | 1835.07] loss=1.63 avg=1.47\n",
            "[969 | 1836.74] loss=1.60 avg=1.47\n",
            "[970 | 1838.41] loss=1.76 avg=1.47\n",
            "[971 | 1840.06] loss=1.32 avg=1.47\n",
            "[972 | 1841.73] loss=1.20 avg=1.47\n",
            "[973 | 1843.40] loss=1.26 avg=1.47\n",
            "[974 | 1845.07] loss=1.18 avg=1.46\n",
            "[975 | 1846.74] loss=1.66 avg=1.47\n",
            "[976 | 1848.41] loss=0.71 avg=1.46\n",
            "[977 | 1850.08] loss=1.26 avg=1.46\n",
            "[978 | 1851.75] loss=2.04 avg=1.46\n",
            "[979 | 1853.42] loss=1.04 avg=1.46\n",
            "[980 | 1855.09] loss=2.00 avg=1.46\n",
            "[981 | 1856.75] loss=1.68 avg=1.47\n",
            "[982 | 1858.42] loss=0.83 avg=1.46\n",
            "[983 | 1860.09] loss=1.16 avg=1.46\n",
            "[984 | 1861.76] loss=1.00 avg=1.45\n",
            "[985 | 1863.43] loss=1.27 avg=1.45\n",
            "[986 | 1865.10] loss=1.99 avg=1.46\n",
            "[987 | 1866.77] loss=0.77 avg=1.45\n",
            "[988 | 1868.44] loss=1.80 avg=1.45\n",
            "[989 | 1870.11] loss=1.16 avg=1.45\n",
            "[990 | 1871.77] loss=1.56 avg=1.45\n",
            "[991 | 1873.44] loss=1.11 avg=1.45\n",
            "[992 | 1875.12] loss=1.55 avg=1.45\n",
            "[993 | 1876.79] loss=0.62 avg=1.44\n",
            "[994 | 1878.45] loss=1.19 avg=1.44\n",
            "[995 | 1880.12] loss=1.09 avg=1.43\n",
            "[996 | 1881.80] loss=1.28 avg=1.43\n",
            "[997 | 1883.47] loss=1.77 avg=1.44\n",
            "[998 | 1885.14] loss=1.74 avg=1.44\n",
            "[999 | 1886.81] loss=0.84 avg=1.43\n",
            "Ran 1010 steps.\n",
            "Saving checkpoint/all_slate_345/model-1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBG7Cq3VYGnr",
        "colab_type": "code",
        "outputId": "7c74cff2-c2ee-433a-c266-2b2f5ccf3b91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "## national review essays training - with 345\n",
        "!PYTHONPATH=src ./train.py --dataset /content/all_national_review.txt --run_name 'all_national_review_345' --model_name '345M'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0627 22:47:37.889118 139705169528704 deprecation_wrapper.py:119] From /content/gpt-2/src/model.py:147: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
            "\n",
            "W0627 22:47:37.898118 139705169528704 deprecation_wrapper.py:119] From /content/gpt-2/src/memory_saving_gradients.py:13: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n",
            "W0627 22:47:37.996006 139705169528704 deprecation_wrapper.py:119] From ./train.py:88: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "W0627 22:47:37.996489 139705169528704 deprecation_wrapper.py:119] From ./train.py:91: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "2019-06-27 22:47:38.004039: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-06-27 22:47:38.004286: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1d61100 executing computations on platform Host. Devices:\n",
            "2019-06-27 22:47:38.004332: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2019-06-27 22:47:38.007166: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\n",
            "2019-06-27 22:47:38.163652: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-27 22:47:38.164439: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1d60840 executing computations on platform CUDA. Devices:\n",
            "2019-06-27 22:47:38.164475: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2019-06-27 22:47:38.164788: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-27 22:47:38.165360: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-06-27 22:47:38.165858: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-06-27 22:47:38.167307: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-06-27 22:47:38.168607: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-06-27 22:47:38.168945: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-06-27 22:47:38.170409: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-06-27 22:47:38.171628: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-06-27 22:47:38.174787: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-06-27 22:47:38.174918: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-27 22:47:38.175342: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-27 22:47:38.175666: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
            "2019-06-27 22:47:38.175722: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-06-27 22:47:38.176689: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-06-27 22:47:38.176715: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n",
            "2019-06-27 22:47:38.176726: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n",
            "2019-06-27 22:47:38.177067: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-27 22:47:38.177518: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-27 22:47:38.178004: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14202 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "W0627 22:47:38.178866 139705169528704 deprecation_wrapper.py:119] From ./train.py:92: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0627 22:47:49.290162 139705169528704 deprecation.py:323] From /content/gpt-2/src/sample.py:65: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "W0627 22:47:49.304963 139705169528704 deprecation.py:323] From /content/gpt-2/src/sample.py:16: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0627 22:47:49.306603 139705169528704 deprecation.py:323] From /content/gpt-2/src/sample.py:70: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.random.categorical` instead.\n",
            "W0627 22:47:49.316590 139705169528704 deprecation_wrapper.py:119] From ./train.py:121: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "W0627 22:48:04.713294 139705169528704 deprecation_wrapper.py:119] From ./train.py:144: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n",
            "W0627 22:48:04.716263 139705169528704 deprecation_wrapper.py:119] From ./train.py:147: The name tf.summary.merge is deprecated. Please use tf.compat.v1.summary.merge instead.\n",
            "\n",
            "W0627 22:48:04.717123 139705169528704 deprecation_wrapper.py:119] From ./train.py:149: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
            "\n",
            "W0627 22:48:04.717907 139705169528704 deprecation_wrapper.py:119] From ./train.py:152: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "Loading checkpoint models/345M/model.ckpt\n",
            "W0627 22:48:18.205542 139705169528704 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "Loading dataset...\n",
            "100% 1/1 [00:01<00:00,  1.28s/it]\n",
            "dataset has 139046 tokens\n",
            "Training...\n",
            "2019-06-27 22:48:32.878302: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n",
            "2019-06-27 22:48:33.615873: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "[1 | 13.59] loss=2.90 avg=2.90\n",
            "[2 | 15.21] loss=3.19 avg=3.05\n",
            "[3 | 16.83] loss=3.10 avg=3.06\n",
            "[4 | 18.47] loss=2.73 avg=2.98\n",
            "[5 | 20.11] loss=3.25 avg=3.03\n",
            "[6 | 21.76] loss=2.48 avg=2.94\n",
            "[7 | 23.42] loss=2.54 avg=2.88\n",
            "[8 | 25.09] loss=2.92 avg=2.88\n",
            "[9 | 26.77] loss=2.97 avg=2.90\n",
            "[10 | 28.44] loss=2.60 avg=2.86\n",
            "[11 | 30.14] loss=2.09 avg=2.79\n",
            "[12 | 31.84] loss=2.89 avg=2.80\n",
            "[13 | 33.56] loss=3.01 avg=2.82\n",
            "[14 | 35.28] loss=2.40 avg=2.78\n",
            "[15 | 36.99] loss=3.02 avg=2.80\n",
            "[16 | 38.73] loss=2.59 avg=2.79\n",
            "[17 | 40.47] loss=2.61 avg=2.78\n",
            "[18 | 42.21] loss=2.77 avg=2.78\n",
            "[19 | 43.95] loss=2.74 avg=2.77\n",
            "[20 | 45.69] loss=2.68 avg=2.77\n",
            "[21 | 47.43] loss=2.94 avg=2.78\n",
            "[22 | 49.15] loss=2.92 avg=2.78\n",
            "[23 | 50.88] loss=2.51 avg=2.77\n",
            "[24 | 52.61] loss=2.73 avg=2.77\n",
            "[25 | 54.32] loss=2.37 avg=2.75\n",
            "[26 | 56.03] loss=3.07 avg=2.77\n",
            "[27 | 57.73] loss=2.64 avg=2.76\n",
            "[28 | 59.42] loss=2.73 avg=2.76\n",
            "[29 | 61.11] loss=3.33 avg=2.78\n",
            "[30 | 62.80] loss=3.23 avg=2.80\n",
            "[31 | 64.47] loss=2.95 avg=2.80\n",
            "[32 | 66.14] loss=2.67 avg=2.80\n",
            "[33 | 67.80] loss=2.73 avg=2.80\n",
            "[34 | 69.46] loss=2.78 avg=2.80\n",
            "[35 | 71.12] loss=2.69 avg=2.79\n",
            "[36 | 72.78] loss=2.80 avg=2.79\n",
            "[37 | 74.43] loss=3.01 avg=2.80\n",
            "[38 | 76.08] loss=2.65 avg=2.79\n",
            "[39 | 77.73] loss=2.53 avg=2.79\n",
            "[40 | 79.38] loss=3.00 avg=2.79\n",
            "[41 | 81.02] loss=2.61 avg=2.79\n",
            "[42 | 82.67] loss=2.54 avg=2.78\n",
            "[43 | 84.31] loss=2.74 avg=2.78\n",
            "[44 | 85.94] loss=2.61 avg=2.77\n",
            "[45 | 87.59] loss=2.60 avg=2.77\n",
            "[46 | 89.22] loss=2.38 avg=2.76\n",
            "[47 | 90.87] loss=2.84 avg=2.76\n",
            "[48 | 92.50] loss=3.28 avg=2.78\n",
            "[49 | 94.14] loss=2.41 avg=2.77\n",
            "[50 | 95.79] loss=2.74 avg=2.77\n",
            "[51 | 97.43] loss=2.71 avg=2.76\n",
            "[52 | 99.07] loss=3.49 avg=2.78\n",
            "[53 | 100.71] loss=2.33 avg=2.77\n",
            "[54 | 102.36] loss=2.85 avg=2.77\n",
            "[55 | 104.01] loss=2.57 avg=2.77\n",
            "[56 | 105.66] loss=2.45 avg=2.76\n",
            "[57 | 107.33] loss=2.88 avg=2.76\n",
            "[58 | 108.98] loss=3.10 avg=2.77\n",
            "[59 | 110.64] loss=3.21 avg=2.78\n",
            "[60 | 112.29] loss=2.74 avg=2.78\n",
            "[61 | 113.95] loss=2.66 avg=2.78\n",
            "[62 | 115.61] loss=3.15 avg=2.78\n",
            "[63 | 117.28] loss=2.86 avg=2.79\n",
            "[64 | 118.95] loss=3.27 avg=2.80\n",
            "[65 | 120.62] loss=2.97 avg=2.80\n",
            "[66 | 122.29] loss=2.22 avg=2.79\n",
            "[67 | 123.97] loss=2.90 avg=2.79\n",
            "[68 | 125.64] loss=2.56 avg=2.79\n",
            "[69 | 127.31] loss=2.80 avg=2.79\n",
            "[70 | 128.98] loss=3.19 avg=2.79\n",
            "[71 | 130.66] loss=2.71 avg=2.79\n",
            "[72 | 132.33] loss=2.52 avg=2.79\n",
            "[73 | 134.00] loss=2.68 avg=2.79\n",
            "[74 | 135.67] loss=2.36 avg=2.78\n",
            "[75 | 137.34] loss=2.52 avg=2.77\n",
            "[76 | 139.01] loss=2.70 avg=2.77\n",
            "[77 | 140.68] loss=2.91 avg=2.77\n",
            "[78 | 142.35] loss=2.92 avg=2.78\n",
            "[79 | 144.03] loss=2.73 avg=2.78\n",
            "[80 | 145.69] loss=2.36 avg=2.77\n",
            "[81 | 147.36] loss=2.82 avg=2.77\n",
            "[82 | 149.02] loss=2.77 avg=2.77\n",
            "[83 | 150.68] loss=2.34 avg=2.76\n",
            "[84 | 152.35] loss=1.89 avg=2.75\n",
            "[85 | 154.01] loss=2.40 avg=2.74\n",
            "[86 | 155.67] loss=2.90 avg=2.74\n",
            "[87 | 157.34] loss=2.97 avg=2.75\n",
            "[88 | 159.01] loss=2.76 avg=2.75\n",
            "[89 | 160.67] loss=2.45 avg=2.74\n",
            "[90 | 162.33] loss=2.81 avg=2.74\n",
            "[91 | 163.98] loss=2.91 avg=2.75\n",
            "[92 | 165.65] loss=2.55 avg=2.74\n",
            "[93 | 167.30] loss=2.78 avg=2.74\n",
            "[94 | 168.97] loss=2.66 avg=2.74\n",
            "[95 | 170.62] loss=2.56 avg=2.74\n",
            "[96 | 172.28] loss=3.05 avg=2.74\n",
            "[97 | 173.94] loss=3.22 avg=2.75\n",
            "[98 | 175.59] loss=2.59 avg=2.75\n",
            "[99 | 177.24] loss=2.90 avg=2.75\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " was going to be able to make sure that you're not missing any of your scheduled meetings, if you can't be there.\n",
            "\n",
            "2) Be careful with the details of your appointments (even if you've hired an interpreter).\n",
            "\n",
            "If you're hired for a job that involves a lot of meetings, then all the detailed details of those meetings should probably be kept to yourself. And, of course, there's the matter of your meetings, which should not be confided in your interpreter. In the event that something goes wrong while you were there, you should try to make sure you report that too.\n",
            "\n",
            "3) Make sure you have a detailed account of who is providing you with information.\n",
            "\n",
            "We all know someone who is good at this, the kind of person who always helps them with their questions, and then keeps them to themselves. In the unlikely event that some unforeseen complication comes up during your interview and it becomes necessary for you to speak through your interpreter, you should tell it all to him/her when time comes.\n",
            "\n",
            "4) Consider a paid interpreter!\n",
            "\n",
            "One of the reasons behind being employed at a multinational firm is that a paid interpreter is a great deal of fun, but it's also sometimes very hard (and expensive) to get an interpreter, especially a one that can speak a certain number of languages. One solution would be to pay your own interpreter, if you can afford one. Or, of course, the interpreter could be hired by your employer, in which case you would have to pay the interpreter as well.\n",
            "\n",
            "5) Make sure it is safe.\n",
            "\n",
            "Remember the time you spent making sure you weren't losing your temper or trying to fight your way out of your job? It's never acceptable to fight your way out of a job when you don't have to. A lot of people become job-related because they get fired for being too rough with the bosses. You should always be at the ready to explain what you believe to be your legal duties or your rights at the time of your interviews and you should tell your interviewser that you are not going to be getting involved if anything goes seriously wrong.\n",
            "\n",
            "Some interviews are really demanding; some are really nice to speak to, and there are always going to be some people who just think it is more fun to be treated well than to have you answer their questions. So, if your job is about getting good results for your employer, the better course of action is never to be too difficult in the matter. Don't be afraid to say something silly or too personal, but if at any time it becomes necessary you tell the interviewer that you weren't going to say what the interviewer asked you to say, you should answer that too.\n",
            "\n",
            "\n",
            "6) Don't be a jerk.\n",
            "The biggest compliment one would ever hear at a job interview is that one is going to be a good, loyal employee. A lot of times, however, this is just a reflection of how you're going to act when you're there. In addition to being the most important thing, your job is about having a positive attitude. Don't just put on a mask of kindness and get on with your role. Don't pretend that you are giving everything you have as you sit in the waiting room, with your face buried in a large white card until your interviewer calls you out.\n",
            "\n",
            "7) Don't be too careful if you get too caught up in your job.\n",
            "\n",
            "Your job is to be the boss, so there's no question about that. However, the thing is, the people who are supposed to make the decisions are going to make those decisions; there's no way around that, and you must take into account them. You need to be ready to accept the criticism and take it head on because you will always get criticized on the job.\n",
            "\n",
            "One very important thing about being a boss is not to overreact to any criticism or let yourself get worked up over trivial things. Be calm, considerate and respectful. Always stay in the middle, always keep an open mind as well as a fair one in order to do that job well. A person is always going to have negative things to say about you, no matter how subtle and how many people you are not going to see or no one will think of you that way.\n",
            "\n",
            "Now, that's all pretty general, but if you are coming up against an issue related to your job that might not be a deal-breaker, ask yourself why it is you should be a boss. There will be situations in which you would rather not have a boss and, by chance, you would be forced to have one.\n",
            "\n",
            "You would not want that to happen to you, and you sure as hell would not want another person at work you didn't want taking the initiative, because you don't like being bullied.\n",
            "\n",
            "I hope that you are as excited about being hired in a job as I am because you know the things you have to put up your hand on:\n",
            "\n",
            "[100 | 205.48] loss=2.27 avg=2.74\n",
            "[101 | 207.14] loss=2.61 avg=2.74\n",
            "[102 | 208.82] loss=3.12 avg=2.75\n",
            "[103 | 210.49] loss=2.51 avg=2.74\n",
            "[104 | 212.16] loss=2.34 avg=2.74\n",
            "[105 | 213.84] loss=2.82 avg=2.74\n",
            "[106 | 215.51] loss=2.34 avg=2.73\n",
            "[107 | 217.19] loss=2.23 avg=2.73\n",
            "[108 | 218.86] loss=2.34 avg=2.72\n",
            "[109 | 220.53] loss=2.75 avg=2.72\n",
            "[110 | 222.21] loss=2.68 avg=2.72\n",
            "[111 | 223.88] loss=2.60 avg=2.72\n",
            "[112 | 225.54] loss=2.79 avg=2.72\n",
            "[113 | 227.21] loss=2.54 avg=2.72\n",
            "[114 | 228.90] loss=2.51 avg=2.71\n",
            "[115 | 230.58] loss=3.02 avg=2.72\n",
            "[116 | 232.24] loss=3.07 avg=2.72\n",
            "[117 | 233.92] loss=2.20 avg=2.72\n",
            "[118 | 235.60] loss=2.87 avg=2.72\n",
            "[119 | 237.28] loss=2.80 avg=2.72\n",
            "[120 | 238.96] loss=2.17 avg=2.71\n",
            "[121 | 240.63] loss=2.90 avg=2.71\n",
            "[122 | 242.31] loss=2.72 avg=2.71\n",
            "[123 | 243.99] loss=2.90 avg=2.72\n",
            "[124 | 245.66] loss=2.27 avg=2.71\n",
            "[125 | 247.34] loss=2.56 avg=2.71\n",
            "[126 | 249.01] loss=2.57 avg=2.71\n",
            "[127 | 250.69] loss=2.20 avg=2.70\n",
            "[128 | 252.37] loss=2.36 avg=2.69\n",
            "[129 | 254.04] loss=2.84 avg=2.70\n",
            "[130 | 255.72] loss=2.54 avg=2.69\n",
            "[131 | 257.40] loss=2.89 avg=2.70\n",
            "[132 | 259.07] loss=2.56 avg=2.70\n",
            "[133 | 260.76] loss=2.70 avg=2.70\n",
            "[134 | 262.44] loss=2.93 avg=2.70\n",
            "[135 | 264.12] loss=2.50 avg=2.70\n",
            "[136 | 265.79] loss=2.91 avg=2.70\n",
            "[137 | 267.47] loss=2.52 avg=2.70\n",
            "[138 | 269.15] loss=2.91 avg=2.70\n",
            "[139 | 270.83] loss=2.49 avg=2.70\n",
            "[140 | 272.50] loss=2.38 avg=2.69\n",
            "[141 | 274.17] loss=2.73 avg=2.69\n",
            "[142 | 275.86] loss=2.64 avg=2.69\n",
            "[143 | 277.54] loss=2.70 avg=2.69\n",
            "[144 | 279.22] loss=2.59 avg=2.69\n",
            "[145 | 280.90] loss=2.61 avg=2.69\n",
            "[146 | 282.58] loss=2.28 avg=2.68\n",
            "[147 | 284.27] loss=2.36 avg=2.68\n",
            "[148 | 285.94] loss=2.45 avg=2.68\n",
            "[149 | 287.61] loss=2.81 avg=2.68\n",
            "[150 | 289.28] loss=2.88 avg=2.68\n",
            "[151 | 290.96] loss=2.71 avg=2.68\n",
            "[152 | 292.63] loss=2.83 avg=2.68\n",
            "[153 | 294.31] loss=2.58 avg=2.68\n",
            "[154 | 295.99] loss=2.28 avg=2.68\n",
            "[155 | 297.66] loss=2.55 avg=2.68\n",
            "[156 | 299.34] loss=2.48 avg=2.67\n",
            "[157 | 301.02] loss=2.28 avg=2.67\n",
            "[158 | 302.69] loss=2.88 avg=2.67\n",
            "[159 | 304.36] loss=2.74 avg=2.67\n",
            "[160 | 306.03] loss=2.08 avg=2.66\n",
            "[161 | 307.71] loss=2.68 avg=2.66\n",
            "[162 | 309.39] loss=2.70 avg=2.66\n",
            "[163 | 311.07] loss=2.52 avg=2.66\n",
            "[164 | 312.76] loss=2.77 avg=2.66\n",
            "[165 | 314.43] loss=2.35 avg=2.66\n",
            "[166 | 316.11] loss=2.66 avg=2.66\n",
            "[167 | 317.77] loss=2.60 avg=2.66\n",
            "[168 | 319.45] loss=1.95 avg=2.65\n",
            "[169 | 321.13] loss=2.68 avg=2.65\n",
            "[170 | 322.81] loss=2.26 avg=2.65\n",
            "[171 | 324.48] loss=2.36 avg=2.64\n",
            "[172 | 326.17] loss=2.74 avg=2.64\n",
            "[173 | 327.84] loss=2.25 avg=2.64\n",
            "[174 | 329.51] loss=2.33 avg=2.64\n",
            "[175 | 331.18] loss=2.27 avg=2.63\n",
            "[176 | 332.86] loss=2.78 avg=2.63\n",
            "[177 | 334.54] loss=2.58 avg=2.63\n",
            "[178 | 336.23] loss=2.48 avg=2.63\n",
            "[179 | 337.90] loss=2.40 avg=2.63\n",
            "[180 | 339.55] loss=2.37 avg=2.63\n",
            "[181 | 341.23] loss=2.93 avg=2.63\n",
            "[182 | 342.91] loss=2.46 avg=2.63\n",
            "[183 | 344.60] loss=2.44 avg=2.62\n",
            "[184 | 346.27] loss=2.39 avg=2.62\n",
            "[185 | 347.95] loss=3.14 avg=2.63\n",
            "[186 | 349.62] loss=2.02 avg=2.62\n",
            "[187 | 351.30] loss=2.01 avg=2.61\n",
            "[188 | 352.97] loss=2.61 avg=2.61\n",
            "[189 | 354.65] loss=2.97 avg=2.62\n",
            "[190 | 356.34] loss=2.33 avg=2.61\n",
            "[191 | 358.01] loss=2.44 avg=2.61\n",
            "[192 | 359.69] loss=2.71 avg=2.61\n",
            "[193 | 361.38] loss=2.19 avg=2.61\n",
            "[194 | 363.06] loss=2.17 avg=2.60\n",
            "[195 | 364.74] loss=2.45 avg=2.60\n",
            "[196 | 366.41] loss=1.95 avg=2.59\n",
            "[197 | 368.08] loss=2.15 avg=2.59\n",
            "[198 | 369.75] loss=1.76 avg=2.58\n",
            "[199 | 371.44] loss=2.32 avg=2.58\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " billion in new infrastructure spending in six years, the money is on fast track to becoming reality. That doesn't mean that Donald Trump has been wrong. However, he has not given us all the facts and is just playing an old-fashioned gameshow show like the last Republican debate.\n",
            "\n",
            "To say that Donald Trump would be the greatest president of the U.S. since Jimmy Carter would require making too much of the \"America First\" rhetoric and too few of the transactional deals that undergird it.\n",
            "\n",
            "We know that his \"America First\" economic nationalism has served as his political base. We also know that his policy of \"America First\" trade protectionism is working wonders by keeping U.S. exports of manufactured products, most notably cars, at home. What Trump wants from a trade regime is competition, not imposition.\n",
            "\n",
            "On trade, though he is a little like the man who, in a tweet, wrote, \"Mexico is paying too much, but I get nothing in return, either. Why not get something in exchange?\"\n",
            "\n",
            "Trump might get something in return here. He was in a position to sell a better wall, and he did. But even so, it is a foolish policy to impose costly tariffs to benefit a nation and its people. Rather, Congress should craft and enact rules that will help American workers and employers — not try to please foreign leaders and benefit only themselves.\n",
            "\n",
            "Moreover, Trump has no problem demanding bribes from foreign governments that may or may not be paid or that won't necessarily produce an advantage. He is a shameless swindler, and his crony capitalism is an indictment of federal government.\n",
            "\n",
            "After his failed attempt at the 1980 candidacy and his failure to win the GOP presidential nomination, he had to go back to reality. He was confronted with a choice: either abandon Americanism and pursue global power — or give up his U.S. and the world's wealth and prestige and live off American taxpayers. The choice was between the kind of internationalism many who support him are committed to and the kind of internationalism that he has been unwilling or unable to give up.\n",
            "\n",
            "Trump, if there is such a thing as a man who can't be defeated, would do well to learn a lesson from the last Republican candidate — and from the fact-checkers of the Clinton campaign.\n",
            "\n",
            "Read more from Eugene Robinson's archive, follow him on Twitter or subscribe to his updates on Facebook. You can also join him Tuesdays at 1 p.m. for a live Q&A.<|endoftext|>We'll get to the story of how the NFL is supposed to help make football a more equitable game.\n",
            "\n",
            "But here is what is remarkable. In a series of actions against the NFL in March — when the league was being investigated over a scheme to shield NFL players — NFL commissioner Roger Goodell, acting on behalf of the league, sought legal advice from NFLPA executive director DeMaurice Smith. In one instance, Smith sought a conference call from Goodell. Smith informed Goodell on March 27. Goodell responded a few days later, after Smith gave him an opportunity to respond. Smith then left Goodell with the message that he needs to prepare a letter stating his position.\n",
            "\n",
            "In other words, Goodell has been told by Smith that he has a responsibility to prepare a letter addressing his concerns. (Smith declined to do the work for Goodell — but he didn't make a big deal out of it or complain.) Smith did not send Goodell a letter, instead, telling him:\n",
            "\n",
            "Here is what Goodell did respond to:\n",
            "\n",
            "In March, Smith had written about his concerns with Goodell, and Goodell had acknowledged them — so, in August, Smith wrote a letter to Goodell. In this letter, Smith said:\n",
            "\n",
            "The NFL has taken action against many former players who have engaged in behavior that is detrimental and unlawful to the NFL franchise and its values. That includes the use of physical violence by ex-players; the use by former players of the language and practices that can create a hostile environment; and the use of discriminatory social media policies by former players that threaten players and adversely affect player-athletes, who often may have limited online communication for fear of being targeted in those same discriminatory policies.\n",
            "\n",
            "We have seen Goodell respond to Smith's concerns, in other words.\n",
            "\n",
            "On Friday morning, I asked Goodell to respond to Smith's letter and his apparent efforts to have him prepare a letter. The NFL declined.\n",
            "\n",
            "So, while Goodell has admitted he needs to act, he has done little or nothing. To what purpose? To what purpose?\n",
            "\n",
            "It seems that the league wants an answer from Goodell, and that the league does not want to hear it.\n",
            "\n",
            "But that is the question. The one Goodell is told to answer.\n",
            "\n",
            "Smith, in his letter, wrote:\n",
            "\n",
            "The NFL needs to address the critical issues raised by IJR and the NFL Players Association including: The NFLPA and their allies in the media, the players and fans\n",
            "\n",
            "[200 | 395.94] loss=2.51 avg=2.58\n",
            "[201 | 397.59] loss=2.10 avg=2.57\n",
            "[202 | 399.26] loss=2.95 avg=2.57\n",
            "[203 | 400.93] loss=1.97 avg=2.57\n",
            "[204 | 402.59] loss=2.21 avg=2.56\n",
            "[205 | 404.25] loss=2.94 avg=2.57\n",
            "[206 | 405.91] loss=2.25 avg=2.56\n",
            "[207 | 407.57] loss=2.70 avg=2.57\n",
            "[208 | 409.23] loss=2.70 avg=2.57\n",
            "[209 | 410.89] loss=2.44 avg=2.57\n",
            "[210 | 412.55] loss=2.51 avg=2.56\n",
            "[211 | 414.21] loss=2.41 avg=2.56\n",
            "[212 | 415.87] loss=2.17 avg=2.56\n",
            "[213 | 417.54] loss=2.10 avg=2.55\n",
            "[214 | 419.21] loss=2.30 avg=2.55\n",
            "[215 | 420.87] loss=2.86 avg=2.55\n",
            "[216 | 422.53] loss=2.66 avg=2.56\n",
            "[217 | 424.20] loss=2.12 avg=2.55\n",
            "[218 | 425.87] loss=2.96 avg=2.55\n",
            "[219 | 427.53] loss=2.43 avg=2.55\n",
            "[220 | 429.20] loss=1.91 avg=2.55\n",
            "[221 | 430.87] loss=1.98 avg=2.54\n",
            "[222 | 432.54] loss=3.00 avg=2.55\n",
            "[223 | 434.22] loss=1.83 avg=2.54\n",
            "[224 | 435.89] loss=2.63 avg=2.54\n",
            "[225 | 437.56] loss=2.44 avg=2.54\n",
            "[226 | 439.23] loss=1.92 avg=2.53\n",
            "[227 | 440.90] loss=2.42 avg=2.53\n",
            "[228 | 442.57] loss=2.13 avg=2.52\n",
            "[229 | 444.26] loss=2.65 avg=2.53\n",
            "[230 | 445.93] loss=2.23 avg=2.52\n",
            "[231 | 447.60] loss=2.44 avg=2.52\n",
            "[232 | 449.26] loss=2.66 avg=2.52\n",
            "[233 | 450.94] loss=2.50 avg=2.52\n",
            "[234 | 452.63] loss=2.39 avg=2.52\n",
            "[235 | 454.30] loss=2.56 avg=2.52\n",
            "[236 | 455.97] loss=2.35 avg=2.52\n",
            "[237 | 457.66] loss=2.28 avg=2.52\n",
            "[238 | 459.33] loss=2.48 avg=2.52\n",
            "[239 | 461.00] loss=2.69 avg=2.52\n",
            "[240 | 462.68] loss=2.05 avg=2.51\n",
            "[241 | 464.36] loss=2.60 avg=2.51\n",
            "[242 | 466.03] loss=2.29 avg=2.51\n",
            "[243 | 467.69] loss=2.61 avg=2.51\n",
            "[244 | 469.36] loss=2.46 avg=2.51\n",
            "[245 | 471.03] loss=2.47 avg=2.51\n",
            "[246 | 472.72] loss=3.04 avg=2.52\n",
            "[247 | 474.40] loss=2.59 avg=2.52\n",
            "[248 | 476.07] loss=2.38 avg=2.52\n",
            "[249 | 477.74] loss=2.28 avg=2.51\n",
            "[250 | 479.42] loss=2.56 avg=2.52\n",
            "[251 | 481.09] loss=3.09 avg=2.52\n",
            "[252 | 482.76] loss=2.43 avg=2.52\n",
            "[253 | 484.44] loss=2.05 avg=2.52\n",
            "[254 | 486.11] loss=3.10 avg=2.52\n",
            "[255 | 487.79] loss=2.64 avg=2.52\n",
            "[256 | 489.46] loss=1.76 avg=2.51\n",
            "[257 | 491.13] loss=2.62 avg=2.52\n",
            "[258 | 492.80] loss=2.95 avg=2.52\n",
            "[259 | 494.47] loss=2.11 avg=2.52\n",
            "[260 | 496.15] loss=2.71 avg=2.52\n",
            "[261 | 497.81] loss=2.28 avg=2.52\n",
            "[262 | 499.49] loss=2.52 avg=2.52\n",
            "[263 | 501.16] loss=2.32 avg=2.51\n",
            "[264 | 502.85] loss=2.74 avg=2.52\n",
            "[265 | 504.51] loss=2.03 avg=2.51\n",
            "[266 | 506.18] loss=2.93 avg=2.52\n",
            "[267 | 507.85] loss=2.10 avg=2.51\n",
            "[268 | 509.52] loss=2.26 avg=2.51\n",
            "[269 | 511.19] loss=2.17 avg=2.50\n",
            "[270 | 512.86] loss=2.56 avg=2.50\n",
            "[271 | 514.53] loss=2.67 avg=2.51\n",
            "[272 | 516.20] loss=2.40 avg=2.51\n",
            "[273 | 517.87] loss=2.29 avg=2.50\n",
            "[274 | 519.54] loss=2.04 avg=2.50\n",
            "[275 | 521.20] loss=2.47 avg=2.50\n",
            "[276 | 522.87] loss=2.19 avg=2.49\n",
            "[277 | 524.54] loss=2.19 avg=2.49\n",
            "[278 | 526.21] loss=2.28 avg=2.49\n",
            "[279 | 527.88] loss=2.69 avg=2.49\n",
            "[280 | 529.55] loss=1.74 avg=2.48\n",
            "[281 | 531.22] loss=2.62 avg=2.48\n",
            "[282 | 532.89] loss=1.50 avg=2.47\n",
            "[283 | 534.56] loss=2.77 avg=2.48\n",
            "[284 | 536.22] loss=2.11 avg=2.47\n",
            "[285 | 537.90] loss=2.94 avg=2.48\n",
            "[286 | 539.57] loss=2.02 avg=2.47\n",
            "[287 | 541.24] loss=2.11 avg=2.47\n",
            "[288 | 542.91] loss=1.90 avg=2.46\n",
            "[289 | 544.59] loss=2.41 avg=2.46\n",
            "[290 | 546.26] loss=2.98 avg=2.47\n",
            "[291 | 547.93] loss=3.00 avg=2.47\n",
            "[292 | 549.60] loss=2.27 avg=2.47\n",
            "[293 | 551.28] loss=2.24 avg=2.47\n",
            "[294 | 552.94] loss=1.43 avg=2.46\n",
            "[295 | 554.62] loss=2.37 avg=2.46\n",
            "[296 | 556.29] loss=1.93 avg=2.45\n",
            "[297 | 557.96] loss=2.43 avg=2.45\n",
            "[298 | 559.64] loss=1.76 avg=2.44\n",
            "[299 | 561.31] loss=2.20 avg=2.44\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " Barr ‘s Senate campaign in 2016. So long as Sessions does not recuse himself from any investigation relating to the Clinton emails probe, Democrats will try to tie it to Trump’s personal involvement in Mueller’s probe of Russian interference in the 2016 election.\n",
            "\n",
            "That will be the point of the special counsel’s letter to Comey. The deputy attorney general gave Comey an ultimatum — either he drop the Clinton investigation or drop Mueller. Comey was, however, not about to drop it. He would not recuse himself — because why would he? If Donald Trump were to be investigated, he is Trump; why would he recuse himself? It would be tantamount to pardoning him. Indeed, if the president is the target of Mueller’s investigation, Trump could be tried for obstruction of justice by a federal court and would most likely be convicted.\n",
            "\n",
            "I mean, come on, folks. The law is on the president’s side; who makes the law? As for Mueller’s investigation, I am sure he will issue a report that concludes, to the best of his recollection, that there is “no basis” for his investigation”. As for the email inquiry, there is evidence in McCabe’s letter to Comey that his investigation began before the March 2, 2016, memo was even written. We will see if that’s the case.\n",
            "\n",
            "What am I saying, you’re just learning? It certainly seems like the president was involved in the decision to fire Mueller and the rationale behind it. Now if there was some legitimate investigation of him, that would be more of a distraction, right?\n",
            "\n",
            "Comey is an honorable man, but it appears that he was so intent on protecting Trump’s reputation that he recused himself from the Clinton probe. There certainly appears to have been no investigation of Trump. It has been suggested (myths about James Comey) that Comey may have been acting on the advice of two senior Justice Department officials, including his boss, in their belief that Rosenstein was an illegitimate nominee who should have been subjected to a judicial hearing. It is hard to imagine why the law forbids Justice Department lawyers from discussing their personal opinions on a nominee before the nominee has been confirmed.\n",
            "\n",
            "In any case, there is no need for Comey to testify. The letter doesn’t say much. The point was that, by not recusing himself from the Clinton investigation, the FBI chief was creating a cloud over his investigation of Trump. Mueller and Acting Attorney General Rod Rosenstein must now decide whether they are going to allow this cloud to grow too big. This is going to be an ugly chapter in their careers if we’re not vigilant and diligent.\n",
            "\n",
            "© 2019, Washington Post Writers Group\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "‘\n",
            "\n",
            "For a time, it might be time to contemplate whether we really do need a special counsel. The case with the president and the Trump campaign has been exhausted.\n",
            "\n",
            "“\n",
            "\n",
            "And so, while waiting for the letter from Rosenstein, I began to think about the best course of action when it comes to prosecuting President Trump. I did not start this thought experiment. I said aloud what I had been thinking since the first time I heard the president say that Barack Obama was not a U.S. citizen, although it did not occur to me that I was asking President Obama to deny the truth of that charge.\n",
            "\n",
            "I did not stop there. Before leaving the White House, I began to consider what, by then, should become clear to anyone who wanted to believe that the president, having learned of the investigation, was not making up facts, that the investigation was not a cover-up, that there was no collusion, and that everything President Obama did could not be characterized as a wiretap, the law of surveillance permitting such monitoring.\n",
            "\n",
            "By then, I had not thought far into the future. My thinking about the president’s case had been confined to the present. I was still waiting for the letter from Rosenstein, waiting for Trump to drop the bombshell charge that he could fire the special counsel. I was waiting, and waiting, and waiting.\n",
            "\n",
            "Then, on Wednesday, Mueller was asked why he had not indicted the president yet, and he replied with a smile: “I’m not going to talk about the special counsel right now!” And I wondered: Was the smile about what we expected? The special counsel, a career prosecutor, and Trump political operative, had the usual reluctance to be seen as a politicized stooge or to do what the president has been asking him to do for months: publicly rebuke the special counsel’s report (the report had, for months, been under review by the Justice Department’s Office of Professional Responsibility; a review carried out by the independent counsels office) and to call the special counsel’s report’s conclusions “outrageous” and “\n",
            "\n",
            "[300 | 585.67] loss=2.10 avg=2.44\n",
            "[301 | 587.33] loss=1.92 avg=2.43\n",
            "[302 | 589.01] loss=2.57 avg=2.43\n",
            "[303 | 590.68] loss=2.37 avg=2.43\n",
            "[304 | 592.34] loss=2.14 avg=2.43\n",
            "[305 | 594.01] loss=1.21 avg=2.42\n",
            "[306 | 595.68] loss=2.68 avg=2.42\n",
            "[307 | 597.35] loss=2.01 avg=2.42\n",
            "[308 | 599.02] loss=2.51 avg=2.42\n",
            "[309 | 600.69] loss=2.38 avg=2.42\n",
            "[310 | 602.36] loss=2.02 avg=2.41\n",
            "[311 | 604.02] loss=2.00 avg=2.41\n",
            "[312 | 605.70] loss=2.42 avg=2.41\n",
            "[313 | 607.36] loss=2.27 avg=2.41\n",
            "[314 | 609.03] loss=2.97 avg=2.41\n",
            "[315 | 610.70] loss=2.27 avg=2.41\n",
            "[316 | 612.38] loss=1.59 avg=2.40\n",
            "[317 | 614.05] loss=2.29 avg=2.40\n",
            "[318 | 615.72] loss=2.42 avg=2.40\n",
            "[319 | 617.39] loss=2.07 avg=2.40\n",
            "[320 | 619.07] loss=2.40 avg=2.40\n",
            "[321 | 620.74] loss=2.62 avg=2.40\n",
            "[322 | 622.39] loss=2.09 avg=2.40\n",
            "[323 | 624.07] loss=2.25 avg=2.40\n",
            "[324 | 625.72] loss=2.50 avg=2.40\n",
            "[325 | 627.39] loss=2.68 avg=2.40\n",
            "[326 | 629.07] loss=2.21 avg=2.40\n",
            "[327 | 630.74] loss=1.78 avg=2.39\n",
            "[328 | 632.41] loss=2.58 avg=2.39\n",
            "[329 | 634.07] loss=2.49 avg=2.39\n",
            "[330 | 635.75] loss=2.61 avg=2.40\n",
            "[331 | 637.42] loss=1.85 avg=2.39\n",
            "[332 | 639.09] loss=2.20 avg=2.39\n",
            "[333 | 640.75] loss=2.29 avg=2.39\n",
            "[334 | 642.42] loss=3.15 avg=2.40\n",
            "[335 | 644.09] loss=2.74 avg=2.40\n",
            "[336 | 645.76] loss=2.32 avg=2.40\n",
            "[337 | 647.43] loss=2.31 avg=2.40\n",
            "[338 | 649.10] loss=1.73 avg=2.39\n",
            "[339 | 650.77] loss=2.37 avg=2.39\n",
            "[340 | 652.42] loss=2.03 avg=2.39\n",
            "[341 | 654.09] loss=2.10 avg=2.38\n",
            "[342 | 655.76] loss=2.78 avg=2.39\n",
            "[343 | 657.42] loss=2.80 avg=2.39\n",
            "[344 | 659.10] loss=1.62 avg=2.38\n",
            "[345 | 660.77] loss=2.68 avg=2.39\n",
            "[346 | 662.42] loss=2.53 avg=2.39\n",
            "[347 | 664.09] loss=2.43 avg=2.39\n",
            "[348 | 665.76] loss=2.58 avg=2.39\n",
            "[349 | 667.43] loss=1.65 avg=2.38\n",
            "[350 | 669.10] loss=2.39 avg=2.38\n",
            "[351 | 670.76] loss=2.25 avg=2.38\n",
            "[352 | 672.43] loss=2.30 avg=2.38\n",
            "[353 | 674.09] loss=2.14 avg=2.38\n",
            "[354 | 675.77] loss=2.66 avg=2.38\n",
            "[355 | 677.43] loss=1.77 avg=2.38\n",
            "[356 | 679.10] loss=1.97 avg=2.37\n",
            "[357 | 680.78] loss=2.58 avg=2.37\n",
            "[358 | 682.45] loss=1.59 avg=2.37\n",
            "[359 | 684.12] loss=2.30 avg=2.36\n",
            "[360 | 685.79] loss=2.49 avg=2.37\n",
            "[361 | 687.47] loss=1.93 avg=2.36\n",
            "[362 | 689.13] loss=1.89 avg=2.36\n",
            "[363 | 690.80] loss=3.32 avg=2.37\n",
            "[364 | 692.47] loss=2.61 avg=2.37\n",
            "[365 | 694.14] loss=1.96 avg=2.36\n",
            "[366 | 695.80] loss=1.39 avg=2.35\n",
            "[367 | 697.47] loss=2.23 avg=2.35\n",
            "[368 | 699.14] loss=2.23 avg=2.35\n",
            "[369 | 700.81] loss=1.76 avg=2.35\n",
            "[370 | 702.48] loss=1.68 avg=2.34\n",
            "[371 | 704.15] loss=1.84 avg=2.33\n",
            "[372 | 705.82] loss=1.68 avg=2.33\n",
            "[373 | 707.49] loss=1.81 avg=2.32\n",
            "[374 | 709.16] loss=1.75 avg=2.32\n",
            "[375 | 710.83] loss=1.59 avg=2.31\n",
            "[376 | 712.50] loss=2.64 avg=2.31\n",
            "[377 | 714.17] loss=2.03 avg=2.31\n",
            "[378 | 715.84] loss=2.08 avg=2.31\n",
            "[379 | 717.51] loss=2.23 avg=2.31\n",
            "[380 | 719.17] loss=2.35 avg=2.31\n",
            "[381 | 720.85] loss=2.67 avg=2.31\n",
            "[382 | 722.52] loss=2.23 avg=2.31\n",
            "[383 | 724.19] loss=2.44 avg=2.31\n",
            "[384 | 725.86] loss=2.57 avg=2.31\n",
            "[385 | 727.53] loss=2.39 avg=2.31\n",
            "[386 | 729.20] loss=2.11 avg=2.31\n",
            "[387 | 730.87] loss=1.55 avg=2.30\n",
            "[388 | 732.54] loss=2.21 avg=2.30\n",
            "[389 | 734.21] loss=2.39 avg=2.30\n",
            "[390 | 735.88] loss=2.82 avg=2.31\n",
            "[391 | 737.55] loss=2.28 avg=2.31\n",
            "[392 | 739.22] loss=2.14 avg=2.31\n",
            "[393 | 740.89] loss=2.31 avg=2.31\n",
            "[394 | 742.56] loss=2.44 avg=2.31\n",
            "[395 | 744.25] loss=2.22 avg=2.31\n",
            "[396 | 745.92] loss=1.91 avg=2.30\n",
            "[397 | 747.60] loss=2.33 avg=2.30\n",
            "[398 | 749.28] loss=1.92 avg=2.30\n",
            "[399 | 750.95] loss=1.95 avg=2.30\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " of such a program.\n",
            "\n",
            "The fact that Trump is the GOP nominee, however, should certainly not exempt him from having to confront this question. What does it mean to have an actual, credible threat of attack? What are the consequences of a nuclear war? I wrote about that topic recently when discussing Russia (for which I am sympathetic, but not necessarily in a very moral manner). The world that we live in is one in which there have been nuclear or chemical wars (or at least the potential for them); there's the possibility that some regime or ideology might carry a nuke or commit something akin to genocide, which would be a terrible catastrophe. However, it isn't always obvious just what type of government or ideology might carry out such an act. And even if it is, we haven't yet seen the consequences of such attacks, which, if the world were truly a model of peace, would be relatively small. Nuclear war would be a much more devastating event because we're dealing with far more lethal weapons; an American attack on Russia would be something far more terrible than a nuclear war between two country states.\n",
            "\n",
            "The United States isn't the only country on the planet. Many of the world's nuclear powers, such as Britain, France, Russia, India, China, Argentina, Pakistan, and Egypt, have long maintained deterrents against war against their neighbors. The United States, for various reasons, has been too timid in challenging those nations, and these countries have responded to those actions by building nuclear weapons. It seems to me that it is wrong and dangerous to do so in the current climate.\n",
            "\n",
            "I don't think this is particularly controversial, beyond my having an extremely limited acquaintance with the issue. In fact, it probably isn't controversial at all. The United States, which is the only major power on the planet (except for France, which has the luxury of being the sole remaining nonnuclear power) is currently engaged in a war with an adversary that is a large, nuclear-armed power — in the case of Russia, which is the second largest country in the world (after China). A substantial, if not overwhelming, portion of the world probably lives under these two massive powers' nuclear umbrella, which presents a unique set of geopolitical and nuclear threats compared to the current situation. There has been some debate as to whether or how many of these threats exist, but the overwhelming case, in my view, is that none should be underestimated in the present environment given the seriousness of our foreign policy challenges.\n",
            "\n",
            "It isn't that I think that the United States should have been intervening directly in the conflicts and crises that led to the present conflicts. I don't think the intervention would have been very helpful, and I certainly don't think that the intervention would have led to the desired outcomes. But the circumstances in which it was undertaken, combined with the realities of the times, make me doubt that it would have been a very good idea.\n",
            "\n",
            "For example, let me give you an example from the present context, because it reflects something I think is relevant in any discussion about nuclear power. If we took a very realistic look at the geopolitical situation, it is not that the world has been in a very peaceful era. The evidence is quite the opposite. A recent study by the Stockholm International Peace Research Institute puts the human-caused-wastage rate of global greenhouse gases about two and a half times higher than the 2° C scenario that many environmental groups want to see. That number doesn't take into account how rapidly emissions from countries are reducing and are likely to be reducing, and how much effort is going into addressing these problems. That's why the rate of climate change, despite the overwhelming scientific evidence, is seen as an urgent challenge, and so it is that countries are pursuing policies that increase their risks of causing more climate change. It's not that we don't want to do something about climate change, I suppose we do, but it is one of those problems that should be decided only after the fact, not only because of all the costs involved in solving it, but also because we should be more focused on the long term benefits of taking action on climate change.\n",
            "\n",
            "The United States has had many attempts to improve things with the language and the terminology, but it seems to me that the current administration, since taking office in January, has demonstrated little indication that it understands the problems with the terminology, and is eager to do more harm than good with the language. What we have now is a situation in which some countries, even those on the Security Council, are using terms like \"regime change,\" and that is the wrong term — which means that, contrary to what some people say, they are not regime-change actors, and are simply trying to resolve long-standing disputes, not make regime-change operations. The term \"regime change\" itself (and the use of the term by the Obama appointees) reflects their unwillingness to distinguish regime change from political and diplomatic intervention, and their desire to continue using the term\n",
            "\n",
            "[400 | 775.42] loss=2.60 avg=2.30\n",
            "[401 | 777.09] loss=2.75 avg=2.30\n",
            "[402 | 778.75] loss=2.36 avg=2.31\n",
            "[403 | 780.41] loss=2.10 avg=2.30\n",
            "[404 | 782.08] loss=2.07 avg=2.30\n",
            "[405 | 783.74] loss=1.93 avg=2.30\n",
            "[406 | 785.41] loss=2.33 avg=2.30\n",
            "[407 | 787.07] loss=2.05 avg=2.29\n",
            "[408 | 788.72] loss=2.95 avg=2.30\n",
            "[409 | 790.36] loss=2.10 avg=2.30\n",
            "[410 | 792.02] loss=2.22 avg=2.30\n",
            "[411 | 793.69] loss=2.11 avg=2.30\n",
            "[412 | 795.35] loss=2.03 avg=2.29\n",
            "[413 | 797.01] loss=2.23 avg=2.29\n",
            "[414 | 798.67] loss=2.52 avg=2.30\n",
            "[415 | 800.33] loss=2.39 avg=2.30\n",
            "[416 | 802.00] loss=1.52 avg=2.29\n",
            "[417 | 803.66] loss=2.57 avg=2.29\n",
            "[418 | 805.33] loss=1.75 avg=2.29\n",
            "[419 | 806.99] loss=1.88 avg=2.28\n",
            "[420 | 808.67] loss=1.99 avg=2.28\n",
            "[421 | 810.34] loss=1.58 avg=2.27\n",
            "[422 | 812.00] loss=2.10 avg=2.27\n",
            "[423 | 813.67] loss=1.05 avg=2.26\n",
            "[424 | 815.34] loss=2.03 avg=2.26\n",
            "[425 | 817.01] loss=2.40 avg=2.26\n",
            "[426 | 818.68] loss=3.04 avg=2.26\n",
            "[427 | 820.35] loss=2.14 avg=2.26\n",
            "[428 | 822.03] loss=2.39 avg=2.26\n",
            "[429 | 823.70] loss=1.12 avg=2.25\n",
            "[430 | 825.38] loss=1.28 avg=2.24\n",
            "[431 | 827.05] loss=1.88 avg=2.24\n",
            "[432 | 828.72] loss=2.84 avg=2.25\n",
            "[433 | 830.39] loss=1.74 avg=2.24\n",
            "[434 | 832.07] loss=2.02 avg=2.24\n",
            "[435 | 833.74] loss=1.96 avg=2.24\n",
            "[436 | 835.41] loss=1.96 avg=2.23\n",
            "[437 | 837.08] loss=1.98 avg=2.23\n",
            "[438 | 838.75] loss=2.51 avg=2.23\n",
            "[439 | 840.43] loss=2.21 avg=2.23\n",
            "[440 | 842.10] loss=2.32 avg=2.23\n",
            "[441 | 843.77] loss=1.87 avg=2.23\n",
            "[442 | 845.46] loss=1.91 avg=2.23\n",
            "[443 | 847.13] loss=1.89 avg=2.22\n",
            "[444 | 848.80] loss=2.66 avg=2.23\n",
            "[445 | 850.48] loss=1.85 avg=2.22\n",
            "[446 | 852.16] loss=1.82 avg=2.22\n",
            "[447 | 853.83] loss=2.56 avg=2.22\n",
            "[448 | 855.50] loss=1.77 avg=2.22\n",
            "[449 | 857.17] loss=1.56 avg=2.21\n",
            "[450 | 858.84] loss=2.31 avg=2.21\n",
            "[451 | 860.53] loss=1.94 avg=2.21\n",
            "[452 | 862.21] loss=1.76 avg=2.21\n",
            "[453 | 863.89] loss=2.67 avg=2.21\n",
            "[454 | 865.56] loss=2.06 avg=2.21\n",
            "[455 | 867.24] loss=1.87 avg=2.21\n",
            "[456 | 868.93] loss=2.13 avg=2.20\n",
            "[457 | 870.60] loss=1.89 avg=2.20\n",
            "[458 | 872.28] loss=2.49 avg=2.20\n",
            "[459 | 873.96] loss=1.96 avg=2.20\n",
            "[460 | 875.63] loss=1.62 avg=2.20\n",
            "[461 | 877.29] loss=1.95 avg=2.19\n",
            "[462 | 878.97] loss=2.37 avg=2.20\n",
            "[463 | 880.64] loss=2.78 avg=2.20\n",
            "[464 | 882.31] loss=1.78 avg=2.20\n",
            "[465 | 883.99] loss=1.81 avg=2.19\n",
            "[466 | 885.66] loss=2.25 avg=2.19\n",
            "[467 | 887.33] loss=2.42 avg=2.20\n",
            "[468 | 889.00] loss=2.18 avg=2.20\n",
            "[469 | 890.68] loss=2.08 avg=2.19\n",
            "[470 | 892.35] loss=2.19 avg=2.19\n",
            "[471 | 894.02] loss=2.15 avg=2.19\n",
            "[472 | 895.70] loss=1.66 avg=2.19\n",
            "[473 | 897.37] loss=2.20 avg=2.19\n",
            "[474 | 899.04] loss=2.23 avg=2.19\n",
            "[475 | 900.72] loss=1.56 avg=2.18\n",
            "[476 | 902.39] loss=1.98 avg=2.18\n",
            "[477 | 904.05] loss=1.72 avg=2.18\n",
            "[478 | 905.71] loss=1.66 avg=2.17\n",
            "[479 | 907.38] loss=2.10 avg=2.17\n",
            "[480 | 909.05] loss=1.93 avg=2.17\n",
            "[481 | 910.72] loss=2.41 avg=2.17\n",
            "[482 | 912.38] loss=1.75 avg=2.17\n",
            "[483 | 914.06] loss=1.94 avg=2.16\n",
            "[484 | 915.73] loss=2.17 avg=2.16\n",
            "[485 | 917.40] loss=3.15 avg=2.17\n",
            "[486 | 919.07] loss=1.48 avg=2.17\n",
            "[487 | 920.74] loss=1.37 avg=2.16\n",
            "[488 | 922.41] loss=2.15 avg=2.16\n",
            "[489 | 924.08] loss=1.91 avg=2.16\n",
            "[490 | 925.74] loss=1.32 avg=2.15\n",
            "[491 | 927.42] loss=2.28 avg=2.15\n",
            "[492 | 929.09] loss=2.70 avg=2.15\n",
            "[493 | 930.75] loss=1.98 avg=2.15\n",
            "[494 | 932.41] loss=2.34 avg=2.15\n",
            "[495 | 934.09] loss=2.32 avg=2.16\n",
            "[496 | 935.77] loss=1.82 avg=2.15\n",
            "[497 | 937.43] loss=1.89 avg=2.15\n",
            "[498 | 939.10] loss=1.07 avg=2.14\n",
            "[499 | 940.77] loss=1.20 avg=2.13\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " story, but I'm also certain. I've been saying it for at least 10 years, and I'm not going anywhere.\n",
            "\n",
            "For now, though, there's only the fact that it isn't true. In addition to the fact that the Obama administration and the administration under former Democratic president Barack Obama generally refused to acknowledge that the president had engaged in torture, the Times report fails to mention at all that as of 2010, at least 17 U.S. attorneys general had already admitted that what the Times is reporting is not a single prosecutable case of torture — including the 18 who are now accusing Trump of conducting waterboarding on the president.\n",
            "\n",
            "The Times also fails to note that former CIA director John Brennan acknowledged that the CIA misled Congress about the nature of enhanced interrogation techniques and that Brennan now says he was mistaken not to inform Congress about them. The report doesn't mention Brennan at all, and he certainly should have, since he is the man on record as contradicting the Times report. But he has not done so, and his deputy, John O. Brennan, has been telling it like it is since before Obama became president — and that, in part, explains the misleading.\n",
            "\n",
            "Here are some of the reasons why:\n",
            "\n",
            "A. Brennan and Brennan loyalists in the Obama administration knew that the public was unlikely to believe this nonsense. A Quinnipiac University poll released the day after Obama left office showed 65 percent of Americans saying Obama had done the right thing. It wasn't unusual for even some of the most skeptical Democrats to embrace him, which was not always easy. Indeed, that pattern continued after Obama left office, with some Democrats, such as former Secretary of State Hillary Clinton, supporting him over Romney despite the evidence showing he had not committed a single felony.\n",
            "\n",
            "B. In addition, Brennan was in a position to know that the public would believe, not the lies of this administration but the lies told by Obama himself for years about the president actually having committed a single crime. And yet he chose to tell the complete opposite. Obama has been proven guilty in the courts, in congressional hearings, and in the pages of the FBI report — and the public has been misled over and over about who really committed whom.\n",
            "\n",
            "C. As I have long suggested, the government cannot be a full partner in the criminal justice process only to the extent that it admits guilt on the spot in court. The government cannot plead innocence and not admit guilt, because only then does the burden of proof shift from the government to the accused and from the accused to the accused. If the public were to have the certainty that the government would plead innocent, it would never, under any circumstances, accept the government at its word about every allegation against the president. The media would have a duty to report the truth, not spin it. The government, in contrast, has to be convinced of its innocence.\n",
            "\n",
            "Here, then, is Brennan telling the government, the government has never admitted guilt. Instead, the government has been lying about the president's guilt — and, therefore, what the public thinks, the public has to be made to believe is not only the case for the government not being guilty.\n",
            "\n",
            "The public doesn't want the public to know.\n",
            "\n",
            "It is time for a change: for the truth to be told, for the truth to be believed, not spun in the media as truth and not sold in the campaign ads.\n",
            "\n",
            "And that, my friends, is the real problem — the media refusing to report the truth, the government refusing to plead guilt on the spot, the media refusing to have to resort to a double standard, saying what it wishes to say but pretending not to believe it when it says it.\n",
            "\n",
            "That the media does not report the truth, not only is not new (see my article on this in 2011) but is a fact of life.\n",
            "\n",
            "That it treats the lie that the president is not guilty as the status quo is not new — it has been so for a very long time. When the government admitted in a 2005 Senate hearing that it had lied about the George W. Bush-led war on terror, the media played dumb on the issue, presenting President Bush as having been a liar.\n",
            "\n",
            "That the government admits the lie about the September 11th attacks and the Boston Marathon bombings in the same breath is something only certain Republicans and some Democrats will embrace is not new. As we have discussed previously, this was the case with the government's official version of when it said it had evidence that led to the suspects, the Bush administration officials who concocted the Bush-era narrative about 9/11. The media played along, as would any politician who does not question the government or its veracity: they reported that the government was telling a false story but not the FBI, the federal law-enforcement agency that investigated the attacks.\n",
            "\n",
            "That the media continues to report the Obama administration lying about the president committing a single crime is a sign that the American people do not need the media\n",
            "\n",
            "[500 | 965.11] loss=0.96 avg=2.12\n",
            "[501 | 966.78] loss=2.47 avg=2.12\n",
            "[502 | 968.45] loss=2.29 avg=2.12\n",
            "[503 | 970.12] loss=1.68 avg=2.12\n",
            "[504 | 971.78] loss=1.15 avg=2.11\n",
            "[505 | 973.46] loss=2.02 avg=2.11\n",
            "[506 | 975.12] loss=1.87 avg=2.11\n",
            "[507 | 976.79] loss=1.39 avg=2.10\n",
            "[508 | 978.46] loss=1.73 avg=2.10\n",
            "[509 | 980.12] loss=2.83 avg=2.10\n",
            "[510 | 981.79] loss=2.10 avg=2.10\n",
            "[511 | 983.45] loss=2.04 avg=2.10\n",
            "[512 | 985.12] loss=0.60 avg=2.09\n",
            "[513 | 986.79] loss=1.63 avg=2.08\n",
            "[514 | 988.45] loss=1.62 avg=2.08\n",
            "[515 | 990.12] loss=1.82 avg=2.08\n",
            "[516 | 991.79] loss=1.80 avg=2.07\n",
            "[517 | 993.46] loss=2.28 avg=2.07\n",
            "[518 | 995.12] loss=1.60 avg=2.07\n",
            "[519 | 996.79] loss=1.66 avg=2.07\n",
            "[520 | 998.47] loss=1.80 avg=2.06\n",
            "[521 | 1000.13] loss=2.07 avg=2.06\n",
            "[522 | 1001.80] loss=2.51 avg=2.07\n",
            "[523 | 1003.45] loss=2.23 avg=2.07\n",
            "[524 | 1005.12] loss=2.31 avg=2.07\n",
            "[525 | 1006.79] loss=1.88 avg=2.07\n",
            "[526 | 1008.46] loss=1.34 avg=2.06\n",
            "[527 | 1010.13] loss=1.69 avg=2.06\n",
            "[528 | 1011.80] loss=1.83 avg=2.06\n",
            "[529 | 1013.47] loss=1.57 avg=2.05\n",
            "[530 | 1015.14] loss=2.32 avg=2.05\n",
            "[531 | 1016.81] loss=3.15 avg=2.07\n",
            "[532 | 1018.48] loss=1.88 avg=2.06\n",
            "[533 | 1020.15] loss=1.91 avg=2.06\n",
            "[534 | 1021.82] loss=1.32 avg=2.05\n",
            "[535 | 1023.49] loss=2.41 avg=2.06\n",
            "[536 | 1025.16] loss=1.60 avg=2.05\n",
            "[537 | 1026.83] loss=2.35 avg=2.06\n",
            "[538 | 1028.50] loss=2.11 avg=2.06\n",
            "[539 | 1030.18] loss=2.06 avg=2.06\n",
            "[540 | 1031.85] loss=2.54 avg=2.06\n",
            "[541 | 1033.53] loss=2.03 avg=2.06\n",
            "[542 | 1035.21] loss=2.20 avg=2.06\n",
            "[543 | 1036.88] loss=1.56 avg=2.06\n",
            "[544 | 1038.55] loss=2.05 avg=2.06\n",
            "[545 | 1040.21] loss=2.97 avg=2.07\n",
            "[546 | 1041.89] loss=2.22 avg=2.07\n",
            "[547 | 1043.56] loss=1.35 avg=2.06\n",
            "[548 | 1045.23] loss=1.65 avg=2.06\n",
            "[549 | 1046.90] loss=2.86 avg=2.07\n",
            "[550 | 1048.57] loss=2.07 avg=2.07\n",
            "[551 | 1050.24] loss=1.97 avg=2.06\n",
            "[552 | 1051.90] loss=2.40 avg=2.07\n",
            "[553 | 1053.58] loss=1.89 avg=2.07\n",
            "[554 | 1055.25] loss=2.01 avg=2.07\n",
            "[555 | 1056.91] loss=1.48 avg=2.06\n",
            "[556 | 1058.58] loss=1.87 avg=2.06\n",
            "[557 | 1060.26] loss=2.02 avg=2.06\n",
            "[558 | 1061.92] loss=1.75 avg=2.05\n",
            "[559 | 1063.59] loss=1.67 avg=2.05\n",
            "[560 | 1065.27] loss=2.62 avg=2.06\n",
            "[561 | 1066.93] loss=1.56 avg=2.05\n",
            "[562 | 1068.60] loss=1.84 avg=2.05\n",
            "[563 | 1070.28] loss=2.40 avg=2.05\n",
            "[564 | 1071.95] loss=1.43 avg=2.05\n",
            "[565 | 1073.62] loss=2.49 avg=2.05\n",
            "[566 | 1075.29] loss=1.97 avg=2.05\n",
            "[567 | 1076.97] loss=1.89 avg=2.05\n",
            "[568 | 1078.65] loss=1.60 avg=2.04\n",
            "[569 | 1080.33] loss=1.99 avg=2.04\n",
            "[570 | 1082.00] loss=1.98 avg=2.04\n",
            "[571 | 1083.67] loss=2.10 avg=2.04\n",
            "[572 | 1085.34] loss=1.90 avg=2.04\n",
            "[573 | 1087.02] loss=2.33 avg=2.04\n",
            "[574 | 1088.70] loss=1.87 avg=2.04\n",
            "[575 | 1090.38] loss=1.24 avg=2.03\n",
            "[576 | 1092.05] loss=1.94 avg=2.03\n",
            "[577 | 1093.72] loss=1.49 avg=2.03\n",
            "[578 | 1095.42] loss=2.34 avg=2.03\n",
            "[579 | 1097.09] loss=2.67 avg=2.04\n",
            "[580 | 1098.76] loss=1.30 avg=2.03\n",
            "[581 | 1100.42] loss=2.08 avg=2.03\n",
            "[582 | 1102.11] loss=1.90 avg=2.03\n",
            "[583 | 1103.77] loss=1.60 avg=2.03\n",
            "[584 | 1105.45] loss=1.69 avg=2.02\n",
            "[585 | 1107.13] loss=2.01 avg=2.02\n",
            "[586 | 1108.81] loss=1.23 avg=2.01\n",
            "[587 | 1110.49] loss=1.92 avg=2.01\n",
            "[588 | 1112.15] loss=1.26 avg=2.01\n",
            "[589 | 1113.83] loss=1.94 avg=2.00\n",
            "[590 | 1115.52] loss=1.68 avg=2.00\n",
            "[591 | 1117.19] loss=2.07 avg=2.00\n",
            "[592 | 1118.88] loss=2.08 avg=2.00\n",
            "[593 | 1120.55] loss=1.71 avg=2.00\n",
            "[594 | 1122.22] loss=1.74 avg=2.00\n",
            "[595 | 1123.90] loss=1.95 avg=2.00\n",
            "[596 | 1125.58] loss=1.16 avg=1.99\n",
            "[597 | 1127.26] loss=1.88 avg=1.99\n",
            "[598 | 1128.93] loss=1.61 avg=1.98\n",
            "[599 | 1130.62] loss=1.77 avg=1.98\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "dn’s account.\n",
            "\n",
            "The account is unverified. There can’t possibly be a true person (as in, real, real person) who actually wrote this account, because the subject of this account does not exist.\n",
            "\n",
            "This proves nothing. If anything, the’s fake account proves that this person could have faked an account at another social media site without being noticed. Since so many people are sharing this account as their real one, this proves that real accounts are being shared by as many people as there are fake accounts.\n",
            "\n",
            "This also proves that there’s a huge, hidden pool of fake people. I’ll make an example of only those people who are posting very false information. Many other people are not. For our purposes, these others need not worry about this. If your account is false, don’t worry about it.\n",
            "\n",
            "A third element to this story’s structure is that, since the news cycle is very short, there’s only one news story that matters here: Clinton’s emails were on Anthony Weiner’s laptop; therefore, Clinton must not have been dishonest — unless the news reports that Clinton used the laptop for other reasons, such as destroying emails. (See: NBC’s report on Weiner’s laptop.)\n",
            "\n",
            "This, of course, is simply not true. See our Fact Checker V. Hillary Clinton Debunked — Misreporting Emails story for more on why. (You can view it here.)\n",
            "\n",
            "In addition, note that after Clinton’s emails were reported, the Weiner story had already been debunked at PolitiFact, at least in part because there was already ample time to conduct the necessary fact-checking. (See: NBC’s Clinton email debunking.) So the time for checking the story already existed.\n",
            "\n",
            "This, however, does not mean this story was not flawed. As we’ve just pointed out, there is no time limit on PolitiFact fact checks; our check was limited to whether the story was false or not. Additionally, there is no reason to believe that the Weiner story was never going to be refuted; it was simply not going to be refuted in the time allotted to us.\n",
            "\n",
            "Of course, that’s the least of their problems. Now, let’s come back to this account’s unverified account. When it was initially reported, this person claimed that this account was one that was “real.” This person then tweeted claiming that the Twitter account was real and not another fake account pretending to be the same person, and that the account was verified.\n",
            "\n",
            "Once again, note that the tweet was not verified. Why? We’ll never know; but if this account is one that is “real” (and we suspect it is, based on the fact that the first tweet was unverified), then there is no reason not to believe that this Twitter account is the same Twitter account that originally tweeted that Clinton’s emails were on Weiner’s laptop. In other “reality check” news, even PolitiFact couldn’t deny that the account was “real.”\n",
            "\n",
            "Let’s now move on to the second account’s status update, which was written just 24 hours before this user claimed that this account was “real.” According to this account’s status update, the Clinton campaign was already dismissing the account’s reports, and that it was “now just a bunch of crazy people with unverified stories.”\n",
            "\n",
            "Here is how this status update begins:\n",
            "\n",
            "Shortly after this “real” status update was written, two people went on CNN and said that the Clinton campaign was “still hearing about the tweets from Anthony Weiner,” and “we’re not denying that.”\n",
            "\n",
            "So, then, did the “real” accounts — which, given the above information, clearly were “real —” or did either of the two accounts suddenly claim to be the same person?\n",
            "\n",
            "By 2:30 p.m., about half way through the first update, this person had tweeted that “we’re just “a bunch of crazy people,” and “we’re not denying that.” After another “we’re not denying that,” another person had gone on CNN to acknowledge that a “real” account was being claimed to be the one that was being tweeted by the real account claiming to be the “real” account. This Twitter person seems to have been an ex-Sanders supporter, or “ex-Bernie-is-a-fraud” type of person.\n",
            "\n",
            "Now, we should point out that this Twitter person was not claiming that there were two “real” accounts\n",
            "\n",
            "[600 | 1155.00] loss=1.34 avg=1.98\n",
            "[601 | 1156.67] loss=3.00 avg=1.99\n",
            "[602 | 1158.34] loss=1.41 avg=1.98\n",
            "[603 | 1160.01] loss=2.16 avg=1.98\n",
            "[604 | 1161.67] loss=2.33 avg=1.98\n",
            "[605 | 1163.32] loss=1.19 avg=1.98\n",
            "[606 | 1164.98] loss=2.50 avg=1.98\n",
            "[607 | 1166.65] loss=1.33 avg=1.98\n",
            "[608 | 1168.31] loss=1.70 avg=1.97\n",
            "[609 | 1169.96] loss=2.34 avg=1.98\n",
            "[610 | 1171.62] loss=1.67 avg=1.97\n",
            "[611 | 1173.29] loss=2.35 avg=1.98\n",
            "[612 | 1174.95] loss=2.39 avg=1.98\n",
            "[613 | 1176.60] loss=1.46 avg=1.98\n",
            "[614 | 1178.26] loss=1.53 avg=1.97\n",
            "[615 | 1179.92] loss=1.67 avg=1.97\n",
            "[616 | 1181.58] loss=1.38 avg=1.96\n",
            "[617 | 1183.25] loss=2.19 avg=1.97\n",
            "[618 | 1184.91] loss=1.38 avg=1.96\n",
            "[619 | 1186.58] loss=1.88 avg=1.96\n",
            "[620 | 1188.25] loss=1.66 avg=1.96\n",
            "[621 | 1189.92] loss=1.60 avg=1.95\n",
            "[622 | 1191.58] loss=1.73 avg=1.95\n",
            "[623 | 1193.25] loss=1.67 avg=1.95\n",
            "[624 | 1194.92] loss=1.81 avg=1.95\n",
            "[625 | 1196.59] loss=1.39 avg=1.94\n",
            "[626 | 1198.27] loss=1.21 avg=1.93\n",
            "[627 | 1199.94] loss=1.70 avg=1.93\n",
            "[628 | 1201.60] loss=1.82 avg=1.93\n",
            "[629 | 1203.26] loss=1.77 avg=1.93\n",
            "[630 | 1204.94] loss=1.39 avg=1.92\n",
            "[631 | 1206.61] loss=2.39 avg=1.93\n",
            "[632 | 1208.28] loss=1.52 avg=1.92\n",
            "[633 | 1209.96] loss=2.13 avg=1.92\n",
            "[634 | 1211.65] loss=1.55 avg=1.92\n",
            "[635 | 1213.31] loss=1.54 avg=1.92\n",
            "[636 | 1214.99] loss=2.06 avg=1.92\n",
            "[637 | 1216.66] loss=1.93 avg=1.92\n",
            "[638 | 1218.33] loss=2.18 avg=1.92\n",
            "[639 | 1220.00] loss=1.03 avg=1.91\n",
            "[640 | 1221.68] loss=2.14 avg=1.91\n",
            "[641 | 1223.37] loss=1.72 avg=1.91\n",
            "[642 | 1225.04] loss=0.75 avg=1.90\n",
            "[643 | 1226.71] loss=2.44 avg=1.91\n",
            "[644 | 1228.39] loss=1.06 avg=1.90\n",
            "[645 | 1230.06] loss=2.07 avg=1.90\n",
            "[646 | 1231.73] loss=1.38 avg=1.89\n",
            "[647 | 1233.43] loss=1.77 avg=1.89\n",
            "[648 | 1235.10] loss=2.31 avg=1.90\n",
            "[649 | 1236.79] loss=1.58 avg=1.89\n",
            "[650 | 1238.46] loss=1.74 avg=1.89\n",
            "[651 | 1240.14] loss=1.24 avg=1.89\n",
            "[652 | 1241.81] loss=1.19 avg=1.88\n",
            "[653 | 1243.50] loss=1.72 avg=1.88\n",
            "[654 | 1245.17] loss=1.79 avg=1.88\n",
            "[655 | 1246.84] loss=1.47 avg=1.87\n",
            "[656 | 1248.51] loss=1.98 avg=1.87\n",
            "[657 | 1250.20] loss=1.74 avg=1.87\n",
            "[658 | 1251.88] loss=2.55 avg=1.88\n",
            "[659 | 1253.57] loss=2.05 avg=1.88\n",
            "[660 | 1255.24] loss=1.88 avg=1.88\n",
            "[661 | 1256.91] loss=1.27 avg=1.87\n",
            "[662 | 1258.59] loss=1.88 avg=1.87\n",
            "[663 | 1260.27] loss=0.89 avg=1.86\n",
            "[664 | 1261.96] loss=1.18 avg=1.86\n",
            "[665 | 1263.63] loss=1.56 avg=1.86\n",
            "[666 | 1265.31] loss=1.96 avg=1.86\n",
            "[667 | 1266.99] loss=1.38 avg=1.85\n",
            "[668 | 1268.66] loss=1.01 avg=1.84\n",
            "[669 | 1270.33] loss=2.02 avg=1.84\n",
            "[670 | 1272.01] loss=1.75 avg=1.84\n",
            "[671 | 1273.69] loss=1.51 avg=1.84\n",
            "[672 | 1275.36] loss=1.64 avg=1.84\n",
            "[673 | 1277.04] loss=2.22 avg=1.84\n",
            "[674 | 1278.73] loss=1.72 avg=1.84\n",
            "[675 | 1280.41] loss=1.06 avg=1.83\n",
            "[676 | 1282.09] loss=2.44 avg=1.84\n",
            "[677 | 1283.76] loss=2.06 avg=1.84\n",
            "[678 | 1285.44] loss=1.90 avg=1.84\n",
            "[679 | 1287.12] loss=1.77 avg=1.84\n",
            "[680 | 1288.80] loss=1.41 avg=1.84\n",
            "[681 | 1290.46] loss=1.68 avg=1.84\n",
            "[682 | 1292.15] loss=1.50 avg=1.83\n",
            "[683 | 1293.83] loss=1.03 avg=1.82\n",
            "[684 | 1295.50] loss=1.23 avg=1.82\n",
            "[685 | 1297.18] loss=2.31 avg=1.82\n",
            "[686 | 1298.85] loss=1.57 avg=1.82\n",
            "[687 | 1300.53] loss=2.05 avg=1.82\n",
            "[688 | 1302.22] loss=1.70 avg=1.82\n",
            "[689 | 1303.89] loss=1.41 avg=1.82\n",
            "[690 | 1305.58] loss=1.74 avg=1.82\n",
            "[691 | 1307.27] loss=1.54 avg=1.81\n",
            "[692 | 1308.95] loss=1.29 avg=1.81\n",
            "[693 | 1310.63] loss=1.60 avg=1.81\n",
            "[694 | 1312.31] loss=1.95 avg=1.81\n",
            "[695 | 1313.98] loss=1.67 avg=1.81\n",
            "[696 | 1315.66] loss=0.92 avg=1.80\n",
            "[697 | 1317.35] loss=1.71 avg=1.80\n",
            "[698 | 1319.03] loss=1.93 avg=1.80\n",
            "[699 | 1320.70] loss=1.49 avg=1.80\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "‼s speech in which he praised the “radical” Islamic extremism that is causing the pain and suffering among Muslims and that the U.S. is the only country that is prepared to deal with the attacks.\n",
            "\n",
            "In fact, the administration has tried to stymie condemnation of the Trump administration, including condemnation of the radical Islamic terrorism embodied in the man who now is president. The State Department has refused to take any responsibility for “inaccurate” or otherwise misstatements made by American officials on the threat from “foreigners” motivated by “extremist beliefs.” This decision implicitly rejects the recommendation of former secretary of state James Baker, “who said in a Washington Post column that he was a “very big supporter” of the policy of “indefinite detention” of people who have traveled to “this new country with the misguided “infallibility” theory.\n",
            "\n",
            "The White House’s position is that the Constitution’s third branch — the president “is the ultimate check on executive power — but the Constitution’s fourth branch is the final arbiter of who is or is not a foreign national, and so Obama could detain anyone he wanted, including U.S.-born citizens who had lived and supported their new homelands in their new countries for decades. The president could now declare as a national emergency any citizen or lawful permanent resident who had not been a foreign national, based on his preferred version of the “infallibility” theory. Those citizens and permanent residents could be denied entry into the United States at the border, with criminal charges dropped, and their citizenship could be suspended.\n",
            "\n",
            "The administration is also taking executive action to revise the Deferred Action for Childhood Arrivals (DACA) program, which was created by the 1996 welfare reform law and has shielded more than 800,000 immigrants from deportation. That decision is being challenged as illegal by the president, who has issued an executive order temporarily suspending the program.\n",
            "\n",
            "Under the guise of enforcing the law, the administration is asserting executive authority that Congress has largely given up on fighting.\n",
            "\n",
            "There is a reason that a majority of former presidents never used the president’s bully pulpit: Presidents have been around longer than almost any of us and so have a more intimate understanding of the problems that confront the government. Unlike presidents, Congressmen and senators have little desire to delegate executive authority, let alone legislate it into oblivion in the interest of rational policies.\n",
            "\n",
            "This is a complex subject, but I can say confidently that it does not require a book. The legislative branch is the administrative arm of the executive agency. Congress has limited authority over the executive branch, but it has the primary responsibility for the functioning of the executive branch — the regulations, procedures, and standards that guide its activities. Congress has abdicated this delicate assignment.\n",
            "\n",
            "The administration has attempted to micromanage the executive branch by refusing to take meaningful action on only a small fraction of the executive’s regulatory burdens. For example, the president tried to eliminate the administrative burden of certifying the legality of executive branch actions through a regulation (described below), and he has ignored the lawfulness of several administrative actions through subverted interpretations of the law (described in a second section of this memorandum). The consequence has been an overreliance on administrative discretion and an inability to coordinate across executive agencies and departments, hampering coordination and reducing efficiency.\n",
            "\n",
            "These failings are compounded by the fact that Congress has not been entirely passive in the executive’s business. In the aftermath of 9/11, Congress enacted three provisions to counter the “presidental power to . . . suspend the entry of all aliens . . . unless there is a finding of national emergency,” and Congress “cannot be forced to fund federal agencies unless it is clearly shown that the funding will be irreparably damaged by forcing them to comply with unconstitutional directives.” The executive order that President Trump signed on February 16, 2017, on his “Muslim Travel Ban” is a good start — but that’s not enough. Congress needs to pass legislation that would permanently and completely eliminate the executive branch’s authority to limit immigration, codify the policy in the law, and require the president to notify Congress each time an immigration restriction is lifted.\n",
            "\n",
            "Congress can pass legislation establishing a level playing field for immigration, require that federal agencies implement strict standards governing the admission of foreign workers, and limit the scope of the president’s discretion in admitting new immigrants. It can pass legislation establishing a federal hiring freeze, requiring that employers demonstrate that they have a “special need” for workers, and requiring that such workers have “a demonstrated commitment to the United States” — or any combination of those provisions — for three years. It can pass legislation requiring that federal agencies report annually to Congress on the number of immigrants who enter the country on visas\n",
            "\n",
            "[700 | 1345.46] loss=1.72 avg=1.79\n",
            "[701 | 1347.11] loss=1.69 avg=1.79\n",
            "[702 | 1348.78] loss=0.95 avg=1.78\n",
            "[703 | 1350.44] loss=1.47 avg=1.78\n",
            "[704 | 1352.10] loss=1.16 avg=1.78\n",
            "[705 | 1353.76] loss=1.83 avg=1.78\n",
            "[706 | 1355.42] loss=1.52 avg=1.77\n",
            "[707 | 1357.08] loss=1.48 avg=1.77\n",
            "[708 | 1358.74] loss=1.56 avg=1.77\n",
            "[709 | 1360.39] loss=1.26 avg=1.76\n",
            "[710 | 1362.06] loss=0.86 avg=1.75\n",
            "[711 | 1363.72] loss=2.25 avg=1.76\n",
            "[712 | 1365.38] loss=1.70 avg=1.76\n",
            "[713 | 1367.05] loss=1.56 avg=1.76\n",
            "[714 | 1368.71] loss=1.69 avg=1.76\n",
            "[715 | 1370.37] loss=1.55 avg=1.75\n",
            "[716 | 1372.04] loss=1.24 avg=1.75\n",
            "[717 | 1373.71] loss=0.97 avg=1.74\n",
            "[718 | 1375.39] loss=2.56 avg=1.75\n",
            "[719 | 1377.06] loss=1.65 avg=1.75\n",
            "[720 | 1378.73] loss=1.41 avg=1.74\n",
            "[721 | 1380.40] loss=1.82 avg=1.75\n",
            "[722 | 1382.07] loss=0.99 avg=1.74\n",
            "[723 | 1383.74] loss=1.77 avg=1.74\n",
            "[724 | 1385.41] loss=1.89 avg=1.74\n",
            "[725 | 1387.08] loss=1.39 avg=1.74\n",
            "[726 | 1388.77] loss=1.29 avg=1.73\n",
            "[727 | 1390.44] loss=1.28 avg=1.73\n",
            "[728 | 1392.12] loss=1.82 avg=1.73\n",
            "[729 | 1393.79] loss=0.69 avg=1.72\n",
            "[730 | 1395.46] loss=1.20 avg=1.71\n",
            "[731 | 1397.15] loss=1.63 avg=1.71\n",
            "[732 | 1398.84] loss=1.75 avg=1.71\n",
            "[733 | 1400.51] loss=1.37 avg=1.71\n",
            "[734 | 1402.20] loss=1.30 avg=1.70\n",
            "[735 | 1403.87] loss=0.76 avg=1.70\n",
            "[736 | 1405.56] loss=1.36 avg=1.69\n",
            "[737 | 1407.24] loss=1.48 avg=1.69\n",
            "[738 | 1408.92] loss=1.28 avg=1.69\n",
            "[739 | 1410.60] loss=1.90 avg=1.69\n",
            "[740 | 1412.29] loss=0.79 avg=1.68\n",
            "[741 | 1413.97] loss=1.65 avg=1.68\n",
            "[742 | 1415.66] loss=1.75 avg=1.68\n",
            "[743 | 1417.34] loss=1.39 avg=1.68\n",
            "[744 | 1419.02] loss=1.54 avg=1.68\n",
            "[745 | 1420.70] loss=2.14 avg=1.68\n",
            "[746 | 1422.38] loss=1.64 avg=1.68\n",
            "[747 | 1424.06] loss=1.01 avg=1.67\n",
            "[748 | 1425.73] loss=1.75 avg=1.67\n",
            "[749 | 1427.40] loss=1.77 avg=1.67\n",
            "[750 | 1429.07] loss=1.46 avg=1.67\n",
            "[751 | 1430.75] loss=1.27 avg=1.67\n",
            "[752 | 1432.42] loss=0.68 avg=1.66\n",
            "[753 | 1434.09] loss=1.47 avg=1.66\n",
            "[754 | 1435.77] loss=1.76 avg=1.66\n",
            "[755 | 1437.44] loss=1.39 avg=1.65\n",
            "[756 | 1439.11] loss=1.61 avg=1.65\n",
            "[757 | 1440.79] loss=2.15 avg=1.66\n",
            "[758 | 1442.46] loss=1.72 avg=1.66\n",
            "[759 | 1444.13] loss=1.51 avg=1.66\n",
            "[760 | 1445.81] loss=0.53 avg=1.65\n",
            "[761 | 1447.47] loss=1.38 avg=1.64\n",
            "[762 | 1449.15] loss=1.55 avg=1.64\n",
            "[763 | 1450.82] loss=1.13 avg=1.64\n",
            "[764 | 1452.49] loss=1.50 avg=1.64\n",
            "[765 | 1454.16] loss=1.36 avg=1.63\n",
            "[766 | 1455.83] loss=0.91 avg=1.63\n",
            "[767 | 1457.50] loss=1.60 avg=1.63\n",
            "[768 | 1459.17] loss=0.71 avg=1.62\n",
            "[769 | 1460.83] loss=1.89 avg=1.62\n",
            "[770 | 1462.50] loss=0.52 avg=1.61\n",
            "[771 | 1464.17] loss=1.32 avg=1.61\n",
            "[772 | 1465.84] loss=1.16 avg=1.60\n",
            "[773 | 1467.51] loss=1.61 avg=1.60\n",
            "[774 | 1469.18] loss=0.92 avg=1.60\n",
            "[775 | 1470.85] loss=1.71 avg=1.60\n",
            "[776 | 1472.51] loss=1.30 avg=1.59\n",
            "[777 | 1474.18] loss=1.21 avg=1.59\n",
            "[778 | 1475.86] loss=1.03 avg=1.58\n",
            "[779 | 1477.53] loss=1.20 avg=1.58\n",
            "[780 | 1479.20] loss=1.14 avg=1.58\n",
            "[781 | 1480.87] loss=1.40 avg=1.57\n",
            "[782 | 1482.54] loss=1.75 avg=1.58\n",
            "[783 | 1484.21] loss=0.44 avg=1.56\n",
            "[784 | 1485.88] loss=1.08 avg=1.56\n",
            "[785 | 1487.55] loss=0.59 avg=1.55\n",
            "[786 | 1489.22] loss=1.06 avg=1.54\n",
            "[787 | 1490.89] loss=0.53 avg=1.53\n",
            "[788 | 1492.56] loss=1.02 avg=1.53\n",
            "[789 | 1494.24] loss=1.75 avg=1.53\n",
            "[790 | 1495.90] loss=1.09 avg=1.53\n",
            "[791 | 1497.58] loss=1.80 avg=1.53\n",
            "[792 | 1499.25] loss=1.57 avg=1.53\n",
            "[793 | 1500.92] loss=1.12 avg=1.53\n",
            "[794 | 1502.59] loss=1.06 avg=1.52\n",
            "[795 | 1504.26] loss=1.93 avg=1.53\n",
            "[796 | 1505.93] loss=1.59 avg=1.53\n",
            "[797 | 1507.60] loss=1.18 avg=1.52\n",
            "[798 | 1509.27] loss=1.38 avg=1.52\n",
            "[799 | 1510.94] loss=1.26 avg=1.52\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "The lawfulness of such actions will depend on the facts surrounding the arrest as well as any related criminal charges. It is also possible that such actions are simply routine stops by police that do not warrant legal intervention.\n",
            "\n",
            "In their effort to control the flow of information, some activists and law enforcement have sought to sanitize some of the more graphic content of the videos that were the basis for the Garner verdict. This is a good thing — more serious problems do not get “pushed under the bus.”\n",
            "\n",
            "But it has also led to dishonest reporting about what officers are actually doing. For instance, one CNN report described an arrest for failure to disperse during which the police officer in question could have been charged with a state’s” “obstruction of justice,” which carries a maximum penalty of up to a year in prison and a $2,500 fine. But this is not what actually took place: The details described were not, as the CNN reporter claimed, the police’s description of the arrest; rather, the officer in question was reviewing the arrest video, not describing it to the police. As it is, many law-enforcement agencies do not report arrest video to local police departments, limiting us to the video of the arrest — the narrative that is, in fact, distorted by the video-monitoring industry.\n",
            "\n",
            "On the other hand, in what is surely a silver lining, New York Governor Andrew Cuomo signed a proclamation on Monday directing police to make the video of all arrests available to the public and to the Legislature within 90 days. The effort to make video of police activity more widely available, combined with a court decision a few weeks ago that the chokehold chokehold chokehold can be used to control the hands and feet of people arrested, will make police-body audio surveillance even more accessible.\n",
            "\n",
            "The media also play a big role in keeping our constitutional rights under fire, as they will be in the days and months to come if we do not stand up to police violence and abuses. Fortunately, the Garner verdict and a host of other landmark cases have made that possible.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "The court has finally rendered some justice to one of the most intractable problems faced by poor black families: affordable childcare.\n",
            "\n",
            "In October, the court found in Kale Williams v. Regents of the University of California, 903 U.S. 805 (2003), that the university’s policy prohibiting its campus child-care centers from housing low-income families in full- or part-time positions violated the Equal Protection Clause of the 14th Amendment. With the stroke of a pen, the court reinstated the ban in California’s high-school equivalency requirements, 740 P.2d 1234 (Cal. 1973).\n",
            "\n",
            "The ban was overturned on appeal in 2012 by the state legislature.\n",
            "\n",
            "The ruling in Williams effectively ended the policy of subsidized child-care for low- and moderate-income families, as well as the six-monthly stipends that helped pay the bills in many cases.\n",
            "\n",
            "Since then a host of state legislatures and school boards have attempted to address the issue of affordable child-care for low-income families through legislation and through higher education reforms. Only last year, however, were lawmakers and regulators on college campuses and in higher education confronted with the reality of the realities faced by many low-income families: There is no politically feasible solution that does not involve raising taxes or imposing severe financial burdens on the families most in need.\n",
            "\n",
            "But the court, in rejecting a legislative solution to the problem of subsidized child care, reaffirmed the basic premise of its landmark Rochin v. California case in 1962: Policies that fail to meet the basic test of being programs of last resort are illegitimate.\n",
            "\n",
            "The ruling effectively ended government support for subsidized child-care for tens of thousands of lower- and moderate-income families in California. With subsidies ending and inflation putting the state in step with inflationary norms, it is the poorest of the working- and middle-income families who have no alternative but to rely on government for child-care. It is a sad reflection on the court that the court in the 1960s and 1970s would now declare subsidized child-care a “program of last resort” even though such programs were the norm before the court came along.\n",
            "\n",
            "Williams required colleges, under penalty of losing federal aid, to accept only low-income families with children, and the policy violated the Four Freedoms of Plurality of Families protected by the 14th Amendment. When the court held that this violated the fifth amendment, it was doing exactly what the court wanted to do anyway — punishing the government for existing, but with the expectation that the government would be able to manage the institutions without interference.\n",
            "\n",
            "The court found that not only was this not possible in California, it was also impractical: Universities have difficulty recruiting, managing and retaining a staff of more than 600 full-time faculty members, and\n",
            "\n",
            "[800 | 1535.33] loss=1.29 avg=1.52\n",
            "[801 | 1537.00] loss=0.74 avg=1.51\n",
            "[802 | 1538.67] loss=1.04 avg=1.50\n",
            "[803 | 1540.34] loss=1.07 avg=1.50\n",
            "[804 | 1542.00] loss=1.15 avg=1.50\n",
            "[805 | 1543.67] loss=1.27 avg=1.49\n",
            "[806 | 1545.34] loss=1.52 avg=1.49\n",
            "[807 | 1547.01] loss=0.38 avg=1.48\n",
            "[808 | 1548.68] loss=1.35 avg=1.48\n",
            "[809 | 1550.35] loss=1.16 avg=1.48\n",
            "[810 | 1552.01] loss=0.97 avg=1.47\n",
            "[811 | 1553.68] loss=1.22 avg=1.47\n",
            "[812 | 1555.36] loss=1.12 avg=1.47\n",
            "[813 | 1557.02] loss=1.03 avg=1.46\n",
            "[814 | 1558.70] loss=0.88 avg=1.46\n",
            "[815 | 1560.37] loss=0.93 avg=1.45\n",
            "[816 | 1562.04] loss=1.03 avg=1.45\n",
            "[817 | 1563.71] loss=1.52 avg=1.45\n",
            "[818 | 1565.38] loss=1.24 avg=1.45\n",
            "[819 | 1567.06] loss=1.80 avg=1.45\n",
            "[820 | 1568.70] loss=1.73 avg=1.45\n",
            "[821 | 1570.38] loss=1.87 avg=1.46\n",
            "[822 | 1572.05] loss=1.10 avg=1.45\n",
            "[823 | 1573.72] loss=1.78 avg=1.46\n",
            "[824 | 1575.39] loss=1.09 avg=1.45\n",
            "[825 | 1577.06] loss=0.94 avg=1.45\n",
            "[826 | 1578.73] loss=2.30 avg=1.46\n",
            "[827 | 1580.40] loss=1.77 avg=1.46\n",
            "[828 | 1582.06] loss=1.11 avg=1.46\n",
            "[829 | 1583.73] loss=1.31 avg=1.45\n",
            "[830 | 1585.40] loss=1.53 avg=1.46\n",
            "[831 | 1587.07] loss=1.40 avg=1.45\n",
            "[832 | 1588.74] loss=0.89 avg=1.45\n",
            "[833 | 1590.41] loss=1.15 avg=1.45\n",
            "[834 | 1592.08] loss=1.73 avg=1.45\n",
            "[835 | 1593.75] loss=1.10 avg=1.45\n",
            "[836 | 1595.42] loss=1.34 avg=1.44\n",
            "[837 | 1597.09] loss=1.54 avg=1.45\n",
            "[838 | 1598.76] loss=2.44 avg=1.46\n",
            "[839 | 1600.43] loss=1.30 avg=1.45\n",
            "[840 | 1602.10] loss=1.39 avg=1.45\n",
            "[841 | 1603.76] loss=1.24 avg=1.45\n",
            "[842 | 1605.43] loss=1.15 avg=1.45\n",
            "[843 | 1607.10] loss=1.55 avg=1.45\n",
            "[844 | 1608.77] loss=0.99 avg=1.44\n",
            "[845 | 1610.45] loss=2.42 avg=1.45\n",
            "[846 | 1612.12] loss=0.94 avg=1.45\n",
            "[847 | 1613.79] loss=0.96 avg=1.44\n",
            "[848 | 1615.46] loss=1.59 avg=1.45\n",
            "[849 | 1617.13] loss=1.88 avg=1.45\n",
            "[850 | 1618.79] loss=0.72 avg=1.44\n",
            "[851 | 1620.46] loss=1.99 avg=1.45\n",
            "[852 | 1622.14] loss=0.82 avg=1.44\n",
            "[853 | 1623.81] loss=0.88 avg=1.44\n",
            "[854 | 1625.48] loss=1.38 avg=1.44\n",
            "[855 | 1627.15] loss=1.60 avg=1.44\n",
            "[856 | 1628.83] loss=1.51 avg=1.44\n",
            "[857 | 1630.50] loss=1.09 avg=1.43\n",
            "[858 | 1632.17] loss=1.24 avg=1.43\n",
            "[859 | 1633.84] loss=0.70 avg=1.43\n",
            "[860 | 1635.51] loss=0.98 avg=1.42\n",
            "[861 | 1637.18] loss=0.47 avg=1.41\n",
            "[862 | 1638.86] loss=0.83 avg=1.41\n",
            "[863 | 1640.53] loss=2.57 avg=1.42\n",
            "[864 | 1642.21] loss=0.93 avg=1.41\n",
            "[865 | 1643.88] loss=1.94 avg=1.42\n",
            "[866 | 1645.57] loss=1.35 avg=1.42\n",
            "[867 | 1647.24] loss=1.69 avg=1.42\n",
            "[868 | 1648.92] loss=1.01 avg=1.42\n",
            "[869 | 1650.59] loss=1.16 avg=1.41\n",
            "[870 | 1652.28] loss=2.28 avg=1.42\n",
            "[871 | 1653.95] loss=1.55 avg=1.42\n",
            "[872 | 1655.62] loss=1.41 avg=1.42\n",
            "[873 | 1657.30] loss=0.86 avg=1.42\n",
            "[874 | 1658.98] loss=1.54 avg=1.42\n",
            "[875 | 1660.65] loss=0.77 avg=1.41\n",
            "[876 | 1662.33] loss=0.86 avg=1.41\n",
            "[877 | 1664.02] loss=1.24 avg=1.40\n",
            "[878 | 1665.69] loss=1.03 avg=1.40\n",
            "[879 | 1667.36] loss=0.71 avg=1.39\n",
            "[880 | 1669.03] loss=1.11 avg=1.39\n",
            "[881 | 1670.70] loss=0.57 avg=1.38\n",
            "[882 | 1672.37] loss=1.53 avg=1.38\n",
            "[883 | 1674.05] loss=1.49 avg=1.39\n",
            "[884 | 1675.73] loss=0.89 avg=1.38\n",
            "[885 | 1677.40] loss=0.29 avg=1.37\n",
            "[886 | 1679.07] loss=1.23 avg=1.37\n",
            "[887 | 1680.74] loss=0.75 avg=1.36\n",
            "[888 | 1682.43] loss=1.30 avg=1.36\n",
            "[889 | 1684.11] loss=1.33 avg=1.36\n",
            "[890 | 1685.79] loss=0.39 avg=1.35\n",
            "[891 | 1687.47] loss=1.06 avg=1.35\n",
            "[892 | 1689.15] loss=1.11 avg=1.35\n",
            "[893 | 1690.82] loss=0.69 avg=1.34\n",
            "[894 | 1692.51] loss=1.50 avg=1.34\n",
            "[895 | 1694.18] loss=1.42 avg=1.34\n",
            "[896 | 1695.87] loss=0.79 avg=1.34\n",
            "[897 | 1697.54] loss=1.64 avg=1.34\n",
            "[898 | 1699.23] loss=0.93 avg=1.34\n",
            "[899 | 1700.91] loss=1.36 avg=1.34\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " food and he is a very religious man. My grandfather was a hard core Communist, although it took my father a while (he was eight) to figure out that. When my father and I first moved to Provo in 1975-76, we had a very tough time even hearing each other speak in hushed voices. My parents had been married just a few months when my father was called to serve as lieutenant governor and as a newly elected member of the Church of Jesus Christ of Latter-day Saints. My father knew that he would, like so many others in Utah, have to choose between serving his state or his faith. When my father began explaining the Church to people, they always referred back to that experience. Then my stepfather, who was Mormon even before he was baptized, came from a long line of Mormons, and my mother, who grew up in a fundamentalist Mormon home after Mormonism took over her family, had been raised in a largely uninterrupted state of apostasy. My father always told me he thought Mormonism was a good thing and that serving as alderman in Provo was a great opportunity for him to learn more about it. Of course, like most good things in Utah, it turned out to be a huge opportunity consumed by other things. In retrospect, that seems like a very poor investment. My father, who is now sixty-three, has gotten much worse. My stepfather is only in his forties, but he is still very much a youthful adult with an established career. How fortunate for our state, where aldermanial candidates face significant pressure to look their age and well past their mid-50s in order to avoid being unseated, but aldermanial campaigns in Utah are surprisingly rare. There is no shortage of people who are ready to step up and make a huge deal of money — and for a buck, too. They must be doing their research. It is only a matter of time before someone writes about the LDS church for the first time, and perhaps my experience of the Provo LDS election would be different had my choice been made by a more experienced adult who had weighed the pros and cons of that campaign. My daughter and I are not Mormon, but we would never recommend it to a stranger. We know we wouldn't recommend it to another family, either. I have to admit: My husband and I were a little taken aback by our own choices. We thought we were just being sensible. My husband had been a lawyer, while I lived herstory as a teacher, and he had known the benefits of living a life dedicated to God. We knew where he had gotten the teachings, and how to apply them. When my husband and I were asked to be part of the decision-making process about our own faith, we were confused. But now we are, too:) My friends and I are committed, as we said, to living faithfully the life of faith. I would never argue that life is full of choices, only that we should be much, much more careful about judging people by the choices they allow themselves to make. The irony is, though, the more careful we get about judging others, the more we can condemn them. When someone is raised in a life of poverty or abuse or oppression or spiritual oppression, is that life worth less than others? When someone is raised in a climate of ridicule or violence or discrimination, ought there to be a special right on earth for that person to suffer? My daughter and I make a decision, and we should never feel pressured or threatened into making a decision for us. With that in mind, I believe we need a national campaign to support the rights of all LGBTQ people to live happy, fulfilling lives. Every American has the right to live their lives confident they will not be marginalized, and free from sadness and regret. I have faith in our country and in our ability to succeed, and my daughter and I know we have the strength to overcome any challenges. But let us not mistake our humility for complacency. The hard work begins now. © 2019, Washington Post Writers Group\n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "On the morning of September 11th, I glanced up from my Twitter feed and saw a message on the head of a child-prodigy: \"Do NOT retweet my naked body🙏.\" From that moment on, it was not just my private thoughts that were being shared, but also the images of my children, many of whom were in the photograph. My reaction was not to be republished by every media outlet in the country. My initial response was hardly a cause for alarm; it was more evidence that, with some remarkable ingenuity, other adults were taking notice. My response was to share it with the images on many people “sick of seeing their kids face to face with ridicule and ignorance.\" There “s no reason to be nasty or mean-spirited—and” I’ve said, “every parent’s instinct tells”—so why was my initial reaction twisted\n",
            "\n",
            "[900 | 1725.29] loss=1.97 avg=1.34\n",
            "[901 | 1726.95] loss=1.20 avg=1.34\n",
            "[902 | 1728.61] loss=1.02 avg=1.34\n",
            "[903 | 1730.26] loss=0.99 avg=1.33\n",
            "[904 | 1731.92] loss=0.88 avg=1.33\n",
            "[905 | 1733.59] loss=1.12 avg=1.33\n",
            "[906 | 1735.25] loss=1.77 avg=1.33\n",
            "[907 | 1736.91] loss=0.56 avg=1.32\n",
            "[908 | 1738.57] loss=0.83 avg=1.32\n",
            "[909 | 1740.23] loss=1.11 avg=1.32\n",
            "[910 | 1741.89] loss=1.28 avg=1.32\n",
            "[911 | 1743.55] loss=1.88 avg=1.32\n",
            "[912 | 1745.22] loss=1.56 avg=1.32\n",
            "[913 | 1746.89] loss=0.85 avg=1.32\n",
            "[914 | 1748.55] loss=1.25 avg=1.32\n",
            "[915 | 1750.20] loss=0.27 avg=1.31\n",
            "[916 | 1751.88] loss=1.08 avg=1.31\n",
            "[917 | 1753.54] loss=1.81 avg=1.31\n",
            "[918 | 1755.22] loss=1.39 avg=1.31\n",
            "[919 | 1756.89] loss=0.83 avg=1.31\n",
            "[920 | 1758.57] loss=0.61 avg=1.30\n",
            "[921 | 1760.24] loss=1.28 avg=1.30\n",
            "[922 | 1761.91] loss=2.05 avg=1.31\n",
            "[923 | 1763.58] loss=1.75 avg=1.31\n",
            "[924 | 1765.25] loss=1.09 avg=1.31\n",
            "[925 | 1766.92] loss=0.51 avg=1.30\n",
            "[926 | 1768.59] loss=1.05 avg=1.30\n",
            "[927 | 1770.26] loss=1.30 avg=1.30\n",
            "[928 | 1771.93] loss=1.94 avg=1.31\n",
            "[929 | 1773.61] loss=1.56 avg=1.31\n",
            "[930 | 1775.28] loss=0.57 avg=1.30\n",
            "[931 | 1776.96] loss=1.09 avg=1.30\n",
            "[932 | 1778.64] loss=1.11 avg=1.30\n",
            "[933 | 1780.32] loss=0.99 avg=1.29\n",
            "[934 | 1782.00] loss=1.02 avg=1.29\n",
            "[935 | 1783.69] loss=0.38 avg=1.28\n",
            "[936 | 1785.38] loss=1.23 avg=1.28\n",
            "[937 | 1787.05] loss=1.99 avg=1.29\n",
            "[938 | 1788.73] loss=0.79 avg=1.28\n",
            "[939 | 1790.42] loss=0.96 avg=1.28\n",
            "[940 | 1792.10] loss=0.38 avg=1.27\n",
            "[941 | 1793.79] loss=1.87 avg=1.28\n",
            "[942 | 1795.47] loss=0.67 avg=1.27\n",
            "[943 | 1797.16] loss=1.28 avg=1.27\n",
            "[944 | 1798.83] loss=1.22 avg=1.27\n",
            "[945 | 1800.52] loss=1.43 avg=1.27\n",
            "[946 | 1802.19] loss=1.17 avg=1.27\n",
            "[947 | 1803.87] loss=0.91 avg=1.27\n",
            "[948 | 1805.54] loss=1.49 avg=1.27\n",
            "[949 | 1807.21] loss=2.01 avg=1.28\n",
            "[950 | 1808.88] loss=0.48 avg=1.27\n",
            "[951 | 1810.55] loss=0.93 avg=1.27\n",
            "[952 | 1812.22] loss=2.00 avg=1.27\n",
            "[953 | 1813.89] loss=1.15 avg=1.27\n",
            "[954 | 1815.55] loss=1.00 avg=1.27\n",
            "[955 | 1817.23] loss=1.48 avg=1.27\n",
            "[956 | 1818.91] loss=0.64 avg=1.27\n",
            "[957 | 1820.58] loss=0.90 avg=1.26\n",
            "[958 | 1822.25] loss=0.92 avg=1.26\n",
            "[959 | 1823.92] loss=1.21 avg=1.26\n",
            "[960 | 1825.59] loss=0.93 avg=1.25\n",
            "[961 | 1827.26] loss=0.71 avg=1.25\n",
            "[962 | 1828.93] loss=0.22 avg=1.24\n",
            "[963 | 1830.60] loss=0.95 avg=1.24\n",
            "[964 | 1832.27] loss=1.03 avg=1.23\n",
            "[965 | 1833.93] loss=1.12 avg=1.23\n",
            "[966 | 1835.60] loss=1.15 avg=1.23\n",
            "[967 | 1837.25] loss=0.67 avg=1.23\n",
            "[968 | 1838.92] loss=0.86 avg=1.22\n",
            "[969 | 1840.59] loss=1.43 avg=1.22\n",
            "[970 | 1842.26] loss=0.46 avg=1.22\n",
            "[971 | 1843.92] loss=1.11 avg=1.22\n",
            "[972 | 1845.59] loss=0.58 avg=1.21\n",
            "[973 | 1847.26] loss=0.61 avg=1.20\n",
            "[974 | 1848.93] loss=0.66 avg=1.20\n",
            "[975 | 1850.60] loss=0.69 avg=1.19\n",
            "[976 | 1852.27] loss=0.50 avg=1.19\n",
            "[977 | 1853.95] loss=1.63 avg=1.19\n",
            "[978 | 1855.62] loss=0.95 avg=1.19\n",
            "[979 | 1857.29] loss=0.67 avg=1.18\n",
            "[980 | 1858.95] loss=1.12 avg=1.18\n",
            "[981 | 1860.63] loss=0.84 avg=1.18\n",
            "[982 | 1862.29] loss=0.81 avg=1.18\n",
            "[983 | 1863.96] loss=0.43 avg=1.17\n",
            "[984 | 1865.66] loss=1.39 avg=1.17\n",
            "[985 | 1867.33] loss=1.26 avg=1.17\n",
            "[986 | 1869.02] loss=0.67 avg=1.17\n",
            "[987 | 1870.71] loss=1.37 avg=1.17\n",
            "[988 | 1872.40] loss=1.21 avg=1.17\n",
            "[989 | 1874.10] loss=0.79 avg=1.16\n",
            "[990 | 1875.78] loss=0.49 avg=1.16\n",
            "[991 | 1877.48] loss=1.45 avg=1.16\n",
            "[992 | 1879.17] loss=0.92 avg=1.16\n",
            "[993 | 1880.86] loss=2.46 avg=1.17\n",
            "[994 | 1882.55] loss=1.01 avg=1.17\n",
            "[995 | 1884.23] loss=1.64 avg=1.17\n",
            "[996 | 1885.91] loss=0.97 avg=1.17\n",
            "[997 | 1887.58] loss=0.98 avg=1.17\n",
            "[998 | 1889.25] loss=1.25 avg=1.17\n",
            "[999 | 1890.92] loss=0.67 avg=1.17\n",
            "Ran 1010 steps.\n",
            "Saving checkpoint/all_national_review_345/model-1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJCZDnhGNA1U",
        "colab_type": "code",
        "outputId": "168649b7-aabf-405b-a40b-022f939c7cb0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "## new york post essays training - with 345\n",
        "!PYTHONPATH=src ./train.py --dataset /content/all_nypost.txt --run_name 'all_nypsot_345' --model_name '345M'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0627 23:20:11.072258 139997260420992 deprecation_wrapper.py:119] From /content/gpt-2/src/model.py:147: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
            "\n",
            "W0627 23:20:11.080600 139997260420992 deprecation_wrapper.py:119] From /content/gpt-2/src/memory_saving_gradients.py:13: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n",
            "W0627 23:20:11.173804 139997260420992 deprecation_wrapper.py:119] From ./train.py:88: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "W0627 23:20:11.174168 139997260420992 deprecation_wrapper.py:119] From ./train.py:91: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "2019-06-27 23:20:11.180492: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-06-27 23:20:11.180759: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1595100 executing computations on platform Host. Devices:\n",
            "2019-06-27 23:20:11.180798: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2019-06-27 23:20:11.183141: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\n",
            "2019-06-27 23:20:11.342598: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-27 23:20:11.343132: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1594840 executing computations on platform CUDA. Devices:\n",
            "2019-06-27 23:20:11.343162: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2019-06-27 23:20:11.343467: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-27 23:20:11.343869: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-06-27 23:20:11.344232: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-06-27 23:20:11.345539: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-06-27 23:20:11.346831: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-06-27 23:20:11.347179: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-06-27 23:20:11.348650: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-06-27 23:20:11.349876: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-06-27 23:20:11.353189: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-06-27 23:20:11.353346: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-27 23:20:11.353754: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-27 23:20:11.354083: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
            "2019-06-27 23:20:11.354141: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-06-27 23:20:11.355099: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-06-27 23:20:11.355125: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n",
            "2019-06-27 23:20:11.355136: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n",
            "2019-06-27 23:20:11.355492: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-27 23:20:11.355923: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-27 23:20:11.356283: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14202 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "W0627 23:20:11.357142 139997260420992 deprecation_wrapper.py:119] From ./train.py:92: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0627 23:20:22.142971 139997260420992 deprecation.py:323] From /content/gpt-2/src/sample.py:65: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "W0627 23:20:22.157008 139997260420992 deprecation.py:323] From /content/gpt-2/src/sample.py:16: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0627 23:20:22.158641 139997260420992 deprecation.py:323] From /content/gpt-2/src/sample.py:70: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.random.categorical` instead.\n",
            "W0627 23:20:22.168884 139997260420992 deprecation_wrapper.py:119] From ./train.py:121: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "W0627 23:20:37.425222 139997260420992 deprecation_wrapper.py:119] From ./train.py:144: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n",
            "W0627 23:20:37.428219 139997260420992 deprecation_wrapper.py:119] From ./train.py:147: The name tf.summary.merge is deprecated. Please use tf.compat.v1.summary.merge instead.\n",
            "\n",
            "W0627 23:20:37.429027 139997260420992 deprecation_wrapper.py:119] From ./train.py:149: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
            "\n",
            "W0627 23:20:37.429758 139997260420992 deprecation_wrapper.py:119] From ./train.py:152: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "Loading checkpoint models/345M/model.ckpt\n",
            "W0627 23:20:51.308665 139997260420992 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "Loading dataset...\n",
            "100% 1/1 [00:00<00:00,  1.11it/s]\n",
            "dataset has 91927 tokens\n",
            "Training...\n",
            "2019-06-27 23:21:05.396652: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n",
            "2019-06-27 23:21:06.112111: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "[1 | 13.35] loss=2.93 avg=2.93\n",
            "[2 | 14.98] loss=2.76 avg=2.84\n",
            "[3 | 16.60] loss=3.17 avg=2.96\n",
            "[4 | 18.22] loss=3.25 avg=3.03\n",
            "[5 | 19.85] loss=3.09 avg=3.04\n",
            "[6 | 21.50] loss=2.63 avg=2.97\n",
            "[7 | 23.16] loss=3.00 avg=2.98\n",
            "[8 | 24.82] loss=3.07 avg=2.99\n",
            "[9 | 26.48] loss=2.78 avg=2.96\n",
            "[10 | 28.15] loss=2.68 avg=2.93\n",
            "[11 | 29.83] loss=2.87 avg=2.93\n",
            "[12 | 31.53] loss=3.01 avg=2.94\n",
            "[13 | 33.23] loss=3.09 avg=2.95\n",
            "[14 | 34.94] loss=2.59 avg=2.92\n",
            "[15 | 36.65] loss=2.71 avg=2.91\n",
            "[16 | 38.39] loss=2.71 avg=2.89\n",
            "[17 | 40.12] loss=2.42 avg=2.86\n",
            "[18 | 41.85] loss=2.65 avg=2.85\n",
            "[19 | 43.59] loss=2.82 avg=2.85\n",
            "[20 | 45.32] loss=2.67 avg=2.84\n",
            "[21 | 47.05] loss=2.61 avg=2.83\n",
            "[22 | 48.78] loss=2.81 avg=2.82\n",
            "[23 | 50.50] loss=2.81 avg=2.82\n",
            "[24 | 52.23] loss=2.65 avg=2.82\n",
            "[25 | 53.94] loss=2.64 avg=2.81\n",
            "[26 | 55.64] loss=2.56 avg=2.80\n",
            "[27 | 57.34] loss=2.45 avg=2.78\n",
            "[28 | 59.03] loss=2.54 avg=2.77\n",
            "[29 | 60.73] loss=2.52 avg=2.76\n",
            "[30 | 62.40] loss=2.67 avg=2.76\n",
            "[31 | 64.07] loss=2.76 avg=2.76\n",
            "[32 | 65.74] loss=2.46 avg=2.75\n",
            "[33 | 67.42] loss=2.65 avg=2.75\n",
            "[34 | 69.08] loss=2.65 avg=2.74\n",
            "[35 | 70.74] loss=2.38 avg=2.73\n",
            "[36 | 72.40] loss=2.69 avg=2.73\n",
            "[37 | 74.05] loss=2.81 avg=2.73\n",
            "[38 | 75.70] loss=2.43 avg=2.72\n",
            "[39 | 77.35] loss=2.42 avg=2.71\n",
            "[40 | 79.00] loss=2.55 avg=2.71\n",
            "[41 | 80.65] loss=3.00 avg=2.72\n",
            "[42 | 82.29] loss=2.97 avg=2.72\n",
            "[43 | 83.93] loss=2.92 avg=2.73\n",
            "[44 | 85.57] loss=2.66 avg=2.73\n",
            "[45 | 87.22] loss=2.42 avg=2.72\n",
            "[46 | 88.86] loss=2.67 avg=2.72\n",
            "[47 | 90.50] loss=2.32 avg=2.71\n",
            "[48 | 92.14] loss=2.90 avg=2.71\n",
            "[49 | 93.78] loss=2.28 avg=2.70\n",
            "[50 | 95.43] loss=2.97 avg=2.71\n",
            "[51 | 97.07] loss=2.85 avg=2.71\n",
            "[52 | 98.71] loss=2.25 avg=2.70\n",
            "[53 | 100.37] loss=2.86 avg=2.70\n",
            "[54 | 102.03] loss=3.14 avg=2.71\n",
            "[55 | 103.67] loss=2.37 avg=2.71\n",
            "[56 | 105.32] loss=2.42 avg=2.70\n",
            "[57 | 106.96] loss=2.95 avg=2.71\n",
            "[58 | 108.61] loss=2.52 avg=2.70\n",
            "[59 | 110.26] loss=2.66 avg=2.70\n",
            "[60 | 111.92] loss=2.96 avg=2.71\n",
            "[61 | 113.58] loss=2.78 avg=2.71\n",
            "[62 | 115.23] loss=2.55 avg=2.70\n",
            "[63 | 116.89] loss=3.03 avg=2.71\n",
            "[64 | 118.55] loss=2.53 avg=2.71\n",
            "[65 | 120.22] loss=2.57 avg=2.70\n",
            "[66 | 121.89] loss=2.41 avg=2.70\n",
            "[67 | 123.55] loss=2.58 avg=2.70\n",
            "[68 | 125.21] loss=2.61 avg=2.69\n",
            "[69 | 126.88] loss=2.91 avg=2.70\n",
            "[70 | 128.56] loss=2.74 avg=2.70\n",
            "[71 | 130.23] loss=2.56 avg=2.70\n",
            "[72 | 131.90] loss=2.58 avg=2.69\n",
            "[73 | 133.57] loss=2.57 avg=2.69\n",
            "[74 | 135.23] loss=2.44 avg=2.69\n",
            "[75 | 136.90] loss=2.30 avg=2.68\n",
            "[76 | 138.57] loss=2.43 avg=2.68\n",
            "[77 | 140.24] loss=2.75 avg=2.68\n",
            "[78 | 141.91] loss=3.00 avg=2.68\n",
            "[79 | 143.58] loss=2.58 avg=2.68\n",
            "[80 | 145.25] loss=2.74 avg=2.68\n",
            "[81 | 146.92] loss=2.17 avg=2.67\n",
            "[82 | 148.58] loss=2.72 avg=2.67\n",
            "[83 | 150.25] loss=2.85 avg=2.68\n",
            "[84 | 151.91] loss=2.77 avg=2.68\n",
            "[85 | 153.58] loss=2.71 avg=2.68\n",
            "[86 | 155.24] loss=2.52 avg=2.68\n",
            "[87 | 156.90] loss=2.67 avg=2.68\n",
            "[88 | 158.56] loss=2.76 avg=2.68\n",
            "[89 | 160.22] loss=2.66 avg=2.68\n",
            "[90 | 161.89] loss=2.53 avg=2.67\n",
            "[91 | 163.55] loss=2.68 avg=2.67\n",
            "[92 | 165.21] loss=2.32 avg=2.67\n",
            "[93 | 166.87] loss=2.41 avg=2.66\n",
            "[94 | 168.55] loss=2.39 avg=2.66\n",
            "[95 | 170.20] loss=2.82 avg=2.66\n",
            "[96 | 171.87] loss=2.80 avg=2.67\n",
            "[97 | 173.54] loss=2.12 avg=2.66\n",
            "[98 | 175.20] loss=3.18 avg=2.66\n",
            "[99 | 176.86] loss=2.70 avg=2.67\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " been one of the most exciting scenes in The Martian, and it may well have been the most interesting.\n",
            "\n",
            "And here, for the first time, we get close up, with the Martian spacecraft's cameras rolling and a tiny hand hovering over the sun – a scene that, to a casual observer, would be an eye-popping sight, as its scope and perspective make it look as if the entire planet is being covered a foot high with dust.\n",
            "\n",
            "But then, we get the Martian sun to get caught in the beam of the huge telescope on board the NASA spacecraft. It turns the Martian day sky sky into the sort of cosmic nightmare you find when you put a telescope on a giant asteroid.\n",
            "\n",
            "In just two hours the astronauts will get a view of the entire Martian surface, with enough detail to calculate the volume of the whole dwarf planet and the distance from its equator, the distance from the equator to its north pole and the distance from the north pole to the north star.\n",
            "\n",
            "It will be the first time anybody on the surface was able to do so. At around the same time, the spacecraft will carry out an extraordinary manoeuvre. This is a little called a \"pitch-correcting manoeuvre\", in which the camera on board makes its way back to a precise point just as the spacecraft has moved slightly over the surface.\n",
            "\n",
            "If the spacecraft continues its path over the surface and reaches the closest point, the cameras' resolution is adjusted, and the spacecraft moves away from the camera. If the spacecraft moves away too far and the cameras get too far away from the correct point, the cameras get caught in the beam.\n",
            "\n",
            "The mission will be a challenge, to say the least. It is supposed to spend around one and a half days at a stretch with just a few hundred metres of surface to cover and a relatively long duration for photography, so astronauts will spend much of this time in the shade.\n",
            "\n",
            "But they will be able to catch the sun, and they will take pictures that will make the trip as exciting as it is remarkable.\n",
            "\n",
            "The journey for Rosetta will be one of the biggest ever, in that it will mark the start of a new stage in the history of mankind – and there will be plenty more to see as the mission continues.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "The discovery makes the comet Churyumov–Gerasimenko the oldest, most massive comet ever discovered\n",
            "\n",
            "After Churyumov–Gerasimenko was first spotted in 2003, it wasn't clear what caused it.\n",
            "\n",
            "Scientists from Europe's University of Leicester said it appeared strange because none of the asteroids that had been seen near its orbit had had any impact marks on them, suggesting the comet had simply been going through a transit – sending out a series of shocks and other particles and causing the comet to orbit around Churyumov and its companion Ceres.\n",
            "\n",
            "But astronomers from Texas A&M University in the US claimed that the comet's orbit – in the way that an asteroid can orbit around an asteroid – made it more likely that it was an asteroid than an asteroid-like object – and so a new theory emerged that it was a comet as it passed through the solar system around the main asteroid belt.\n",
            "\n",
            "Asteriovox: Comet Churyumov-Gerasimenko a comet as it passes through the solar system – and an asteroid in its tail\n",
            "\n",
            "Then, the researchers concluded that the comet was an asteroid, and had formed around the asteroid Phobos, which lies just 400,000 miles from the Sun. Comet Churyumov was discovered after a 13-year search by Italian astronomer Vincenzo Gaffini in 1973.\n",
            "\n",
            "It is one of many icy bodies which pass through the sun every day and reach their closest approaches by the end of April, at which point they will appear as a small bright dot between the two largest asteroids, both of which are millions of miles wide.\n",
            "\n",
            "They will disappear in two months' time, when our Sun appears in the sky.\n",
            "\n",
            "Astronomers originally suspected that the comet was an asteroid, because it had made a small and unusual transit around Churyumov, but then they realised that it wasn't a comet.\n",
            "\n",
            "The comet is just 2km across, and its transit – in which it travels through the solar system before disappearing – occurs as the comet travels across the inner solar system. The transit doesn't take place for much longer.\n",
            "\n",
            "When Churyumov was first discovered, it had been observed for only a few hours a day. As the comet passed by, we got much more frequent views of it, and on July 31 it returned to the sun for the first time at 21:11 GMT and went through an eccentric orbit for a time.\n",
            "\n",
            "It did not cause visible impact marks on Earth – it was quite harmless – but some geologists believed it was an asteroid.\n",
            "\n",
            "By the time Churyumov was found, Comet Churyumov was\n",
            "\n",
            "[100 | 204.49] loss=2.42 avg=2.66\n",
            "[101 | 206.16] loss=2.50 avg=2.66\n",
            "[102 | 207.84] loss=2.95 avg=2.66\n",
            "[103 | 209.49] loss=2.80 avg=2.67\n",
            "[104 | 211.17] loss=2.73 avg=2.67\n",
            "[105 | 212.85] loss=2.51 avg=2.66\n",
            "[106 | 214.50] loss=2.26 avg=2.66\n",
            "[107 | 216.18] loss=2.38 avg=2.65\n",
            "[108 | 217.85] loss=2.94 avg=2.66\n",
            "[109 | 219.52] loss=2.33 avg=2.65\n",
            "[110 | 221.19] loss=2.34 avg=2.65\n",
            "[111 | 222.87] loss=2.32 avg=2.64\n",
            "[112 | 224.54] loss=2.22 avg=2.64\n",
            "[113 | 226.21] loss=2.34 avg=2.63\n",
            "[114 | 227.88] loss=2.48 avg=2.63\n",
            "[115 | 229.56] loss=2.83 avg=2.63\n",
            "[116 | 231.23] loss=2.33 avg=2.63\n",
            "[117 | 232.90] loss=2.20 avg=2.62\n",
            "[118 | 234.58] loss=2.55 avg=2.62\n",
            "[119 | 236.25] loss=2.69 avg=2.62\n",
            "[120 | 237.93] loss=2.28 avg=2.62\n",
            "[121 | 239.60] loss=2.24 avg=2.61\n",
            "[122 | 241.27] loss=2.39 avg=2.61\n",
            "[123 | 242.93] loss=2.50 avg=2.61\n",
            "[124 | 244.60] loss=2.72 avg=2.61\n",
            "[125 | 246.28] loss=2.66 avg=2.61\n",
            "[126 | 247.94] loss=2.24 avg=2.60\n",
            "[127 | 249.62] loss=2.65 avg=2.61\n",
            "[128 | 251.29] loss=2.44 avg=2.60\n",
            "[129 | 252.96] loss=2.75 avg=2.61\n",
            "[130 | 254.62] loss=2.19 avg=2.60\n",
            "[131 | 256.30] loss=2.35 avg=2.60\n",
            "[132 | 257.97] loss=2.07 avg=2.59\n",
            "[133 | 259.64] loss=2.67 avg=2.59\n",
            "[134 | 261.31] loss=2.62 avg=2.59\n",
            "[135 | 262.98] loss=2.68 avg=2.59\n",
            "[136 | 264.64] loss=2.59 avg=2.59\n",
            "[137 | 266.31] loss=2.46 avg=2.59\n",
            "[138 | 267.98] loss=2.32 avg=2.59\n",
            "[139 | 269.65] loss=2.74 avg=2.59\n",
            "[140 | 271.32] loss=2.36 avg=2.59\n",
            "[141 | 272.99] loss=2.48 avg=2.58\n",
            "[142 | 274.66] loss=2.16 avg=2.58\n",
            "[143 | 276.33] loss=3.06 avg=2.58\n",
            "[144 | 278.00] loss=2.32 avg=2.58\n",
            "[145 | 279.67] loss=2.26 avg=2.58\n",
            "[146 | 281.34] loss=2.46 avg=2.58\n",
            "[147 | 283.01] loss=2.23 avg=2.57\n",
            "[148 | 284.68] loss=2.61 avg=2.57\n",
            "[149 | 286.35] loss=2.16 avg=2.57\n",
            "[150 | 288.02] loss=2.38 avg=2.56\n",
            "[151 | 289.70] loss=2.26 avg=2.56\n",
            "[152 | 291.36] loss=2.22 avg=2.56\n",
            "[153 | 293.04] loss=2.03 avg=2.55\n",
            "[154 | 294.70] loss=2.06 avg=2.54\n",
            "[155 | 296.37] loss=1.93 avg=2.53\n",
            "[156 | 298.04] loss=2.51 avg=2.53\n",
            "[157 | 299.72] loss=2.88 avg=2.54\n",
            "[158 | 301.37] loss=2.30 avg=2.54\n",
            "[159 | 303.05] loss=2.38 avg=2.53\n",
            "[160 | 304.72] loss=2.06 avg=2.53\n",
            "[161 | 306.38] loss=2.28 avg=2.52\n",
            "[162 | 308.04] loss=2.40 avg=2.52\n",
            "[163 | 309.71] loss=2.04 avg=2.52\n",
            "[164 | 311.38] loss=2.19 avg=2.51\n",
            "[165 | 313.05] loss=2.82 avg=2.52\n",
            "[166 | 314.72] loss=2.48 avg=2.52\n",
            "[167 | 316.39] loss=2.09 avg=2.51\n",
            "[168 | 318.06] loss=2.72 avg=2.51\n",
            "[169 | 319.73] loss=2.61 avg=2.51\n",
            "[170 | 321.40] loss=2.58 avg=2.52\n",
            "[171 | 323.06] loss=2.59 avg=2.52\n",
            "[172 | 324.74] loss=2.74 avg=2.52\n",
            "[173 | 326.41] loss=1.98 avg=2.51\n",
            "[174 | 328.08] loss=2.02 avg=2.51\n",
            "[175 | 329.75] loss=2.23 avg=2.50\n",
            "[176 | 331.42] loss=2.67 avg=2.51\n",
            "[177 | 333.08] loss=1.96 avg=2.50\n",
            "[178 | 334.76] loss=2.89 avg=2.50\n",
            "[179 | 336.43] loss=2.56 avg=2.50\n",
            "[180 | 338.10] loss=2.03 avg=2.50\n",
            "[181 | 339.75] loss=2.49 avg=2.50\n",
            "[182 | 341.42] loss=2.02 avg=2.49\n",
            "[183 | 343.10] loss=2.68 avg=2.50\n",
            "[184 | 344.76] loss=2.28 avg=2.49\n",
            "[185 | 346.43] loss=2.30 avg=2.49\n",
            "[186 | 348.11] loss=2.76 avg=2.49\n",
            "[187 | 349.78] loss=2.16 avg=2.49\n",
            "[188 | 351.44] loss=2.71 avg=2.49\n",
            "[189 | 353.11] loss=2.59 avg=2.49\n",
            "[190 | 354.78] loss=2.44 avg=2.49\n",
            "[191 | 356.45] loss=2.04 avg=2.49\n",
            "[192 | 358.11] loss=2.79 avg=2.49\n",
            "[193 | 359.77] loss=2.24 avg=2.49\n",
            "[194 | 361.42] loss=1.84 avg=2.48\n",
            "[195 | 363.09] loss=2.33 avg=2.48\n",
            "[196 | 364.76] loss=2.27 avg=2.48\n",
            "[197 | 366.43] loss=1.97 avg=2.47\n",
            "[198 | 368.09] loss=2.29 avg=2.47\n",
            "[199 | 369.76] loss=2.16 avg=2.46\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            ", but a few steps forward and they may have just found a new favorite. This photo of the new NASA orbiter, called the ExoMars rover, which is scheduled to land on Mars on May 29, 2012, was taken in March by ESA's Mars Express spacecraft while it was orbiting Mars just over the north-west face of ExoMars in April 2011. ...\n",
            "\n",
            "At a time when the NASA budget is facing a funding gap and with President Obama considering new NASA policies to balance the budget, there is an interesting development out there in the United States, although you might think that these people are the Republicans. What they're talking about is a radical departure from the way America's space program has evolved over the last 50 years. What they believe NASA can do is radically outpace its status quo, which was established more than a century ago by a Republican president — Dwight Eisenhower.\n",
            "\n",
            "What Eisenhower had to do with it is to give the United States a role in space exploration and create what became known as the \"Eisenhower Plan.\" Eisenhower created the Institute of Advanced Study, which is one of the very first institutes in the United States to accept foreign students and conduct research in space. He also opened the Space Research Center of the Air Force in Little Rock in 1952, to provide research and test facilities for the United States to compete in the space race against other nations to explore outer space.\n",
            "\n",
            "Eisenhower went on to create the United States Space Command. He created the International Astronautical Organization (IAO) to give the United States a better role in space exploration. He created the National Aeronautics and Space Administration, which was created in 1955. The purpose of this agency is to do research that will help our nation compete with the Soviet Union. They put together the first satellite in 1957, the Sputnik 1.\n",
            "\n",
            "Eisenhower was a big proponent of military space development. He was also a big proponent of technology advances that would help us compete with the Soviets. There was a time when we had to develop atomic devices and other advanced technology to compete with the Soviets. It wasn't until the 1990s that we were able to compete with them.\n",
            "\n",
            "By creating the Institute of Advanced Studies, Eisenhower got the United States out of its traditional role as a second-rate power that we still are, and he also created a role in space development that would help us get the United States in the top position in space.\n",
            "\n",
            "The new Institute for Advanced Studies is an intergovernmental institute. It is a noncommercial organization, so it doesn't receive any Federal funding. It wants to encourage the private sector to develop the technologies in space, because the private sector will create the solutions to problems in space that we can't be solved by the United States in space.\n",
            "\n",
            "The Institute for Advanced Studies wants to be the voice of the American people on matters of national security. We are supposed to be the voice of the American people, because that's what we were established to do. Eisenhower wanted to take that role back, and he created the Institute for Advanced Studies. He created the space agency of the United States with the IAS. He created the Space Science and Technology Advisory Panel, which is a group of experts on space issues and policy with recommendations. He also created the Advisory Council on Space-Based Infrared Astronomy and directed NASA to create the Spaceborne Infrared Mapper as a permanent instrument on the International Space Station. This is a revolutionary new research instrument to collect and catalog infrared light. These are really the only infrared instruments in the world. They are going to tell us something about space that we can never know with existing instruments.\n",
            "\n",
            "He created the Air Force Institute of Science to be one of the leading centers of research in air-to-air missile development. He created NASA's Space Science Center and it is now focused on space flight and space flight systems. He created the National Academy of Sciences and created NASA-NIS to conduct the majority of America's national security policy. NASA was created to be the nation's leading force in space research. Now we are the prime mover in space technology. Eisenhower was a big proponent of space research, and he was a huge advocate of a national security force in space.\n",
            "\n",
            "This new institute would be run by the Institute of Advanced Studies, not NASA. NASA is currently running an initiative called NASA Innovative Advanced Concepts, but its goal is to run a new institute. It wants to establish a National Advisory Board for ideas for technologies that will ultimately make the United States the dominant space power. This institute would be run by the institute of advanced studies, not NASA.\n",
            "\n",
            "By creating the Institute for Advancement of Higher Education, Ike envisioned that we would lead the world on the next generation of technology. He proposed the creation of a National Advanced College of Education. He proposed the National Advanced Enterprise School. What Ike and the IAS is proposing is a different future for America in which we lead the world at work in the next generation of innovation and\n",
            "\n",
            "[200 | 394.13] loss=2.19 avg=2.46\n",
            "[201 | 395.80] loss=1.89 avg=2.45\n",
            "[202 | 397.47] loss=1.83 avg=2.45\n",
            "[203 | 399.15] loss=2.10 avg=2.44\n",
            "[204 | 400.81] loss=2.91 avg=2.45\n",
            "[205 | 402.48] loss=1.60 avg=2.44\n",
            "[206 | 404.15] loss=2.83 avg=2.44\n",
            "[207 | 405.82] loss=1.70 avg=2.44\n",
            "[208 | 407.49] loss=2.35 avg=2.43\n",
            "[209 | 409.15] loss=2.51 avg=2.44\n",
            "[210 | 410.82] loss=1.81 avg=2.43\n",
            "[211 | 412.49] loss=2.24 avg=2.43\n",
            "[212 | 414.15] loss=1.82 avg=2.42\n",
            "[213 | 415.82] loss=2.08 avg=2.42\n",
            "[214 | 417.48] loss=2.33 avg=2.41\n",
            "[215 | 419.14] loss=1.52 avg=2.40\n",
            "[216 | 420.79] loss=2.77 avg=2.41\n",
            "[217 | 422.46] loss=2.66 avg=2.41\n",
            "[218 | 424.14] loss=2.73 avg=2.41\n",
            "[219 | 425.80] loss=2.12 avg=2.41\n",
            "[220 | 427.46] loss=2.42 avg=2.41\n",
            "[221 | 429.13] loss=2.48 avg=2.41\n",
            "[222 | 430.79] loss=1.77 avg=2.41\n",
            "[223 | 432.46] loss=2.81 avg=2.41\n",
            "[224 | 434.12] loss=2.68 avg=2.41\n",
            "[225 | 435.79] loss=2.17 avg=2.41\n",
            "[226 | 437.46] loss=2.07 avg=2.41\n",
            "[227 | 439.12] loss=2.39 avg=2.41\n",
            "[228 | 440.79] loss=2.36 avg=2.41\n",
            "[229 | 442.45] loss=2.68 avg=2.41\n",
            "[230 | 444.12] loss=1.97 avg=2.40\n",
            "[231 | 445.79] loss=1.70 avg=2.40\n",
            "[232 | 447.46] loss=1.79 avg=2.39\n",
            "[233 | 449.13] loss=1.79 avg=2.38\n",
            "[234 | 450.79] loss=2.36 avg=2.38\n",
            "[235 | 452.45] loss=2.24 avg=2.38\n",
            "[236 | 454.11] loss=2.16 avg=2.38\n",
            "[237 | 455.78] loss=2.37 avg=2.38\n",
            "[238 | 457.45] loss=2.40 avg=2.38\n",
            "[239 | 459.11] loss=2.17 avg=2.38\n",
            "[240 | 460.77] loss=2.06 avg=2.37\n",
            "[241 | 462.45] loss=2.33 avg=2.37\n",
            "[242 | 464.12] loss=2.52 avg=2.37\n",
            "[243 | 465.79] loss=2.07 avg=2.37\n",
            "[244 | 467.46] loss=2.68 avg=2.37\n",
            "[245 | 469.12] loss=2.47 avg=2.38\n",
            "[246 | 470.78] loss=2.36 avg=2.37\n",
            "[247 | 472.46] loss=2.68 avg=2.38\n",
            "[248 | 474.13] loss=2.19 avg=2.38\n",
            "[249 | 475.79] loss=2.04 avg=2.37\n",
            "[250 | 477.46] loss=2.10 avg=2.37\n",
            "[251 | 479.13] loss=2.67 avg=2.37\n",
            "[252 | 480.80] loss=2.09 avg=2.37\n",
            "[253 | 482.47] loss=2.38 avg=2.37\n",
            "[254 | 484.14] loss=1.58 avg=2.36\n",
            "[255 | 485.80] loss=2.21 avg=2.36\n",
            "[256 | 487.47] loss=2.32 avg=2.36\n",
            "[257 | 489.14] loss=2.15 avg=2.36\n",
            "[258 | 490.79] loss=1.89 avg=2.35\n",
            "[259 | 492.45] loss=2.42 avg=2.35\n",
            "[260 | 494.12] loss=1.83 avg=2.35\n",
            "[261 | 495.78] loss=1.87 avg=2.34\n",
            "[262 | 497.45] loss=2.20 avg=2.34\n",
            "[263 | 499.11] loss=2.67 avg=2.34\n",
            "[264 | 500.79] loss=1.68 avg=2.34\n",
            "[265 | 502.46] loss=1.77 avg=2.33\n",
            "[266 | 504.13] loss=2.34 avg=2.33\n",
            "[267 | 505.81] loss=2.30 avg=2.33\n",
            "[268 | 507.48] loss=2.28 avg=2.33\n",
            "[269 | 509.16] loss=2.10 avg=2.33\n",
            "[270 | 510.83] loss=2.15 avg=2.33\n",
            "[271 | 512.49] loss=2.41 avg=2.33\n",
            "[272 | 514.16] loss=1.96 avg=2.32\n",
            "[273 | 515.82] loss=2.34 avg=2.32\n",
            "[274 | 517.49] loss=1.54 avg=2.31\n",
            "[275 | 519.17] loss=2.34 avg=2.31\n",
            "[276 | 520.84] loss=2.16 avg=2.31\n",
            "[277 | 522.51] loss=1.55 avg=2.30\n",
            "[278 | 524.17] loss=1.60 avg=2.30\n",
            "[279 | 525.85] loss=2.11 avg=2.30\n",
            "[280 | 527.52] loss=1.81 avg=2.29\n",
            "[281 | 529.19] loss=2.10 avg=2.29\n",
            "[282 | 530.86] loss=2.36 avg=2.29\n",
            "[283 | 532.53] loss=1.30 avg=2.28\n",
            "[284 | 534.19] loss=2.16 avg=2.28\n",
            "[285 | 535.86] loss=1.93 avg=2.27\n",
            "[286 | 537.53] loss=1.58 avg=2.27\n",
            "[287 | 539.20] loss=2.52 avg=2.27\n",
            "[288 | 540.87] loss=2.24 avg=2.27\n",
            "[289 | 542.54] loss=2.03 avg=2.27\n",
            "[290 | 544.21] loss=2.61 avg=2.27\n",
            "[291 | 545.88] loss=1.46 avg=2.26\n",
            "[292 | 547.55] loss=1.73 avg=2.26\n",
            "[293 | 549.21] loss=1.92 avg=2.25\n",
            "[294 | 550.88] loss=1.76 avg=2.25\n",
            "[295 | 552.55] loss=3.15 avg=2.26\n",
            "[296 | 554.22] loss=1.97 avg=2.25\n",
            "[297 | 555.89] loss=2.02 avg=2.25\n",
            "[298 | 557.56] loss=1.77 avg=2.25\n",
            "[299 | 559.24] loss=1.97 avg=2.24\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "orthy ’till the end. ”\n",
            "\n",
            "And this ’s the message of ’All In With Chris Hayes” an afternoon special that will air live from the White House, featuring interviews with Obama, Vice President Joe Biden and Senate Majority Leader Harry Reid.\n",
            "\n",
            "This is how Obama ’s left-wing revolution will play out in real time: In the hour after the speech, viewers will see Biden speak a few minutes later, during the ’Super Bowl of Joy” broadcast on NBC.\n",
            "\n",
            "The message is: Watch the 'Super Bowl of Joy” to see Biden speak in the hour we get the “All In” live.\n",
            "\n",
            "Obama is playing the waiting game. Biden is the play­er. He doesn’t have to speak. All’   of the audience’s time will be up in the hour after Biden speaks. (But all of Obama’s fans would have to watch the show anyway.)\n",
            "\n",
            "Obama is doing himself a disservice by not engaging right away on the Trans-Pacific Partnership. He’ll look like a lame duck, while Biden’s fans are guaranteed to be sitting uncomfortably, thinking: “Who’s boss here?”\n",
            "\n",
            "The best bet for a president of the United States is to stick to the issues that matter to voters — like income inequality, the threat of a third world war against the United States or the urgent need for the government to make the lives of ordinary people better. In politics, you’re never too late.\n",
            "\n",
            "If Biden is able to get the “Obama” crowd through this speech, Obama will have set an example for the future. If he’s not, there will be a lot to prove in the final two and a half hours.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "The last thing America needs is for its Supreme Court to get in the way.\n",
            "\n",
            "The court approved all 50 executive branch appointments until now with three major rulings that reversed decades of precedent and gave the president the power to nominate and the power to fire U.S. attorneys, including for serious crimes like bribery and obstruction of justice.\n",
            "\n",
            "That was the first time the court had overturned a presidential appointee. The two other reversals came in 2006 and 2012. The court will review more than 100 executive branch nominations this term and reviewable through a four-year window next term, giving the Trump White House about two years to prepare for a possible five-to-four conservative court decision overturning virtually all the executive branch's actions since the late 1950s.\n",
            "\n",
            "All 50 judges now have the power to block appointments.\n",
            "\n",
            "The president and attorney general get a grace period of three months. It should last an awfully long time. Trump has given no indication he intends to follow the Obama-era practice of nominating judges in their first months in office, as his legal advisers have suggested.\n",
            "\n",
            "That gives Trump and his attorneys an opening to argue that they have full legislative authority to put together the court without the need for a two-thirds vote.\n",
            "\n",
            "The argument, while potentially significant, will go relatively quickly. Just three years after Democrats controlled the Senate and the White House, Trump made recess appointments and announced ­removal of nearly 800 U.S. attorneys.\n",
            "\n",
            "It was a relatively minor, if noteworthy, development, and while Trump is now a president himself, he should be able to use the new ­rudimentary procedure to argue ­that the two-thirds approval requirement did not apply.\n",
            "\n",
            "He has done just that. A group of Democratic senators wrote him a letter urging him to reverse course.\n",
            "\n",
            "Trump’s argument may be better suited to one of political theater than argument.\n",
            "\n",
            "He could spend much of this week using the issue to try to alienate allies and delegitimize the court, all the while saying the court has acted unfairly against him and has the wrong ­guidance on what constitutes a violation of the constitutional separation of powers. This is exactly the wrong argument.\n",
            "\n",
            "The court has never had a reason to rule against the president. His constitutional powers are so broad, and the court’s independence under the Constitution so thin, any claim that the president is acting in a manner unreviewable by the other branches of our government ­ in the form of judicial overreach — should be easily ­settled.\n",
            "\n",
            "If Trump wanted a ruling, he could always try to change the law to stop his ­administrations from using recess appointments to fill ­up vacancies, but there is no reason to think he would. He was elected to fill the court’s vacancies, not to amend the law.\n",
            "\n",
            "Trump is not arguing on principle that the Constitution requires that the court give him an up-or-down vote on all these executive branch nominations within two years. That is not a plausible argument for a vote, and should have been treated as such\n",
            "\n",
            "[300 | 583.71] loss=1.95 avg=2.24\n",
            "[301 | 585.38] loss=1.94 avg=2.24\n",
            "[302 | 587.04] loss=2.43 avg=2.24\n",
            "[303 | 588.70] loss=1.92 avg=2.24\n",
            "[304 | 590.35] loss=2.05 avg=2.23\n",
            "[305 | 592.01] loss=2.24 avg=2.23\n",
            "[306 | 593.66] loss=1.12 avg=2.22\n",
            "[307 | 595.31] loss=1.26 avg=2.21\n",
            "[308 | 596.96] loss=1.98 avg=2.21\n",
            "[309 | 598.61] loss=2.62 avg=2.21\n",
            "[310 | 600.27] loss=2.23 avg=2.21\n",
            "[311 | 601.93] loss=2.40 avg=2.22\n",
            "[312 | 603.59] loss=2.54 avg=2.22\n",
            "[313 | 605.25] loss=1.67 avg=2.21\n",
            "[314 | 606.89] loss=2.58 avg=2.22\n",
            "[315 | 608.56] loss=2.43 avg=2.22\n",
            "[316 | 610.22] loss=2.11 avg=2.22\n",
            "[317 | 611.89] loss=2.22 avg=2.22\n",
            "[318 | 613.55] loss=1.44 avg=2.21\n",
            "[319 | 615.21] loss=1.87 avg=2.21\n",
            "[320 | 616.88] loss=1.51 avg=2.20\n",
            "[321 | 618.55] loss=2.00 avg=2.20\n",
            "[322 | 620.20] loss=2.03 avg=2.20\n",
            "[323 | 621.87] loss=2.50 avg=2.20\n",
            "[324 | 623.54] loss=1.72 avg=2.19\n",
            "[325 | 625.21] loss=1.62 avg=2.19\n",
            "[326 | 626.89] loss=2.27 avg=2.19\n",
            "[327 | 628.56] loss=2.24 avg=2.19\n",
            "[328 | 630.23] loss=2.25 avg=2.19\n",
            "[329 | 631.89] loss=1.39 avg=2.18\n",
            "[330 | 633.56] loss=2.20 avg=2.18\n",
            "[331 | 635.24] loss=2.03 avg=2.18\n",
            "[332 | 636.91] loss=1.86 avg=2.18\n",
            "[333 | 638.58] loss=1.69 avg=2.17\n",
            "[334 | 640.25] loss=1.82 avg=2.17\n",
            "[335 | 641.93] loss=2.22 avg=2.17\n",
            "[336 | 643.60] loss=2.21 avg=2.17\n",
            "[337 | 645.27] loss=1.67 avg=2.16\n",
            "[338 | 646.94] loss=1.80 avg=2.16\n",
            "[339 | 648.62] loss=1.94 avg=2.16\n",
            "[340 | 650.30] loss=1.66 avg=2.15\n",
            "[341 | 651.97] loss=2.01 avg=2.15\n",
            "[342 | 653.65] loss=1.66 avg=2.15\n",
            "[343 | 655.33] loss=1.86 avg=2.14\n",
            "[344 | 657.01] loss=1.73 avg=2.14\n",
            "[345 | 658.69] loss=1.97 avg=2.14\n",
            "[346 | 660.37] loss=1.94 avg=2.14\n",
            "[347 | 662.04] loss=1.47 avg=2.13\n",
            "[348 | 663.71] loss=1.04 avg=2.12\n",
            "[349 | 665.38] loss=1.91 avg=2.11\n",
            "[350 | 667.07] loss=1.65 avg=2.11\n",
            "[351 | 668.75] loss=1.48 avg=2.10\n",
            "[352 | 670.43] loss=2.25 avg=2.11\n",
            "[353 | 672.11] loss=1.30 avg=2.10\n",
            "[354 | 673.79] loss=3.07 avg=2.11\n",
            "[355 | 675.46] loss=1.32 avg=2.10\n",
            "[356 | 677.13] loss=1.85 avg=2.10\n",
            "[357 | 678.80] loss=1.90 avg=2.09\n",
            "[358 | 680.49] loss=2.33 avg=2.10\n",
            "[359 | 682.16] loss=1.52 avg=2.09\n",
            "[360 | 683.85] loss=1.27 avg=2.08\n",
            "[361 | 685.53] loss=1.40 avg=2.08\n",
            "[362 | 687.21] loss=2.81 avg=2.08\n",
            "[363 | 688.90] loss=1.28 avg=2.07\n",
            "[364 | 690.57] loss=1.88 avg=2.07\n",
            "[365 | 692.26] loss=1.68 avg=2.07\n",
            "[366 | 693.94] loss=1.81 avg=2.07\n",
            "[367 | 695.61] loss=1.17 avg=2.06\n",
            "[368 | 697.30] loss=2.85 avg=2.06\n",
            "[369 | 698.97] loss=2.99 avg=2.07\n",
            "[370 | 700.65] loss=1.82 avg=2.07\n",
            "[371 | 702.32] loss=2.45 avg=2.08\n",
            "[372 | 704.00] loss=1.95 avg=2.07\n",
            "[373 | 705.67] loss=1.19 avg=2.07\n",
            "[374 | 707.36] loss=1.97 avg=2.06\n",
            "[375 | 709.03] loss=2.14 avg=2.07\n",
            "[376 | 710.71] loss=2.26 avg=2.07\n",
            "[377 | 712.38] loss=2.53 avg=2.07\n",
            "[378 | 714.06] loss=1.71 avg=2.07\n",
            "[379 | 715.73] loss=2.19 avg=2.07\n",
            "[380 | 717.40] loss=2.26 avg=2.07\n",
            "[381 | 719.07] loss=1.84 avg=2.07\n",
            "[382 | 720.74] loss=1.55 avg=2.06\n",
            "[383 | 722.43] loss=2.11 avg=2.06\n",
            "[384 | 724.10] loss=1.54 avg=2.06\n",
            "[385 | 725.77] loss=1.90 avg=2.06\n",
            "[386 | 727.44] loss=1.55 avg=2.05\n",
            "[387 | 729.12] loss=1.58 avg=2.05\n",
            "[388 | 730.78] loss=1.87 avg=2.05\n",
            "[389 | 732.45] loss=1.52 avg=2.04\n",
            "[390 | 734.12] loss=1.73 avg=2.04\n",
            "[391 | 735.80] loss=1.44 avg=2.03\n",
            "[392 | 737.48] loss=1.66 avg=2.03\n",
            "[393 | 739.17] loss=1.62 avg=2.02\n",
            "[394 | 740.84] loss=1.39 avg=2.02\n",
            "[395 | 742.52] loss=2.11 avg=2.02\n",
            "[396 | 744.21] loss=2.89 avg=2.03\n",
            "[397 | 745.88] loss=1.84 avg=2.02\n",
            "[398 | 747.56] loss=1.04 avg=2.01\n",
            "[399 | 749.24] loss=2.26 avg=2.02\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "‾s political agenda in Washington is to repeal and replace Obamacare — not simply defund it.\n",
            "\n",
            "President Trump’s decision to keep the government funded and move forward on his legislative agenda could be judged — by almost any metric — as a success. Trump’s Democratic opposition, on the other hand, is beginning to appear nervous. We are likely to see a showdown over funding the government at some point, and there is little doubt that Democrats would prefer inaction over a fight.\n",
            "\n",
            "The reality is that there is little Trump could do to avoid a shutdown. He can always use the threat of default to force Democrats to fund the government — a move that would probably enrage Trump and some of his lawmakers, since default would not only send a signal to the Chinese but to Russia and China, too, where their governments disapprove of the West turning a blind eye to human rights abuses.\n",
            "\n",
            "In fact, Trump could choose to use default to force Democrats under Obama to reform their ­aboriginal policies and move away from laws that have harmed Native Americans. These policies represent the best potential path to restoring traditional Native American culture and civilization.\n",
            "\n",
            "\n",
            "\n",
            "That Trump could choose these options, and to a greater extent choose to pursue these options, is commendable. That he chose them instead of pursuing a solution that would have solved the problem of illegal immigration is deeply worrying.\n",
            "\n",
            "If Trump chose these options, it would show a willingness to use the threat of default to achieve his partisan aims. He could be showing us a new willingness to resort to force to advance his policies.\n",
            "\n",
            "If he chose these options, it would show us that he is open to the kind of behavior that has enabled him to take over and succeed in office. And it would show us he is willing to disregard basic ­rules of political accountability.\n",
            "\n",
            "In other words, this doesn’t just show us the odd — even petty — thing Trump might like to do, this shows us how politicians and the forces they serve will behave in the presence of even tiny, unacceptable misbehavior.\n",
            "\n",
            "If ­Democrats and many Republicans want to get this budget off the ground, they will have to abandon policies that would make it harder for people to get ahead, and they will have to show us they care about people like Erica Smeal, who couldn’t even get ahead because of her ­illegal immigration status.\n",
            "\n",
            "If they don’t want to do that, they must show Trump and his friends that they will do everything in their power to keep him from doing his most ­impeccable offenses, because they can’t avoid him. And they will see a willingness to ignore basic rules of political accountability as their willingness to do something terrible and ­unthinkable to him.\n",
            "\n",
            "Matt Viser is a national political reporter for Politico.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "President Trump is still reeling from a bruising week that included two humiliating news conferences and a federal investigation that may have revealed his ­deliberate plan to fire special counsel Robert Mueller.\n",
            "\n",
            "It is good that Mueller’s investigation found no wrongdoing, but the president has a hard time taking it personally. After all, it was his appointed special prosecutor — not him — who recommended to Deputy Attorney General Rod Rosenstein that Mueller step aside.\n",
            "\n",
            "It wasn’t just Rosenstein, it was Mueller’s four deputy ­attorneys general, as well as both intelligence chiefs, and several senior FBI and CIA officials and several senior Department of Justice officials.\n",
            "\n",
            "If Trump were to fire Rosenstein, there would be uproar in Congress, likely led by the likes of Eliot Spitzer and Joe Biden. Trump could rightly make the argument that he had no choice because Attorney General William Barr had already said that the Mueller probe would continue.\n",
            "\n",
            "It’s not as though Trump didn’t intend to fire Mueller, either. As the New York Times reported on Thursday, he “was prepared to fire Mueller if it were believed that he had obstructed justice in the investigation. That would have raised the possibility of a second inquiry, as well, which could have undermined his narrative that he had an independent investigative commission.”\n",
            "\n",
            "Trump could argue that firing Mueller was part of a larger pattern of self-dealing that has undercut his presidency’s legitimacy, too. And he would be right. After ­obstructing Mueller for so long, if Trump had just taken his complaint about Russia seriously, it would have forced the president to confront his own recklessness and personal failings.\n",
            "\n",
            "As I wrote Thursday, President Trump’s initial claim that former acting Attorney General Sally Yates told him that he could fire Mueller wasn’t accurate. Yates told Yates’s superiors at Justice that there was no legal basis for Trump’s original travel ban, but White House Counsel Donald McGahn’s response to Yates’s report is instructive. He insisted that her testimony had nothing to do with that — even after an inspector general�\n",
            "\n",
            "[400 | 773.60] loss=2.03 avg=2.02\n",
            "[401 | 775.25] loss=1.91 avg=2.02\n",
            "[402 | 776.91] loss=1.67 avg=2.01\n",
            "[403 | 778.56] loss=2.17 avg=2.01\n",
            "[404 | 780.22] loss=1.77 avg=2.01\n",
            "[405 | 781.88] loss=1.58 avg=2.01\n",
            "[406 | 783.53] loss=1.43 avg=2.00\n",
            "[407 | 785.19] loss=2.12 avg=2.00\n",
            "[408 | 786.85] loss=1.37 avg=2.00\n",
            "[409 | 788.51] loss=1.78 avg=1.99\n",
            "[410 | 790.17] loss=1.95 avg=1.99\n",
            "[411 | 791.83] loss=2.00 avg=1.99\n",
            "[412 | 793.47] loss=1.97 avg=1.99\n",
            "[413 | 795.13] loss=1.93 avg=1.99\n",
            "[414 | 796.79] loss=1.48 avg=1.99\n",
            "[415 | 798.46] loss=2.28 avg=1.99\n",
            "[416 | 800.12] loss=1.57 avg=1.99\n",
            "[417 | 801.79] loss=1.49 avg=1.98\n",
            "[418 | 803.46] loss=1.44 avg=1.98\n",
            "[419 | 805.13] loss=1.88 avg=1.97\n",
            "[420 | 806.80] loss=1.82 avg=1.97\n",
            "[421 | 808.48] loss=1.93 avg=1.97\n",
            "[422 | 810.14] loss=2.21 avg=1.97\n",
            "[423 | 811.82] loss=1.85 avg=1.97\n",
            "[424 | 813.48] loss=1.83 avg=1.97\n",
            "[425 | 815.16] loss=1.58 avg=1.97\n",
            "[426 | 816.83] loss=2.47 avg=1.97\n",
            "[427 | 818.50] loss=2.48 avg=1.98\n",
            "[428 | 820.17] loss=2.54 avg=1.98\n",
            "[429 | 821.84] loss=1.63 avg=1.98\n",
            "[430 | 823.51] loss=1.23 avg=1.97\n",
            "[431 | 825.18] loss=1.33 avg=1.97\n",
            "[432 | 826.85] loss=1.35 avg=1.96\n",
            "[433 | 828.51] loss=1.58 avg=1.96\n",
            "[434 | 830.20] loss=1.56 avg=1.95\n",
            "[435 | 831.87] loss=1.89 avg=1.95\n",
            "[436 | 833.54] loss=1.42 avg=1.95\n",
            "[437 | 835.20] loss=2.02 avg=1.95\n",
            "[438 | 836.89] loss=2.39 avg=1.95\n",
            "[439 | 838.58] loss=2.01 avg=1.95\n",
            "[440 | 840.25] loss=1.73 avg=1.95\n",
            "[441 | 841.93] loss=0.94 avg=1.94\n",
            "[442 | 843.61] loss=1.53 avg=1.94\n",
            "[443 | 845.28] loss=1.86 avg=1.93\n",
            "[444 | 846.96] loss=1.67 avg=1.93\n",
            "[445 | 848.64] loss=1.57 avg=1.93\n",
            "[446 | 850.32] loss=1.97 avg=1.93\n",
            "[447 | 852.00] loss=1.73 avg=1.93\n",
            "[448 | 853.67] loss=1.52 avg=1.92\n",
            "[449 | 855.34] loss=1.28 avg=1.92\n",
            "[450 | 857.02] loss=1.82 avg=1.92\n",
            "[451 | 858.70] loss=1.84 avg=1.91\n",
            "[452 | 860.37] loss=1.35 avg=1.91\n",
            "[453 | 862.06] loss=1.67 avg=1.91\n",
            "[454 | 863.73] loss=1.85 avg=1.91\n",
            "[455 | 865.42] loss=1.14 avg=1.90\n",
            "[456 | 867.09] loss=1.96 avg=1.90\n",
            "[457 | 868.75] loss=1.14 avg=1.89\n",
            "[458 | 870.43] loss=1.22 avg=1.88\n",
            "[459 | 872.10] loss=1.72 avg=1.88\n",
            "[460 | 873.79] loss=1.94 avg=1.88\n",
            "[461 | 875.46] loss=0.82 avg=1.87\n",
            "[462 | 877.15] loss=1.41 avg=1.87\n",
            "[463 | 878.82] loss=1.09 avg=1.86\n",
            "[464 | 880.51] loss=1.90 avg=1.86\n",
            "[465 | 882.18] loss=0.94 avg=1.85\n",
            "[466 | 883.85] loss=0.82 avg=1.84\n",
            "[467 | 885.52] loss=2.02 avg=1.84\n",
            "[468 | 887.21] loss=1.59 avg=1.84\n",
            "[469 | 888.88] loss=1.39 avg=1.84\n",
            "[470 | 890.55] loss=1.05 avg=1.83\n",
            "[471 | 892.22] loss=1.27 avg=1.82\n",
            "[472 | 893.90] loss=1.07 avg=1.81\n",
            "[473 | 895.58] loss=1.54 avg=1.81\n",
            "[474 | 897.26] loss=1.29 avg=1.81\n",
            "[475 | 898.93] loss=2.12 avg=1.81\n",
            "[476 | 900.61] loss=2.01 avg=1.81\n",
            "[477 | 902.28] loss=1.01 avg=1.80\n",
            "[478 | 903.95] loss=1.80 avg=1.80\n",
            "[479 | 905.63] loss=1.79 avg=1.80\n",
            "[480 | 907.30] loss=1.20 avg=1.80\n",
            "[481 | 908.97] loss=1.52 avg=1.79\n",
            "[482 | 910.64] loss=0.75 avg=1.78\n",
            "[483 | 912.31] loss=0.76 avg=1.77\n",
            "[484 | 913.99] loss=1.27 avg=1.77\n",
            "[485 | 915.65] loss=0.98 avg=1.76\n",
            "[486 | 917.33] loss=1.22 avg=1.76\n",
            "[487 | 918.99] loss=2.27 avg=1.76\n",
            "[488 | 920.67] loss=1.29 avg=1.76\n",
            "[489 | 922.34] loss=1.15 avg=1.75\n",
            "[490 | 924.02] loss=0.68 avg=1.74\n",
            "[491 | 925.70] loss=1.85 avg=1.74\n",
            "[492 | 927.38] loss=1.53 avg=1.74\n",
            "[493 | 929.05] loss=1.54 avg=1.74\n",
            "[494 | 930.73] loss=2.00 avg=1.74\n",
            "[495 | 932.41] loss=0.75 avg=1.73\n",
            "[496 | 934.09] loss=2.31 avg=1.73\n",
            "[497 | 935.77] loss=1.26 avg=1.73\n",
            "[498 | 937.45] loss=2.14 avg=1.73\n",
            "[499 | 939.13] loss=1.31 avg=1.73\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " Russian Democratic Republic and the former Soviet Union — was likely to benefit from such a move. Putin could argue that the sanctions were meant to punish the country for its annexation of Crimea while in possession of the Black Sea peninsula. Or even worse, as he has done before, he could claim they were a ruse to obtain a no-fly zone over rebel-held eastern Ukraine — which, had he sought one, would have established Russia on a legal firm majestically situated above those besieged by the West.\n",
            "\n",
            "The West has learned its lesson. As John Kerry, Obama's former secretary of state, put it, the sanctions \"made no [material] difference\" to Putin. There are some, it seems, who still think so.\n",
            "\n",
            "Juan Cole is a foreign policy commentator for CNN and author of \" Putin In Disarray: How Vladimir Putin Is Regaining the Initiative for a New Century. \"\n",
            "\n",
            "Read more:\n",
            "\n",
            "The Russian economy has shrunk by more than a third in five years and inflation is running at more than 50 percent, says the official website of Russia's central bank.\n",
            "\n",
            "Putin has lost a fifth of his power at home in the past year. Now he wants to reverse course, warn Republican presidential candidates Marco Rubio and Mike Huckabee — and warn Obama as well.\n",
            "\n",
            "Now comes the hard part: Who will take on the Russia threat in a manner that will not embolden Putin?\n",
            "\n",
            "That's how the GOP and its candidates are approaching the Obama legacy. They can't help but respond with a dose of the Cold War against the backdrop of a sickening visual display: a man wrapped in a red shirt with the words ''Menshevik -- Defender of the Fatherland**s only weapon.''\n",
            "\n",
            "\n",
            "TOPICS:\n",
            "\n",
            "Foreign Affairs\n",
            "\n",
            "News/Current Events\n",
            "\n",
            "Politics/Elections\n",
            "\n",
            "KEYWORDS:\n",
            "\n",
            "crudeoil\n",
            "\n",
            "\n",
            "To: jdolley\n",
            "\n",
            "Well, he never quite got that sick feeling in the pit of his stomach after his run-in with the fed muck.\n",
            "\n",
            "\n",
            "To: jdolley\n",
            "\n",
            "The 'Russian Federation' is another acronym for the Federal Republic of Independent States of Central Asia.\n",
            "\n",
            "\n",
            "by 2 posted onby The Patriot (America Never Waits)\n",
            "\n",
            "To: jdolley\n",
            "\n",
            "No wonder those who say Obama was naïve to think he could get Russia to abandon its traditional anti-American behavior are now saying he was equally naïve not to anticipate its development.\n",
            "\n",
            "\n",
            "To: jdolley\n",
            "\n",
            "No wonder those who say Obama was naïve to think he could get Russia to abandon its traditional anti-American behavior are now saying he was equally naïve not to anticipate its development.\n",
            "\n",
            "\n",
            "As it happens, in 2008 Russia joined the WTO and is now one of the few rich countries which benefits from the falling prices of oil.\n",
            "\n",
            "\n",
            "To: jdolley\n",
            "\n",
            "Obama should have anticipated Russia's rise from the ashes of the Soviet Union. The question, however, is whether he was prepared to take the necessary drastic measures to stop it.\n",
            "\n",
            "\n",
            "To: jdolley\n",
            "\n",
            "What is the difference between these two examples? One was a complete collapse following the second Chechen war and the other was followed by a very modest recovery - a gradual recovery for which Russia gets no benefits in prices, but a steady rise in living standards based on falling prices - which the U.S. enjoys no greater benefits than the fall in prices for Soviet exports.\n",
            "\n",
            "\n",
            "To: jdolley\n",
            "\n",
            "Yes, we are seeing signs that economic recovery in the United States may be picking up. The first month of 2014 represented the best jobs growth since September 2007 - and a 3.3-percent increase over the month of January.\n",
            "\n",
            "\n",
            "To: jdolley\n",
            "\n",
            "It's like they didn’t realize what the Russians had become in the 1990s. They should've guessed that a world economy based on cheap oil could have severe global consequences.\n",
            "\n",
            "\n",
            "To: jdolley\n",
            "\n",
            "\n",
            "by 11 of 17 ­ ­TB\n",
            "\n",
            "\n",
            "by 12 of 17 “Wake up, America!” They lied to you, they betrayed you, and now they’re betraying you.\n",
            "\n",
            "\n",
            "by 14 of 17 Meryl Streep — “Shakespeare”\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "by 15 of 17 “A Very Russian Décor”\n",
            "\n",
            "\n",
            "by 16 of 17 “I Could Never Get What I Needed for Myself,” Now I’m Just a Russian Décor!\n",
            "\n",
            "\n",
            "by 17 of 17 “In Russian Hand,” They Don’t Know If I Can Continue!!!\n",
            "\n",
            "\n",
            "by 18 of 17 I’m a Russian Décor —“Hedonism for Hiden Russians”\n",
            "\n",
            "\n",
            "by 20 of 17 “Rudy”<|endoftext|>The latest installment in the epic Pokémon saga, 'The Last Pokémon Ranger: The Answer',\n",
            "\n",
            "[500 | 963.57] loss=1.24 avg=1.72\n",
            "[501 | 965.22] loss=1.54 avg=1.72\n",
            "[502 | 966.89] loss=1.75 avg=1.72\n",
            "[503 | 968.55] loss=1.54 avg=1.72\n",
            "[504 | 970.21] loss=1.53 avg=1.72\n",
            "[505 | 971.88] loss=1.40 avg=1.72\n",
            "[506 | 973.54] loss=1.31 avg=1.71\n",
            "[507 | 975.20] loss=0.54 avg=1.70\n",
            "[508 | 976.85] loss=1.30 avg=1.70\n",
            "[509 | 978.52] loss=1.28 avg=1.69\n",
            "[510 | 980.19] loss=0.99 avg=1.68\n",
            "[511 | 981.85] loss=1.38 avg=1.68\n",
            "[512 | 983.51] loss=2.59 avg=1.69\n",
            "[513 | 985.16] loss=1.37 avg=1.69\n",
            "[514 | 986.83] loss=1.48 avg=1.69\n",
            "[515 | 988.50] loss=0.88 avg=1.68\n",
            "[516 | 990.17] loss=1.10 avg=1.67\n",
            "[517 | 991.82] loss=1.17 avg=1.67\n",
            "[518 | 993.49] loss=1.56 avg=1.67\n",
            "[519 | 995.16] loss=0.96 avg=1.66\n",
            "[520 | 996.83] loss=0.93 avg=1.65\n",
            "[521 | 998.50] loss=1.22 avg=1.65\n",
            "[522 | 1000.17] loss=1.59 avg=1.65\n",
            "[523 | 1001.84] loss=1.54 avg=1.65\n",
            "[524 | 1003.51] loss=1.50 avg=1.64\n",
            "[525 | 1005.18] loss=1.79 avg=1.65\n",
            "[526 | 1006.85] loss=1.10 avg=1.64\n",
            "[527 | 1008.52] loss=1.14 avg=1.63\n",
            "[528 | 1010.19] loss=1.64 avg=1.63\n",
            "[529 | 1011.87] loss=1.30 avg=1.63\n",
            "[530 | 1013.53] loss=1.18 avg=1.63\n",
            "[531 | 1015.20] loss=1.00 avg=1.62\n",
            "[532 | 1016.87] loss=1.77 avg=1.62\n",
            "[533 | 1018.54] loss=2.33 avg=1.63\n",
            "[534 | 1020.21] loss=1.09 avg=1.62\n",
            "[535 | 1021.89] loss=1.59 avg=1.62\n",
            "[536 | 1023.57] loss=1.97 avg=1.63\n",
            "[537 | 1025.25] loss=1.66 avg=1.63\n",
            "[538 | 1026.94] loss=1.17 avg=1.62\n",
            "[539 | 1028.62] loss=1.31 avg=1.62\n",
            "[540 | 1030.30] loss=1.02 avg=1.61\n",
            "[541 | 1031.98] loss=1.04 avg=1.61\n",
            "[542 | 1033.67] loss=0.86 avg=1.60\n",
            "[543 | 1035.34] loss=0.84 avg=1.59\n",
            "[544 | 1037.03] loss=1.58 avg=1.59\n",
            "[545 | 1038.72] loss=1.07 avg=1.59\n",
            "[546 | 1040.40] loss=1.63 avg=1.59\n",
            "[547 | 1042.09] loss=1.04 avg=1.58\n",
            "[548 | 1043.76] loss=1.37 avg=1.58\n",
            "[549 | 1045.46] loss=0.90 avg=1.57\n",
            "[550 | 1047.13] loss=1.55 avg=1.57\n",
            "[551 | 1048.81] loss=0.70 avg=1.56\n",
            "[552 | 1050.50] loss=0.94 avg=1.56\n",
            "[553 | 1052.18] loss=1.19 avg=1.55\n",
            "[554 | 1053.87] loss=0.72 avg=1.55\n",
            "[555 | 1055.54] loss=0.93 avg=1.54\n",
            "[556 | 1057.21] loss=1.14 avg=1.54\n",
            "[557 | 1058.89] loss=1.64 avg=1.54\n",
            "[558 | 1060.57] loss=1.87 avg=1.54\n",
            "[559 | 1062.24] loss=1.71 avg=1.54\n",
            "[560 | 1063.92] loss=2.20 avg=1.55\n",
            "[561 | 1065.59] loss=1.10 avg=1.54\n",
            "[562 | 1067.26] loss=2.06 avg=1.55\n",
            "[563 | 1068.92] loss=1.90 avg=1.55\n",
            "[564 | 1070.60] loss=1.80 avg=1.56\n",
            "[565 | 1072.27] loss=1.49 avg=1.55\n",
            "[566 | 1073.94] loss=2.06 avg=1.56\n",
            "[567 | 1075.60] loss=1.65 avg=1.56\n",
            "[568 | 1077.27] loss=1.45 avg=1.56\n",
            "[569 | 1078.95] loss=1.63 avg=1.56\n",
            "[570 | 1080.62] loss=1.44 avg=1.56\n",
            "[571 | 1082.28] loss=1.12 avg=1.55\n",
            "[572 | 1083.95] loss=1.15 avg=1.55\n",
            "[573 | 1085.61] loss=1.00 avg=1.54\n",
            "[574 | 1087.28] loss=0.94 avg=1.54\n",
            "[575 | 1088.95] loss=1.19 avg=1.54\n",
            "[576 | 1090.62] loss=1.47 avg=1.53\n",
            "[577 | 1092.27] loss=1.90 avg=1.54\n",
            "[578 | 1093.93] loss=1.59 avg=1.54\n",
            "[579 | 1095.61] loss=0.84 avg=1.53\n",
            "[580 | 1097.27] loss=1.33 avg=1.53\n",
            "[581 | 1098.93] loss=0.82 avg=1.52\n",
            "[582 | 1100.61] loss=2.07 avg=1.53\n",
            "[583 | 1102.27] loss=1.12 avg=1.52\n",
            "[584 | 1103.94] loss=1.04 avg=1.52\n",
            "[585 | 1105.62] loss=1.14 avg=1.52\n",
            "[586 | 1107.30] loss=2.17 avg=1.52\n",
            "[587 | 1108.97] loss=1.21 avg=1.52\n",
            "[588 | 1110.64] loss=0.94 avg=1.51\n",
            "[589 | 1112.32] loss=1.31 avg=1.51\n",
            "[590 | 1114.00] loss=1.06 avg=1.51\n",
            "[591 | 1115.69] loss=1.53 avg=1.51\n",
            "[592 | 1117.37] loss=1.38 avg=1.51\n",
            "[593 | 1119.06] loss=1.20 avg=1.50\n",
            "[594 | 1120.75] loss=0.80 avg=1.50\n",
            "[595 | 1122.43] loss=1.50 avg=1.50\n",
            "[596 | 1124.12] loss=0.75 avg=1.49\n",
            "[597 | 1125.81] loss=2.50 avg=1.50\n",
            "[598 | 1127.50] loss=0.81 avg=1.49\n",
            "[599 | 1129.20] loss=1.47 avg=1.49\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " sent the Democrats a gift card.\n",
            "\n",
            "He also turned them into a political football — and used the team that didn’t play to his personal gain.\n",
            "\n",
            "In the midterms, he’s hoping the base will elect Democratic senators and representatives who will vote for his budget, and then he can add an additional $1 trillion in new spending.\n",
            "\n",
            "It’s a grand game of chicken, but if the plan comes to fruition, the president’s plans would be even more modest because the cuts are designed to take place over a two-decade period.\n",
            "\n",
            "It’s worth pointing out that under President Trump, defense spending has increased by more than 25 percent — and that’s without taking into account hundreds of billions more in new taxes and spending stemming from the health care overhaul.\n",
            "\n",
            "Under President Barack Obama, defense spending was projected to increase by 3 percent annually through 2035, and then by an inflation-indexed 2.2 percent annually after that.\n",
            "\n",
            "Under President Trump, we are on the cusp of a 3 percent average annual rate of increase; inflation-indexed; and 2.2 percent. If inflation continues its forward march and economic growth weakens, inflation-indexed is the price we have to pay if we’re going to sustain the growth and long-term success of our country.\n",
            "\n",
            "For someone who has made it his life’s work to turn the United States into a banana republic, this is an ­uncomfortable fact to ­realize.\n",
            "\n",
            "But what’s the use of crying inflation, if you can’t confront it and defend it?\n",
            "\n",
            "Twitter: @RichLowry\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "With his heart set on a bipartisan tax reform deal, Rep. Charlie Dent’s (R-Pa.) fate appeared sealed on Tuesday night.\n",
            "\n",
            "But in a stunning turn of events, the freshman Dent’s office announced that he would undergo heart surgery last week — and that he would require a second surgery — after which he will not be returning to Congress.\n",
            "\n",
            "“In the final days of his first term, I made a difficult decision that will not change my priorities for the 2018 session. As I have said before, there can be no tax reform without a path to coverage for the nearly nine million Americans who currently don’t have access to coverage, and I’m committed to working with Democrats and Republicans on a common-sense solution, no matter how difficult that may be, including with religious institutions and individuals who refuse to treat people’s health.”\n",
            "\n",
            "Dent is not the first member of Congress to postpone or ultimately kill his or her tax reform plans just days before an anticipated vote. In February and March, Rep. Tim Huelskamp (R-Kansas) announced that he was ­retiring after ­his first term, and in August, Rep. Charlie Dent canceled his retirement plan and announced plans to run for governor of Pennsylvania.\n",
            "\n",
            "This time, however, no other members of Congress were ­exposed to the facts of Dent’s ­medical emergency — and his sudden cancellation didn’t prompt widespread media coverage.\n",
            "\n",
            "The Philadelphia Inquirer reported that he underwent two ­arousal heart surgeries in the last year, one in January and another in August. He also underwent angioplasty to correct a ­defect in his coronary artery labiopathophysel, or the coronary blood-brain barrier, and small coronary angioplasty to correct a narrowed coronary artery, which could have compromised his heart rate monitoring, the paper reported.\n",
            "\n",
            "But the Washington Post reported Wednesday night that in ­his announcement on his plans for ­retirement, ­Dent ­understated the ­reason for his ­retirement by noting that he had a daughter under 2 months old.\n",
            "\n",
            "Rep. Carlos Curbelo (R-Fla.) announced last week that he would drop out of the 2018 election “at the earliest,” and The Washington Post’s ­reports Wednesday night and Thursday morning show that he did so shortly after announcing his retirement. The Miami Herald reports that he announced the ­detachment in an exclusive interview with The Post.\n",
            "\n",
            "Curbelo announced his plans to ­retire early because he felt he had no hope of retaking his Republican-held seat in Florida, and ­now that he’s out of the race, there’s no one else left to take it. He’s up against former state Sen. Corrine Brown, who announced her intention to run in 2018, and the “younger, less well-known ”Curtis Lamont, who finished third in 2014.\n",
            "\n",
            "By dropping out of the election before it was competitive, Curbelo’s move may have hurt his chances — although he claims his moves were designed to give his voters another reason to vote.\n",
            "\n",
            "Even\n",
            "\n",
            "[600 | 1153.78] loss=0.62 avg=1.48\n",
            "[601 | 1155.44] loss=1.39 avg=1.48\n",
            "[602 | 1157.10] loss=1.10 avg=1.48\n",
            "[603 | 1158.76] loss=0.84 avg=1.47\n",
            "[604 | 1160.41] loss=1.33 avg=1.47\n",
            "[605 | 1162.06] loss=0.77 avg=1.46\n",
            "[606 | 1163.72] loss=0.86 avg=1.46\n",
            "[607 | 1165.37] loss=1.51 avg=1.46\n",
            "[608 | 1167.02] loss=1.01 avg=1.45\n",
            "[609 | 1168.67] loss=0.68 avg=1.45\n",
            "[610 | 1170.32] loss=1.39 avg=1.44\n",
            "[611 | 1171.97] loss=1.76 avg=1.45\n",
            "[612 | 1173.63] loss=0.86 avg=1.44\n",
            "[613 | 1175.28] loss=0.88 avg=1.44\n",
            "[614 | 1176.92] loss=1.27 avg=1.43\n",
            "[615 | 1178.58] loss=1.13 avg=1.43\n",
            "[616 | 1180.24] loss=2.28 avg=1.44\n",
            "[617 | 1181.90] loss=1.37 avg=1.44\n",
            "[618 | 1183.55] loss=1.04 avg=1.44\n",
            "[619 | 1185.22] loss=1.03 avg=1.43\n",
            "[620 | 1186.87] loss=0.92 avg=1.43\n",
            "[621 | 1188.54] loss=0.50 avg=1.42\n",
            "[622 | 1190.20] loss=1.10 avg=1.41\n",
            "[623 | 1191.86] loss=0.61 avg=1.41\n",
            "[624 | 1193.54] loss=0.79 avg=1.40\n",
            "[625 | 1195.21] loss=1.02 avg=1.40\n",
            "[626 | 1196.89] loss=1.34 avg=1.39\n",
            "[627 | 1198.57] loss=1.41 avg=1.40\n",
            "[628 | 1200.25] loss=1.03 avg=1.39\n",
            "[629 | 1201.92] loss=0.64 avg=1.38\n",
            "[630 | 1203.60] loss=0.50 avg=1.38\n",
            "[631 | 1205.29] loss=1.00 avg=1.37\n",
            "[632 | 1206.95] loss=0.85 avg=1.37\n",
            "[633 | 1208.64] loss=1.01 avg=1.36\n",
            "[634 | 1210.32] loss=1.66 avg=1.37\n",
            "[635 | 1212.00] loss=1.38 avg=1.37\n",
            "[636 | 1213.68] loss=1.26 avg=1.36\n",
            "[637 | 1215.35] loss=1.59 avg=1.37\n",
            "[638 | 1217.04] loss=0.61 avg=1.36\n",
            "[639 | 1218.73] loss=1.50 avg=1.36\n",
            "[640 | 1220.40] loss=1.24 avg=1.36\n",
            "[641 | 1222.09] loss=1.50 avg=1.36\n",
            "[642 | 1223.76] loss=0.69 avg=1.35\n",
            "[643 | 1225.43] loss=0.51 avg=1.35\n",
            "[644 | 1227.12] loss=0.54 avg=1.34\n",
            "[645 | 1228.79] loss=0.93 avg=1.33\n",
            "[646 | 1230.48] loss=1.04 avg=1.33\n",
            "[647 | 1232.17] loss=1.52 avg=1.33\n",
            "[648 | 1233.86] loss=1.20 avg=1.33\n",
            "[649 | 1235.54] loss=0.85 avg=1.33\n",
            "[650 | 1237.21] loss=1.64 avg=1.33\n",
            "[651 | 1238.89] loss=1.57 avg=1.33\n",
            "[652 | 1240.57] loss=1.28 avg=1.33\n",
            "[653 | 1242.24] loss=0.98 avg=1.33\n",
            "[654 | 1243.91] loss=0.69 avg=1.32\n",
            "[655 | 1245.58] loss=1.43 avg=1.32\n",
            "[656 | 1247.26] loss=1.25 avg=1.32\n",
            "[657 | 1248.93] loss=0.55 avg=1.31\n",
            "[658 | 1250.61] loss=0.68 avg=1.31\n",
            "[659 | 1252.28] loss=1.17 avg=1.31\n",
            "[660 | 1253.94] loss=2.16 avg=1.32\n",
            "[661 | 1255.60] loss=0.63 avg=1.31\n",
            "[662 | 1257.27] loss=0.84 avg=1.30\n",
            "[663 | 1258.93] loss=1.32 avg=1.30\n",
            "[664 | 1260.60] loss=1.21 avg=1.30\n",
            "[665 | 1262.27] loss=0.76 avg=1.30\n",
            "[666 | 1263.94] loss=0.94 avg=1.29\n",
            "[667 | 1265.61] loss=0.33 avg=1.28\n",
            "[668 | 1267.28] loss=0.81 avg=1.28\n",
            "[669 | 1268.95] loss=0.36 avg=1.27\n",
            "[670 | 1270.62] loss=1.19 avg=1.27\n",
            "[671 | 1272.29] loss=1.08 avg=1.27\n",
            "[672 | 1273.95] loss=0.77 avg=1.26\n",
            "[673 | 1275.63] loss=1.16 avg=1.26\n",
            "[674 | 1277.30] loss=1.16 avg=1.26\n",
            "[675 | 1278.97] loss=1.26 avg=1.26\n",
            "[676 | 1280.64] loss=0.96 avg=1.26\n",
            "[677 | 1282.31] loss=0.50 avg=1.25\n",
            "[678 | 1283.97] loss=0.89 avg=1.25\n",
            "[679 | 1285.64] loss=0.80 avg=1.24\n",
            "[680 | 1287.29] loss=1.53 avg=1.25\n",
            "[681 | 1288.96] loss=0.52 avg=1.24\n",
            "[682 | 1290.62] loss=1.09 avg=1.24\n",
            "[683 | 1292.29] loss=1.60 avg=1.24\n",
            "[684 | 1293.97] loss=0.94 avg=1.24\n",
            "[685 | 1295.64] loss=0.64 avg=1.23\n",
            "[686 | 1297.31] loss=0.67 avg=1.23\n",
            "[687 | 1298.98] loss=1.44 avg=1.23\n",
            "[688 | 1300.65] loss=0.54 avg=1.22\n",
            "[689 | 1302.32] loss=1.39 avg=1.22\n",
            "[690 | 1303.99] loss=1.32 avg=1.22\n",
            "[691 | 1305.66] loss=0.60 avg=1.22\n",
            "[692 | 1307.33] loss=0.40 avg=1.21\n",
            "[693 | 1309.01] loss=1.00 avg=1.21\n",
            "[694 | 1310.68] loss=0.99 avg=1.20\n",
            "[695 | 1312.35] loss=0.89 avg=1.20\n",
            "[696 | 1314.02] loss=1.04 avg=1.20\n",
            "[697 | 1315.69] loss=1.37 avg=1.20\n",
            "[698 | 1317.37] loss=0.59 avg=1.20\n",
            "[699 | 1319.03] loss=0.51 avg=1.19\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "Even so, it is worth noting that the FBI director, James Comey, wrote a letter exonerating Hillary Clinton just one day after the release of the new emails. The fact that the investigation fell apart at the first sign of impropriety only serves to underscore the importance of a thorough and independent probe.\n",
            "\n",
            "In the new book 'Deep State,' former Justice Department official Daniel Richman documents the inner workings of the FBI and shows how the bureau has been hijacked by political partisans eager for months to inflict maximum damage on President Trump.\n",
            "\n",
            "In 2017, the bureau was charged with enforcing a 2000 law aimed at battling Russian interference in the 2016 election. It is charged with enforcing 28 existing laws, many of them aimed at disrupting the Trump presidency.\n",
            "\n",
            "Yet from the start, the Mueller probe has been framed not just against the then-candidate but also with a special suspicion for Trump. The indictment charged that from January 2016 through early 2017, from the Trump Organization to its for-profit Atlantic City casinos, “Donald J. Trump became, and remained, a primary and only‑in‑sole source of income for himself and his family.”\n",
            "\n",
            "This is a profoundly anti-Trump motivation. The idea is that because Trump controls the government and because he is wealthy enough to have his own lawyers and trusted enough to have his own FBI director, he can avoid any apparent self-dealing in the way of conflicts of interest while still being held to account for his ­minor role in a Russian conspiracy.\n",
            "\n",
            "It is absurd. But now we know why. The Mueller probe has produced not just indictments against Trump but also almost simultaneous admissions from his former aides that ­undermined his fealty to the president. The revelation that Robert Mueller got his first briefing on the Russia collusion conspiracy while the president was still technically president ­charges a direct ­obstruction of justice.\n",
            "\n",
            "The Democrats have seized on this development to demand that Mueller be permitted to testify under oath about his relationship with Justice Department lawyers — an outrageous overreach that could lead to frivolous impeachment proceedings.\n",
            "\n",
            "I don’t see how the Constitution permits this. The Framers understood that the power to declare war is specifically enumerated in the Bill of Rights, and they gave Congress broad leeway to give effect to the declarations. The Fourth Amendment prohibits excessive seizures and searches, but it does not prohibit arrests. Likewise, the Fifth Amendment requires that seizures and searches be justified by probable cause.\n",
            "\n",
            "Congress has complied with all three requirements. It has declared three ­anticipatory wars with foreign armies: the Russian invasion of Ukraine, the Chinese invasion of Alaska and the Turkish invasion of Greece. Congress has declared these wars national emergencies, giving it authorized powers to use all necessary means to protect American lives and property.\n",
            "\n",
            "Today’s developments in Syria offer even more reason for immediate action. Syria has ­already declared war on the People’ Alliance, the alliance of Kurds, Armenians and other minorities that Russia ­recognized in November. President Trump should immediately declare that all Syrian Kurdish forces are terrorists, expel them from the country and return their Kurdish languages and culture to them.\n",
            "\n",
            "Russia knows this because Syrian President Bashar al-Assad signed up for it in the first place. Russian intelligence agencies know this because Assad routinely says he wants to ­degrade and eventually destroy the Syrian Kurds.\n",
            "\n",
            "Trump should act now to stop Assad before it’s too late. He should call upon Russia and China to do more to stop the regime of Syrian President Bashar al-Assad. And he should urge the regime of Syrian President Bashar al-Assad to come to the table and join the Russian-Chinese-Kerry plan to destroy ISIS.\n",
            "\n",
            "If Trump does not act, the world will see what happens when a president behaves irresponsibly around the world. That Trump does act shows that he is worthy of our admiration and support. It also shows that we can still lead if we take deliberate and overwhelming action.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "When President Trump boasted about sexually assaulting women, he was not merely belittling established norms but also betraying one of our country’s greatest strengths: the willingness to buck politically powerful historical figures.\n",
            "\n",
            "The New York Times reported that Trump told Russian officials during the White House transition that he could use Russian state funding to ­mock ­both Clintons. The goal, Trump surmised, is to discredit both men and their policies.\n",
            "\n",
            "If this sounds familiar, it should. It should ­also ring true in light of the latest revelations about President Trump’s campaign dealings with Russia and whether or not there was actually collusion with the Kremlin.\n",
            "\n",
            "It’s entirely possible that the Trump administration didn’t know this man was Vasyl Ivoff — the real name of a KGB asset who served briefly in the U.S. government — or that he was working on his behalf in 2016. But that doesn’t make ­himnℓposh.\n",
            "\n",
            "\n",
            "\n",
            "[700 | 1343.52] loss=1.92 avg=1.20\n",
            "[701 | 1345.19] loss=1.75 avg=1.20\n",
            "[702 | 1346.86] loss=0.75 avg=1.20\n",
            "[703 | 1348.53] loss=0.94 avg=1.19\n",
            "[704 | 1350.19] loss=0.87 avg=1.19\n",
            "[705 | 1351.86] loss=1.44 avg=1.19\n",
            "[706 | 1353.54] loss=1.06 avg=1.19\n",
            "[707 | 1355.19] loss=0.95 avg=1.19\n",
            "[708 | 1356.86] loss=1.05 avg=1.19\n",
            "[709 | 1358.52] loss=0.95 avg=1.19\n",
            "[710 | 1360.18] loss=0.46 avg=1.18\n",
            "[711 | 1361.84] loss=0.37 avg=1.17\n",
            "[712 | 1363.49] loss=0.67 avg=1.17\n",
            "[713 | 1365.16] loss=0.28 avg=1.16\n",
            "[714 | 1366.82] loss=0.63 avg=1.15\n",
            "[715 | 1368.49] loss=0.74 avg=1.15\n",
            "[716 | 1370.15] loss=1.27 avg=1.15\n",
            "[717 | 1371.82] loss=0.81 avg=1.15\n",
            "[718 | 1373.48] loss=1.26 avg=1.15\n",
            "[719 | 1375.14] loss=0.44 avg=1.14\n",
            "[720 | 1376.81] loss=0.54 avg=1.13\n",
            "[721 | 1378.48] loss=0.99 avg=1.13\n",
            "[722 | 1380.13] loss=0.39 avg=1.12\n",
            "[723 | 1381.80] loss=0.81 avg=1.12\n",
            "[724 | 1383.47] loss=0.65 avg=1.12\n",
            "[725 | 1385.13] loss=1.67 avg=1.12\n",
            "[726 | 1386.80] loss=0.64 avg=1.12\n",
            "[727 | 1388.47] loss=0.40 avg=1.11\n",
            "[728 | 1390.13] loss=2.02 avg=1.12\n",
            "[729 | 1391.79] loss=0.68 avg=1.11\n",
            "[730 | 1393.46] loss=1.22 avg=1.12\n",
            "[731 | 1395.13] loss=0.97 avg=1.11\n",
            "[732 | 1396.79] loss=1.35 avg=1.12\n",
            "[733 | 1398.46] loss=1.27 avg=1.12\n",
            "[734 | 1400.14] loss=0.64 avg=1.11\n",
            "[735 | 1401.81] loss=1.02 avg=1.11\n",
            "[736 | 1403.48] loss=1.34 avg=1.11\n",
            "[737 | 1405.15] loss=0.81 avg=1.11\n",
            "[738 | 1406.81] loss=0.51 avg=1.11\n",
            "[739 | 1408.46] loss=1.15 avg=1.11\n",
            "[740 | 1410.12] loss=0.44 avg=1.10\n",
            "[741 | 1411.78] loss=1.25 avg=1.10\n",
            "[742 | 1413.46] loss=1.02 avg=1.10\n",
            "[743 | 1415.12] loss=1.19 avg=1.10\n",
            "[744 | 1416.78] loss=0.85 avg=1.10\n",
            "[745 | 1418.47] loss=0.60 avg=1.09\n",
            "[746 | 1420.14] loss=1.51 avg=1.10\n",
            "[747 | 1421.81] loss=1.08 avg=1.10\n",
            "[748 | 1423.48] loss=1.21 avg=1.10\n",
            "[749 | 1425.15] loss=1.02 avg=1.10\n",
            "[750 | 1426.82] loss=0.90 avg=1.10\n",
            "[751 | 1428.49] loss=0.49 avg=1.09\n",
            "[752 | 1430.16] loss=1.55 avg=1.09\n",
            "[753 | 1431.83] loss=0.69 avg=1.09\n",
            "[754 | 1433.50] loss=0.97 avg=1.09\n",
            "[755 | 1435.17] loss=1.09 avg=1.09\n",
            "[756 | 1436.84] loss=0.90 avg=1.09\n",
            "[757 | 1438.51] loss=0.58 avg=1.08\n",
            "[758 | 1440.19] loss=0.84 avg=1.08\n",
            "[759 | 1441.87] loss=0.57 avg=1.07\n",
            "[760 | 1443.54] loss=0.41 avg=1.07\n",
            "[761 | 1445.21] loss=1.28 avg=1.07\n",
            "[762 | 1446.89] loss=0.51 avg=1.06\n",
            "[763 | 1448.56] loss=1.00 avg=1.06\n",
            "[764 | 1450.23] loss=0.18 avg=1.06\n",
            "[765 | 1451.91] loss=0.83 avg=1.05\n",
            "[766 | 1453.58] loss=0.23 avg=1.04\n",
            "[767 | 1455.25] loss=1.49 avg=1.05\n",
            "[768 | 1456.93] loss=0.95 avg=1.05\n",
            "[769 | 1458.61] loss=0.67 avg=1.04\n",
            "[770 | 1460.28] loss=1.11 avg=1.04\n",
            "[771 | 1461.95] loss=1.16 avg=1.05\n",
            "[772 | 1463.64] loss=0.59 avg=1.04\n",
            "[773 | 1465.31] loss=0.75 avg=1.04\n",
            "[774 | 1467.00] loss=0.51 avg=1.03\n",
            "[775 | 1468.67] loss=0.52 avg=1.03\n",
            "[776 | 1470.37] loss=0.94 avg=1.03\n",
            "[777 | 1472.05] loss=1.82 avg=1.04\n",
            "[778 | 1473.72] loss=0.41 avg=1.03\n",
            "[779 | 1475.39] loss=0.58 avg=1.02\n",
            "[780 | 1477.08] loss=0.33 avg=1.02\n",
            "[781 | 1478.76] loss=0.60 avg=1.01\n",
            "[782 | 1480.43] loss=0.84 avg=1.01\n",
            "[783 | 1482.10] loss=0.64 avg=1.01\n",
            "[784 | 1483.77] loss=0.77 avg=1.01\n",
            "[785 | 1485.46] loss=0.98 avg=1.01\n",
            "[786 | 1487.13] loss=0.97 avg=1.00\n",
            "[787 | 1488.82] loss=0.73 avg=1.00\n",
            "[788 | 1490.49] loss=0.58 avg=1.00\n",
            "[789 | 1492.18] loss=0.99 avg=1.00\n",
            "[790 | 1493.85] loss=0.58 avg=0.99\n",
            "[791 | 1495.53] loss=2.24 avg=1.01\n",
            "[792 | 1497.21] loss=0.41 avg=1.00\n",
            "[793 | 1498.89] loss=1.41 avg=1.00\n",
            "[794 | 1500.58] loss=0.71 avg=1.00\n",
            "[795 | 1502.25] loss=0.51 avg=1.00\n",
            "[796 | 1503.92] loss=0.27 avg=0.99\n",
            "[797 | 1505.59] loss=0.30 avg=0.98\n",
            "[798 | 1507.27] loss=0.64 avg=0.98\n",
            "[799 | 1508.95] loss=0.29 avg=0.97\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " generating a very large, negative effect. That will be difficult to reverse because we are doing it ourselves by accepting refugees.\n",
            "\n",
            "Meanwhile, a lot of Muslim immigrants are radicalized teenagers radicalized by radicalized Internet speech — which should come as no surprise, given their background.\n",
            "\n",
            "The fact is, we don’t have a choice. The federal government has the authority to accept refugees, and it can do so. The states should follow their own laws and take them.\n",
            "\n",
            "But if the federal government continues acting like a gang of around-the-clock, angry refugees, the American people will no longer accept it. As with all policies, there is consensus on one thing: The American dream is about choice. It is a right.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "We are witnessing the birth of what appears to be a Trumpian nativism. This is strange, because Trump is an out-and-out nationalists who detest globalism and resentment. He detests being misunderstood and outplayed. This is why, as a candidate, he famously declared, “If Mexico is unwilling to pay for the Southern Border Collision Trials to address the Sensational Status of our Country, I will not accept the Presidency, In My Opinion.”\n",
            "\n",
            "So why is this happening here?\n",
            "\n",
            "Well, Trump clearly has no love for the New York Rangers, having taken issue with their perceived participation in the city sports empire’s siloing of the city while also deriding their Mexican owner.\n",
            "\n",
            "He has also taken issue with the NHL for having its logo all over the place ­, a move that has reportedly made it harder for him to sell baseball in Texas.\n",
            "\n",
            "Finally, he has taken umbrage at a national mood that sees sports fans as boorish, loud, irrational and unpatriotic. Trump sees all this as what it is: Official evidence of the media conspiracy against him, an overabundance of which has led to ­accommodating elements on both sides of the Atlantic.\n",
            "\n",
            "All of this has ­confounded and infuriated Trump. In one of the few moments in which he seems to genuinely care whether or not what’s being said is true, the president-elect has now, via Twitter, ordered government agencies to find a way of making sure what he says is taken seriously.\n",
            "\n",
            "The New York Rangers, it seems, fall into the category of bad idea which, under normal circumstances, someone from the federal government trying to execute would be able to direct through sheer will.\n",
            "\n",
            "Perhaps Trump sees them as a way, through which he can get at some of the unsaid unsaid: He thinks that the Texas attorney general made a stupid ­decision not to ­attorney up when she was with the ­Attkisson file, and think about what that says about the way the world works there. Or perhaps he’s simply mad at the NHL for not ­attempting to help.\n",
            "\n",
            "Whatever the reason, the NHL is now taking a beating on the world stage. The NHL has enjoyed a warm welcome in Brazil, Argentina and, most recently, Turkey. This is partly because the NHL perceives itself as an international organization, rather than a U.S.-based soccer league.\n",
            "\n",
            "On the other hand, the NHL’s presence in Turkey came at a terrible political time, as the government cracked down on the ­MİTK network. While it’s understandable that a soccer league would want to avoid a repeat of the 1960s and 1970s, it’s foolish to try to dictate when governments act in the interests of all people.\n",
            "\n",
            "The NHL could have played a role in pushing back against the ideas the case for the membership of Turkey in the NHL was built on. But it didn’t want to, because it didn’t want to be seen as playing politics with pro wrestling.\n",
            "\n",
            "The NHL didn’t want to be seen as pandering to Recep Tayyip Erdoğan. Erdoğan saw the NHL’s funding cut from the 2018-19 season and felt aggrieved, so in November he filed suit in a Istanbul federal court. The case is still moving along, but so is the NHL’s political climate in this country.\n",
            "\n",
            "The NHL didn’t want to be seen as giving succor to Erdoğan, either. The Turkish government saw the NHL losing money and believed the league was somehow working with the Turkish government in an effort to undercut the government.\n",
            "\n",
            "The NHL didn’t want to be seen as appeasing Erdoğan, either. The Turkish government saw the NHL losing money and was disappointed that the NHL was willing to entertain anti-government ideas. It was eager for the NHL to spend more to support its players and facilities and youth hockey programs. The NHL didn’t spend enough on players.\n",
            "\n",
            "The NHL didn’t want to be seen as placating the Turkish government, either.\n",
            "\n",
            "[800 | 1533.75] loss=0.70 avg=0.97\n",
            "[801 | 1535.42] loss=0.40 avg=0.96\n",
            "[802 | 1537.07] loss=0.69 avg=0.96\n",
            "[803 | 1538.74] loss=0.64 avg=0.96\n",
            "[804 | 1540.41] loss=1.55 avg=0.96\n",
            "[805 | 1542.08] loss=1.49 avg=0.97\n",
            "[806 | 1543.76] loss=0.77 avg=0.97\n",
            "[807 | 1545.42] loss=0.71 avg=0.96\n",
            "[808 | 1547.09] loss=0.44 avg=0.96\n",
            "[809 | 1548.76] loss=1.25 avg=0.96\n",
            "[810 | 1550.43] loss=0.60 avg=0.96\n",
            "[811 | 1552.10] loss=0.24 avg=0.95\n",
            "[812 | 1553.76] loss=0.67 avg=0.95\n",
            "[813 | 1555.42] loss=0.94 avg=0.95\n",
            "[814 | 1557.09] loss=0.61 avg=0.94\n",
            "[815 | 1558.75] loss=0.74 avg=0.94\n",
            "[816 | 1560.41] loss=1.02 avg=0.94\n",
            "[817 | 1562.06] loss=0.68 avg=0.94\n",
            "[818 | 1563.71] loss=0.43 avg=0.94\n",
            "[819 | 1565.38] loss=0.58 avg=0.93\n",
            "[820 | 1567.05] loss=0.58 avg=0.93\n",
            "[821 | 1568.71] loss=0.49 avg=0.92\n",
            "[822 | 1570.38] loss=0.41 avg=0.92\n",
            "[823 | 1572.04] loss=0.17 avg=0.91\n",
            "[824 | 1573.71] loss=0.40 avg=0.91\n",
            "[825 | 1575.39] loss=0.69 avg=0.90\n",
            "[826 | 1577.06] loss=0.86 avg=0.90\n",
            "[827 | 1578.74] loss=0.46 avg=0.90\n",
            "[828 | 1580.42] loss=0.46 avg=0.90\n",
            "[829 | 1582.08] loss=0.71 avg=0.89\n",
            "[830 | 1583.76] loss=1.22 avg=0.90\n",
            "[831 | 1585.45] loss=0.89 avg=0.90\n",
            "[832 | 1587.12] loss=0.35 avg=0.89\n",
            "[833 | 1588.79] loss=0.72 avg=0.89\n",
            "[834 | 1590.46] loss=0.85 avg=0.89\n",
            "[835 | 1592.14] loss=0.50 avg=0.89\n",
            "[836 | 1593.82] loss=0.20 avg=0.88\n",
            "[837 | 1595.51] loss=0.16 avg=0.87\n",
            "[838 | 1597.19] loss=0.81 avg=0.87\n",
            "[839 | 1598.87] loss=0.40 avg=0.87\n",
            "[840 | 1600.55] loss=0.27 avg=0.86\n",
            "[841 | 1602.24] loss=0.24 avg=0.85\n",
            "[842 | 1603.92] loss=0.50 avg=0.85\n",
            "[843 | 1605.59] loss=0.29 avg=0.84\n",
            "[844 | 1607.27] loss=0.70 avg=0.84\n",
            "[845 | 1608.95] loss=0.76 avg=0.84\n",
            "[846 | 1610.63] loss=0.54 avg=0.84\n",
            "[847 | 1612.31] loss=0.55 avg=0.84\n",
            "[848 | 1613.99] loss=0.26 avg=0.83\n",
            "[849 | 1615.67] loss=2.09 avg=0.84\n",
            "[850 | 1617.34] loss=0.21 avg=0.84\n",
            "[851 | 1619.00] loss=1.06 avg=0.84\n",
            "[852 | 1620.68] loss=1.01 avg=0.84\n",
            "[853 | 1622.37] loss=0.53 avg=0.84\n",
            "[854 | 1624.04] loss=1.02 avg=0.84\n",
            "[855 | 1625.71] loss=0.74 avg=0.84\n",
            "[856 | 1627.38] loss=0.29 avg=0.83\n",
            "[857 | 1629.05] loss=0.37 avg=0.83\n",
            "[858 | 1630.72] loss=0.53 avg=0.83\n",
            "[859 | 1632.40] loss=1.24 avg=0.83\n",
            "[860 | 1634.08] loss=0.92 avg=0.83\n",
            "[861 | 1635.75] loss=0.35 avg=0.83\n",
            "[862 | 1637.42] loss=1.10 avg=0.83\n",
            "[863 | 1639.09] loss=0.39 avg=0.82\n",
            "[864 | 1640.76] loss=0.85 avg=0.82\n",
            "[865 | 1642.43] loss=0.47 avg=0.82\n",
            "[866 | 1644.10] loss=0.31 avg=0.82\n",
            "[867 | 1645.77] loss=0.72 avg=0.81\n",
            "[868 | 1647.44] loss=0.25 avg=0.81\n",
            "[869 | 1649.12] loss=1.10 avg=0.81\n",
            "[870 | 1650.80] loss=0.48 avg=0.81\n",
            "[871 | 1652.48] loss=0.74 avg=0.81\n",
            "[872 | 1654.15] loss=0.37 avg=0.80\n",
            "[873 | 1655.83] loss=0.29 avg=0.80\n",
            "[874 | 1657.50] loss=0.33 avg=0.79\n",
            "[875 | 1659.18] loss=0.30 avg=0.79\n",
            "[876 | 1660.83] loss=0.99 avg=0.79\n",
            "[877 | 1662.51] loss=1.23 avg=0.80\n",
            "[878 | 1664.18] loss=0.49 avg=0.79\n",
            "[879 | 1665.86] loss=0.40 avg=0.79\n",
            "[880 | 1667.53] loss=0.47 avg=0.79\n",
            "[881 | 1669.20] loss=0.72 avg=0.78\n",
            "[882 | 1670.87] loss=0.66 avg=0.78\n",
            "[883 | 1672.54] loss=0.38 avg=0.78\n",
            "[884 | 1674.21] loss=0.19 avg=0.77\n",
            "[885 | 1675.88] loss=0.32 avg=0.77\n",
            "[886 | 1677.55] loss=0.34 avg=0.76\n",
            "[887 | 1679.22] loss=0.38 avg=0.76\n",
            "[888 | 1680.88] loss=0.67 avg=0.76\n",
            "[889 | 1682.55] loss=0.64 avg=0.76\n",
            "[890 | 1684.21] loss=0.74 avg=0.76\n",
            "[891 | 1685.87] loss=0.62 avg=0.76\n",
            "[892 | 1687.54] loss=1.22 avg=0.76\n",
            "[893 | 1689.21] loss=0.29 avg=0.76\n",
            "[894 | 1690.88] loss=0.67 avg=0.76\n",
            "[895 | 1692.55] loss=0.47 avg=0.75\n",
            "[896 | 1694.22] loss=0.32 avg=0.75\n",
            "[897 | 1695.89] loss=0.60 avg=0.75\n",
            "[898 | 1697.56] loss=0.64 avg=0.75\n",
            "[899 | 1699.23] loss=0.55 avg=0.74\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " never just a matter of going the distance, though—they need to be ready for a long, hard fight. To win, a boxer must be willing to fight with almost any wound, without qualification.\n",
            "\n",
            "This essay appears in the August 2014 issue of SPORTS ILLUSTRATED<|endoftext|>The US government has approved the first shipments of two new Hellfire missiles to Ukraine, bringing to 36 the number of Russian-made missiles under strict international missile-control obligations.\n",
            "\n",
            "The approval came on Tuesday in a presidential directive signed by President Barack Obama to provide Kiev with 72 missiles in response to Russia's seizure of Ukrainian navy vessels in the Black Sea.\n",
            "\n",
            "Get our daily newsletter Upgrade your inbox and get our Daily Dispatch and Editor's Picks.\n",
            "\n",
            "\n",
            "Russia responded with 279 cruise missiles it had seized from the Ukrainian navy. By contrast, America has 2,726 cruise missiles in its inventory, and so has plenty of retaliation capacity for any future attack.\n",
            "\n",
            "The new anti-missiles come just a day after Russia annexed the Crimean peninsula, which it then held up as a bridgehead for an invasion. Washington fears the move opens the country up to an all-out war with Russia.\n",
            "\n",
            "The missile deliveries also come after a warm and helpful meeting between President Vladimir Putin of Russia and President Barack Obama of America last week. The leaders agreed to expand their partnership on the fight against terrorism, particularly in the fight against the Islamic State group. Mr Putin agreed to lift sanctions aimed at punishing it for the invasion and occupation of neighbouring Syria and Iraq.\n",
            "\n",
            "The Kremlin has since taken a tougher line with Iran, tightening sanctions there and cutting off key supply routes for Jihadist militants into Syria. This is partly because it fears if Tehran is pushed too far towards a bomb it could provoke a wider war and again cut off a supply line to the so-called caliph state of Pakistan.\n",
            "\n",
            "All that said, Washington believes it has a winning hand here. Mr Putin has rejected American proposals to ease the sanctions and seems to be willing to give way on Pakistan and Iran. Mr Obama seems to be willing to give way on Russia, which might work out well for both of them.\n",
            "\n",
            "The new anti-missiles will provide much-needed cover against a potential Russian-Iranian nuclear war. But the real prize may be the lifting of the crippling sanctions banning exports of Russian arms. That would provide the United States with a bulwark against an Iran determined only to make nuclear weapons.\n",
            "\n",
            "Such sanctions are harming American industry and threatening the economy. The new sanctions will make it easier for American firms to do business with Russia, giving American firms a leg up over our enemies and opening the way for much-needed American jobs here at home.\n",
            "\n",
            "\n",
            "\n",
            "Such sanctions have also been a stumbling block in the path of other states looking to join the nuclear club—Japan in the mid-2000s, South Korea in the late-2000s, other non-nuclear-armed Asian nations since the fall of the Soviet Union. The lifting of sanctions on Russia would lead to a sea change in how states approach nuclear policy. Nations that don’t want their partners to acquire a nuclear weapon will feel pressured to do so, and countries that cheat will feel punished.\n",
            "\n",
            "The lifting of sanctions on Russia would also lead to other positive changes in American foreign policy. For one thing, the United States would be free to pursue its own national interests at home while seeking cooperation with our friends and foes alike. That includes ensuring that our allies and foes have what we consider essential partnerships, regardless of their national character.\n",
            "\n",
            "And the lifting of sanctions on Russia would lead to a corresponding rise in American exports. Our exports to the former Cold War foes would boomerang back, raising our prices and dampening our economic growth.\n",
            "\n",
            "Such an accord would also lead to greater cooperation on counterterrorism and common ground with the African Union, the Central American countries rattled by the new jihad, the Indo-Pacific region prone to greater American influence, and with Israel, whose security is at stake.\n",
            "\n",
            "Finally, the lifting of sanctions would lead to a real effort on the part of the Kremlin to reduce the number of deployed anti-aircraft guns in Russia. That would lead to a reduction in the number of anti-aircraft guns in the air, leading to less danger to passenger aircraft and to a reduction in the number of people killed when Russian fighters take to the skies over Ukraine.\n",
            "\n",
            "Given these prospects, the American people deserve no less than full implementation of the agreement after seven years. That is why I have directed my administration to work constructively with President Putin as we pursue common but separate goals, and I look forward to announcing further steps at the United Nations in the days ahead.\n",
            "\n",
            "\n",
            "\n",
            "Of course, he would do so, rightly, as well. But the American people don’t get to choose the course Putin takes (or doesn’t take). In his speech, he simply gave the Russians what they wanted: an opening to press their\n",
            "\n",
            "[900 | 1723.95] loss=0.34 avg=0.74\n",
            "[901 | 1725.62] loss=0.20 avg=0.73\n",
            "[902 | 1727.29] loss=0.93 avg=0.74\n",
            "[903 | 1728.95] loss=0.27 avg=0.73\n",
            "[904 | 1730.62] loss=0.32 avg=0.73\n",
            "[905 | 1732.28] loss=0.61 avg=0.73\n",
            "[906 | 1733.95] loss=0.58 avg=0.73\n",
            "[907 | 1735.62] loss=1.02 avg=0.73\n",
            "[908 | 1737.28] loss=0.30 avg=0.72\n",
            "[909 | 1738.93] loss=0.50 avg=0.72\n",
            "[910 | 1740.58] loss=0.63 avg=0.72\n",
            "[911 | 1742.25] loss=1.19 avg=0.73\n",
            "[912 | 1743.90] loss=0.77 avg=0.73\n",
            "[913 | 1745.56] loss=0.86 avg=0.73\n",
            "[914 | 1747.23] loss=0.35 avg=0.72\n",
            "[915 | 1748.89] loss=0.27 avg=0.72\n",
            "[916 | 1750.54] loss=0.35 avg=0.72\n",
            "[917 | 1752.21] loss=0.37 avg=0.71\n",
            "[918 | 1753.86] loss=0.24 avg=0.71\n",
            "[919 | 1755.52] loss=0.30 avg=0.70\n",
            "[920 | 1757.18] loss=0.59 avg=0.70\n",
            "[921 | 1758.85] loss=0.16 avg=0.70\n",
            "[922 | 1760.52] loss=0.38 avg=0.69\n",
            "[923 | 1762.18] loss=0.22 avg=0.69\n",
            "[924 | 1763.86] loss=1.09 avg=0.69\n",
            "[925 | 1765.54] loss=0.30 avg=0.69\n",
            "[926 | 1767.21] loss=0.29 avg=0.68\n",
            "[927 | 1768.88] loss=0.24 avg=0.68\n",
            "[928 | 1770.55] loss=0.36 avg=0.68\n",
            "[929 | 1772.22] loss=0.24 avg=0.67\n",
            "[930 | 1773.89] loss=0.32 avg=0.67\n",
            "[931 | 1775.56] loss=0.65 avg=0.67\n",
            "[932 | 1777.23] loss=0.72 avg=0.67\n",
            "[933 | 1778.90] loss=0.39 avg=0.67\n",
            "[934 | 1780.59] loss=0.44 avg=0.66\n",
            "[935 | 1782.27] loss=0.30 avg=0.66\n",
            "[936 | 1783.95] loss=0.84 avg=0.66\n",
            "[937 | 1785.64] loss=0.66 avg=0.66\n",
            "[938 | 1787.31] loss=0.09 avg=0.66\n",
            "[939 | 1789.01] loss=0.21 avg=0.65\n",
            "[940 | 1790.69] loss=0.60 avg=0.65\n",
            "[941 | 1792.38] loss=0.96 avg=0.65\n",
            "[942 | 1794.05] loss=0.22 avg=0.65\n",
            "[943 | 1795.72] loss=0.16 avg=0.65\n",
            "[944 | 1797.40] loss=0.34 avg=0.64\n",
            "[945 | 1799.07] loss=0.71 avg=0.64\n",
            "[946 | 1800.75] loss=0.42 avg=0.64\n",
            "[947 | 1802.42] loss=0.40 avg=0.64\n",
            "[948 | 1804.09] loss=0.47 avg=0.64\n",
            "[949 | 1805.78] loss=0.39 avg=0.63\n",
            "[950 | 1807.45] loss=0.68 avg=0.63\n",
            "[951 | 1809.12] loss=0.57 avg=0.63\n",
            "[952 | 1810.80] loss=0.30 avg=0.63\n",
            "[953 | 1812.48] loss=0.34 avg=0.63\n",
            "[954 | 1814.14] loss=0.40 avg=0.63\n",
            "[955 | 1815.82] loss=0.48 avg=0.62\n",
            "[956 | 1817.48] loss=0.66 avg=0.62\n",
            "[957 | 1819.15] loss=0.33 avg=0.62\n",
            "[958 | 1820.82] loss=0.42 avg=0.62\n",
            "[959 | 1822.49] loss=0.58 avg=0.62\n",
            "[960 | 1824.16] loss=0.65 avg=0.62\n",
            "[961 | 1825.83] loss=0.23 avg=0.62\n",
            "[962 | 1827.50] loss=0.63 avg=0.62\n",
            "[963 | 1829.17] loss=0.48 avg=0.61\n",
            "[964 | 1830.84] loss=0.73 avg=0.62\n",
            "[965 | 1832.51] loss=0.15 avg=0.61\n",
            "[966 | 1834.18] loss=0.29 avg=0.61\n",
            "[967 | 1835.86] loss=0.47 avg=0.61\n",
            "[968 | 1837.52] loss=0.12 avg=0.60\n",
            "[969 | 1839.20] loss=0.61 avg=0.60\n",
            "[970 | 1840.86] loss=0.25 avg=0.60\n",
            "[971 | 1842.53] loss=0.62 avg=0.60\n",
            "[972 | 1844.19] loss=0.60 avg=0.60\n",
            "[973 | 1845.87] loss=0.14 avg=0.59\n",
            "[974 | 1847.54] loss=0.39 avg=0.59\n",
            "[975 | 1849.22] loss=0.52 avg=0.59\n",
            "[976 | 1850.89] loss=0.51 avg=0.59\n",
            "[977 | 1852.56] loss=0.31 avg=0.59\n",
            "[978 | 1854.23] loss=0.55 avg=0.59\n",
            "[979 | 1855.90] loss=0.74 avg=0.59\n",
            "[980 | 1857.58] loss=0.46 avg=0.59\n",
            "[981 | 1859.25] loss=0.26 avg=0.58\n",
            "[982 | 1860.92] loss=0.32 avg=0.58\n",
            "[983 | 1862.59] loss=0.35 avg=0.58\n",
            "[984 | 1864.26] loss=0.35 avg=0.58\n",
            "[985 | 1865.93] loss=0.79 avg=0.58\n",
            "[986 | 1867.60] loss=1.77 avg=0.59\n",
            "[987 | 1869.27] loss=0.38 avg=0.59\n",
            "[988 | 1870.94] loss=0.50 avg=0.59\n",
            "[989 | 1872.61] loss=0.48 avg=0.59\n",
            "[990 | 1874.28] loss=0.63 avg=0.59\n",
            "[991 | 1875.95] loss=0.51 avg=0.59\n",
            "[992 | 1877.62] loss=0.37 avg=0.58\n",
            "[993 | 1879.29] loss=0.18 avg=0.58\n",
            "[994 | 1880.95] loss=0.42 avg=0.58\n",
            "[995 | 1882.62] loss=0.19 avg=0.57\n",
            "[996 | 1884.28] loss=0.48 avg=0.57\n",
            "[997 | 1885.96] loss=0.89 avg=0.58\n",
            "[998 | 1887.63] loss=1.23 avg=0.58\n",
            "[999 | 1889.30] loss=0.36 avg=0.58\n",
            "Ran 1010 steps.\n",
            "Saving checkpoint/all_nypsot_345/model-1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDxmeWIewzvH",
        "colab_type": "code",
        "outputId": "61686e8e-a75b-48c6-ee3d-217af64ff5b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "## short story training - with 345\n",
        "!PYTHONPATH=src ./train.py --dataset /content/all_short_stories.txt --run_name 'all_short_stories_345' --model_name '345M'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0628 01:48:52.913891 139766784378752 deprecation_wrapper.py:119] From /content/gpt-2/model.py:147: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
            "\n",
            "W0628 01:48:52.936126 139766784378752 deprecation_wrapper.py:119] From /content/gpt-2/src/memory_saving_gradients.py:13: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n",
            "W0628 01:48:53.018906 139766784378752 deprecation_wrapper.py:119] From ./train.py:88: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "W0628 01:48:53.019173 139766784378752 deprecation_wrapper.py:119] From ./train.py:91: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "2019-06-28 01:48:53.129439: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-06-28 01:48:53.133149: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2a65100 executing computations on platform Host. Devices:\n",
            "2019-06-28 01:48:53.133236: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2019-06-28 01:48:53.157287: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\n",
            "2019-06-28 01:48:53.347230: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-28 01:48:53.347836: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2a64840 executing computations on platform CUDA. Devices:\n",
            "2019-06-28 01:48:53.347870: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2019-06-28 01:48:53.348233: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-28 01:48:53.348608: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-06-28 01:48:53.359252: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-06-28 01:48:53.529146: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-06-28 01:48:53.610621: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-06-28 01:48:53.632518: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-06-28 01:48:53.826871: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-06-28 01:48:53.933258: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-06-28 01:48:54.295609: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-06-28 01:48:54.295904: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-28 01:48:54.296435: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-28 01:48:54.296763: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
            "2019-06-28 01:48:54.300533: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-06-28 01:48:54.301714: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-06-28 01:48:54.301764: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n",
            "2019-06-28 01:48:54.301794: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n",
            "2019-06-28 01:48:54.308137: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-28 01:48:54.310991: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-28 01:48:54.313140: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14202 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "W0628 01:48:54.316301 139766784378752 deprecation_wrapper.py:119] From ./train.py:92: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0628 01:49:04.923466 139766784378752 deprecation.py:323] From /content/gpt-2/sample.py:65: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "W0628 01:49:04.937444 139766784378752 deprecation.py:323] From /content/gpt-2/sample.py:16: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0628 01:49:04.939330 139766784378752 deprecation.py:323] From /content/gpt-2/sample.py:70: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.random.categorical` instead.\n",
            "W0628 01:49:04.949174 139766784378752 deprecation_wrapper.py:119] From ./train.py:121: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "W0628 01:49:20.288250 139766784378752 deprecation_wrapper.py:119] From ./train.py:144: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n",
            "W0628 01:49:20.291301 139766784378752 deprecation_wrapper.py:119] From ./train.py:147: The name tf.summary.merge is deprecated. Please use tf.compat.v1.summary.merge instead.\n",
            "\n",
            "W0628 01:49:20.292139 139766784378752 deprecation_wrapper.py:119] From ./train.py:149: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
            "\n",
            "W0628 01:49:20.292846 139766784378752 deprecation_wrapper.py:119] From ./train.py:152: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "Loading checkpoint models/345M/model.ckpt\n",
            "W0628 01:49:33.937984 139766784378752 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "Loading dataset...\n",
            "100% 1/1 [00:03<00:00,  3.64s/it]\n",
            "dataset has 531349 tokens\n",
            "Training...\n",
            "2019-06-28 01:49:56.426909: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n",
            "2019-06-28 01:49:57.074251: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "[1 | 12.23] loss=3.01 avg=3.01\n",
            "[2 | 13.67] loss=3.39 avg=3.20\n",
            "[3 | 15.13] loss=3.67 avg=3.36\n",
            "[4 | 16.58] loss=2.92 avg=3.25\n",
            "[5 | 18.03] loss=3.18 avg=3.24\n",
            "[6 | 19.49] loss=3.69 avg=3.31\n",
            "[7 | 20.94] loss=3.02 avg=3.27\n",
            "[8 | 22.39] loss=3.37 avg=3.28\n",
            "[9 | 23.85] loss=3.52 avg=3.31\n",
            "[10 | 25.31] loss=2.90 avg=3.27\n",
            "[11 | 26.78] loss=2.79 avg=3.22\n",
            "[12 | 28.24] loss=3.19 avg=3.22\n",
            "[13 | 29.70] loss=3.37 avg=3.23\n",
            "[14 | 31.15] loss=3.44 avg=3.25\n",
            "[15 | 32.62] loss=3.61 avg=3.27\n",
            "[16 | 34.08] loss=3.21 avg=3.27\n",
            "[17 | 35.54] loss=3.75 avg=3.30\n",
            "[18 | 37.01] loss=3.20 avg=3.29\n",
            "[19 | 38.48] loss=3.65 avg=3.31\n",
            "[20 | 39.94] loss=3.37 avg=3.32\n",
            "[21 | 41.41] loss=3.95 avg=3.35\n",
            "[22 | 42.87] loss=3.63 avg=3.36\n",
            "[23 | 44.34] loss=2.79 avg=3.34\n",
            "[24 | 45.81] loss=3.34 avg=3.34\n",
            "[25 | 47.27] loss=3.37 avg=3.34\n",
            "[26 | 48.74] loss=2.73 avg=3.31\n",
            "[27 | 50.20] loss=3.36 avg=3.31\n",
            "[28 | 51.67] loss=2.87 avg=3.30\n",
            "[29 | 53.14] loss=3.29 avg=3.30\n",
            "[30 | 54.60] loss=3.35 avg=3.30\n",
            "[31 | 56.07] loss=3.31 avg=3.30\n",
            "[32 | 57.54] loss=3.22 avg=3.30\n",
            "[33 | 59.01] loss=3.07 avg=3.29\n",
            "[34 | 60.48] loss=3.18 avg=3.28\n",
            "[35 | 61.95] loss=2.92 avg=3.27\n",
            "[36 | 63.42] loss=3.04 avg=3.26\n",
            "[37 | 64.88] loss=3.29 avg=3.26\n",
            "[38 | 66.35] loss=3.63 avg=3.28\n",
            "[39 | 67.82] loss=3.00 avg=3.27\n",
            "[40 | 69.30] loss=3.29 avg=3.27\n",
            "[41 | 70.78] loss=3.05 avg=3.26\n",
            "[42 | 72.25] loss=2.19 avg=3.23\n",
            "[43 | 73.73] loss=3.10 avg=3.23\n",
            "[44 | 75.19] loss=3.28 avg=3.23\n",
            "[45 | 76.66] loss=3.39 avg=3.23\n",
            "[46 | 78.13] loss=3.29 avg=3.23\n",
            "[47 | 79.61] loss=2.79 avg=3.22\n",
            "[48 | 81.08] loss=2.73 avg=3.21\n",
            "[49 | 82.55] loss=2.75 avg=3.20\n",
            "[50 | 84.02] loss=3.31 avg=3.20\n",
            "[51 | 85.49] loss=3.12 avg=3.20\n",
            "[52 | 86.96] loss=2.82 avg=3.19\n",
            "[53 | 88.43] loss=3.94 avg=3.21\n",
            "[54 | 89.91] loss=3.13 avg=3.21\n",
            "[55 | 91.37] loss=3.18 avg=3.20\n",
            "[56 | 92.84] loss=3.68 avg=3.22\n",
            "[57 | 94.31] loss=3.04 avg=3.21\n",
            "[58 | 95.78] loss=3.11 avg=3.21\n",
            "[59 | 97.25] loss=2.95 avg=3.20\n",
            "[60 | 98.73] loss=3.37 avg=3.21\n",
            "[61 | 100.20] loss=3.09 avg=3.20\n",
            "[62 | 101.67] loss=3.23 avg=3.21\n",
            "[63 | 103.14] loss=3.13 avg=3.20\n",
            "[64 | 104.61] loss=3.52 avg=3.21\n",
            "[65 | 106.08] loss=2.83 avg=3.20\n",
            "[66 | 107.55] loss=3.33 avg=3.20\n",
            "[67 | 109.03] loss=3.27 avg=3.21\n",
            "[68 | 110.50] loss=2.91 avg=3.20\n",
            "[69 | 111.97] loss=3.06 avg=3.20\n",
            "[70 | 113.43] loss=3.44 avg=3.20\n",
            "[71 | 114.91] loss=3.01 avg=3.20\n",
            "[72 | 116.38] loss=2.84 avg=3.19\n",
            "[73 | 117.85] loss=3.38 avg=3.20\n",
            "[74 | 119.32] loss=3.38 avg=3.20\n",
            "[75 | 120.79] loss=3.41 avg=3.20\n",
            "[76 | 122.28] loss=3.70 avg=3.21\n",
            "[77 | 123.75] loss=3.53 avg=3.22\n",
            "[78 | 125.22] loss=3.30 avg=3.22\n",
            "[79 | 126.69] loss=3.64 avg=3.23\n",
            "[80 | 128.16] loss=3.17 avg=3.23\n",
            "[81 | 129.64] loss=3.10 avg=3.22\n",
            "[82 | 131.11] loss=3.52 avg=3.23\n",
            "[83 | 132.58] loss=3.15 avg=3.23\n",
            "[84 | 134.05] loss=3.07 avg=3.23\n",
            "[85 | 135.52] loss=2.75 avg=3.22\n",
            "[86 | 136.99] loss=3.11 avg=3.22\n",
            "[87 | 138.47] loss=3.14 avg=3.21\n",
            "[88 | 139.94] loss=3.37 avg=3.22\n",
            "[89 | 141.42] loss=3.35 avg=3.22\n",
            "[90 | 142.90] loss=3.29 avg=3.22\n",
            "[91 | 144.37] loss=3.44 avg=3.22\n",
            "[92 | 145.85] loss=2.87 avg=3.22\n",
            "[93 | 147.33] loss=3.33 avg=3.22\n",
            "[94 | 148.81] loss=3.25 avg=3.22\n",
            "[95 | 150.28] loss=3.09 avg=3.22\n",
            "[96 | 151.75] loss=3.16 avg=3.22\n",
            "[97 | 153.22] loss=3.32 avg=3.22\n",
            "[98 | 154.70] loss=2.88 avg=3.21\n",
            "[99 | 156.17] loss=3.67 avg=3.22\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " but that she would be on her way!\n",
            "\n",
            "She did not have to wait long; her escort was just about the turn of the corner of the alley where the old shop was now. After making his way back and forth between the two little houses where the old shop stood on its own side, he arrived at his destination where he expected to find his destination. For this reason, he had not intended to take the street with his escort. The streets on which he would pass were narrow and dark, and on the third floor of the old shop no one would pass without being noticed.\n",
            "\n",
            "On returning to the street, he passed with ease from street to street. Now and then a light would be thrown upon the street through which he passed, and then the crowd of people who formed it would descend into the streets. On the streets were the small houses which were erected to house the poor. At night these houses would fall into disrepair, and it was not too much to hope that after a few hours of this a man could find a dwelling on the street on which he had spent the night. On the other hand, if he chose the road which led to the market he would make his exit in a hurry, when the crowds would come, and then he could spend the night.\n",
            "\n",
            "With that thought in mind, he made his way to the corner, and at the same time his companion, who was still keeping up with him, came up and offered him a seat. They had to sit facing each other on the threshold of the little shop. This was a simple and comfortable place; it was also quite deserted. The old fellow had no money, so it was better than nothing.\n",
            "\n",
            "The young man was no longer of the age where he could be of some service to the common purse. After all, he was only an Englishman, and with his old English money he was of no use to his country, or the poor. He had just reached the age when he was almost to a person of importance in the world, so far as the world was concerned; and this he supposed would be long ago. He was of the country and the old man, and had a way of seeing and perceiving things which the Englishman in the street could not see and perceive!\n",
            "\n",
            "He was also of the country; for that man was quite old. He had been a very great farmer. He was also of the country, for, after all, the land and its people have a way of finding and perceiving that old men are of long age. He had also long been acquainted with the old storehouse in the market-place. The owner had been a stranger to him, and now that he was with his master there lay a great deal of trouble for the Englishman to find the owner. The old shop-keeper was a stranger even to him, and, after all, a stranger was not always a household man, because he had his own business to undertake. And if the shop-keeper had gone about to help himself, would he be still in charge?\n",
            "\n",
            "The two of them had a good time, and then, after some time, they went to sit in their seats, but the doors to the shop were not kept at all! And one was still sitting there, and the other was sitting in one of the two little windows. The one was quite cold, and with its door broken away she thought that perhaps someone had left something in the storeroom. She wished to be sure that there was nothing in it.\n",
            "\n",
            "When both were sitting there, a servant came in and asked in a very low voice, \"Is there anything in it?\" She who was sitting beside him, was very silent. But then the servant, with his own voice, said, \"Hark, there, here!\" And he brought out a bottle of cold brandy. It was quite cold, and the only thing in it was a single piece of wood which was wrapped about it. The little windows, however, were open, and when she saw the empty bottle she wished to see what had been poured in.\n",
            "\n",
            "She saw a very light blue-green liquid poured out. She was quite sure that it was champagne--not real champagne, but the liquid that she had seen in the old bottles. She put on a few glasses and, after tasting it, her countenance became very cheerful and the door opened. But, because it was so late in the evening, there was no one in the shop, and there was no one in the street to look at the new bottles. So, on finding himself in the dark, he took a seat on the threshold of the old shop and sat there till the clock struck four.\n",
            "\n",
            "The street had been long closed down, but there was one alley which was open, and no one could pass through it--that's why the two gentlemen at the corners had not gone in. As for the two strangers on the other side, they stood in silence, but there\n",
            "\n",
            "[100 | 182.18] loss=3.09 avg=3.22\n",
            "[101 | 183.67] loss=3.11 avg=3.22\n",
            "[102 | 185.15] loss=3.29 avg=3.22\n",
            "[103 | 186.63] loss=3.48 avg=3.22\n",
            "[104 | 188.10] loss=3.31 avg=3.22\n",
            "[105 | 189.58] loss=2.81 avg=3.22\n",
            "[106 | 191.06] loss=3.32 avg=3.22\n",
            "[107 | 192.54] loss=2.87 avg=3.21\n",
            "[108 | 194.02] loss=3.11 avg=3.21\n",
            "[109 | 195.49] loss=3.04 avg=3.21\n",
            "[110 | 196.97] loss=3.58 avg=3.21\n",
            "[111 | 198.45] loss=2.67 avg=3.21\n",
            "[112 | 199.93] loss=2.99 avg=3.20\n",
            "[113 | 201.41] loss=3.14 avg=3.20\n",
            "[114 | 202.89] loss=3.27 avg=3.20\n",
            "[115 | 204.36] loss=3.15 avg=3.20\n",
            "[116 | 205.84] loss=3.34 avg=3.20\n",
            "[117 | 207.33] loss=3.32 avg=3.21\n",
            "[118 | 208.80] loss=2.94 avg=3.20\n",
            "[119 | 210.28] loss=3.04 avg=3.20\n",
            "[120 | 211.77] loss=2.98 avg=3.20\n",
            "[121 | 213.24] loss=3.48 avg=3.20\n",
            "[122 | 214.72] loss=3.76 avg=3.21\n",
            "[123 | 216.20] loss=3.10 avg=3.21\n",
            "[124 | 217.68] loss=3.13 avg=3.21\n",
            "[125 | 219.16] loss=2.91 avg=3.20\n",
            "[126 | 220.64] loss=3.39 avg=3.20\n",
            "[127 | 222.13] loss=3.28 avg=3.21\n",
            "[128 | 223.61] loss=2.99 avg=3.20\n",
            "[129 | 225.08] loss=3.52 avg=3.21\n",
            "[130 | 226.56] loss=2.73 avg=3.20\n",
            "[131 | 228.03] loss=2.69 avg=3.19\n",
            "[132 | 229.51] loss=2.78 avg=3.19\n",
            "[133 | 230.99] loss=3.11 avg=3.19\n",
            "[134 | 232.48] loss=2.95 avg=3.18\n",
            "[135 | 233.95] loss=2.49 avg=3.17\n",
            "[136 | 235.43] loss=3.24 avg=3.18\n",
            "[137 | 236.92] loss=3.54 avg=3.18\n",
            "[138 | 238.40] loss=2.79 avg=3.17\n",
            "[139 | 239.88] loss=3.38 avg=3.18\n",
            "[140 | 241.36] loss=2.87 avg=3.17\n",
            "[141 | 242.84] loss=3.22 avg=3.17\n",
            "[142 | 244.32] loss=2.62 avg=3.17\n",
            "[143 | 245.80] loss=3.14 avg=3.17\n",
            "[144 | 247.29] loss=3.24 avg=3.17\n",
            "[145 | 248.76] loss=3.15 avg=3.17\n",
            "[146 | 250.25] loss=2.96 avg=3.16\n",
            "[147 | 251.73] loss=3.36 avg=3.17\n",
            "[148 | 253.20] loss=3.12 avg=3.17\n",
            "[149 | 254.68] loss=2.55 avg=3.16\n",
            "[150 | 256.16] loss=3.32 avg=3.16\n",
            "[151 | 257.64] loss=3.24 avg=3.16\n",
            "[152 | 259.12] loss=2.95 avg=3.16\n",
            "[153 | 260.60] loss=3.01 avg=3.16\n",
            "[154 | 262.08] loss=3.07 avg=3.16\n",
            "[155 | 263.56] loss=3.38 avg=3.16\n",
            "[156 | 265.04] loss=3.24 avg=3.16\n",
            "[157 | 266.52] loss=3.31 avg=3.16\n",
            "[158 | 267.99] loss=2.86 avg=3.16\n",
            "[159 | 269.47] loss=3.10 avg=3.16\n",
            "[160 | 270.96] loss=3.09 avg=3.16\n",
            "[161 | 272.44] loss=3.32 avg=3.16\n",
            "[162 | 273.91] loss=2.90 avg=3.16\n",
            "[163 | 275.40] loss=3.03 avg=3.15\n",
            "[164 | 276.87] loss=3.23 avg=3.15\n",
            "[165 | 278.36] loss=2.88 avg=3.15\n",
            "[166 | 279.83] loss=3.15 avg=3.15\n",
            "[167 | 281.31] loss=2.86 avg=3.15\n",
            "[168 | 282.79] loss=2.90 avg=3.14\n",
            "[169 | 284.28] loss=3.18 avg=3.14\n",
            "[170 | 285.76] loss=2.94 avg=3.14\n",
            "[171 | 287.25] loss=2.89 avg=3.14\n",
            "[172 | 288.73] loss=2.68 avg=3.13\n",
            "[173 | 290.21] loss=3.08 avg=3.13\n",
            "[174 | 291.69] loss=2.56 avg=3.13\n",
            "[175 | 293.17] loss=3.09 avg=3.13\n",
            "[176 | 294.65] loss=3.07 avg=3.13\n",
            "[177 | 296.13] loss=3.28 avg=3.13\n",
            "[178 | 297.61] loss=3.20 avg=3.13\n",
            "[179 | 299.09] loss=2.89 avg=3.13\n",
            "[180 | 300.57] loss=3.18 avg=3.13\n",
            "[181 | 302.05] loss=3.17 avg=3.13\n",
            "[182 | 303.53] loss=2.90 avg=3.12\n",
            "[183 | 305.01] loss=2.90 avg=3.12\n",
            "[184 | 306.49] loss=3.41 avg=3.12\n",
            "[185 | 307.97] loss=2.98 avg=3.12\n",
            "[186 | 309.45] loss=3.94 avg=3.13\n",
            "[187 | 310.94] loss=2.79 avg=3.13\n",
            "[188 | 312.42] loss=2.62 avg=3.12\n",
            "[189 | 313.90] loss=3.25 avg=3.12\n",
            "[190 | 315.38] loss=3.33 avg=3.13\n",
            "[191 | 316.86] loss=3.02 avg=3.12\n",
            "[192 | 318.35] loss=3.56 avg=3.13\n",
            "[193 | 319.83] loss=3.61 avg=3.14\n",
            "[194 | 321.31] loss=3.07 avg=3.13\n",
            "[195 | 322.79] loss=3.31 avg=3.14\n",
            "[196 | 324.27] loss=2.39 avg=3.13\n",
            "[197 | 325.75] loss=2.50 avg=3.12\n",
            "[198 | 327.23] loss=2.74 avg=3.12\n",
            "[199 | 328.72] loss=3.43 avg=3.12\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " curls, and his little sister-in-law. It may, indeed, have been too little because she had too much to say. All the while the boy waited, without speaking another word, his arms folded across his thighs like the arms of an idol, for his wife. His mother, who was in the room before him, sat her little hand on her daughter's, and her head leaned against his elbow, as if to whisper, \"My child, what is the meaning of everything? What is the meaning of all that I know?\"\n",
            "\n",
            "\"Good God! There is more going to be done than I ever saw in my life,\" the girl exclaimed.\n",
            "\n",
            "\"There is a lot, my dear,\" the boy said, and with his hand on her daughter's cheek she said, \"It is too hard, my dear. Too hard. But there cannot be a worse place for a child than that awful room with the big door in the middle, and the devil, all waiting in a big closet. There is no way to stand it. There can never be anything much more dreadful.\"\n",
            "\n",
            "\"Then I should have nothing right in the world to do but to go to the little house of the devil?\" asked the girl, with tears in her eyes. \"Is it not too little? It is too little.\"\n",
            "\n",
            "\"But you have got to go now, my dear,\" said the boy. \"Do not be afraid, my dear. I feel all right, and it does not matter. What have you got to tell the devil, or the old mother if you come here? He will hear it if he sees it.\"\n",
            "\n",
            "\"I could not tell you to make a sign like this,\" said the girl. She clutched her hands tight.\n",
            "\n",
            "He drew his arms close together, and he whispered:\n",
            "\n",
            "\"My child, if you will stay in that closet, and see the devil come, if you will stay there alone, when there is not a man in the whole house, he will say to you, \"My little child! Will you do what a stranger says to me?'\"\n",
            "\n",
            "\"You can make a sign like that,\" said the girl, with tears in her eyes.\n",
            "\n",
            "He said:\n",
            "\n",
            "\"The devil doesn't have to see it to be afraid.\"\n",
            "\n",
            "The little girl was a little worried. At first she was frightened for fear of the mother as well as for fear of the boy. But as time went on the little child realized that she was being taken advantage of, and she knew that he was getting used to it. She was frightened and cried.\n",
            "\n",
            "He went outside the door and stood in the doorway, still speaking to himself:\n",
            "\n",
            "\"My boy,\" he said, \"if you can come in there in the closet, and then be afraid. I would be the first to see the damned thing.\"\n",
            "\n",
            "Then he said:\n",
            "\n",
            "\"My daughter, don't be afraid. Go out there and cry. You don't have to come in there with the devil in the closet. I know it well: they only go out there when the wind blows. When the wind blows, they come in and wait in the closet. It is the devil they come in.\"\n",
            "\n",
            "So she went out there, and in the corner went the little girl. \"Come inside, my dear,\" said she, as she put her arms and knees under one another. \"They won't come in. The whole house is full of people, and they will only see what the devil has told.\" She held on tightly a little girl.\n",
            "\n",
            "\"Go in there, my child, please,\" said he, and took her in his arms, and embraced her.\n",
            "\n",
            "She was a little troubled for fear of the devil, and she was getting used to it. However, she knew all too well, and knew that she had been taken advantage of. And when she had closed her hands in his arms, she said, \"What a wonderful thing the devil has told me. What a dreadful thing!\" and was crying on.\n",
            "\n",
            "\"It is the devil,\" he said. \"He is waiting for me. It is the devil.\"\n",
            "\n",
            "\"What can we do about it?\"\n",
            "\n",
            "\"We can stay here. It is too safe. They don't want it to blow. It would only make you think more of it to have it blow at home.\"\n",
            "\n",
            "But she was too nervous to take off. He kept her, and then he went quietly to bed.\n",
            "\n",
            "\n",
            "The old woman, who was a widow of a family of merchants at Cibonia, was married, and having taken her last child into her family, was very anxious to give her another. It was no small thing for her to marry a second person. For she could make no children of men, and she was very fond of her only son-in-law, who was the great grandson of her nephew, who had married a lady\n",
            "\n",
            "[200 | 351.98] loss=3.00 avg=3.12\n",
            "[201 | 353.46] loss=2.64 avg=3.11\n",
            "[202 | 354.94] loss=2.85 avg=3.11\n",
            "[203 | 356.42] loss=3.68 avg=3.12\n",
            "[204 | 357.90] loss=3.22 avg=3.12\n",
            "[205 | 359.38] loss=3.28 avg=3.12\n",
            "[206 | 360.86] loss=3.33 avg=3.12\n",
            "[207 | 362.34] loss=2.65 avg=3.12\n",
            "[208 | 363.82] loss=2.73 avg=3.11\n",
            "[209 | 365.29] loss=3.15 avg=3.11\n",
            "[210 | 366.78] loss=3.55 avg=3.12\n",
            "[211 | 368.25] loss=3.10 avg=3.12\n",
            "[212 | 369.73] loss=3.31 avg=3.12\n",
            "[213 | 371.21] loss=3.60 avg=3.13\n",
            "[214 | 372.69] loss=3.49 avg=3.13\n",
            "[215 | 374.17] loss=3.61 avg=3.13\n",
            "[216 | 375.65] loss=3.03 avg=3.13\n",
            "[217 | 377.14] loss=3.36 avg=3.14\n",
            "[218 | 378.61] loss=3.11 avg=3.14\n",
            "[219 | 380.10] loss=3.05 avg=3.13\n",
            "[220 | 381.57] loss=2.73 avg=3.13\n",
            "[221 | 383.05] loss=3.48 avg=3.13\n",
            "[222 | 384.53] loss=3.36 avg=3.14\n",
            "[223 | 386.01] loss=3.11 avg=3.14\n",
            "[224 | 387.49] loss=3.04 avg=3.14\n",
            "[225 | 388.98] loss=3.04 avg=3.13\n",
            "[226 | 390.46] loss=3.27 avg=3.14\n",
            "[227 | 391.94] loss=3.39 avg=3.14\n",
            "[228 | 393.42] loss=3.10 avg=3.14\n",
            "[229 | 394.90] loss=3.37 avg=3.14\n",
            "[230 | 396.39] loss=2.78 avg=3.14\n",
            "[231 | 397.86] loss=3.19 avg=3.14\n",
            "[232 | 399.33] loss=2.87 avg=3.13\n",
            "[233 | 400.81] loss=3.28 avg=3.14\n",
            "[234 | 402.30] loss=3.51 avg=3.14\n",
            "[235 | 403.78] loss=2.89 avg=3.14\n",
            "[236 | 405.25] loss=3.42 avg=3.14\n",
            "[237 | 406.73] loss=2.64 avg=3.13\n",
            "[238 | 408.21] loss=3.43 avg=3.14\n",
            "[239 | 409.70] loss=2.99 avg=3.14\n",
            "[240 | 411.18] loss=2.72 avg=3.13\n",
            "[241 | 412.66] loss=2.84 avg=3.13\n",
            "[242 | 414.14] loss=2.76 avg=3.12\n",
            "[243 | 415.62] loss=3.49 avg=3.13\n",
            "[244 | 417.11] loss=2.85 avg=3.13\n",
            "[245 | 418.58] loss=3.19 avg=3.13\n",
            "[246 | 420.06] loss=3.38 avg=3.13\n",
            "[247 | 421.54] loss=2.99 avg=3.13\n",
            "[248 | 423.02] loss=2.71 avg=3.12\n",
            "[249 | 424.50] loss=2.84 avg=3.12\n",
            "[250 | 425.99] loss=3.15 avg=3.12\n",
            "[251 | 427.47] loss=3.10 avg=3.12\n",
            "[252 | 428.95] loss=3.48 avg=3.12\n",
            "[253 | 430.44] loss=2.99 avg=3.12\n",
            "[254 | 431.92] loss=3.34 avg=3.12\n",
            "[255 | 433.40] loss=2.82 avg=3.12\n",
            "[256 | 434.88] loss=2.64 avg=3.12\n",
            "[257 | 436.36] loss=2.88 avg=3.11\n",
            "[258 | 437.84] loss=2.68 avg=3.11\n",
            "[259 | 439.33] loss=2.90 avg=3.11\n",
            "[260 | 440.82] loss=2.75 avg=3.10\n",
            "[261 | 442.30] loss=3.07 avg=3.10\n",
            "[262 | 443.78] loss=3.36 avg=3.11\n",
            "[263 | 445.26] loss=2.91 avg=3.10\n",
            "[264 | 446.74] loss=3.78 avg=3.11\n",
            "[265 | 448.22] loss=2.90 avg=3.11\n",
            "[266 | 449.70] loss=3.48 avg=3.11\n",
            "[267 | 451.19] loss=3.22 avg=3.11\n",
            "[268 | 452.67] loss=2.86 avg=3.11\n",
            "[269 | 454.14] loss=3.46 avg=3.11\n",
            "[270 | 455.62] loss=3.31 avg=3.12\n",
            "[271 | 457.11] loss=3.34 avg=3.12\n",
            "[272 | 458.59] loss=2.89 avg=3.12\n",
            "[273 | 460.07] loss=3.09 avg=3.12\n",
            "[274 | 461.56] loss=2.79 avg=3.11\n",
            "[275 | 463.04] loss=2.86 avg=3.11\n",
            "[276 | 464.52] loss=3.07 avg=3.11\n",
            "[277 | 466.00] loss=2.99 avg=3.11\n",
            "[278 | 467.49] loss=3.83 avg=3.12\n",
            "[279 | 468.96] loss=3.15 avg=3.12\n",
            "[280 | 470.45] loss=3.82 avg=3.12\n",
            "[281 | 471.94] loss=3.52 avg=3.13\n",
            "[282 | 473.42] loss=4.02 avg=3.14\n",
            "[283 | 474.89] loss=3.02 avg=3.14\n",
            "[284 | 476.38] loss=2.86 avg=3.13\n",
            "[285 | 477.86] loss=3.36 avg=3.14\n",
            "[286 | 479.34] loss=2.84 avg=3.13\n",
            "[287 | 480.82] loss=2.62 avg=3.13\n",
            "[288 | 482.30] loss=2.66 avg=3.12\n",
            "[289 | 483.79] loss=3.25 avg=3.12\n",
            "[290 | 485.27] loss=3.37 avg=3.13\n",
            "[291 | 486.75] loss=2.79 avg=3.12\n",
            "[292 | 488.24] loss=2.81 avg=3.12\n",
            "[293 | 489.72] loss=3.04 avg=3.12\n",
            "[294 | 491.20] loss=2.57 avg=3.11\n",
            "[295 | 492.68] loss=3.80 avg=3.12\n",
            "[296 | 494.16] loss=2.64 avg=3.11\n",
            "[297 | 495.64] loss=2.92 avg=3.11\n",
            "[298 | 497.11] loss=3.16 avg=3.11\n",
            "[299 | 498.60] loss=3.41 avg=3.12\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " in the city. As the carriages came nearer the crowd began to gather. In front of the houses were two lines of police officers carrying revolvers, which was the rule. There were police vans on both sides of the road. They were manned by a young woman who carried an enormous stick. She had a big grin on her face and a smile on her face was a smile.\n",
            "\n",
            "\"How were you doing?\" asked an officer.\n",
            "\n",
            "\"Oh, fine,\" she said. \"Very well. We brought in a few people to check their papers on the first floor. We want to check their papers.\"\n",
            "\n",
            "There was a huge commotion on the second floor. In the living room there was a group of frightened girls and some boys who were all frightened. Then the children came out again as if frightened. The carriages came out from the city. People stood at the doorway, in the streets, at the gate. The first officer, who was a very good soldier, came as close to the children as if their lives depended on him, and he talked to them. He went in with a white man as his driver, and they were all waiting on the second level of the carriages, where the police arrived soon after. There was even a policeman, an officer of the guard.\n",
            "\n",
            "\"How are you, Sir?\" he asked the little girls, putting his head into the open doorway. \"How are you?\" asked the little boy.\n",
            "\n",
            "\"Just fine, thank you very much.\"\n",
            "\n",
            "They waited for an hour. Then they were going out into the city streets. They took a long time in the street. When the carriage doors and people came running out, they stopped and sat down in the street, and then they walked along the pavement and along the shops. Then the policemen walked out in the street too, and, to make matters worse, there were some police cars that happened to pass by. There were some people who came out on to the pavement, so the policemen stopped and stood for a moment talking to some person. Then they went in, and there were about three dozen policemen standing in the street. One man in blue had his hat on and he was holding a cigar with which he was smoking a cigarette.\n",
            "\n",
            "\"Come with me,\" said the policeman. \"I am the inspector, and I am going to the second floor. I want to see your papers.\"\n",
            "\n",
            "Then the inspector, who was in blue, followed him. He held the cigar and said one word to him when he was near enough. Then he turned round and went down the street. After a few minutes, the inspector came back and held the cigar and said, \"Thank you very much.\"\n",
            "\n",
            "The inspector went back to his car. \"I want to know that the third floor, where the police are going to keep their police car for officers to drive, is right there right there!\"\n",
            "\n",
            "\"Right there?\" said the little girl.\n",
            "\n",
            "\"Yes, it is on the first floor where the police went away from the first level. It is right there.\"\n",
            "\n",
            "\"You see a policeman coming back into the street?\"\n",
            "\n",
            "\"Yes. He is carrying the policeman's badge, with the number on it. And there are people standing on all sides waiting for him to go in and see how the second or the third floor lives.\"\n",
            "\n",
            "\"Well, how many?\" the little girl asked.\n",
            "\n",
            "\"Well, I think it is about twelve, twelve to fourteen policemen, three guards, and a woman and a child, who came to see this policeman.\"\n",
            "\n",
            "\"Well, how many were there?\"\n",
            "\n",
            "\"Well, I don't know. Well, I see the police officers coming back into the street. They were all stopped, and there were people standing there with the policeman's badge on and the policeman's number on.\"\n",
            "\n",
            "The little girl, smiling, said, \"Well, that must be more policemen.\"\n",
            "\n",
            "\"Well, I don't know,\" said the inspector. He held the cigar. \"It seems to be an old policeman in blue who is taking a young policeman who is carrying the policeman's badge.\"\n",
            "\n",
            "The inspector took another cigar and kept his eyes on the policeman, as if he were a witness to a crime. The little girl, smiling, said, \"Well, that must be more policemen. They are all getting to come back to see where the old policeman is going.\" And the inspector said, \"What do you want to ask the policeman?\" and the little girl, smiling, answered, \"What do I want of the policeman?\"\n",
            "\n",
            "\"I want the policeman's badge,\" said the inspector, looking towards the policeman, as if he was a witness to a crime. So the policeman, with the policeman's badge on, went through the door. The inspector turned his head and, looking round, began to listen. There was a car, a small car, with a white man as it's driver\n",
            "\n",
            "[300 | 521.86] loss=3.71 avg=3.12\n",
            "[301 | 523.34] loss=3.16 avg=3.12\n",
            "[302 | 524.82] loss=2.67 avg=3.12\n",
            "[303 | 526.30] loss=2.57 avg=3.11\n",
            "[304 | 527.79] loss=3.15 avg=3.11\n",
            "[305 | 529.27] loss=3.41 avg=3.12\n",
            "[306 | 530.75] loss=2.78 avg=3.11\n",
            "[307 | 532.23] loss=3.19 avg=3.11\n",
            "[308 | 533.70] loss=2.84 avg=3.11\n",
            "[309 | 535.18] loss=3.64 avg=3.12\n",
            "[310 | 536.66] loss=3.49 avg=3.12\n",
            "[311 | 538.15] loss=3.18 avg=3.12\n",
            "[312 | 539.63] loss=2.49 avg=3.11\n",
            "[313 | 541.11] loss=3.11 avg=3.11\n",
            "[314 | 542.58] loss=3.28 avg=3.12\n",
            "[315 | 544.06] loss=3.25 avg=3.12\n",
            "[316 | 545.54] loss=3.30 avg=3.12\n",
            "[317 | 547.02] loss=3.06 avg=3.12\n",
            "[318 | 548.50] loss=3.04 avg=3.12\n",
            "[319 | 549.97] loss=3.22 avg=3.12\n",
            "[320 | 551.46] loss=3.02 avg=3.12\n",
            "[321 | 552.94] loss=3.15 avg=3.12\n",
            "[322 | 554.42] loss=2.69 avg=3.11\n",
            "[323 | 555.89] loss=3.18 avg=3.11\n",
            "[324 | 557.38] loss=2.93 avg=3.11\n",
            "[325 | 558.86] loss=2.76 avg=3.11\n",
            "[326 | 560.34] loss=2.80 avg=3.11\n",
            "[327 | 561.82] loss=3.06 avg=3.10\n",
            "[328 | 563.30] loss=3.02 avg=3.10\n",
            "[329 | 564.79] loss=2.90 avg=3.10\n",
            "[330 | 566.27] loss=3.19 avg=3.10\n",
            "[331 | 567.75] loss=2.94 avg=3.10\n",
            "[332 | 569.23] loss=3.35 avg=3.10\n",
            "[333 | 570.71] loss=3.42 avg=3.11\n",
            "[334 | 572.19] loss=2.46 avg=3.10\n",
            "[335 | 573.67] loss=3.03 avg=3.10\n",
            "[336 | 575.15] loss=3.23 avg=3.10\n",
            "[337 | 576.64] loss=2.77 avg=3.10\n",
            "[338 | 578.12] loss=2.70 avg=3.09\n",
            "[339 | 579.60] loss=3.04 avg=3.09\n",
            "[340 | 581.08] loss=2.64 avg=3.09\n",
            "[341 | 582.56] loss=2.92 avg=3.09\n",
            "[342 | 584.04] loss=3.27 avg=3.09\n",
            "[343 | 585.53] loss=2.73 avg=3.08\n",
            "[344 | 587.01] loss=3.45 avg=3.09\n",
            "[345 | 588.49] loss=3.14 avg=3.09\n",
            "[346 | 589.98] loss=3.62 avg=3.09\n",
            "[347 | 591.46] loss=3.18 avg=3.10\n",
            "[348 | 592.94] loss=2.70 avg=3.09\n",
            "[349 | 594.42] loss=3.20 avg=3.09\n",
            "[350 | 595.91] loss=3.32 avg=3.09\n",
            "[351 | 597.38] loss=3.23 avg=3.10\n",
            "[352 | 598.87] loss=3.32 avg=3.10\n",
            "[353 | 600.35] loss=3.00 avg=3.10\n",
            "[354 | 601.83] loss=3.28 avg=3.10\n",
            "[355 | 603.31] loss=3.80 avg=3.11\n",
            "[356 | 604.78] loss=2.46 avg=3.10\n",
            "[357 | 606.27] loss=3.17 avg=3.10\n",
            "[358 | 607.75] loss=2.81 avg=3.10\n",
            "[359 | 609.24] loss=2.86 avg=3.09\n",
            "[360 | 610.72] loss=2.97 avg=3.09\n",
            "[361 | 612.20] loss=2.79 avg=3.09\n",
            "[362 | 613.68] loss=3.18 avg=3.09\n",
            "[363 | 615.17] loss=2.72 avg=3.09\n",
            "[364 | 616.65] loss=3.09 avg=3.09\n",
            "[365 | 618.13] loss=3.52 avg=3.09\n",
            "[366 | 619.61] loss=3.29 avg=3.09\n",
            "[367 | 621.08] loss=2.90 avg=3.09\n",
            "[368 | 622.57] loss=2.60 avg=3.09\n",
            "[369 | 624.05] loss=2.94 avg=3.09\n",
            "[370 | 625.54] loss=3.04 avg=3.09\n",
            "[371 | 627.01] loss=3.14 avg=3.09\n",
            "[372 | 628.49] loss=2.73 avg=3.08\n",
            "[373 | 629.96] loss=3.25 avg=3.08\n",
            "[374 | 631.45] loss=2.80 avg=3.08\n",
            "[375 | 632.93] loss=3.05 avg=3.08\n",
            "[376 | 634.40] loss=2.95 avg=3.08\n",
            "[377 | 635.88] loss=2.77 avg=3.08\n",
            "[378 | 637.36] loss=3.30 avg=3.08\n",
            "[379 | 638.84] loss=2.44 avg=3.07\n",
            "[380 | 640.31] loss=3.01 avg=3.07\n",
            "[381 | 641.79] loss=3.04 avg=3.07\n",
            "[382 | 643.28] loss=3.13 avg=3.07\n",
            "[383 | 644.76] loss=2.39 avg=3.06\n",
            "[384 | 646.24] loss=3.69 avg=3.07\n",
            "[385 | 647.73] loss=2.97 avg=3.07\n",
            "[386 | 649.20] loss=3.46 avg=3.07\n",
            "[387 | 650.69] loss=3.27 avg=3.08\n",
            "[388 | 652.18] loss=2.72 avg=3.07\n",
            "[389 | 653.66] loss=2.81 avg=3.07\n",
            "[390 | 655.14] loss=3.24 avg=3.07\n",
            "[391 | 656.62] loss=3.11 avg=3.07\n",
            "[392 | 658.11] loss=3.12 avg=3.07\n",
            "[393 | 659.59] loss=3.26 avg=3.07\n",
            "[394 | 661.07] loss=3.38 avg=3.08\n",
            "[395 | 662.55] loss=3.55 avg=3.08\n",
            "[396 | 664.04] loss=3.47 avg=3.09\n",
            "[397 | 665.52] loss=3.46 avg=3.09\n",
            "[398 | 667.00] loss=2.56 avg=3.08\n",
            "[399 | 668.48] loss=2.93 avg=3.08\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " res.\n",
            "\n",
            "The three men went back again, having made their escape. Mr. and Mrs. Ketchum were dead and Mr. Devereux was in jail, having escaped in a hurry. His wife was an old woman who had been living in the house the whole of the winter, so that Mrs. Ketchum had been living with her and had been unable to save her.\n",
            "\n",
            "Mr. Ketchum was one of those who had suffered from the cold, and the cold had caused him to miss his wife more than any of these men, but the winter he was away he found his wife and children, and they went through the streets without talking. Mr. Wills used to go on through with his mother one day, and Mrs. Ketchum talked a great deal. She used to play with little John to make his cheeks look white and make it look like they were in a bad temper in the way she played with John. She would take him up to the room where she grew up and take him to play in the hall, and he would sit quietly there crying and saying nothing, and then when he grew up he would hear his mother crying too.\n",
            "\n",
            "And then Mr. Wills would look at the two children and he would say: \"Oh, they are only children, John and Wills. What a foolish thing all the world is to care for a child who is never to be seen. But I don't care any more about this than you. I've got a house to make. I can get a job for myself and a wife and a little boy and take care of him. I don't care as much!\" And I'd say: \"Oh, but your wife and little boy and your mother are so very poor, but if they got us a house they'd have to work with us to save their little bread, and I'd do anything to get them a decent home. I'd work myself and you would work, and I'd take you to work if we ever came to have another boy.\"\n",
            "\n",
            "But he never wanted to make one of those terrible decisions. All the time he was trying to talk with the little girl and the little boy with the heart of a lion. There was a great deal of play that these three poor, cold, heartless, heartless, heartless people had to go through at the end, and the heart was very cold, and the heart knew a woman was a man when she thought of another woman, but the heart was all right when she thought of another woman just like herself. They lived that way until they went over, and then they had to go back into it all over again so that the heart remained that beautiful, that hot, that kind, so lovely, so warm in that heart.\n",
            "\n",
            "This winter I thought of the heart and I began to think of the words and I began to think of what an ideal heart would be like and I had an idea, and I said: \"Oh, I'll have my own heart, and I'll have every man's heart. I know I'll be a woman, and that will be the heart of every man who has ever lived, and I'll have every man's heart and I'll let every man have a heart as well as I have.\"\n",
            "\n",
            "And it happened that some time after Mr. Ketchum had been at one of his friends' houses and it had become clear that he'd been spending more time to himself than was natural to a man, his friend had told Mr. Devereux; and Mr. Devereux had been at Mr. Ketchum's house for the Christmas holidays and been taking him all out.\n",
            "\n",
            "\"Oh, God, Mr. Ketchum!\" said Mr. Devereux. \"Mr. Devereux is going into a man's heart, isn't he? Mr. Devereux knows it's a man's heart and he has a man's heart inside. I shall have no more of your heart than Mr. Devereux has of yours. I'll be in the heart of your whole, and let every man have a heart, Mr. Devereux's heart and mine as well.\"\n",
            "\n",
            "What a heart was that! It was as beautiful as any man could have painted, every shade of it.\n",
            "\n",
            "You know how they say that a man's soul can last but a week or so, and this was a great week. It was an interesting week because each and every one of those little things that I said about a husband and a wife were in the beginning of some real, a real life, and the soul did last for a long time.\n",
            "\n",
            "And then after I had my first year home there came a summer of some weeks when I went through life like a woman who'd begun to live as a man.\n",
            "\n",
            "And that summer was the summer when I had the first child.\n",
            "\n",
            "And when I was pregnant with that baby I was a young man\n",
            "\n",
            "[400 | 691.78] loss=3.39 avg=3.09\n",
            "[401 | 693.26] loss=3.18 avg=3.09\n",
            "[402 | 694.74] loss=3.15 avg=3.09\n",
            "[403 | 696.21] loss=3.23 avg=3.09\n",
            "[404 | 697.70] loss=2.78 avg=3.09\n",
            "[405 | 699.18] loss=2.39 avg=3.08\n",
            "[406 | 700.67] loss=2.81 avg=3.08\n",
            "[407 | 702.14] loss=3.32 avg=3.08\n",
            "[408 | 703.62] loss=2.70 avg=3.07\n",
            "[409 | 705.10] loss=3.35 avg=3.08\n",
            "[410 | 706.58] loss=2.63 avg=3.07\n",
            "[411 | 708.06] loss=2.47 avg=3.07\n",
            "[412 | 709.54] loss=3.08 avg=3.07\n",
            "[413 | 711.02] loss=2.77 avg=3.06\n",
            "[414 | 712.49] loss=3.06 avg=3.06\n",
            "[415 | 713.98] loss=2.89 avg=3.06\n",
            "[416 | 715.47] loss=3.19 avg=3.06\n",
            "[417 | 716.95] loss=2.70 avg=3.06\n",
            "[418 | 718.42] loss=3.71 avg=3.07\n",
            "[419 | 719.91] loss=2.04 avg=3.06\n",
            "[420 | 721.39] loss=3.43 avg=3.06\n",
            "[421 | 722.87] loss=3.43 avg=3.06\n",
            "[422 | 724.35] loss=2.78 avg=3.06\n",
            "[423 | 725.83] loss=3.46 avg=3.06\n",
            "[424 | 727.31] loss=3.56 avg=3.07\n",
            "[425 | 728.80] loss=3.58 avg=3.07\n",
            "[426 | 730.28] loss=3.20 avg=3.08\n",
            "[427 | 731.76] loss=3.09 avg=3.08\n",
            "[428 | 733.25] loss=2.89 avg=3.07\n",
            "[429 | 734.73] loss=2.68 avg=3.07\n",
            "[430 | 736.21] loss=2.96 avg=3.07\n",
            "[431 | 737.70] loss=3.09 avg=3.07\n",
            "[432 | 739.18] loss=3.33 avg=3.07\n",
            "[433 | 740.66] loss=3.07 avg=3.07\n",
            "[434 | 742.13] loss=2.97 avg=3.07\n",
            "[435 | 743.62] loss=3.16 avg=3.07\n",
            "[436 | 745.10] loss=2.62 avg=3.07\n",
            "[437 | 746.58] loss=2.78 avg=3.06\n",
            "[438 | 748.07] loss=2.92 avg=3.06\n",
            "[439 | 749.55] loss=2.68 avg=3.06\n",
            "[440 | 751.03] loss=2.99 avg=3.06\n",
            "[441 | 752.50] loss=2.93 avg=3.06\n",
            "[442 | 753.98] loss=3.04 avg=3.06\n",
            "[443 | 755.45] loss=3.11 avg=3.06\n",
            "[444 | 756.93] loss=2.80 avg=3.06\n",
            "[445 | 758.42] loss=3.27 avg=3.06\n",
            "[446 | 759.90] loss=3.10 avg=3.06\n",
            "[447 | 761.38] loss=3.16 avg=3.06\n",
            "[448 | 762.86] loss=3.15 avg=3.06\n",
            "[449 | 764.35] loss=3.36 avg=3.06\n",
            "[450 | 765.83] loss=2.92 avg=3.06\n",
            "[451 | 767.31] loss=2.82 avg=3.06\n",
            "[452 | 768.80] loss=3.59 avg=3.06\n",
            "[453 | 770.29] loss=1.89 avg=3.05\n",
            "[454 | 771.76] loss=2.86 avg=3.05\n",
            "[455 | 773.25] loss=3.03 avg=3.05\n",
            "[456 | 774.73] loss=3.29 avg=3.05\n",
            "[457 | 776.21] loss=2.55 avg=3.05\n",
            "[458 | 777.69] loss=3.45 avg=3.05\n",
            "[459 | 779.17] loss=3.29 avg=3.05\n",
            "[460 | 780.64] loss=2.33 avg=3.05\n",
            "[461 | 782.12] loss=3.22 avg=3.05\n",
            "[462 | 783.59] loss=2.92 avg=3.05\n",
            "[463 | 785.07] loss=3.07 avg=3.05\n",
            "[464 | 786.55] loss=2.24 avg=3.04\n",
            "[465 | 788.04] loss=3.53 avg=3.04\n",
            "[466 | 789.52] loss=2.79 avg=3.04\n",
            "[467 | 791.00] loss=3.08 avg=3.04\n",
            "[468 | 792.48] loss=3.28 avg=3.04\n",
            "[469 | 793.96] loss=2.90 avg=3.04\n",
            "[470 | 795.44] loss=2.82 avg=3.04\n",
            "[471 | 796.91] loss=2.46 avg=3.03\n",
            "[472 | 798.39] loss=2.94 avg=3.03\n",
            "[473 | 799.86] loss=3.00 avg=3.03\n",
            "[474 | 801.35] loss=2.79 avg=3.03\n",
            "[475 | 802.82] loss=3.46 avg=3.04\n",
            "[476 | 804.30] loss=3.22 avg=3.04\n",
            "[477 | 805.78] loss=2.76 avg=3.03\n",
            "[478 | 807.27] loss=2.92 avg=3.03\n",
            "[479 | 808.75] loss=3.27 avg=3.04\n",
            "[480 | 810.23] loss=2.91 avg=3.03\n",
            "[481 | 811.72] loss=3.19 avg=3.04\n",
            "[482 | 813.20] loss=3.39 avg=3.04\n",
            "[483 | 814.68] loss=3.13 avg=3.04\n",
            "[484 | 816.16] loss=3.49 avg=3.04\n",
            "[485 | 817.64] loss=2.78 avg=3.04\n",
            "[486 | 819.12] loss=3.30 avg=3.04\n",
            "[487 | 820.61] loss=2.85 avg=3.04\n",
            "[488 | 822.09] loss=3.09 avg=3.04\n",
            "[489 | 823.57] loss=3.75 avg=3.05\n",
            "[490 | 825.06] loss=2.83 avg=3.05\n",
            "[491 | 826.53] loss=3.46 avg=3.05\n",
            "[492 | 828.01] loss=3.08 avg=3.05\n",
            "[493 | 829.50] loss=2.76 avg=3.05\n",
            "[494 | 830.98] loss=2.37 avg=3.04\n",
            "[495 | 832.46] loss=2.92 avg=3.04\n",
            "[496 | 833.94] loss=3.17 avg=3.04\n",
            "[497 | 835.41] loss=3.07 avg=3.04\n",
            "[498 | 836.89] loss=3.23 avg=3.05\n",
            "[499 | 838.37] loss=2.95 avg=3.04\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " unwredded. They did not want to hear his voice. \"My God,\" he repeated in a low voice, as though calling a prayer, \"my God, what are you doing with us?\" A few hours later at lunch he was eating his usual plate of spaghetti and meatballs with fruit and salad; after an instant he took from the table the red and white fruit of his \"redemption\" of his wife. Suddenly he sat up and gazed at his wife with a look of amazement. He said, in a soft voice, \"I have never seen you this angry and discontented in all my life. I am only to be the shepherd's boy, so to speak.\"\n",
            "\n",
            "She was quite sure she had known that for years at least. She thought this was a great change, and that she was getting accustomed to it; what she did not know was that, so long as she was a poor woman, she had never been without her husband, whose kindness had always been essential to her existence. His kindness had made her an even richer woman and made her his wife, whom he had loved, an even richer woman.\n",
            "\n",
            "This is the story told by the poor, by the poor. Some poor men said the other day to their wives: \"Poor wives, you are so poor! All your children are little and are going to be sold, and they will never have the money you have. It is a terrible thing that we should have to suffer through for nothing, when your children are at a time when you can get in all you want. Do not be frightened. The poor woman has nothing. You know, don't you? You had nothing to eat and nothing to do, but now you have nothing but what can be bought at the market. The poor woman is helpless now. She is not at all sure of her lot, because it is so simple, and because she is not at all sure what she would do if it mattered in the end if she got nothing. The poor woman is a woman not yet ready to let go. She is not sure of her own happiness at last. She has not felt the life of the world, or the love of her children; she has not felt their joy or grief and sorrow and loneliness. Poor woman! You and your children are not in love here on earth, but on earth no one cares about anybody. Now she has absolutely nothing to live for, and the man who is the shepherd comes, and you have nothing to take for yourself; you are miserable now. Don't try to make yourself happy. You will never get anything! You are a woman of sorrows, and the poor woman does not know what death is!\" And these poor men said \"Poor wives, you do not know what a wretched woman you are! Your children and your husband are all dead because you have not bought the bread they require. You have been so foolish, and you should have had more wisdom. Do not be discouraged. Your children and husband are not dead at the door. They are alive and they are happy. There is no need to live so wretchedly now. They have been born, and have died, and you do not know what to do with them!\" And these poor husbands said: \"I do not know the man who is to pay for the bread we need. My poor and wretched wife and children have had so much misfortune in life: they have lived, they have died, and no one has paid for them. They are happy. They go to sleep like little angels with joy in their eyes. Why not take care of them like poor people? They will never die. There is no need to take away their happiness.\"\n",
            "\n",
            "It does no good to talk about the poor, but I found out that men of that kind are called \"sick\" and \"feral\" because they do not know what to do with their misery: they cannot get anything with it. They can live in misery forever, if they would live with what they have. That is the case with men of my class. A man of that class, who could get nothing, is called \"sick and wild,\" and his wife goes to Heaven. I am thinking of such a man.<|endoftext|>The story of the Battle of the Bulge, as told to a reporter in Berlin, is told in one sentence: \"The enemy was surrounded. He could not see. He could not get through. And in the center of that confusion was his sergeant's rifle which had killed him.\"\n",
            "\n",
            "One hears of a battle quite often: at its earliest period one or two days prior to the actual battle, the reader hears about it, then learns afterwards that in the trenches one finds a man who had been shot in the shoulder and who was going on a rampage, his rifle sticking to his flesh. He had been shot after the last shot, and his bullet had torn his skin open, spilling into the cold dead air. This man--my man--was one of the best of\n",
            "\n",
            "[500 | 861.73] loss=2.85 avg=3.04\n",
            "[501 | 863.21] loss=2.87 avg=3.04\n",
            "[502 | 864.68] loss=3.19 avg=3.04\n",
            "[503 | 866.17] loss=2.57 avg=3.04\n",
            "[504 | 867.66] loss=3.21 avg=3.04\n",
            "[505 | 869.14] loss=3.49 avg=3.04\n",
            "[506 | 870.62] loss=2.92 avg=3.04\n",
            "[507 | 872.09] loss=3.77 avg=3.05\n",
            "[508 | 873.58] loss=3.14 avg=3.05\n",
            "[509 | 875.06] loss=2.63 avg=3.05\n",
            "[510 | 876.54] loss=3.17 avg=3.05\n",
            "[511 | 878.02] loss=3.09 avg=3.05\n",
            "[512 | 879.50] loss=2.59 avg=3.04\n",
            "[513 | 880.98] loss=2.78 avg=3.04\n",
            "[514 | 882.46] loss=2.92 avg=3.04\n",
            "[515 | 883.94] loss=3.08 avg=3.04\n",
            "[516 | 885.41] loss=2.95 avg=3.04\n",
            "[517 | 886.90] loss=3.24 avg=3.04\n",
            "[518 | 888.38] loss=2.88 avg=3.04\n",
            "[519 | 889.85] loss=3.54 avg=3.04\n",
            "[520 | 891.33] loss=2.73 avg=3.04\n",
            "[521 | 892.81] loss=2.82 avg=3.04\n",
            "[522 | 894.30] loss=2.88 avg=3.04\n",
            "[523 | 895.78] loss=2.54 avg=3.03\n",
            "[524 | 897.26] loss=3.68 avg=3.04\n",
            "[525 | 898.74] loss=3.22 avg=3.04\n",
            "[526 | 900.22] loss=2.60 avg=3.04\n",
            "[527 | 901.70] loss=2.73 avg=3.03\n",
            "[528 | 903.19] loss=3.16 avg=3.03\n",
            "[529 | 904.66] loss=3.02 avg=3.03\n",
            "[530 | 906.14] loss=2.80 avg=3.03\n",
            "[531 | 907.63] loss=3.21 avg=3.03\n",
            "[532 | 909.11] loss=3.21 avg=3.04\n",
            "[533 | 910.59] loss=2.91 avg=3.03\n",
            "[534 | 912.07] loss=2.71 avg=3.03\n",
            "[535 | 913.55] loss=2.93 avg=3.03\n",
            "[536 | 915.03] loss=2.91 avg=3.03\n",
            "[537 | 916.50] loss=3.01 avg=3.03\n",
            "[538 | 917.98] loss=3.35 avg=3.03\n",
            "[539 | 919.46] loss=2.64 avg=3.03\n",
            "[540 | 920.95] loss=2.83 avg=3.03\n",
            "[541 | 922.43] loss=2.80 avg=3.02\n",
            "[542 | 923.91] loss=2.95 avg=3.02\n",
            "[543 | 925.39] loss=3.08 avg=3.02\n",
            "[544 | 926.87] loss=3.12 avg=3.02\n",
            "[545 | 928.36] loss=2.79 avg=3.02\n",
            "[546 | 929.84] loss=2.97 avg=3.02\n",
            "[547 | 931.32] loss=2.48 avg=3.02\n",
            "[548 | 932.80] loss=2.72 avg=3.01\n",
            "[549 | 934.28] loss=2.94 avg=3.01\n",
            "[550 | 935.76] loss=2.75 avg=3.01\n",
            "[551 | 937.24] loss=2.86 avg=3.01\n",
            "[552 | 938.73] loss=2.75 avg=3.01\n",
            "[553 | 940.21] loss=2.84 avg=3.00\n",
            "[554 | 941.69] loss=3.03 avg=3.00\n",
            "[555 | 943.17] loss=2.52 avg=3.00\n",
            "[556 | 944.64] loss=3.06 avg=3.00\n",
            "[557 | 946.12] loss=2.57 avg=3.00\n",
            "[558 | 947.60] loss=3.40 avg=3.00\n",
            "[559 | 949.08] loss=3.10 avg=3.00\n",
            "[560 | 950.57] loss=3.38 avg=3.00\n",
            "[561 | 952.05] loss=2.91 avg=3.00\n",
            "[562 | 953.54] loss=2.77 avg=3.00\n",
            "[563 | 955.01] loss=3.12 avg=3.00\n",
            "[564 | 956.50] loss=3.32 avg=3.01\n",
            "[565 | 957.98] loss=2.77 avg=3.00\n",
            "[566 | 959.46] loss=3.48 avg=3.01\n",
            "[567 | 960.94] loss=3.06 avg=3.01\n",
            "[568 | 962.43] loss=3.49 avg=3.01\n",
            "[569 | 963.90] loss=3.76 avg=3.02\n",
            "[570 | 965.38] loss=3.52 avg=3.03\n",
            "[571 | 966.86] loss=3.22 avg=3.03\n",
            "[572 | 968.34] loss=3.06 avg=3.03\n",
            "[573 | 969.82] loss=3.07 avg=3.03\n",
            "[574 | 971.30] loss=2.98 avg=3.03\n",
            "[575 | 972.78] loss=2.99 avg=3.03\n",
            "[576 | 974.26] loss=3.35 avg=3.03\n",
            "[577 | 975.74] loss=2.53 avg=3.03\n",
            "[578 | 977.23] loss=3.53 avg=3.03\n",
            "[579 | 978.71] loss=2.87 avg=3.03\n",
            "[580 | 980.19] loss=3.27 avg=3.03\n",
            "[581 | 981.67] loss=2.37 avg=3.02\n",
            "[582 | 983.15] loss=3.10 avg=3.03\n",
            "[583 | 984.64] loss=3.03 avg=3.03\n",
            "[584 | 986.12] loss=3.03 avg=3.03\n",
            "[585 | 987.60] loss=2.80 avg=3.02\n",
            "[586 | 989.08] loss=3.24 avg=3.03\n",
            "[587 | 990.55] loss=3.56 avg=3.03\n",
            "[588 | 992.03] loss=2.32 avg=3.02\n",
            "[589 | 993.51] loss=3.13 avg=3.02\n",
            "[590 | 994.98] loss=2.59 avg=3.02\n",
            "[591 | 996.46] loss=3.24 avg=3.02\n",
            "[592 | 997.94] loss=3.10 avg=3.02\n",
            "[593 | 999.40] loss=2.98 avg=3.02\n",
            "[594 | 1000.89] loss=2.80 avg=3.02\n",
            "[595 | 1002.37] loss=2.89 avg=3.02\n",
            "[596 | 1003.85] loss=3.56 avg=3.03\n",
            "[597 | 1005.33] loss=3.03 avg=3.03\n",
            "[598 | 1006.81] loss=3.24 avg=3.03\n",
            "[599 | 1008.29] loss=3.23 avg=3.03\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " which will not have anything to do with the other. It will be a sort of little town outside all that, and it will have been put together by the labor of one man! But it is all gone; the old walls have been taken out and laid again. A small building is standing where the old walls used to be, and it's a storehouse, and the town people want it.\n",
            "\n",
            "\"We will just go on like this,\" the lady's voice was weak and shrill. \"I have been in such a hurry, and I can't keep up with you all the time. I've got to put a cupboard under your feet. I should take you to dinner and dinner dinner dinner, you see, but it's all so crowded, you know. I have to do something to make the time pass when you are so small. I've been having so much trouble and fuss, and I can't take it in. I can't think!\"\n",
            "\n",
            "And suddenly the lady looked at the children and thought for a moment--\n",
            "\n",
            "\"Why, what a waste!\" she said. \"There's no use trying to make it up again. My children have been so well taken care of, and if I haven't had much trouble with you, they've been taken care of so well. You've had a great deal to suffer, but--you are quite a good child, you know! You've been put through the ringer, and you are learning to have something of an opinion--to have your way. What are you afraid of? I have been very much busy with you, and if you would just get with me and go on, I think you would be very happy. And you've been the biggest help to me, for everything. Come along with me, and I'll do anything. But I don't want any trouble on your account.\"\n",
            "\n",
            "And as she spoke, the boy's eyes were moist, and his hands were trembling--all his strength was in the little chest. \"I think I shall go with her to bed,\" he murmured, with an air of horror on his lips. \"She'll put a door on it, and we'll go out together in the dark, and she'll come here with a bad cough and a little fever--she'll be so tired when we get back.\" And all that--all the trouble, all the suffering under her eyes, and everything was for nothing. \"Don't you think that would be a good thing for us, for it's an easy thing to have children who do want to go on! And you--you have your reason; we have our reason--but I was very much interested in hearing what you thought, and if you had any good to say, I thought it was only right that the old woman might hear your good and take part of it, too. Besides, she's an old woman, with her wrinkles. You might as well say she's an aunt! Come along with us together, and I am ready to go to the bottom of the house in the dark; I have had enough of this little town.\"\n",
            "\n",
            "And just then a horrible sob and an awful shaking came over all the children. As a mother would do, they all began to cry. \"Well!\" they had all cried, together, and then the old lady rose. She was very frightened, and she was going to throw herself out into the cold night. \"It's not right,\" she would say, \"but I am not giving up now. It's not fair, and the old lady is in such a hurry!\" She shook, and there was a great deal of strain in her mouth, and she looked up at the children; one of her eyes was red, and she saw the child look down at it--and she would only growl and scream, so that the little chest would shake and cry and shake and cry as well. \"It isn't fair, I know it isn't right,\" she wouldn't say it out loud, but she would only shake, and growl as long as they would go.\n",
            "\n",
            "\"Why are you standing there?\" the other children cried.\n",
            "\n",
            "\"I don't know,\" the old lady would say. \"I think it may be an old thing, and I don't want to do it any more for her. But, I'm frightened for her.\"\n",
            "\n",
            "And they sat and listened to her, and in them was all the strength that the old woman had to stand there, to shake till an oar or a chair would catch them under the arms and make them come down, and she was frightened, and they could hardly be quieted, and so they would all scream and moan and cry, and the old woman trembled and trembled and cried, and the children were as much frightened as she.\n",
            "\n",
            "\"Why are you all trembling?\" the old lady would cry. \"Come along with us, and I am ready to go to the bottom of\n",
            "\n",
            "[600 | 1031.56] loss=3.39 avg=3.03\n",
            "[601 | 1033.04] loss=3.07 avg=3.03\n",
            "[602 | 1034.52] loss=2.79 avg=3.03\n",
            "[603 | 1036.00] loss=2.91 avg=3.03\n",
            "[604 | 1037.48] loss=2.82 avg=3.03\n",
            "[605 | 1038.96] loss=3.05 avg=3.03\n",
            "[606 | 1040.45] loss=3.07 avg=3.03\n",
            "[607 | 1041.93] loss=3.12 avg=3.03\n",
            "[608 | 1043.41] loss=3.03 avg=3.03\n",
            "[609 | 1044.88] loss=3.22 avg=3.03\n",
            "[610 | 1046.36] loss=3.07 avg=3.03\n",
            "[611 | 1047.84] loss=2.67 avg=3.03\n",
            "[612 | 1049.32] loss=3.04 avg=3.03\n",
            "[613 | 1050.80] loss=2.53 avg=3.02\n",
            "[614 | 1052.29] loss=2.61 avg=3.02\n",
            "[615 | 1053.77] loss=2.90 avg=3.02\n",
            "[616 | 1055.25] loss=2.68 avg=3.01\n",
            "[617 | 1056.74] loss=3.26 avg=3.02\n",
            "[618 | 1058.22] loss=3.47 avg=3.02\n",
            "[619 | 1059.70] loss=2.61 avg=3.02\n",
            "[620 | 1061.18] loss=2.70 avg=3.01\n",
            "[621 | 1062.67] loss=3.18 avg=3.02\n",
            "[622 | 1064.14] loss=2.75 avg=3.01\n",
            "[623 | 1065.62] loss=3.16 avg=3.01\n",
            "[624 | 1067.10] loss=2.87 avg=3.01\n",
            "[625 | 1068.58] loss=2.57 avg=3.01\n",
            "[626 | 1070.06] loss=2.51 avg=3.00\n",
            "[627 | 1071.54] loss=2.62 avg=3.00\n",
            "[628 | 1073.02] loss=2.85 avg=3.00\n",
            "[629 | 1074.50] loss=2.78 avg=3.00\n",
            "[630 | 1075.99] loss=2.68 avg=2.99\n",
            "[631 | 1077.46] loss=2.99 avg=2.99\n",
            "[632 | 1078.95] loss=3.32 avg=3.00\n",
            "[633 | 1080.43] loss=2.72 avg=2.99\n",
            "[634 | 1081.91] loss=3.45 avg=3.00\n",
            "[635 | 1083.39] loss=3.02 avg=3.00\n",
            "[636 | 1084.88] loss=3.16 avg=3.00\n",
            "[637 | 1086.36] loss=3.31 avg=3.00\n",
            "[638 | 1087.85] loss=2.74 avg=3.00\n",
            "[639 | 1089.34] loss=2.87 avg=3.00\n",
            "[640 | 1090.82] loss=3.32 avg=3.00\n",
            "[641 | 1092.31] loss=2.80 avg=3.00\n",
            "[642 | 1093.78] loss=2.88 avg=3.00\n",
            "[643 | 1095.27] loss=3.30 avg=3.00\n",
            "[644 | 1096.76] loss=3.00 avg=3.00\n",
            "[645 | 1098.24] loss=3.25 avg=3.00\n",
            "[646 | 1099.71] loss=3.40 avg=3.01\n",
            "[647 | 1101.19] loss=3.69 avg=3.01\n",
            "[648 | 1102.67] loss=3.40 avg=3.02\n",
            "[649 | 1104.16] loss=3.18 avg=3.02\n",
            "[650 | 1105.64] loss=2.95 avg=3.02\n",
            "[651 | 1107.13] loss=2.73 avg=3.02\n",
            "[652 | 1108.60] loss=3.13 avg=3.02\n",
            "[653 | 1110.09] loss=2.83 avg=3.02\n",
            "[654 | 1111.57] loss=3.30 avg=3.02\n",
            "[655 | 1113.05] loss=2.67 avg=3.02\n",
            "[656 | 1114.53] loss=2.98 avg=3.01\n",
            "[657 | 1116.02] loss=2.95 avg=3.01\n",
            "[658 | 1117.49] loss=2.73 avg=3.01\n",
            "[659 | 1118.98] loss=2.92 avg=3.01\n",
            "[660 | 1120.46] loss=2.97 avg=3.01\n",
            "[661 | 1121.94] loss=2.87 avg=3.01\n",
            "[662 | 1123.42] loss=3.47 avg=3.01\n",
            "[663 | 1124.90] loss=2.94 avg=3.01\n",
            "[664 | 1126.39] loss=2.63 avg=3.01\n",
            "[665 | 1127.87] loss=2.93 avg=3.01\n",
            "[666 | 1129.36] loss=2.98 avg=3.01\n",
            "[667 | 1130.84] loss=3.51 avg=3.01\n",
            "[668 | 1132.32] loss=2.73 avg=3.01\n",
            "[669 | 1133.81] loss=3.00 avg=3.01\n",
            "[670 | 1135.29] loss=2.51 avg=3.00\n",
            "[671 | 1136.77] loss=2.93 avg=3.00\n",
            "[672 | 1138.25] loss=2.79 avg=3.00\n",
            "[673 | 1139.73] loss=2.86 avg=3.00\n",
            "[674 | 1141.22] loss=3.24 avg=3.00\n",
            "[675 | 1142.70] loss=2.70 avg=3.00\n",
            "[676 | 1144.18] loss=2.54 avg=3.00\n",
            "[677 | 1145.67] loss=3.27 avg=3.00\n",
            "[678 | 1147.15] loss=2.51 avg=2.99\n",
            "[679 | 1148.64] loss=2.52 avg=2.99\n",
            "[680 | 1150.11] loss=2.75 avg=2.99\n",
            "[681 | 1151.59] loss=2.38 avg=2.98\n",
            "[682 | 1153.07] loss=2.88 avg=2.98\n",
            "[683 | 1154.55] loss=2.89 avg=2.98\n",
            "[684 | 1156.03] loss=2.75 avg=2.98\n",
            "[685 | 1157.51] loss=3.20 avg=2.98\n",
            "[686 | 1159.00] loss=2.93 avg=2.98\n",
            "[687 | 1160.48] loss=2.94 avg=2.98\n",
            "[688 | 1161.96] loss=2.97 avg=2.98\n",
            "[689 | 1163.45] loss=3.00 avg=2.98\n",
            "[690 | 1164.94] loss=3.43 avg=2.98\n",
            "[691 | 1166.42] loss=3.19 avg=2.98\n",
            "[692 | 1167.90] loss=2.58 avg=2.98\n",
            "[693 | 1169.38] loss=2.48 avg=2.97\n",
            "[694 | 1170.86] loss=3.02 avg=2.98\n",
            "[695 | 1172.35] loss=3.04 avg=2.98\n",
            "[696 | 1173.83] loss=3.34 avg=2.98\n",
            "[697 | 1175.31] loss=2.98 avg=2.98\n",
            "[698 | 1176.80] loss=2.97 avg=2.98\n",
            "[699 | 1178.28] loss=2.61 avg=2.98\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " interroads, it seemed to me somewhat curious and peculiar. The old stone street in the centre was no longer at once full of people, but it became the residence of a race which were a different sort of people. The men were very pale men; the women were very pale women - and in the middle was a great white child; and the child was white. There was a great white face under their heads; and there were very white hands on their shoulders; and the child was smiling. Then I said:\n",
            "\n",
            "\"What is the meaning of that? Is it the expression of the soul of the child, or is it a human expression?\"\n",
            "\n",
            "And there was a great silence.\n",
            "\n",
            "\"Is it the expression of the soul of the child,\" said a man, \"or is it only of the child? That will be better for you to know.\"\n",
            "\n",
            "And he sat down before me, and I asked him:\n",
            "\n",
            "\"Do you think that the child has an immortal soul?\"\n",
            "\n",
            "\"Of course he has,\" he said.\n",
            "\n",
            "A large man, with enormous dark hair which was tied with white threads, with a very large beard, with a very large nose; the men who came to visit me with the children, were very tall men, and at last an old man, he was very old, and very old was the child. He was white and black with a thin beard, and he had a very small chin, and a very long nose - and his mouth was always closed.\n",
            "\n",
            "\"The child has an immortal soul,\" said he, \"and I hope he will live long after you all; for my life will be spent in you, as in a grain of rice from the garden, and you, as part of the grain, as in a grain of corn.\"\n",
            "\n",
            "At the end of four months I had to send a present for a very large white child: and the mother of the little white child gave me a very small white present.\n",
            "\n",
            "When I was going home, I stopped and I asked the old man:\n",
            "\n",
            "\"Do you wish me to eat a piece of bread and a piece of butter? I want to eat something.\"\n",
            "\n",
            "\"No,\" he said; \"now come down and rest; the children are already hungry.\"\n",
            "\n",
            "My wife and I ate bread and butter without rest, and then we became more tired; and she fell asleep.\n",
            "\n",
            "\"I cannot sleep again now,\" said the old man, \"because there is a great hunger to be killed; and it is time I went into the market and killed some people.\"\n",
            "\n",
            "Then he went out into the market and killed many thousand people.\n",
            "\n",
            "One night, the old man went to sleep; and the next morning he came home again and said:\n",
            "\n",
            "\"Well, it is time to kill people again.\"\n",
            "\n",
            "And he ate a little bit of food. They say they do not know where his family are. And the old man fell asleep again, and he died. And the sun became much hotter than it had been when I came to this land.\n",
            "\n",
            "You see, I came to this land by a different path.\n",
            "\n",
            "When they had buried him, they had carried him to great stones, and he had been buried in a grave where I had buried my brother; and some women, men, and children had put food in him -- and he ate it -- and he did not seem to be alive any more; for he was not breathing.\n",
            "\n",
            "Then the man said:\n",
            "\n",
            "\"Why does my food have no breath? Is it death?\"\n",
            "\n",
            "\"He has no mouth,\" said one of the women. \"He has made himself as one of you -- a human being. Take him out; he is dead.\"\n",
            "\n",
            "Then he said to his wife:\n",
            "\n",
            "\"If it is not death, it is nothing. Why should I not eat?\"\n",
            "\n",
            "And she said to him: -- \"If it is not death, it is nothing; because there will be food for you; for a man has the life of a man; just as you have the life of a man; the life of a man, as you are; and the life of what you and this one have made, as you are. Take this man out, and he will have no life.\"\n",
            "\n",
            "And he made himself a small pot, and he put out in it one drop a day, and then he became very thin and thin. And there was nothing to eat; and he slept, and he died, and he made another pot into which the same drop of food was rolled, and he made another, another, and another, and next he died.\n",
            "\n",
            "Next day he rolled out again one drop a day, and then he could not move himself, for he was dead. And he made another pot and rolled out some more one drop of food, and then he did not move. And all day long he died; and at night\n",
            "\n",
            "[700 | 1201.61] loss=2.84 avg=2.97\n",
            "[701 | 1203.08] loss=2.89 avg=2.97\n",
            "[702 | 1204.56] loss=3.07 avg=2.97\n",
            "[703 | 1206.04] loss=2.42 avg=2.97\n",
            "[704 | 1207.52] loss=2.71 avg=2.97\n",
            "[705 | 1209.01] loss=3.30 avg=2.97\n",
            "[706 | 1210.49] loss=2.74 avg=2.97\n",
            "[707 | 1211.98] loss=2.68 avg=2.96\n",
            "[708 | 1213.46] loss=3.41 avg=2.97\n",
            "[709 | 1214.94] loss=2.91 avg=2.97\n",
            "[710 | 1216.42] loss=2.80 avg=2.97\n",
            "[711 | 1217.91] loss=2.42 avg=2.96\n",
            "[712 | 1219.39] loss=2.96 avg=2.96\n",
            "[713 | 1220.87] loss=2.67 avg=2.96\n",
            "[714 | 1222.35] loss=2.32 avg=2.95\n",
            "[715 | 1223.84] loss=3.22 avg=2.95\n",
            "[716 | 1225.32] loss=2.96 avg=2.95\n",
            "[717 | 1226.80] loss=2.48 avg=2.95\n",
            "[718 | 1228.28] loss=2.90 avg=2.95\n",
            "[719 | 1229.77] loss=3.16 avg=2.95\n",
            "[720 | 1231.25] loss=2.64 avg=2.95\n",
            "[721 | 1232.74] loss=2.42 avg=2.94\n",
            "[722 | 1234.22] loss=2.83 avg=2.94\n",
            "[723 | 1235.70] loss=2.54 avg=2.94\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZpo3AqpNBXX",
        "colab_type": "code",
        "outputId": "66898c7f-3b13-49de-f8c4-b02aa6378e58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "## short story training \n",
        "!PYTHONPATH=src ./train.py --dataset /content/all_short_stories.txt --run_name 'all_short_stories' --model_name '117M'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0630 04:21:27.512420 140328742782848 deprecation_wrapper.py:119] From /content/gpt-2/src/model.py:147: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
            "\n",
            "W0630 04:21:27.527328 140328742782848 deprecation_wrapper.py:119] From /content/gpt-2/src/memory_saving_gradients.py:13: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n",
            "W0630 04:21:27.613547 140328742782848 deprecation_wrapper.py:119] From ./train.py:87: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "W0630 04:21:27.613832 140328742782848 deprecation_wrapper.py:119] From ./train.py:90: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "2019-06-30 04:21:27.631013: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-06-30 04:21:27.632788: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x17152c0 executing computations on platform Host. Devices:\n",
            "2019-06-30 04:21:27.632825: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2019-06-30 04:21:27.637794: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\n",
            "2019-06-30 04:21:27.865570: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-30 04:21:27.866072: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1714840 executing computations on platform CUDA. Devices:\n",
            "2019-06-30 04:21:27.866099: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2019-06-30 04:21:27.866305: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-30 04:21:27.866705: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-06-30 04:21:27.873930: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-06-30 04:21:28.044331: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-06-30 04:21:28.121665: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-06-30 04:21:28.146910: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-06-30 04:21:28.340437: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-06-30 04:21:28.442496: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-06-30 04:21:28.785678: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-06-30 04:21:28.785925: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-30 04:21:28.786474: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-30 04:21:28.786843: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
            "2019-06-30 04:21:28.789460: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-06-30 04:21:28.791439: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-06-30 04:21:28.791469: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n",
            "2019-06-30 04:21:28.791479: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n",
            "2019-06-30 04:21:28.797488: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-30 04:21:28.797958: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-30 04:21:28.798373: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14202 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "W0630 04:21:28.799177 140328742782848 deprecation_wrapper.py:119] From ./train.py:91: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0630 04:21:34.244274 140328742782848 deprecation.py:323] From /content/gpt-2/src/sample.py:65: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "W0630 04:21:34.258172 140328742782848 deprecation.py:323] From /content/gpt-2/src/sample.py:16: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0630 04:21:34.259821 140328742782848 deprecation.py:323] From /content/gpt-2/src/sample.py:70: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.random.categorical` instead.\n",
            "W0630 04:21:34.269958 140328742782848 deprecation_wrapper.py:119] From ./train.py:120: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "W0630 04:21:39.993435 140328742782848 deprecation_wrapper.py:119] From ./train.py:143: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n",
            "W0630 04:21:39.996579 140328742782848 deprecation_wrapper.py:119] From ./train.py:146: The name tf.summary.merge is deprecated. Please use tf.compat.v1.summary.merge instead.\n",
            "\n",
            "W0630 04:21:39.997320 140328742782848 deprecation_wrapper.py:119] From ./train.py:148: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
            "\n",
            "W0630 04:21:39.997937 140328742782848 deprecation_wrapper.py:119] From ./train.py:151: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "Loading checkpoint models/117M/model.ckpt\n",
            "W0630 04:21:45.785471 140328742782848 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "Loading dataset...\n",
            "100% 1/1 [00:03<00:00,  3.74s/it]\n",
            "dataset has 531349 tokens\n",
            "Training...\n",
            "2019-06-30 04:21:53.488603: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n",
            "2019-06-30 04:21:53.747114: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "[1 | 4.73] loss=3.91 avg=3.91\n",
            "[2 | 5.20] loss=3.54 avg=3.72\n",
            "[3 | 5.68] loss=3.94 avg=3.79\n",
            "[4 | 6.15] loss=3.35 avg=3.68\n",
            "[5 | 6.62] loss=3.12 avg=3.57\n",
            "[6 | 7.09] loss=3.00 avg=3.47\n",
            "[7 | 7.56] loss=3.88 avg=3.53\n",
            "[8 | 8.04] loss=3.08 avg=3.47\n",
            "[9 | 8.51] loss=3.82 avg=3.51\n",
            "[10 | 8.99] loss=3.62 avg=3.52\n",
            "[11 | 9.46] loss=3.29 avg=3.50\n",
            "[12 | 9.94] loss=3.26 avg=3.48\n",
            "[13 | 10.41] loss=2.99 avg=3.44\n",
            "[14 | 10.89] loss=3.13 avg=3.42\n",
            "[15 | 11.37] loss=3.73 avg=3.44\n",
            "[16 | 11.85] loss=3.32 avg=3.43\n",
            "[17 | 12.33] loss=3.52 avg=3.44\n",
            "[18 | 12.81] loss=3.23 avg=3.42\n",
            "[19 | 13.29] loss=3.38 avg=3.42\n",
            "[20 | 13.77] loss=3.21 avg=3.41\n",
            "[21 | 14.25] loss=3.45 avg=3.41\n",
            "[22 | 14.74] loss=3.20 avg=3.40\n",
            "[23 | 15.22] loss=3.79 avg=3.42\n",
            "[24 | 15.71] loss=3.67 avg=3.43\n",
            "[25 | 16.19] loss=3.51 avg=3.44\n",
            "[26 | 16.68] loss=2.87 avg=3.41\n",
            "[27 | 17.16] loss=3.81 avg=3.43\n",
            "[28 | 17.65] loss=2.93 avg=3.41\n",
            "[29 | 18.13] loss=3.51 avg=3.41\n",
            "[30 | 18.62] loss=4.13 avg=3.44\n",
            "[31 | 19.11] loss=3.81 avg=3.45\n",
            "[32 | 19.60] loss=3.16 avg=3.44\n",
            "[33 | 20.10] loss=3.38 avg=3.44\n",
            "[34 | 20.59] loss=3.34 avg=3.44\n",
            "[35 | 21.08] loss=3.09 avg=3.43\n",
            "[36 | 21.57] loss=3.20 avg=3.42\n",
            "[37 | 22.06] loss=3.61 avg=3.42\n",
            "[38 | 22.56] loss=3.44 avg=3.42\n",
            "[39 | 23.04] loss=3.15 avg=3.42\n",
            "[40 | 23.53] loss=3.60 avg=3.42\n",
            "[41 | 24.02] loss=3.38 avg=3.42\n",
            "[42 | 24.51] loss=3.26 avg=3.42\n",
            "[43 | 25.00] loss=3.73 avg=3.42\n",
            "[44 | 25.49] loss=3.73 avg=3.43\n",
            "[45 | 25.98] loss=3.82 avg=3.44\n",
            "[46 | 26.48] loss=3.20 avg=3.44\n",
            "[47 | 26.97] loss=3.71 avg=3.44\n",
            "[48 | 27.46] loss=2.94 avg=3.43\n",
            "[49 | 27.96] loss=3.75 avg=3.44\n",
            "[50 | 28.45] loss=3.19 avg=3.43\n",
            "[51 | 28.94] loss=3.75 avg=3.44\n",
            "[52 | 29.44] loss=3.66 avg=3.45\n",
            "[53 | 29.94] loss=3.16 avg=3.44\n",
            "[54 | 30.44] loss=2.92 avg=3.43\n",
            "[55 | 30.94] loss=2.98 avg=3.42\n",
            "[56 | 31.44] loss=3.63 avg=3.42\n",
            "[57 | 31.94] loss=3.60 avg=3.43\n",
            "[58 | 32.45] loss=2.70 avg=3.41\n",
            "[59 | 32.95] loss=3.52 avg=3.41\n",
            "[60 | 33.45] loss=3.23 avg=3.41\n",
            "[61 | 33.96] loss=3.19 avg=3.40\n",
            "[62 | 34.47] loss=3.13 avg=3.40\n",
            "[63 | 34.98] loss=3.28 avg=3.39\n",
            "[64 | 35.49] loss=3.24 avg=3.39\n",
            "[65 | 36.00] loss=3.76 avg=3.40\n",
            "[66 | 36.52] loss=3.57 avg=3.40\n",
            "[67 | 37.03] loss=2.77 avg=3.39\n",
            "[68 | 37.54] loss=3.51 avg=3.39\n",
            "[69 | 38.05] loss=3.50 avg=3.39\n",
            "[70 | 38.56] loss=3.67 avg=3.40\n",
            "[71 | 39.07] loss=3.87 avg=3.41\n",
            "[72 | 39.59] loss=3.84 avg=3.42\n",
            "[73 | 40.10] loss=3.15 avg=3.41\n",
            "[74 | 40.61] loss=3.33 avg=3.41\n",
            "[75 | 41.12] loss=3.32 avg=3.41\n",
            "[76 | 41.63] loss=3.89 avg=3.42\n",
            "[77 | 42.15] loss=3.00 avg=3.41\n",
            "[78 | 42.67] loss=3.21 avg=3.41\n",
            "[79 | 43.19] loss=3.01 avg=3.40\n",
            "[80 | 43.71] loss=3.32 avg=3.40\n",
            "[81 | 44.22] loss=3.22 avg=3.39\n",
            "[82 | 44.74] loss=4.42 avg=3.41\n",
            "[83 | 45.26] loss=3.76 avg=3.42\n",
            "[84 | 45.79] loss=3.57 avg=3.42\n",
            "[85 | 46.30] loss=3.65 avg=3.43\n",
            "[86 | 46.83] loss=3.87 avg=3.43\n",
            "[87 | 47.36] loss=3.68 avg=3.44\n",
            "[88 | 47.89] loss=2.88 avg=3.43\n",
            "[89 | 48.42] loss=3.79 avg=3.43\n",
            "[90 | 48.95] loss=3.56 avg=3.44\n",
            "[91 | 49.47] loss=2.88 avg=3.43\n",
            "[92 | 50.00] loss=3.73 avg=3.43\n",
            "[93 | 50.53] loss=3.11 avg=3.43\n",
            "[94 | 51.06] loss=3.80 avg=3.43\n",
            "[95 | 51.59] loss=3.32 avg=3.43\n",
            "[96 | 52.12] loss=3.99 avg=3.44\n",
            "[97 | 52.65] loss=3.19 avg=3.44\n",
            "[98 | 53.18] loss=3.09 avg=3.43\n",
            "[99 | 53.71] loss=3.63 avg=3.43\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "); but he went on to play in the Eastern Conference. He made the Western Conference regular season roster in 1981, but he was recalled to the club's regular season in 1992 just three years later when he failed to make the team in a game in Montreal.\n",
            "\n",
            "\"I've always loved the sport,\" said Moustakas, who made a point of being always the lead vocalist in the band when he was playing. \"I've played drums and guitar, and I played in the United States. I was a bit disinterested in the sport because it was my favorite thing to do.\"\n",
            "\n",
            "When Moustakas heard of the new contract they had not received from the Montreal Canadiens it changed his life.\n",
            "\n",
            "\"Everybody in Montreal was on the line to have a contract,\" Moustakas said. \"I've made three good years. In my experience, nothing else mattered.\n",
            "\n",
            "\"When you're here, you've got your chance, you know what I mean. But the truth of the matter is, I didn't get it.\"\n",
            "\n",
            "Moustakas is still playing the guitar and drums, but he has become an influential voice for the Canadiens' new, more popular band.\n",
            "\n",
            "\"It's different playing the drums. The first time I played drums in Montreal had come with some other band I'd been playing. That was when I was signed by the Canadiens (in March, 1982), and we played the National Hockey League game in Canada, where I was playing.\"\n",
            "\n",
            "He knew his next band, the Canadiens, so it wasn't a surprise to him to see what the three years would do to them. \"We were going to sign one another. It'd be great. You'd make it a better group if that happened.\"\n",
            "\n",
            "Moustakas had the opportunity to go up against some of the greatest music in world history when the Canadiens entered the regular season with a 4-10-1 record. That was during one of the best stretches in the league, with an overall lead that extended to seven games.\n",
            "\n",
            "In that situation, Montreal's captain Jean-Gabriel Page had just been killed in a vehicle accident in an ice hockey accident while driving home from an evening concert at a hotel.\n",
            "\n",
            "\"It was great hearing the music during the game,\" Moustakas recalled.\n",
            "\n",
            "Page was rushed to hospital as soon as the game resumed with the Canadiens, but Moustakas got some bad news. The news was bad enough. He would miss the next 24 game, which the Canadiens went on to win. This time, he was back in New York, a day after finishing the season as the player leading scorer in the AHL.\n",
            "\n",
            "Page had a couple of bad nights in Philadelphia, which forced him to sign with the Flyers. He also missed three games with a lower-body injury, and it got to have an effect as well.\n",
            "\n",
            "On April 28 that year he was signed by the Canadiens, and the next day he was in Philadelphia to be the starting defenseman. On May 31 against Montreal, he was called up in the second game, and he became the first player to make 53 saves when he finished the season with a 2-6-2 record and a six-1-5 record over seven playoff games. Moustakas came back to his old club, but the club went 4-11-2 and was at home on a two-game series that lasted six games.\n",
            "\n",
            "There had been rumors that Moustakas was suffering from leukemia, but that didn't stop him from playing. He would always go to the hospital with his two younger sisters, and he would walk to a dentist for those early stages of chemotherapy treatments that his family hadn't considered.\n",
            "\n",
            "\"It was pretty special,\" said Moustakas. \"When I saw him in the dressing room, my heart was beating so good. I just wished he was well tomorrow.\"\n",
            "\n",
            "When they decided to part ways he would go to Buffalo, where he had just earned his first regular season cap and a contract to keep the team. He had a lot of friends back home. His mother was a nurse at Buffalo Children's Hospital, and he attended some school games there. He played on the first team during his playing days and on the third, when he was a student there playing in a game of men's lacrosse. He played on the first ever national championship team and the first-ever hockey tournament.\n",
            "\n",
            "One day at his home in Buffalo, he told his parents that he was looking forward to a lot.\n",
            "\n",
            "\"It was pretty important in a couple of ways,\" says Page. \"My family said I must go. Everybody wanted a part of it. Everybody wanted to be a part of it. So I went back to that place. It was all good to know they were still with us, and they had a lot of good wishes for me.\"\n",
            "\n",
            "Moustas grew up on the West Coast, and in\n",
            "\n",
            "[100 | 66.31] loss=3.26 avg=3.43\n",
            "[101 | 66.85] loss=3.54 avg=3.43\n",
            "[102 | 67.39] loss=3.54 avg=3.43\n",
            "[103 | 67.92] loss=3.75 avg=3.44\n",
            "[104 | 68.45] loss=3.08 avg=3.43\n",
            "[105 | 68.98] loss=3.58 avg=3.44\n",
            "[106 | 69.51] loss=2.93 avg=3.43\n",
            "[107 | 70.04] loss=3.66 avg=3.43\n",
            "[108 | 70.57] loss=3.99 avg=3.44\n",
            "[109 | 71.10] loss=3.20 avg=3.44\n",
            "[110 | 71.62] loss=3.16 avg=3.43\n",
            "[111 | 72.15] loss=3.55 avg=3.43\n",
            "[112 | 72.68] loss=2.98 avg=3.43\n",
            "[113 | 73.21] loss=3.88 avg=3.43\n",
            "[114 | 73.74] loss=3.04 avg=3.43\n",
            "[115 | 74.27] loss=3.21 avg=3.43\n",
            "[116 | 74.80] loss=3.06 avg=3.42\n",
            "[117 | 75.32] loss=3.23 avg=3.42\n",
            "[118 | 75.84] loss=3.01 avg=3.41\n",
            "[119 | 76.37] loss=3.63 avg=3.41\n",
            "[120 | 76.90] loss=4.01 avg=3.42\n",
            "[121 | 77.42] loss=3.53 avg=3.42\n",
            "[122 | 77.94] loss=3.47 avg=3.43\n",
            "[123 | 78.47] loss=3.48 avg=3.43\n",
            "[124 | 78.99] loss=3.53 avg=3.43\n",
            "[125 | 79.51] loss=3.61 avg=3.43\n",
            "[126 | 80.03] loss=3.08 avg=3.43\n",
            "[127 | 80.55] loss=3.22 avg=3.42\n",
            "[128 | 81.07] loss=3.52 avg=3.42\n",
            "[129 | 81.59] loss=3.56 avg=3.43\n",
            "[130 | 82.11] loss=3.30 avg=3.42\n",
            "[131 | 82.62] loss=3.59 avg=3.43\n",
            "[132 | 83.14] loss=3.59 avg=3.43\n",
            "[133 | 83.65] loss=3.48 avg=3.43\n",
            "[134 | 84.17] loss=3.19 avg=3.43\n",
            "[135 | 84.68] loss=3.86 avg=3.43\n",
            "[136 | 85.19] loss=3.26 avg=3.43\n",
            "[137 | 85.70] loss=2.84 avg=3.42\n",
            "[138 | 86.21] loss=3.71 avg=3.43\n",
            "[139 | 86.72] loss=3.22 avg=3.42\n",
            "[140 | 87.23] loss=2.76 avg=3.41\n",
            "[141 | 87.74] loss=4.23 avg=3.42\n",
            "[142 | 88.25] loss=2.99 avg=3.42\n",
            "[143 | 88.76] loss=3.53 avg=3.42\n",
            "[144 | 89.27] loss=3.67 avg=3.42\n",
            "[145 | 89.78] loss=3.94 avg=3.43\n",
            "[146 | 90.29] loss=3.69 avg=3.43\n",
            "[147 | 90.80] loss=3.29 avg=3.43\n",
            "[148 | 91.31] loss=3.60 avg=3.43\n",
            "[149 | 91.82] loss=3.44 avg=3.43\n",
            "[150 | 92.33] loss=3.64 avg=3.44\n",
            "[151 | 92.84] loss=3.69 avg=3.44\n",
            "[152 | 93.35] loss=3.67 avg=3.44\n",
            "[153 | 93.87] loss=3.10 avg=3.44\n",
            "[154 | 94.37] loss=3.53 avg=3.44\n",
            "[155 | 94.88] loss=3.28 avg=3.44\n",
            "[156 | 95.39] loss=3.12 avg=3.43\n",
            "[157 | 95.90] loss=3.31 avg=3.43\n",
            "[158 | 96.41] loss=3.58 avg=3.43\n",
            "[159 | 96.92] loss=4.24 avg=3.44\n",
            "[160 | 97.43] loss=3.15 avg=3.44\n",
            "[161 | 97.94] loss=3.54 avg=3.44\n",
            "[162 | 98.45] loss=3.13 avg=3.44\n",
            "[163 | 98.96] loss=3.83 avg=3.44\n",
            "[164 | 99.47] loss=3.74 avg=3.45\n",
            "[165 | 99.98] loss=2.90 avg=3.44\n",
            "[166 | 100.49] loss=3.29 avg=3.44\n",
            "[167 | 101.00] loss=3.72 avg=3.44\n",
            "[168 | 101.51] loss=3.33 avg=3.44\n",
            "[169 | 102.01] loss=3.70 avg=3.44\n",
            "[170 | 102.51] loss=3.05 avg=3.44\n",
            "[171 | 103.02] loss=3.25 avg=3.44\n",
            "[172 | 103.53] loss=3.57 avg=3.44\n",
            "[173 | 104.03] loss=3.97 avg=3.44\n",
            "[174 | 104.53] loss=3.81 avg=3.45\n",
            "[175 | 105.04] loss=3.54 avg=3.45\n",
            "[176 | 105.55] loss=3.04 avg=3.44\n",
            "[177 | 106.06] loss=3.17 avg=3.44\n",
            "[178 | 106.57] loss=3.58 avg=3.44\n",
            "[179 | 107.07] loss=3.41 avg=3.44\n",
            "[180 | 107.58] loss=4.04 avg=3.45\n",
            "[181 | 108.08] loss=3.20 avg=3.45\n",
            "[182 | 108.59] loss=3.95 avg=3.45\n",
            "[183 | 109.10] loss=3.07 avg=3.45\n",
            "[184 | 109.61] loss=3.73 avg=3.45\n",
            "[185 | 110.12] loss=3.12 avg=3.45\n",
            "[186 | 110.62] loss=3.09 avg=3.44\n",
            "[187 | 111.12] loss=3.68 avg=3.45\n",
            "[188 | 111.63] loss=3.58 avg=3.45\n",
            "[189 | 112.14] loss=3.25 avg=3.45\n",
            "[190 | 112.65] loss=3.11 avg=3.44\n",
            "[191 | 113.16] loss=3.34 avg=3.44\n",
            "[192 | 113.66] loss=3.76 avg=3.44\n",
            "[193 | 114.17] loss=2.68 avg=3.44\n",
            "[194 | 114.68] loss=3.28 avg=3.43\n",
            "[195 | 115.19] loss=3.91 avg=3.44\n",
            "[196 | 115.70] loss=3.69 avg=3.44\n",
            "[197 | 116.21] loss=3.10 avg=3.44\n",
            "[198 | 116.72] loss=3.69 avg=3.44\n",
            "[199 | 117.23] loss=3.17 avg=3.44\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " trench is as big a place as it's ever been; a dark hill, or in the town of Blossomsbury.\n",
            "\n",
            "\"And as I lie at peace, the man who came to me now, who was his best friend, says, 'I see I see that his soul has an evil eye,' but I say, 'He has an evil eye, and he is a bad man.'\"\n",
            "\n",
            "\"He shall live,\" said the old man, \"and in him will come the good heart that loves me, and in me the soul of his heart,\" whereupon he said, in the most mildest manner, \"Let this man go; he hath an evil eye; but he who knows who it is that fears him is going to give it to him; for he would have it to himself.\"\n",
            "\n",
            "But there was, in the course of the next hour, the appearance of a tall, stout man on a black silk coat and breeches. His head was very round, and he was about eleven. He was an upright man, and a sharp tongue. He was wearing a very coarse coat, and his black trousers were as smooth as cotton, and he had a black silk scarf about his neck and shoulders, and a silk clasp about his waist.\n",
            "\n",
            "\"I've been a long time an admirer of his,\" said the man; \"and yet I cannot tell you of all I've said for his sake, or of any person but myself, that I am so un-advised.\"\n",
            "\n",
            "He looked very kindly upon the old gentleman, and said, smiling very little, \"Now, Mr. Murnane, and your own good affection, we have here a very interesting report, very interesting for you. You can imagine how much time has passed since you first met him on the spot, and how dear was the little girl you called 'Saffron,' and how much sorrow you had suffered when she first arrived at us. You were a young friend and old-man, and your mind would be set on any account of the death of your best friend.'\n",
            "\n",
            "\"And we were very much surprised and alarmed in the first place to know that a large, stout man in a gold cap and a grey hat, like yours, would be in possession of your old friends.\n",
            "\n",
            "\"Well, it must be that your friend's soul has an evil eye; so I say, 'I understand that his soul loves you, and in him will come the good heart that loves me, and in me the soul of his heart,' which life itself has come to love you.\n",
            "\n",
            "\"And when it was well understood that he had good eyes and good ears, he sat down and said, in a tone which I do not understand, 'That is very much like the old man; and I think I must go,' and the old man did as he had done, and began saying things which he did not speak of, and this he did not say: 'Here is what I have just said.' He had done this little conversation, for it was the only thing which he had said on that occasion.\n",
            "\n",
            "\"To all our friends and friends whom your kindness has so pleased.\n",
            "\n",
            "\"And the old man was a very brave man, and he, after having been carried down as a prisoner to the brig, had the privilege of going out, and I will tell the story of how he got in his boat as you have thought proper, and did so.\n",
            "\n",
            "\"Mr. Murnane asked him, \"What sort of ship you took in the first place?\" He replied, \"We took a boat that had been sold on Mr. Murnane's side on August 23rd.\"\n",
            "\n",
            "\"No,\" said the old man; \"I did not expect any such sort of company; I suppose that it was one of the most wretched ships; it would seem that you were afraid of the danger, because there was a very large merchant ship at your side, along with the old man. It must have been your ship all the time.\n",
            "\n",
            "\"And the ship you called 'Saffron,' when it arrived a few days before, and which had been sold to you on the spot, went to your little friend, and when the old merchantman had said how much sorrow you had suffered when he first heard of her arrival, he said to you, \"That is my friend, and his soul is my friend, and he has been an admiral and admiral's cousin and admiral's wife, and he has fought two wars against the Spaniards, and has taken a ship all the way from Spain on his behalf, and where he had a wife, but has never been able to get them; and in the course of his journey he went to the Spanish town and killed two Spaniards.\n",
            "\n",
            "\"And it was this that I must tell you about. It was the captain of the vessel, whose name was Pia, which was sold to\n",
            "\n",
            "[200 | 128.59] loss=3.09 avg=3.43\n",
            "[201 | 129.10] loss=3.84 avg=3.44\n",
            "[202 | 129.61] loss=3.09 avg=3.43\n",
            "[203 | 130.12] loss=3.56 avg=3.44\n",
            "[204 | 130.64] loss=3.52 avg=3.44\n",
            "[205 | 131.14] loss=3.31 avg=3.44\n",
            "[206 | 131.65] loss=3.83 avg=3.44\n",
            "[207 | 132.16] loss=2.99 avg=3.43\n",
            "[208 | 132.67] loss=3.49 avg=3.44\n",
            "[209 | 133.18] loss=3.13 avg=3.43\n",
            "[210 | 133.69] loss=3.27 avg=3.43\n",
            "[211 | 134.21] loss=3.54 avg=3.43\n",
            "[212 | 134.73] loss=2.88 avg=3.42\n",
            "[213 | 135.24] loss=3.64 avg=3.43\n",
            "[214 | 135.75] loss=3.04 avg=3.42\n",
            "[215 | 136.26] loss=3.51 avg=3.42\n",
            "[216 | 136.78] loss=3.24 avg=3.42\n",
            "[217 | 137.29] loss=3.85 avg=3.43\n",
            "[218 | 137.81] loss=3.09 avg=3.42\n",
            "[219 | 138.32] loss=3.74 avg=3.43\n",
            "[220 | 138.83] loss=2.98 avg=3.42\n",
            "[221 | 139.35] loss=3.53 avg=3.42\n",
            "[222 | 139.86] loss=3.21 avg=3.42\n",
            "[223 | 140.37] loss=4.06 avg=3.43\n",
            "[224 | 140.89] loss=3.34 avg=3.43\n",
            "[225 | 141.41] loss=3.16 avg=3.42\n",
            "[226 | 141.92] loss=3.30 avg=3.42\n",
            "[227 | 142.43] loss=3.99 avg=3.43\n",
            "[228 | 142.95] loss=3.58 avg=3.43\n",
            "[229 | 143.47] loss=3.02 avg=3.43\n",
            "[230 | 143.98] loss=3.58 avg=3.43\n",
            "[231 | 144.50] loss=3.68 avg=3.43\n",
            "[232 | 145.02] loss=3.49 avg=3.43\n",
            "[233 | 145.54] loss=3.19 avg=3.43\n",
            "[234 | 146.05] loss=3.31 avg=3.43\n",
            "[235 | 146.56] loss=3.14 avg=3.42\n",
            "[236 | 147.07] loss=3.10 avg=3.42\n",
            "[237 | 147.59] loss=3.33 avg=3.42\n",
            "[238 | 148.11] loss=4.01 avg=3.43\n",
            "[239 | 148.62] loss=3.34 avg=3.42\n",
            "[240 | 149.14] loss=3.19 avg=3.42\n",
            "[241 | 149.66] loss=3.59 avg=3.42\n",
            "[242 | 150.17] loss=3.63 avg=3.43\n",
            "[243 | 150.69] loss=3.30 avg=3.42\n",
            "[244 | 151.21] loss=3.36 avg=3.42\n",
            "[245 | 151.72] loss=3.99 avg=3.43\n",
            "[246 | 152.24] loss=4.21 avg=3.44\n",
            "[247 | 152.77] loss=3.38 avg=3.44\n",
            "[248 | 153.28] loss=3.49 avg=3.44\n",
            "[249 | 153.81] loss=3.53 avg=3.44\n",
            "[250 | 154.32] loss=2.94 avg=3.43\n",
            "[251 | 154.85] loss=3.25 avg=3.43\n",
            "[252 | 155.36] loss=3.70 avg=3.44\n",
            "[253 | 155.88] loss=3.26 avg=3.43\n",
            "[254 | 156.40] loss=3.62 avg=3.44\n",
            "[255 | 156.92] loss=3.14 avg=3.43\n",
            "[256 | 157.44] loss=3.22 avg=3.43\n",
            "[257 | 157.95] loss=3.09 avg=3.43\n",
            "[258 | 158.46] loss=3.17 avg=3.42\n",
            "[259 | 158.98] loss=3.23 avg=3.42\n",
            "[260 | 159.50] loss=3.51 avg=3.42\n",
            "[261 | 160.02] loss=3.56 avg=3.42\n",
            "[262 | 160.53] loss=3.52 avg=3.42\n",
            "[263 | 161.05] loss=4.01 avg=3.43\n",
            "[264 | 161.56] loss=3.36 avg=3.43\n",
            "[265 | 162.07] loss=3.63 avg=3.43\n",
            "[266 | 162.59] loss=3.58 avg=3.43\n",
            "[267 | 163.11] loss=3.39 avg=3.43\n",
            "[268 | 163.62] loss=3.46 avg=3.43\n",
            "[269 | 164.13] loss=2.80 avg=3.43\n",
            "[270 | 164.65] loss=3.44 avg=3.43\n",
            "[271 | 165.17] loss=3.08 avg=3.42\n",
            "[272 | 165.68] loss=3.50 avg=3.42\n",
            "[273 | 166.19] loss=3.77 avg=3.43\n",
            "[274 | 166.71] loss=3.46 avg=3.43\n",
            "[275 | 167.23] loss=2.85 avg=3.42\n",
            "[276 | 167.74] loss=3.84 avg=3.43\n",
            "[277 | 168.25] loss=3.16 avg=3.42\n",
            "[278 | 168.76] loss=3.21 avg=3.42\n",
            "[279 | 169.28] loss=3.01 avg=3.42\n",
            "[280 | 169.79] loss=2.99 avg=3.41\n",
            "[281 | 170.30] loss=3.10 avg=3.41\n",
            "[282 | 170.81] loss=3.20 avg=3.41\n",
            "[283 | 171.33] loss=3.43 avg=3.41\n",
            "[284 | 171.85] loss=3.05 avg=3.40\n",
            "[285 | 172.36] loss=3.86 avg=3.41\n",
            "[286 | 172.88] loss=2.87 avg=3.40\n",
            "[287 | 173.40] loss=3.72 avg=3.41\n",
            "[288 | 173.91] loss=2.93 avg=3.40\n",
            "[289 | 174.43] loss=3.06 avg=3.40\n",
            "[290 | 174.94] loss=3.03 avg=3.39\n",
            "[291 | 175.46] loss=3.48 avg=3.39\n",
            "[292 | 175.97] loss=3.91 avg=3.40\n",
            "[293 | 176.48] loss=4.01 avg=3.41\n",
            "[294 | 176.99] loss=3.76 avg=3.41\n",
            "[295 | 177.50] loss=3.21 avg=3.41\n",
            "[296 | 178.01] loss=3.29 avg=3.41\n",
            "[297 | 178.53] loss=3.28 avg=3.41\n",
            "[298 | 179.04] loss=2.48 avg=3.40\n",
            "[299 | 179.55] loss=3.75 avg=3.40\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            ", the men were not far. There were two more, and we all turned our attention to the next. We entered that house from the street, which we had seen before, and we came into the house. The room was furnished with a stone wall, a table, a large wall-chair, a door, and two small window piers. That was also the room of the one of the women, whom we had met while we were sleeping. She was about seventeen feet tall, and standing about fifty with a pale face and a thin build, with brown eyes and short, long hair, with dark scarlet hair. As we passed the room she began to stir. She seemed to be in pain, and she turned and drew her knees together toward us, while she began to cry out upon her little finger. The tears were thick, and she started to cry again. As we passed the room she started to cry. We knew that she was suffering from a fever, and when we saw her it seemed to us that she was no longer in such pain, even in a very small way. There was no pain. There we looked. There were blood, and there was an old-smelling plaster, which was now dead, and there was a white staining, which we saw the following morning, and which was still fresh today. There was another old-smelling plaster, and we looked. There was even a glass-spiked door, which was already dark, and there came from us a very pale face which we did not see before. But the same old-smelling plaster which we see now, was old, and it was standing upon a tall, slender, broad-brimmed floor, which stood about five feet high. It rested on a great, bare, open-misted stone wall, which was also covered with a thin, pale, blotchy layer of stone. This was the door which the woman had gone into, and it was very thick, and it was in such a state, that there was as much danger of going into the room as of being in the room. There was blood and a large hole in a small door-post; the old-smelling plaster was white, and there was blood all over. Then there was a bloodstained lamp, which had not died of old age, but when some men were sleeping they would wake a little, and when we went in there we saw blood all over. Then we saw that the room where we had come into it had been covered with a great white spot, which was still, even now, in such a condition that it remained in such a state. And the woman had gone into that room without knowing her condition; but that was no time to lie with her, because our eyes saw every thing, and the light of our eyes was very strong. There were also black staining on her fingers. The old-smilling plaster was still living at the moment when the old-smelling plaster was alive. Then there was a great hole in the wall-post, and there were many blood stains; there was also blood all over the roof-pits, but we did not look on them, lest in our haste to see and see these things we might be killed. All these things we had seen of those who had been taken away, and we were too alarmed to do anything. It was clear that when the woman went into the room she had been killed, and this is what happened: There were some persons lying there, and we saw them, and they appeared to us very pale, pale, and then they began to cough. We did not look at them for many minutes, because we thought that they would vanish, but they were still growing pale and faint, as we had seen before. There were some other persons lying there, and we did not see any. Then there was another old-smelling plaster, which was standing there on a tall, slender, stout, broad-brimmed table, which lay, even now, on a tall, broad-brimmed, large, wide floor. There were very pale faces and white, blotchy skin, and there was a hole in a small, dark-colored wall-post, and a great hole in the wall-post. This was an old-smelling plaster, and it was standing on a tall, broad-brimmed, large, wide-brimmed table. There was a hole in the wall-post into which there came, from one of the old-smilling plaster. There was a blotchy, white, blotchy skin and a hole in the wall-post; the old-smilling plaster was white, as we had seen; there was also a blotchy spot all over the roof-pits. Then there appeared to us two young men who were lying down in a very slight position on the table, and they were evidently crying, crying and shivering; but we did not look at them, because we\n",
            "\n",
            "[300 | 190.83] loss=2.63 avg=3.39\n",
            "[301 | 191.35] loss=3.35 avg=3.39\n",
            "[302 | 191.87] loss=3.73 avg=3.39\n",
            "[303 | 192.39] loss=3.67 avg=3.40\n",
            "[304 | 192.90] loss=3.63 avg=3.40\n",
            "[305 | 193.42] loss=3.21 avg=3.40\n",
            "[306 | 193.93] loss=3.77 avg=3.40\n",
            "[307 | 194.43] loss=4.23 avg=3.41\n",
            "[308 | 194.94] loss=3.12 avg=3.41\n",
            "[309 | 195.46] loss=3.34 avg=3.41\n",
            "[310 | 195.98] loss=3.13 avg=3.40\n",
            "[311 | 196.49] loss=2.68 avg=3.40\n",
            "[312 | 197.00] loss=3.91 avg=3.40\n",
            "[313 | 197.51] loss=4.03 avg=3.41\n",
            "[314 | 198.02] loss=3.71 avg=3.41\n",
            "[315 | 198.53] loss=2.96 avg=3.41\n",
            "[316 | 199.04] loss=3.35 avg=3.41\n",
            "[317 | 199.55] loss=3.50 avg=3.41\n",
            "[318 | 200.06] loss=3.55 avg=3.41\n",
            "[319 | 200.57] loss=3.17 avg=3.41\n",
            "[320 | 201.07] loss=3.52 avg=3.41\n",
            "[321 | 201.58] loss=3.58 avg=3.41\n",
            "[322 | 202.09] loss=3.35 avg=3.41\n",
            "[323 | 202.60] loss=3.32 avg=3.41\n",
            "[324 | 203.11] loss=3.16 avg=3.40\n",
            "[325 | 203.63] loss=3.86 avg=3.41\n",
            "[326 | 204.15] loss=3.82 avg=3.41\n",
            "[327 | 204.66] loss=3.53 avg=3.41\n",
            "[328 | 205.17] loss=3.72 avg=3.42\n",
            "[329 | 205.68] loss=3.66 avg=3.42\n",
            "[330 | 206.19] loss=3.75 avg=3.42\n",
            "[331 | 206.70] loss=3.50 avg=3.42\n",
            "[332 | 207.21] loss=3.42 avg=3.42\n",
            "[333 | 207.72] loss=3.48 avg=3.43\n",
            "[334 | 208.23] loss=3.06 avg=3.42\n",
            "[335 | 208.74] loss=3.66 avg=3.42\n",
            "[336 | 209.25] loss=3.31 avg=3.42\n",
            "[337 | 209.76] loss=3.51 avg=3.42\n",
            "[338 | 210.27] loss=3.03 avg=3.42\n",
            "[339 | 210.78] loss=3.06 avg=3.42\n",
            "[340 | 211.29] loss=3.67 avg=3.42\n",
            "[341 | 211.80] loss=3.07 avg=3.41\n",
            "[342 | 212.31] loss=3.20 avg=3.41\n",
            "[343 | 212.82] loss=3.69 avg=3.42\n",
            "[344 | 213.33] loss=3.62 avg=3.42\n",
            "[345 | 213.84] loss=3.30 avg=3.42\n",
            "[346 | 214.35] loss=3.01 avg=3.41\n",
            "[347 | 214.86] loss=3.00 avg=3.41\n",
            "[348 | 215.37] loss=3.71 avg=3.41\n",
            "[349 | 215.88] loss=3.60 avg=3.41\n",
            "[350 | 216.40] loss=3.43 avg=3.41\n",
            "[351 | 216.91] loss=3.82 avg=3.42\n",
            "[352 | 217.43] loss=3.10 avg=3.41\n",
            "[353 | 217.94] loss=3.07 avg=3.41\n",
            "[354 | 218.44] loss=3.84 avg=3.42\n",
            "[355 | 218.96] loss=3.39 avg=3.41\n",
            "[356 | 219.46] loss=3.35 avg=3.41\n",
            "[357 | 219.98] loss=3.06 avg=3.41\n",
            "[358 | 220.48] loss=3.24 avg=3.41\n",
            "[359 | 220.99] loss=4.03 avg=3.42\n",
            "[360 | 221.50] loss=3.44 avg=3.42\n",
            "[361 | 222.01] loss=3.92 avg=3.42\n",
            "[362 | 222.52] loss=3.50 avg=3.42\n",
            "[363 | 223.03] loss=2.99 avg=3.42\n",
            "[364 | 223.54] loss=3.64 avg=3.42\n",
            "[365 | 224.05] loss=3.69 avg=3.42\n",
            "[366 | 224.56] loss=3.50 avg=3.42\n",
            "[367 | 225.08] loss=3.08 avg=3.42\n",
            "[368 | 225.58] loss=3.79 avg=3.42\n",
            "[369 | 226.09] loss=3.55 avg=3.42\n",
            "[370 | 226.60] loss=2.75 avg=3.42\n",
            "[371 | 227.11] loss=3.21 avg=3.42\n",
            "[372 | 227.62] loss=3.42 avg=3.42\n",
            "[373 | 228.13] loss=3.56 avg=3.42\n",
            "[374 | 228.64] loss=3.68 avg=3.42\n",
            "[375 | 229.15] loss=3.56 avg=3.42\n",
            "[376 | 229.66] loss=3.82 avg=3.43\n",
            "[377 | 230.18] loss=3.68 avg=3.43\n",
            "[378 | 230.70] loss=3.30 avg=3.43\n",
            "[379 | 231.21] loss=3.61 avg=3.43\n",
            "[380 | 231.72] loss=3.32 avg=3.43\n",
            "[381 | 232.23] loss=3.71 avg=3.43\n",
            "[382 | 232.74] loss=3.82 avg=3.43\n",
            "[383 | 233.26] loss=3.28 avg=3.43\n",
            "[384 | 233.78] loss=3.44 avg=3.43\n",
            "[385 | 234.29] loss=3.01 avg=3.43\n",
            "[386 | 234.80] loss=3.59 avg=3.43\n",
            "[387 | 235.31] loss=2.92 avg=3.42\n",
            "[388 | 235.82] loss=3.46 avg=3.43\n",
            "[389 | 236.33] loss=2.93 avg=3.42\n",
            "[390 | 236.84] loss=3.83 avg=3.42\n",
            "[391 | 237.35] loss=4.09 avg=3.43\n",
            "[392 | 237.86] loss=2.84 avg=3.43\n",
            "[393 | 238.38] loss=3.43 avg=3.43\n",
            "[394 | 238.89] loss=3.38 avg=3.42\n",
            "[395 | 239.41] loss=3.51 avg=3.43\n",
            "[396 | 239.93] loss=2.96 avg=3.42\n",
            "[397 | 240.44] loss=3.89 avg=3.43\n",
            "[398 | 240.95] loss=3.23 avg=3.42\n",
            "[399 | 241.46] loss=3.39 avg=3.42\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " real-deal and a big deal, I can just move on.\n",
            "\n",
            "We get to dinner but we quickly go out and we go a little too. We're back to a quiet place, where we are busy talking but really in the dark room and I can't see the door. The boy has the gun and is about to draw a breath and he says -\n",
            "\n",
            "\"There!\" And he starts in a very loud voice, and, as it were, he doesn't answer the door.\n",
            "\n",
            "After dinner, we take a ride on the express train to the station.\n",
            "\n",
            "We're in the back of the carriage, waiting on station carriages. The other passengers are sitting on the platform, and there's a heavy car being driven in front; a white cabs are in the passenger seat, while a woman in a top hat lies by the door.\n",
            "\n",
            "There is a big, white car parked on the platform. The engine is off, and we get to a little cab-house. We wait. There's another cab-house with a very white lady who is sitting on the white cab-door opposite us, and the driver of the passenger car is an old man of good standing.\n",
            "\n",
            "Finally we get downstairs and take the train home with us.\n",
            "\n",
            "As far as the other passengers are concerned, it's not the police station. There is a hotel in the neighborhood now. We are waiting for a coach to arrive.\n",
            "\n",
            "We get back up, walk back to the station and we take off our luggage. The coach goes slowly down the street. We are going through the dark. There's another red car, this one of white, and this one of brown. Everybody's sitting still in the train, and one of them is sitting looking off at the sky. The coach is on the other side of the room. Nobody knows where we are. The coach comes into a corner at a certain pace. The coach is very slow.\n",
            "\n",
            "We get back to the hotel, and after a minute or two we are sitting at an empty table, which is about half empty. It is very cold. A few seats away, there is a white woman sitting on a chair in front of the white cabs in the cab-house. And two white men with yellow hair stand in front of it, but they are waiting impatiently for a chance to take a nap. The cab comes down and there are two white ladies sitting on the table next to everybody. And one is talking very ruefully, as it is not good to say to him and everybody.\n",
            "\n",
            "We sit and watch the train with a certain amount of curiosity.\n",
            "\n",
            "We can't believe our eyes and our thoughts. The train has stopped after a little time and the two white men have gone out of here. But they look at us and they don't take us seriously and they are saying things which we do not understand.\n",
            "\n",
            "So then we have to go and wait for it. There are people waiting out the door waiting and they don't notice us going in. But they do notice us and they ask us where we are and they are waiting for us.\n",
            "\n",
            "We go and wait, for an hour. We listen, but nobody is listening to us. Then they say to one another,\n",
            "\n",
            "\"Good morning. It's been seven hours since we went into the hotel, but we are still waiting. Please excuse us for this. We didn't see the hotel when you came here.\"\n",
            "\n",
            "They say to another,\n",
            "\n",
            "\"Yes. Can we get to the hotel and wait there?\"\n",
            "\n",
            "\"Yes.\"\n",
            "\n",
            "\"What do you mean?\"\n",
            "\n",
            "\"My friend came here for a dinner.\" A second later, some of the other passengers began to tell the story of how you came here. The waiter says:\n",
            "\n",
            "\"Good morning, Mr. and Mrs. Lark. I never came here for some reason. I came here to buy a wine, because I didn't want to spend a second's pay on a wine. I've got a bottle of wine in my back pocket but I'm not making any money and I don't have a penny left to spend on the wine. What was the reason you came here?\"\n",
            "\n",
            "All of us said something like this. But when we got to the hotel we found that there was an empty table in front of everybody's seat. We were sitting in a corner trying to get some food.\n",
            "\n",
            "The waiter says:\n",
            "\n",
            "\"I've been in the hotel for two days but I'm not making any money.\"\n",
            "\n",
            "Then he says:\n",
            "\n",
            "\"Well, it's a good deal for you. I'm glad to have lunch here today, just in case something happens.\"\n",
            "\n",
            "We sit as long as we can.\n",
            "\n",
            "We wait until the end of the three-minute silence at the hotel for Lark to come to us. But there is a great deal of silence in Lark's room.\n",
            "\n",
            "[400 | 253.00] loss=3.74 avg=3.43\n",
            "[401 | 253.51] loss=2.90 avg=3.42\n",
            "[402 | 254.03] loss=2.87 avg=3.42\n",
            "[403 | 254.54] loss=3.45 avg=3.42\n",
            "[404 | 255.06] loss=3.02 avg=3.41\n",
            "[405 | 255.57] loss=3.36 avg=3.41\n",
            "[406 | 256.09] loss=3.38 avg=3.41\n",
            "[407 | 256.59] loss=3.49 avg=3.41\n",
            "[408 | 257.11] loss=2.95 avg=3.41\n",
            "[409 | 257.62] loss=2.85 avg=3.40\n",
            "[410 | 258.14] loss=4.07 avg=3.41\n",
            "[411 | 258.65] loss=3.58 avg=3.41\n",
            "[412 | 259.17] loss=3.57 avg=3.41\n",
            "[413 | 259.68] loss=3.84 avg=3.42\n",
            "[414 | 260.20] loss=2.93 avg=3.41\n",
            "[415 | 260.72] loss=3.52 avg=3.41\n",
            "[416 | 261.22] loss=3.73 avg=3.42\n",
            "[417 | 261.73] loss=2.72 avg=3.41\n",
            "[418 | 262.25] loss=3.45 avg=3.41\n",
            "[419 | 262.76] loss=3.01 avg=3.40\n",
            "[420 | 263.28] loss=2.65 avg=3.40\n",
            "[421 | 263.79] loss=3.39 avg=3.40\n",
            "[422 | 264.30] loss=3.12 avg=3.39\n",
            "[423 | 264.81] loss=3.65 avg=3.40\n",
            "[424 | 265.33] loss=3.40 avg=3.40\n",
            "[425 | 265.85] loss=3.17 avg=3.39\n",
            "[426 | 266.36] loss=3.79 avg=3.40\n",
            "[427 | 266.87] loss=3.07 avg=3.40\n",
            "[428 | 267.39] loss=2.91 avg=3.39\n",
            "[429 | 267.91] loss=3.51 avg=3.39\n",
            "[430 | 268.42] loss=3.31 avg=3.39\n",
            "[431 | 268.93] loss=3.51 avg=3.39\n",
            "[432 | 269.44] loss=3.60 avg=3.39\n",
            "[433 | 269.96] loss=3.24 avg=3.39\n",
            "[434 | 270.48] loss=3.69 avg=3.40\n",
            "[435 | 270.99] loss=3.88 avg=3.40\n",
            "[436 | 271.50] loss=3.43 avg=3.40\n",
            "[437 | 272.01] loss=3.42 avg=3.40\n",
            "[438 | 272.52] loss=4.02 avg=3.41\n",
            "[439 | 273.04] loss=3.52 avg=3.41\n",
            "[440 | 273.55] loss=3.43 avg=3.41\n",
            "[441 | 274.07] loss=3.44 avg=3.41\n",
            "[442 | 274.57] loss=3.41 avg=3.41\n",
            "[443 | 275.08] loss=2.55 avg=3.40\n",
            "[444 | 275.59] loss=3.90 avg=3.40\n",
            "[445 | 276.11] loss=3.41 avg=3.41\n",
            "[446 | 276.63] loss=3.17 avg=3.40\n",
            "[447 | 277.14] loss=3.06 avg=3.40\n",
            "[448 | 277.65] loss=2.84 avg=3.39\n",
            "[449 | 278.16] loss=3.35 avg=3.39\n",
            "[450 | 278.67] loss=3.29 avg=3.39\n",
            "[451 | 279.18] loss=3.23 avg=3.39\n",
            "[452 | 279.69] loss=3.05 avg=3.39\n",
            "[453 | 280.20] loss=3.22 avg=3.39\n",
            "[454 | 280.71] loss=3.22 avg=3.38\n",
            "[455 | 281.23] loss=2.95 avg=3.38\n",
            "[456 | 281.74] loss=2.88 avg=3.37\n",
            "[457 | 282.25] loss=4.03 avg=3.38\n",
            "[458 | 282.76] loss=3.44 avg=3.38\n",
            "[459 | 283.27] loss=3.25 avg=3.38\n",
            "[460 | 283.78] loss=2.92 avg=3.38\n",
            "[461 | 284.29] loss=3.35 avg=3.38\n",
            "[462 | 284.80] loss=3.53 avg=3.38\n",
            "[463 | 285.31] loss=3.51 avg=3.38\n",
            "[464 | 285.82] loss=3.86 avg=3.38\n",
            "[465 | 286.33] loss=3.47 avg=3.38\n",
            "[466 | 286.85] loss=3.73 avg=3.39\n",
            "[467 | 287.37] loss=3.48 avg=3.39\n",
            "[468 | 287.88] loss=3.25 avg=3.39\n",
            "[469 | 288.40] loss=3.38 avg=3.39\n",
            "[470 | 288.91] loss=3.76 avg=3.39\n",
            "[471 | 289.43] loss=3.57 avg=3.39\n",
            "[472 | 289.94] loss=3.45 avg=3.39\n",
            "[473 | 290.45] loss=3.68 avg=3.40\n",
            "[474 | 290.96] loss=3.36 avg=3.40\n",
            "[475 | 291.47] loss=2.61 avg=3.39\n",
            "[476 | 291.99] loss=3.43 avg=3.39\n",
            "[477 | 292.50] loss=3.53 avg=3.39\n",
            "[478 | 293.01] loss=3.40 avg=3.39\n",
            "[479 | 293.52] loss=3.13 avg=3.39\n",
            "[480 | 294.03] loss=3.45 avg=3.39\n",
            "[481 | 294.54] loss=3.73 avg=3.39\n",
            "[482 | 295.05] loss=3.90 avg=3.40\n",
            "[483 | 295.56] loss=2.95 avg=3.39\n",
            "[484 | 296.08] loss=3.62 avg=3.39\n",
            "[485 | 296.59] loss=3.72 avg=3.40\n",
            "[486 | 297.11] loss=3.21 avg=3.40\n",
            "[487 | 297.62] loss=3.59 avg=3.40\n",
            "[488 | 298.13] loss=3.47 avg=3.40\n",
            "[489 | 298.64] loss=3.23 avg=3.40\n",
            "[490 | 299.15] loss=3.54 avg=3.40\n",
            "[491 | 299.66] loss=3.51 avg=3.40\n",
            "[492 | 300.17] loss=3.11 avg=3.40\n",
            "[493 | 300.69] loss=2.90 avg=3.39\n",
            "[494 | 301.20] loss=3.44 avg=3.39\n",
            "[495 | 301.71] loss=2.90 avg=3.39\n",
            "[496 | 302.22] loss=3.25 avg=3.38\n",
            "[497 | 302.74] loss=3.30 avg=3.38\n",
            "[498 | 303.26] loss=2.77 avg=3.38\n",
            "[499 | 303.77] loss=3.54 avg=3.38\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " and he was a little drowsy. A white man was walking slowly in the open road. The black man, whom he had met at work in the woods, was still looking for the little black boy, he said. The black man stopped at the foot of a tree and turned and saw the black boy. With his hands in the woods he said:\n",
            "\n",
            "\"The devil is here, this is my boy.\"\n",
            "\n",
            "And he paused. The white man saw him. And his face was white, and his forehead was long black. And he said--\n",
            "\n",
            "\"The devil is in a big wig.\"\n",
            "\n",
            "And they walked together. The black man stood up and stood beside the old black boy, and they kissed. The old black boy was asleep. The black man saw the old black boy standing beside him, and looked to the white man, and said;\n",
            "\n",
            "\"The devil is here.\"\n",
            "\n",
            "And they were silent for a long time.\n",
            "\n",
            "They stood on a tall hill, and looked at the trees, and at the black boy in the woods. The woman sat on a throne, and looked up; and a woman behind a lamp, and said:\n",
            "\n",
            "\"Let's watch together.\"\n",
            "\n",
            "The man sat down, and kissed the woman. He then lifted his eyes to hers. He said:\n",
            "\n",
            "\"Good evening, ladies and gentlemen.\"\n",
            "\n",
            "Then he stood, and kissed the woman. He said,\n",
            "\n",
            "\"And now it is.\"\n",
            "\n",
            "The woman rose, and said,\n",
            "\n",
            "\"Good evening.\"\n",
            "\n",
            "They stood on a tall hill, together, and looked at the trees. They walked together; both stood silent and looking at the trees. And the old Black Man looked to the woman at the lamp, and said,\n",
            "\n",
            "\"It is good to see you again. I should not have known what to do.\"\n",
            "\n",
            "The woman stood as if she had seen him come into the woods. She turned, and looked at the men. She smiled. There was a great line of trees. And she said to the man:\n",
            "\n",
            "\"Good afternoon, dear young man.\"\n",
            "\n",
            "The woman laughed and sat up. She said:\n",
            "\n",
            "\"Then shall we be here again, now.\"\n",
            "\n",
            "Finally, the old Black Man came to the end of the hill; and he sat down, and kissed the women, and said:\n",
            "\n",
            "\"And now you shall go and pray.\"\n",
            "\n",
            "They sat in silence, and gazed at the black man, and said:\n",
            "\n",
            "\"The devil is in a big wig.\"\n",
            "\n",
            "And they sat together. The old Black Man stood up and stood beside the old black boy, and they kissed. The woman came forward, and said:\n",
            "\n",
            "\"The devil is here.\"\n",
            "\n",
            "And they stood silent again. The old man came to the end of the hill. The black man stood up and stood beside the old black boy, and they kissed. The woman came forward. She said:\n",
            "\n",
            "\"The devil is here.\"\n",
            "\n",
            "And they stood silent and looked at the black man, and said;\n",
            "\n",
            "\"The devil is here .\"\n",
            "\n",
            "They sat together, and looked into the trees. They walked together. And the old Black Man said:\n",
            "\n",
            "\"A good night!\"\n",
            "\n",
            "The woman rose, and said:\n",
            "\n",
            "\"Good night, beautiful little woman.\"\n",
            "\n",
            "And the old Black Man looked to the woman:\n",
            "\n",
            "\"The devil is in a wig.\"\n",
            "\n",
            "They stood silent again. The old Black Man looked to the woman:\n",
            "\n",
            "\"Good night, beautiful little woman.\"\n",
            "\n",
            "They stood silent and looked into the trees. They walked together. And the old Black Man said to the woman,\n",
            "\n",
            "\"The devil is in a wig.\"\n",
            "\n",
            "They stood silent and looked at the black man, and said;\n",
            "\n",
            "\"The devil is in a wig.\"\n",
            "\n",
            "They kissed. There was a great line of trees. And they said,\n",
            "\n",
            "\"Good evening, beautiful young man.\"\n",
            "\n",
            "And the woman rose. She said:\n",
            "\n",
            "\"Good night, beautiful old man.\"\n",
            "\n",
            "They stood silent and gazed at the old man, and said:\n",
            "\n",
            "\"The devil is in a wig.\"\n",
            "\n",
            "They stood silent and looked into the trees. They walked together. And the Black Man said:\n",
            "\n",
            "\"The devil is in a wig.\"\n",
            "\n",
            "The woman took the handhold, and said, \"Good night, beautiful young man.\" She said:\n",
            "\n",
            "\"Good night.\"\n",
            "\n",
            "Then she came down from the hill and lifted the little black boy. She looked very slowly into the wood:\n",
            "\n",
            "\"I have seen him before, beautiful young man.\"\n",
            "\n",
            "Then she put the little black boy to bed, and stood up: And the old man rose, and said:\n",
            "\n",
            "\"Good night, beautiful young man.\"\n",
            "\n",
            "Now there was a deep and bright blue mist, and there\n",
            "\n",
            "[500 | 315.06] loss=3.13 avg=3.38\n",
            "[501 | 315.57] loss=3.12 avg=3.37\n",
            "[502 | 316.09] loss=3.29 avg=3.37\n",
            "[503 | 316.59] loss=3.29 avg=3.37\n",
            "[504 | 317.12] loss=2.99 avg=3.37\n",
            "[505 | 317.63] loss=3.22 avg=3.37\n",
            "[506 | 318.15] loss=2.75 avg=3.36\n",
            "[507 | 318.65] loss=3.78 avg=3.37\n",
            "[508 | 319.16] loss=3.47 avg=3.37\n",
            "[509 | 319.68] loss=3.02 avg=3.36\n",
            "[510 | 320.20] loss=3.64 avg=3.37\n",
            "[511 | 320.71] loss=3.61 avg=3.37\n",
            "[512 | 321.22] loss=3.28 avg=3.37\n",
            "[513 | 321.73] loss=3.28 avg=3.37\n",
            "[514 | 322.24] loss=2.73 avg=3.36\n",
            "[515 | 322.76] loss=3.76 avg=3.36\n",
            "[516 | 323.28] loss=3.59 avg=3.37\n",
            "[517 | 323.79] loss=3.38 avg=3.37\n",
            "[518 | 324.31] loss=3.95 avg=3.37\n",
            "[519 | 324.82] loss=2.92 avg=3.37\n",
            "[520 | 325.34] loss=3.02 avg=3.36\n",
            "[521 | 325.86] loss=3.41 avg=3.36\n",
            "[522 | 326.37] loss=3.63 avg=3.37\n",
            "[523 | 326.88] loss=3.30 avg=3.37\n",
            "[524 | 327.39] loss=3.68 avg=3.37\n",
            "[525 | 327.90] loss=3.65 avg=3.37\n",
            "[526 | 328.41] loss=3.11 avg=3.37\n",
            "[527 | 328.92] loss=3.40 avg=3.37\n",
            "[528 | 329.43] loss=3.37 avg=3.37\n",
            "[529 | 329.93] loss=3.28 avg=3.37\n",
            "[530 | 330.44] loss=3.49 avg=3.37\n",
            "[531 | 330.95] loss=3.45 avg=3.37\n",
            "[532 | 331.46] loss=3.47 avg=3.37\n",
            "[533 | 331.97] loss=3.15 avg=3.37\n",
            "[534 | 332.48] loss=2.83 avg=3.36\n",
            "[535 | 332.99] loss=3.51 avg=3.37\n",
            "[536 | 333.50] loss=3.54 avg=3.37\n",
            "[537 | 334.01] loss=3.44 avg=3.37\n",
            "[538 | 334.53] loss=3.46 avg=3.37\n",
            "[539 | 335.04] loss=3.96 avg=3.38\n",
            "[540 | 335.56] loss=3.91 avg=3.38\n",
            "[541 | 336.07] loss=3.00 avg=3.38\n",
            "[542 | 336.58] loss=3.44 avg=3.38\n",
            "[543 | 337.09] loss=3.84 avg=3.38\n",
            "[544 | 337.60] loss=3.40 avg=3.38\n",
            "[545 | 338.10] loss=2.94 avg=3.38\n",
            "[546 | 338.61] loss=3.73 avg=3.38\n",
            "[547 | 339.12] loss=3.08 avg=3.38\n",
            "[548 | 339.63] loss=2.70 avg=3.37\n",
            "[549 | 340.14] loss=3.83 avg=3.38\n",
            "[550 | 340.65] loss=3.19 avg=3.37\n",
            "[551 | 341.16] loss=3.15 avg=3.37\n",
            "[552 | 341.68] loss=3.45 avg=3.37\n",
            "[553 | 342.20] loss=3.20 avg=3.37\n",
            "[554 | 342.71] loss=2.65 avg=3.36\n",
            "[555 | 343.22] loss=3.41 avg=3.36\n",
            "[556 | 343.73] loss=2.79 avg=3.36\n",
            "[557 | 344.24] loss=3.60 avg=3.36\n",
            "[558 | 344.75] loss=3.18 avg=3.36\n",
            "[559 | 345.27] loss=3.13 avg=3.36\n",
            "[560 | 345.78] loss=3.11 avg=3.35\n",
            "[561 | 346.30] loss=3.67 avg=3.36\n",
            "[562 | 346.81] loss=3.82 avg=3.36\n",
            "[563 | 347.32] loss=3.51 avg=3.36\n",
            "[564 | 347.83] loss=3.85 avg=3.37\n",
            "[565 | 348.34] loss=3.33 avg=3.37\n",
            "[566 | 348.85] loss=2.57 avg=3.36\n",
            "[567 | 349.36] loss=3.11 avg=3.36\n",
            "[568 | 349.87] loss=3.26 avg=3.36\n",
            "[569 | 350.38] loss=3.39 avg=3.36\n",
            "[570 | 350.89] loss=3.34 avg=3.36\n",
            "[571 | 351.39] loss=3.72 avg=3.36\n",
            "[572 | 351.91] loss=3.21 avg=3.36\n",
            "[573 | 352.42] loss=3.36 avg=3.36\n",
            "[574 | 352.93] loss=3.64 avg=3.36\n",
            "[575 | 353.44] loss=3.30 avg=3.36\n",
            "[576 | 353.95] loss=2.94 avg=3.36\n",
            "[577 | 354.46] loss=3.07 avg=3.35\n",
            "[578 | 354.97] loss=3.73 avg=3.36\n",
            "[579 | 355.48] loss=3.46 avg=3.36\n",
            "[580 | 355.99] loss=3.27 avg=3.36\n",
            "[581 | 356.50] loss=3.20 avg=3.36\n",
            "[582 | 357.02] loss=3.06 avg=3.35\n",
            "[583 | 357.53] loss=3.56 avg=3.36\n",
            "[584 | 358.04] loss=2.91 avg=3.35\n",
            "[585 | 358.55] loss=3.15 avg=3.35\n",
            "[586 | 359.08] loss=3.85 avg=3.35\n",
            "[587 | 359.60] loss=3.60 avg=3.36\n",
            "[588 | 360.11] loss=3.66 avg=3.36\n",
            "[589 | 360.62] loss=3.03 avg=3.36\n",
            "[590 | 361.13] loss=3.41 avg=3.36\n",
            "[591 | 361.64] loss=2.86 avg=3.35\n",
            "[592 | 362.14] loss=2.85 avg=3.35\n",
            "[593 | 362.66] loss=3.09 avg=3.34\n",
            "[594 | 363.16] loss=3.65 avg=3.35\n",
            "[595 | 363.68] loss=3.65 avg=3.35\n",
            "[596 | 364.19] loss=3.19 avg=3.35\n",
            "[597 | 364.69] loss=3.28 avg=3.35\n",
            "[598 | 365.20] loss=2.96 avg=3.34\n",
            "[599 | 365.71] loss=3.73 avg=3.35\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " Conversation said, \"How can we be so poor as to go through it?\"\n",
            "\n",
            "\"I suppose we can, and when we have, we shall be a little better than we will otherwise have been.\"\n",
            "\n",
            "\"You will never tell me what you have never been and why you never believe me.\"\n",
            "\n",
            "\"You had better listen to something else. I did, I do believe that's the whole point.\"\n",
            "\n",
            "\"You may listen, I believe you said. You may not.\"\n",
            "\n",
            "\"You said that, but I want you to listen.\"\n",
            "\n",
            "\"I don't believe I can listen to what I have to say.\"\n",
            "\n",
            "\"It is all mine.\"\n",
            "\n",
            "\"That's what you say; I'm not going to say anything else about it.\"\n",
            "\n",
            "\"It's not in you; I'm going to tell you everything.\"\n",
            "\n",
            "\"I think.\"\n",
            "\n",
            "\"It's not in you, man.\"\n",
            "\n",
            "\"You have some things to say about how you feel.\"\n",
            "\n",
            "\"I'm not going to go into details here, but I shall put what I have to say in the beginning. I believe it to be true. I believe it. I don't think we should go through it.\"\n",
            "\n",
            "\"I see that, forgive me, and I wish I could explain my feeling here.\"\n",
            "\n",
            "\"You must admit, though I think it very important, that you do not think we ought to go through it.\"\n",
            "\n",
            "\"Well, I believe I should have.\"\n",
            "\n",
            "\"It is my feeling that we would not go through at all. I think we should try, as soon as possible, to have a little conversation, and if we had nothing to say, it would be very valuable to me. Well, I suppose it has been a while since we had any conversation, and I am pretty sure that we have. I wish I had no fear; I think I am quite ready to answer.\"\n",
            "\n",
            "The woman in a long coat stood at the end of his cane and gazed in the direction of the window. It was cold, and she looked out across the street from the house at the house where she had lived for over two months.\n",
            "\n",
            "\"You have heard, indeed, that I have done these things.\"\n",
            "\n",
            "\"I do wish I had to know what you say.\"\n",
            "\n",
            "\"You say that you always go through it.\"\n",
            "\n",
            "\"I say that I have heard from a number of persons that I haven't.\"\n",
            "\n",
            "\"But you are the only one who is capable of hearing what we must, and if I had any of them to speak with, I ought to tell them how they feel.\"\n",
            "\n",
            "The woman in a long coat turned her white face down her legs, and the light on the windows gave a warm light to her eyes.\n",
            "\n",
            "\"What do you mean by that?\"\n",
            "\n",
            "\"Oh, the truth was, if we had been alone, I wouldn't have gone through the whole thing.\"\n",
            "\n",
            "\"Well, I know I can't go through it, and I don't suppose I ought to, considering that I should need to. But there's one thing I know about it, as far as I'm concerned. I suppose I am not going to go through it either, though not without some pressure.\"\n",
            "\n",
            "The woman in the long coat looked at the ground and shook her head. \"That's the matter. You know, there are some people who say that we should go through it. I don't want to be in any way to be in any way, but I do wish that we could have some conversation before we began.\"\n",
            "\n",
            "\"No, because I don't think we ought to. It is not in the law. I am not going to go through it, because I think it might ruin us; but I would suppose that we should do it together. Why don't we do that in general?\"\n",
            "\n",
            "\"But we need to talk, because it would be very important to be at that house. Of course, it is not necessary that we must talk.\"\n",
            "\n",
            "\"What is it?\"\n",
            "\n",
            "\"You may say, 'cause I will tell you about it.\"\n",
            "\n",
            "\"Why?\"\n",
            "\n",
            "\"Because the law says we will have to talk until we have got everything together. But that is not in the law, man. I can tell you what I really think.\"\n",
            "\n",
            "\"I am going to tell you everything,\" she said. \"Now, don't you think that if it all goes as we suppose it will we should start now? We don't have the money to start this time. We have got to figure things out when we get ready for this thing. This is it, sir—good gracious.\"\n",
            "\n",
            "She turned towards the door, and spoke softly to it.\n",
            "\n",
            "\"I think the law says that we shall have to start together. Well, we shouldn't, for the most part, but we do have to find\n",
            "\n",
            "[600 | 376.97] loss=3.63 avg=3.35\n",
            "[601 | 377.48] loss=2.85 avg=3.35\n",
            "[602 | 378.00] loss=3.40 avg=3.35\n",
            "[603 | 378.52] loss=3.09 avg=3.34\n",
            "[604 | 379.04] loss=3.08 avg=3.34\n",
            "[605 | 379.55] loss=3.68 avg=3.34\n",
            "[606 | 380.07] loss=3.59 avg=3.35\n",
            "[607 | 380.58] loss=2.96 avg=3.34\n",
            "[608 | 381.09] loss=2.94 avg=3.34\n",
            "[609 | 381.61] loss=2.81 avg=3.33\n",
            "[610 | 382.13] loss=4.03 avg=3.34\n",
            "[611 | 382.63] loss=3.43 avg=3.34\n",
            "[612 | 383.14] loss=3.04 avg=3.34\n",
            "[613 | 383.65] loss=3.53 avg=3.34\n",
            "[614 | 384.17] loss=3.21 avg=3.34\n",
            "[615 | 384.68] loss=2.87 avg=3.33\n",
            "[616 | 385.20] loss=3.54 avg=3.34\n",
            "[617 | 385.72] loss=3.42 avg=3.34\n",
            "[618 | 386.23] loss=2.81 avg=3.33\n",
            "[619 | 386.73] loss=3.10 avg=3.33\n",
            "[620 | 387.25] loss=3.97 avg=3.34\n",
            "[621 | 387.76] loss=3.53 avg=3.34\n",
            "[622 | 388.27] loss=3.14 avg=3.34\n",
            "[623 | 388.79] loss=3.08 avg=3.33\n",
            "[624 | 389.30] loss=3.20 avg=3.33\n",
            "[625 | 389.82] loss=3.60 avg=3.33\n",
            "[626 | 390.33] loss=3.18 avg=3.33\n",
            "[627 | 390.84] loss=2.61 avg=3.33\n",
            "[628 | 391.35] loss=3.31 avg=3.33\n",
            "[629 | 391.86] loss=3.89 avg=3.33\n",
            "[630 | 392.37] loss=3.46 avg=3.33\n",
            "[631 | 392.89] loss=3.51 avg=3.33\n",
            "[632 | 393.41] loss=3.31 avg=3.33\n",
            "[633 | 393.92] loss=3.48 avg=3.34\n",
            "[634 | 394.42] loss=3.19 avg=3.33\n",
            "[635 | 394.94] loss=3.17 avg=3.33\n",
            "[636 | 395.44] loss=3.30 avg=3.33\n",
            "[637 | 395.96] loss=3.32 avg=3.33\n",
            "[638 | 396.48] loss=3.42 avg=3.33\n",
            "[639 | 396.99] loss=3.73 avg=3.34\n",
            "[640 | 397.50] loss=2.90 avg=3.33\n",
            "[641 | 398.02] loss=3.42 avg=3.33\n",
            "[642 | 398.54] loss=3.41 avg=3.33\n",
            "[643 | 399.05] loss=3.34 avg=3.33\n",
            "[644 | 399.56] loss=3.37 avg=3.33\n",
            "[645 | 400.07] loss=3.28 avg=3.33\n",
            "[646 | 400.58] loss=3.26 avg=3.33\n",
            "[647 | 401.09] loss=3.40 avg=3.33\n",
            "[648 | 401.61] loss=3.33 avg=3.33\n",
            "[649 | 402.13] loss=3.30 avg=3.33\n",
            "[650 | 402.64] loss=3.31 avg=3.33\n",
            "[651 | 403.15] loss=3.41 avg=3.33\n",
            "[652 | 403.66] loss=2.85 avg=3.33\n",
            "[653 | 404.17] loss=3.45 avg=3.33\n",
            "[654 | 404.68] loss=2.94 avg=3.33\n",
            "[655 | 405.19] loss=3.21 avg=3.33\n",
            "[656 | 405.71] loss=3.54 avg=3.33\n",
            "[657 | 406.22] loss=3.44 avg=3.33\n",
            "[658 | 406.73] loss=2.64 avg=3.32\n",
            "[659 | 407.24] loss=3.35 avg=3.32\n",
            "[660 | 407.76] loss=3.82 avg=3.33\n",
            "[661 | 408.28] loss=3.26 avg=3.33\n",
            "[662 | 408.79] loss=2.87 avg=3.32\n",
            "[663 | 409.30] loss=3.49 avg=3.32\n",
            "[664 | 409.81] loss=3.44 avg=3.32\n",
            "[665 | 410.33] loss=3.18 avg=3.32\n",
            "[666 | 410.84] loss=3.21 avg=3.32\n",
            "[667 | 411.36] loss=3.94 avg=3.33\n",
            "[668 | 411.86] loss=3.41 avg=3.33\n",
            "[669 | 412.37] loss=3.79 avg=3.33\n",
            "[670 | 412.88] loss=2.66 avg=3.33\n",
            "[671 | 413.40] loss=3.43 avg=3.33\n",
            "[672 | 413.92] loss=2.97 avg=3.32\n",
            "[673 | 414.43] loss=3.62 avg=3.33\n",
            "[674 | 414.94] loss=3.33 avg=3.33\n",
            "[675 | 415.45] loss=3.51 avg=3.33\n",
            "[676 | 415.96] loss=3.04 avg=3.33\n",
            "[677 | 416.46] loss=2.62 avg=3.32\n",
            "[678 | 416.98] loss=3.36 avg=3.32\n",
            "[679 | 417.50] loss=3.80 avg=3.32\n",
            "[680 | 418.01] loss=3.66 avg=3.33\n",
            "[681 | 418.52] loss=2.98 avg=3.32\n",
            "[682 | 419.03] loss=3.43 avg=3.33\n",
            "[683 | 419.54] loss=3.43 avg=3.33\n",
            "[684 | 420.06] loss=3.11 avg=3.32\n",
            "[685 | 420.57] loss=3.70 avg=3.33\n",
            "[686 | 421.09] loss=2.59 avg=3.32\n",
            "[687 | 421.60] loss=3.51 avg=3.32\n",
            "[688 | 422.11] loss=3.38 avg=3.32\n",
            "[689 | 422.62] loss=3.16 avg=3.32\n",
            "[690 | 423.13] loss=3.26 avg=3.32\n",
            "[691 | 423.63] loss=3.30 avg=3.32\n",
            "[692 | 424.15] loss=4.02 avg=3.33\n",
            "[693 | 424.66] loss=2.67 avg=3.32\n",
            "[694 | 425.18] loss=3.04 avg=3.32\n",
            "[695 | 425.69] loss=2.92 avg=3.31\n",
            "[696 | 426.20] loss=2.94 avg=3.31\n",
            "[697 | 426.71] loss=2.86 avg=3.31\n",
            "[698 | 427.22] loss=3.18 avg=3.30\n",
            "[699 | 427.73] loss=3.25 avg=3.30\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " which is not always true.\n",
            "\n",
            "\n",
            "She was always pretty, but it was more difficult to keep her eyes from her father's nose when he was a student. The hair was very soft and silvery to the touch. They were always on her neck, and this was especially in middle-aged boys. And on her neck and back.\n",
            "\n",
            "\n",
            "\"Now the boy is going to have to have a good temper,\" said she, as I saw her in the kitchen. \"He may start a fire. I know he has to get to his room to-day. The kitchen has to be ransacked to the best of his ability. The kitchen has to be taken out, and the kitchen with it. The father may have to drive into a neighbor's house or something. But it is better to work at him, but the father must have the keys in the kitchen. I know he is going to have to start a fire, but the mother must have the keys.\"\n",
            "\n",
            "\n",
            "My stepfather looked up and saw that I was dressed in old clothes, and was trying to say words.\n",
            "\n",
            "\n",
            "\"Are you all right, Mrs. Wright?\"\n",
            "\n",
            "\"Yes,\" murmured his stepmother in a tone of quiet.\n",
            "\n",
            "I had been dressed pretty since we were born. My stepmother was not so sure, but it was good she told me now.\n",
            "\n",
            "\n",
            "\"I don't know,\" she said to me.\n",
            "\n",
            "\n",
            "So I knew she did that when she went off, and I had the whole world in this house. I had a very large house, but I had a little room.\n",
            "\n",
            "\n",
            "\"It's all the way down toward town, Mrs. Wright,\" I told her. She kept repeating this as she went and walked.\n",
            "\n",
            "\n",
            "\"You must bring the keys in,\" I said, because I needed them.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "If anybody wanted his money he might go and buy it with his whole life. But that was an old woman, an older friend, and she wanted to take the money from me. I said: \"I want the money just as I've always wanted it.\"\n",
            "\n",
            "\n",
            "\"Why?\" she asked, in a deep, angry voice.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\"Well, it's nice to see the money,\" I replied, as I sat down at the table with my old friend. \"But it's much better to have it in my house. My father would not let me take it without a permit, and I know that he will have to do it for me when all is finished.\"\n",
            "\n",
            "\n",
            "She looked at me with a cold sense, and looked about her. \"Are you all right?\" she asked.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\"I am, Mrs. Wright,\" I replied.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\"Oh,\" she cried in a voice which was deep and heavy, inaudible in our old village, and almost oppressive.\n",
            "\n",
            "\n",
            "\"And,\" she began more gradually, \"how can I live with you if you come and find me here?\"\n",
            "\n",
            "\n",
            "She answered, \"I'm not sure if you mean it in writing, and I don't care so much. Of course your sister will never tell you. But as it is, your father is going to have to have them. If he can't come to-morrow he has to have the keys for his room, and he does not deserve to live with you like that.\"\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "So I was going to go and see my best friend come home. It was a little winter morning, and we used to spend a good deal of the afternoon going round town and talking to the people. And then we came home and went to hear the bell.\n",
            "\n",
            "\n",
            "\n",
            "\"Wright, Wright,\" said I. \"I think it's true that the boy is going to use the key. How come his parents should know that I want it?\"\n",
            "\n",
            "\n",
            "I said:\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\"Mrs. Wright, I shall give you the money you want by tomorrow.\"\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "That was exactly what they wanted. I went and bought the whole thing at a market in a town called Loyola, on the way to the town of Loyola, in the town of O'Fallon, a good-natured district on the Western shore of Lake Okeechobee. My stepmother and I met them in a room together.\n",
            "\n",
            "\n",
            "\"Can't you buy another, Wright?\" said I. \"You may go to the grocer for something else.\"\n",
            "\n",
            "\n",
            "They went downstairs and bought another pound of gold, and another, when they came at the door.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\"Where is the safe?\" I inquired, but said:\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " \"You may go to the grocery,\" said the stepmother. \"I don't want you coming into this house.\"\n",
            "\n",
            "\n",
            "I knew she would be a little nervous, and the only thing I could think about was going to buy the little gold with her little purse.\n",
            "\n",
            "\n",
            "\"You can go to the grocer,\" said she. Now I\n",
            "\n",
            "[700 | 438.99] loss=3.68 avg=3.31\n",
            "[701 | 439.50] loss=3.38 avg=3.31\n",
            "[702 | 440.02] loss=2.93 avg=3.30\n",
            "[703 | 440.53] loss=2.93 avg=3.30\n",
            "[704 | 441.04] loss=2.66 avg=3.29\n",
            "[705 | 441.55] loss=2.67 avg=3.29\n",
            "[706 | 442.06] loss=2.70 avg=3.28\n",
            "[707 | 442.57] loss=3.28 avg=3.28\n",
            "[708 | 443.08] loss=3.59 avg=3.29\n",
            "[709 | 443.59] loss=2.67 avg=3.28\n",
            "[710 | 444.10] loss=2.80 avg=3.27\n",
            "[711 | 444.61] loss=3.44 avg=3.28\n",
            "[712 | 445.12] loss=3.62 avg=3.28\n",
            "[713 | 445.63] loss=3.09 avg=3.28\n",
            "[714 | 446.15] loss=3.57 avg=3.28\n",
            "[715 | 446.66] loss=3.03 avg=3.28\n",
            "[716 | 447.17] loss=3.28 avg=3.28\n",
            "[717 | 447.68] loss=3.54 avg=3.28\n",
            "[718 | 448.19] loss=2.79 avg=3.28\n",
            "[719 | 448.70] loss=3.04 avg=3.27\n",
            "[720 | 449.22] loss=3.53 avg=3.28\n",
            "[721 | 449.74] loss=3.15 avg=3.27\n",
            "[722 | 450.25] loss=3.24 avg=3.27\n",
            "[723 | 450.76] loss=3.09 avg=3.27\n",
            "[724 | 451.27] loss=3.67 avg=3.28\n",
            "[725 | 451.78] loss=3.12 avg=3.28\n",
            "[726 | 452.29] loss=2.93 avg=3.27\n",
            "[727 | 452.80] loss=3.01 avg=3.27\n",
            "[728 | 453.31] loss=2.76 avg=3.26\n",
            "[729 | 453.82] loss=3.16 avg=3.26\n",
            "[730 | 454.33] loss=2.31 avg=3.25\n",
            "[731 | 454.84] loss=3.20 avg=3.25\n",
            "[732 | 455.35] loss=3.48 avg=3.26\n",
            "[733 | 455.86] loss=3.61 avg=3.26\n",
            "[734 | 456.37] loss=3.66 avg=3.26\n",
            "[735 | 456.87] loss=3.23 avg=3.26\n",
            "[736 | 457.39] loss=3.36 avg=3.26\n",
            "[737 | 457.89] loss=3.30 avg=3.26\n",
            "[738 | 458.40] loss=2.63 avg=3.26\n",
            "[739 | 458.91] loss=2.82 avg=3.25\n",
            "[740 | 459.42] loss=3.09 avg=3.25\n",
            "[741 | 459.93] loss=3.35 avg=3.25\n",
            "[742 | 460.44] loss=2.94 avg=3.25\n",
            "[743 | 460.95] loss=3.31 avg=3.25\n",
            "[744 | 461.46] loss=3.35 avg=3.25\n",
            "[745 | 461.96] loss=3.16 avg=3.25\n",
            "[746 | 462.48] loss=2.88 avg=3.25\n",
            "[747 | 463.00] loss=3.29 avg=3.25\n",
            "[748 | 463.51] loss=3.64 avg=3.25\n",
            "[749 | 464.02] loss=3.10 avg=3.25\n",
            "[750 | 464.54] loss=3.33 avg=3.25\n",
            "[751 | 465.04] loss=3.76 avg=3.26\n",
            "[752 | 465.55] loss=2.86 avg=3.25\n",
            "[753 | 466.06] loss=3.15 avg=3.25\n",
            "[754 | 466.58] loss=2.96 avg=3.25\n",
            "[755 | 467.10] loss=2.96 avg=3.24\n",
            "[756 | 467.61] loss=3.47 avg=3.25\n",
            "[757 | 468.12] loss=3.29 avg=3.25\n",
            "[758 | 468.63] loss=3.37 avg=3.25\n",
            "[759 | 469.14] loss=2.73 avg=3.24\n",
            "[760 | 469.65] loss=2.98 avg=3.24\n",
            "[761 | 470.16] loss=3.23 avg=3.24\n",
            "[762 | 470.67] loss=3.33 avg=3.24\n",
            "[763 | 471.18] loss=3.46 avg=3.24\n",
            "[764 | 471.69] loss=2.89 avg=3.24\n",
            "[765 | 472.21] loss=3.66 avg=3.24\n",
            "[766 | 472.72] loss=2.94 avg=3.24\n",
            "[767 | 473.23] loss=3.22 avg=3.24\n",
            "[768 | 473.74] loss=3.14 avg=3.24\n",
            "[769 | 474.25] loss=3.74 avg=3.24\n",
            "[770 | 474.76] loss=2.99 avg=3.24\n",
            "[771 | 475.27] loss=3.38 avg=3.24\n",
            "[772 | 475.78] loss=3.59 avg=3.25\n",
            "[773 | 476.29] loss=3.03 avg=3.25\n",
            "[774 | 476.81] loss=3.18 avg=3.24\n",
            "[775 | 477.33] loss=2.59 avg=3.24\n",
            "[776 | 477.84] loss=3.17 avg=3.24\n",
            "[777 | 478.35] loss=2.74 avg=3.23\n",
            "[778 | 478.86] loss=3.21 avg=3.23\n",
            "[779 | 479.37] loss=3.31 avg=3.23\n",
            "[780 | 479.89] loss=3.50 avg=3.24\n",
            "[781 | 480.40] loss=3.30 avg=3.24\n",
            "[782 | 480.92] loss=3.64 avg=3.24\n",
            "[783 | 481.43] loss=3.37 avg=3.24\n",
            "[784 | 481.95] loss=3.37 avg=3.24\n",
            "[785 | 482.46] loss=3.58 avg=3.25\n",
            "[786 | 482.98] loss=3.12 avg=3.24\n",
            "[787 | 483.49] loss=3.34 avg=3.25\n",
            "[788 | 484.00] loss=3.35 avg=3.25\n",
            "[789 | 484.51] loss=2.88 avg=3.24\n",
            "[790 | 485.02] loss=3.68 avg=3.25\n",
            "[791 | 485.54] loss=3.44 avg=3.25\n",
            "[792 | 486.05] loss=3.03 avg=3.25\n",
            "[793 | 486.56] loss=3.13 avg=3.25\n",
            "[794 | 487.07] loss=2.63 avg=3.24\n",
            "[795 | 487.59] loss=3.48 avg=3.24\n",
            "[796 | 488.11] loss=3.68 avg=3.25\n",
            "[797 | 488.62] loss=3.26 avg=3.25\n",
            "[798 | 489.13] loss=3.53 avg=3.25\n",
            "[799 | 489.64] loss=3.24 avg=3.25\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "ray and the rest of the house would be burned down if they did not make a way for it.\n",
            "\n",
            "But not this time.\n",
            "\n",
            "It was a little after dusk, and in a few minutes the whole sky was silent. The first moment it was visible was the following day.\n",
            "\n",
            "The next day was the following day. In the same instant, the house was at the very top-point, and the entire world was illuminated and silent.\n",
            "\n",
            "This was indeed quite extraordinary. In the beginning, I was almost speechless.\n",
            "\n",
            "But as soon as the lamp was turned on, and at night I went to sleep, so was the whole world, and the world was silent. Now after the fire, I began to cry more. There was not the least indication of how long I stood or how far I would go. Sometimes I could make out a little line of fire in a square square. With a little help of my hands, I drew nearer and nearer.\n",
            "\n",
            "That was the moment when I found the very light. And that moment is the moment that when this fire took hold of my heart and my mind there was not the slightest sensation of relief or apprehension, for it had nothing to do with anything. For it was only that a little fire in a square square was burning, to such a degree that the rest of the world was not there to begin with.\n",
            "\n",
            "I cried. I cried harder. In its place lay my entire body. Without that little fire, there would have been no life. The light seemed brighter! But to my whole brain, my eyes, at a distance, I saw nothing at all at all--no picture at all; the fire was gone. All I loved was lost and nothing was left. Not even the light could explain why I said these words at all.\n",
            "\n",
            "And now the time for me to go, for once I realized that I would have to ask for permission to go for a walk. I did not want to get my hands on the keys. So, I went into the house and took my keys. I went down the block. The one door I had closed was broken.\n",
            "\n",
            "Then I began to cry more.\n",
            "\n",
            "A little after this I said again, I cried harder.\n",
            "\n",
            "A little after this I started to cry again.\n",
            "\n",
            "A little after this I stopped crying.\n",
            "\n",
            "Then the darkness seemed as full of flame as it may have been. A little after this I had no idea what to say. I did not know what could be more terrible and terrible than the terrible silence of this house alone.\n",
            "\n",
            "It was only in a moment that I heard two terrible voices, but then I said, I want to go away. And the door of the house opened. And a voice cried in the air. I knew it was a voice that was screaming again. And after so many horrible cries, I went away.\n",
            "\n",
            "This time the man who had come in, and who had become so angry at me was the man who had married me; and that man belonged to my family. Now, I don't know who came in and who became so mad that they all went away. And I don't know how I was to know that this fire was about to be burnt away.\n",
            "\n",
            "And if you remember, you may hear the scene in the book of the Dead by Carl Schulz.\n",
            "\n",
            "Now there are many stories in which life seemed to be a kind of endless, unbroken cycle in which the mind and body were divided into two parts. The first is one of the story told in the book of St. John. It is a book of poetry written in the very first year of the Church. If we look at the beginning of the story we should notice that St. John says that he was not able to write. In the same way that we read old poems and old people saying great things, that they had done great things, it is very easy to see why there should be a certain kind of darkness between the two halves of nature.\n",
            "\n",
            "This idea of silence seems to be not only very beautiful in itself, but to me as a poet, I think it has more in common with the other ideas in the Book of St. John. People like the Virgin Mary, which is written in that same year of St. John, and whom St. John had loved, or whom Henry knew and who he loved, and who lived alone in Rome, could tell with no other way of saying that the silence between the two halves of nature was beautiful in comparison with what the other thing could do! They could tell with no other way of saying it. And when one of the nuns said, \"That was not the thing we should do,\" they went away. St. John goes back again before he was able to write again. This story is a part of an old poem in the Church that was written in that year of St. John, and whose name was St. John.\n",
            "\n",
            "\n",
            "\n",
            "[800 | 500.94] loss=2.65 avg=3.24\n",
            "[801 | 501.45] loss=2.60 avg=3.24\n",
            "[802 | 501.96] loss=3.53 avg=3.24\n",
            "[803 | 502.47] loss=3.02 avg=3.24\n",
            "[804 | 502.98] loss=3.51 avg=3.24\n",
            "[805 | 503.49] loss=3.68 avg=3.24\n",
            "[806 | 504.01] loss=3.21 avg=3.24\n",
            "[807 | 504.53] loss=3.41 avg=3.25\n",
            "[808 | 505.04] loss=3.07 avg=3.24\n",
            "[809 | 505.55] loss=3.59 avg=3.25\n",
            "[810 | 506.06] loss=3.51 avg=3.25\n",
            "[811 | 506.57] loss=2.95 avg=3.25\n",
            "[812 | 507.08] loss=3.33 avg=3.25\n",
            "[813 | 507.60] loss=3.04 avg=3.25\n",
            "[814 | 508.12] loss=3.13 avg=3.25\n",
            "[815 | 508.63] loss=2.77 avg=3.24\n",
            "[816 | 509.15] loss=3.64 avg=3.24\n",
            "[817 | 509.65] loss=3.58 avg=3.25\n",
            "[818 | 510.17] loss=2.90 avg=3.24\n",
            "[819 | 510.69] loss=3.33 avg=3.25\n",
            "[820 | 511.20] loss=2.96 avg=3.24\n",
            "[821 | 511.72] loss=2.87 avg=3.24\n",
            "[822 | 512.23] loss=3.04 avg=3.24\n",
            "[823 | 512.74] loss=2.59 avg=3.23\n",
            "[824 | 513.25] loss=2.91 avg=3.23\n",
            "[825 | 513.76] loss=3.10 avg=3.23\n",
            "[826 | 514.27] loss=3.04 avg=3.22\n",
            "[827 | 514.78] loss=3.37 avg=3.23\n",
            "[828 | 515.29] loss=3.18 avg=3.22\n",
            "[829 | 515.80] loss=3.09 avg=3.22\n",
            "[830 | 516.31] loss=3.29 avg=3.22\n",
            "[831 | 516.82] loss=2.73 avg=3.22\n",
            "[832 | 517.33] loss=3.86 avg=3.23\n",
            "[833 | 517.84] loss=2.92 avg=3.22\n",
            "[834 | 518.35] loss=3.19 avg=3.22\n",
            "[835 | 518.86] loss=3.18 avg=3.22\n",
            "[836 | 519.37] loss=3.33 avg=3.22\n",
            "[837 | 519.88] loss=3.75 avg=3.23\n",
            "[838 | 520.39] loss=3.14 avg=3.23\n",
            "[839 | 520.91] loss=3.53 avg=3.23\n",
            "[840 | 521.41] loss=3.48 avg=3.23\n",
            "[841 | 521.92] loss=3.64 avg=3.24\n",
            "[842 | 522.43] loss=3.46 avg=3.24\n",
            "[843 | 522.95] loss=2.48 avg=3.23\n",
            "[844 | 523.45] loss=3.46 avg=3.23\n",
            "[845 | 523.96] loss=2.97 avg=3.23\n",
            "[846 | 524.47] loss=2.87 avg=3.23\n",
            "[847 | 524.98] loss=4.04 avg=3.24\n",
            "[848 | 525.49] loss=2.57 avg=3.23\n",
            "[849 | 526.00] loss=3.54 avg=3.23\n",
            "[850 | 526.52] loss=3.84 avg=3.24\n",
            "[851 | 527.02] loss=2.81 avg=3.23\n",
            "[852 | 527.53] loss=3.78 avg=3.24\n",
            "[853 | 528.04] loss=3.29 avg=3.24\n",
            "[854 | 528.55] loss=3.10 avg=3.24\n",
            "[855 | 529.06] loss=2.90 avg=3.24\n",
            "[856 | 529.57] loss=3.35 avg=3.24\n",
            "[857 | 530.08] loss=2.73 avg=3.23\n",
            "[858 | 530.59] loss=2.85 avg=3.23\n",
            "[859 | 531.10] loss=3.80 avg=3.23\n",
            "[860 | 531.60] loss=2.79 avg=3.23\n",
            "[861 | 532.11] loss=3.19 avg=3.23\n",
            "[862 | 532.62] loss=3.54 avg=3.23\n",
            "[863 | 533.13] loss=2.65 avg=3.23\n",
            "[864 | 533.64] loss=3.58 avg=3.23\n",
            "[865 | 534.16] loss=3.24 avg=3.23\n",
            "[866 | 534.66] loss=3.17 avg=3.23\n",
            "[867 | 535.18] loss=3.91 avg=3.24\n",
            "[868 | 535.68] loss=3.30 avg=3.24\n",
            "[869 | 536.19] loss=3.09 avg=3.23\n",
            "[870 | 536.70] loss=3.29 avg=3.24\n",
            "[871 | 537.21] loss=3.44 avg=3.24\n",
            "[872 | 537.72] loss=3.95 avg=3.24\n",
            "[873 | 538.23] loss=3.33 avg=3.25\n",
            "[874 | 538.74] loss=3.03 avg=3.24\n",
            "[875 | 539.25] loss=3.37 avg=3.24\n",
            "[876 | 539.76] loss=3.18 avg=3.24\n",
            "[877 | 540.27] loss=3.38 avg=3.25\n",
            "[878 | 540.78] loss=3.01 avg=3.24\n",
            "[879 | 541.29] loss=3.27 avg=3.24\n",
            "[880 | 541.80] loss=3.14 avg=3.24\n",
            "[881 | 542.31] loss=3.24 avg=3.24\n",
            "[882 | 542.82] loss=3.59 avg=3.25\n",
            "[883 | 543.33] loss=3.15 avg=3.24\n",
            "[884 | 543.83] loss=3.47 avg=3.25\n",
            "[885 | 544.34] loss=3.21 avg=3.25\n",
            "[886 | 544.85] loss=3.54 avg=3.25\n",
            "[887 | 545.36] loss=2.92 avg=3.25\n",
            "[888 | 545.87] loss=3.34 avg=3.25\n",
            "[889 | 546.38] loss=3.13 avg=3.25\n",
            "[890 | 546.89] loss=2.93 avg=3.24\n",
            "[891 | 547.40] loss=3.25 avg=3.24\n",
            "[892 | 547.92] loss=3.37 avg=3.24\n",
            "[893 | 548.44] loss=3.16 avg=3.24\n",
            "[894 | 548.95] loss=2.83 avg=3.24\n",
            "[895 | 549.46] loss=3.59 avg=3.24\n",
            "[896 | 549.96] loss=3.28 avg=3.24\n",
            "[897 | 550.48] loss=3.55 avg=3.25\n",
            "[898 | 551.00] loss=3.42 avg=3.25\n",
            "[899 | 551.51] loss=3.61 avg=3.25\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " the \"sensible\" opinion that the Russian government must be stopped; there can be no denying that the entire world is in peril. \"The United States is a great power; it is, after all, the United States,\" he said. \"Is it not true that the Russian government is not afraid of criticism?\" I said. \"Let me know in the meanwhile whether the President has been in that state, Mr. President.\" \"You will hear, will you?\" \"Yes.\" \"Why do you talk too much?\" \"I am not going to give you another chance, Mr. President.\" \"Well, I think it is no great shame to me to listen to you, Mr. President.\" \"We must all be careful to listen. The enemy is not afraid to look at us. We have to show ourselves.\" \"True,\" I replied; \"but don't you think it would be a good idea, Mr. President? He is not only in that state - he is in another!\" This was a great deal more a speech than I had intended to give it, and I am not sure that I had meant what the President was saying in the speech before me. \"My dear friend, I have not thought it appropriate to express a single word which I had heard over and through your house, without at least giving it your tacit consent.\" \"My dear friend, I would tell you the truth,\" I said, as if to warn you. \"My dear friend, I have not thought that it would be proper to show yourself like this; it was an idea you had heard before, you know. I thought you were going to get in trouble yourself. Now let me ask you, Mr. President, have you ever been to Moscow where the foreign minister used to speak by example, the expression \"Russian President?\" I told you so to me. I do not know whether it was my imagination or yours that thought it necessary to show yourself like that. The only reason why I think you were not asked to be is that I knew what you meant; the language used by the foreign minister was not the way to express something in an ordinary fashion, so the idea of it was not worth a moment's talk or a thought in any other way. To you, Mr. President, I say: \"Do you not feel that you were in the right, Mr. President?\" I said, with great satisfaction. \"I have,\" I said, with all the courage I had to suggest; for I ought not to have done it if I had known that the Russians would refuse me! \"To you, Mr. President, I reply: \"You must remember that in the last few weeks a huge blow has been taken against the country. The people have had their best intentions, but they have not the courage, and they hope that they cannot have it themselves. There is nothing to be gained from it; but to speak your mind and show yourself like this, Mr. President, I say to those who work at this very minute: \"I am sorry, I have not spoken in a while, for I have never spoken.\" \"What am you saying?\" they cried, in tones of indignation; they could not bear to be told that their lips were open and their hands held out. \"Why are you speaking this language, Mr. President? What does it matter?\" \"That which is to be said, Mr. President, is not to be said in this tongue,\" I said; \"and what is the harm in having spoken it?\" \"I can see no harm in such things as you say -- the country will be ready to carry out the sentence of the United States in case of the Russian government being informed.\" \"Now you have explained yourself, for I tell you that you are very sorry.\" \"Yes, dear,\" I said. \"I am not sorry; I can never understand how you, who have talked, can express any more than you have to, by means of those words you have read. I am sorry, Mr. President, for you did not like what I said; perhaps you should have told me to put my hand where yours always holds mine, too.\" \"Oh, my dear! why were you speaking that language?\" \"What, dear, you say that? The Russian Government knows that; it should not be afraid of criticism.\" \"Is it then of advantage to them that you speak that language?\" \"That is not altogether my fault, Mr. President. I do not understand you to mean to say that. What I mean to state, Mr. President, is that I think you are not a well-formed man. When you take the Russian language, even in that one word, at me you are very silly. You use such words to mean to speak this language. You do this, though, in order to be able to say the words I have said that are to be used to mean them. You do that, for, as you say, I would only speak this language at the dinner now\n",
            "\n",
            "[900 | 562.86] loss=2.66 avg=3.25\n",
            "[901 | 563.36] loss=3.65 avg=3.25\n",
            "[902 | 563.88] loss=3.08 avg=3.25\n",
            "[903 | 564.39] loss=3.21 avg=3.25\n",
            "[904 | 564.91] loss=3.38 avg=3.25\n",
            "[905 | 565.43] loss=3.56 avg=3.25\n",
            "[906 | 565.94] loss=2.98 avg=3.25\n",
            "[907 | 566.45] loss=3.36 avg=3.25\n",
            "[908 | 566.96] loss=3.17 avg=3.25\n",
            "[909 | 567.47] loss=3.41 avg=3.25\n",
            "[910 | 567.98] loss=3.06 avg=3.25\n",
            "[911 | 568.50] loss=3.19 avg=3.25\n",
            "[912 | 569.01] loss=3.38 avg=3.25\n",
            "[913 | 569.53] loss=3.18 avg=3.25\n",
            "[914 | 570.05] loss=3.41 avg=3.25\n",
            "[915 | 570.56] loss=3.75 avg=3.26\n",
            "[916 | 571.07] loss=3.28 avg=3.26\n",
            "[917 | 571.58] loss=3.11 avg=3.25\n",
            "[918 | 572.09] loss=3.47 avg=3.26\n",
            "[919 | 572.60] loss=3.69 avg=3.26\n",
            "[920 | 573.11] loss=3.61 avg=3.26\n",
            "[921 | 573.63] loss=2.55 avg=3.26\n",
            "[922 | 574.14] loss=3.01 avg=3.25\n",
            "[923 | 574.66] loss=2.99 avg=3.25\n",
            "[924 | 575.17] loss=3.63 avg=3.26\n",
            "[925 | 575.69] loss=3.78 avg=3.26\n",
            "[926 | 576.20] loss=3.42 avg=3.26\n",
            "[927 | 576.70] loss=3.12 avg=3.26\n",
            "[928 | 577.22] loss=3.15 avg=3.26\n",
            "[929 | 577.73] loss=3.49 avg=3.26\n",
            "[930 | 578.24] loss=3.09 avg=3.26\n",
            "[931 | 578.76] loss=3.47 avg=3.26\n",
            "[932 | 579.28] loss=2.73 avg=3.26\n",
            "[933 | 579.79] loss=3.36 avg=3.26\n",
            "[934 | 580.30] loss=3.22 avg=3.26\n",
            "[935 | 580.81] loss=3.22 avg=3.26\n",
            "[936 | 581.32] loss=3.15 avg=3.26\n",
            "[937 | 581.83] loss=3.13 avg=3.26\n",
            "[938 | 582.34] loss=2.34 avg=3.25\n",
            "[939 | 582.86] loss=3.21 avg=3.25\n",
            "[940 | 583.38] loss=2.51 avg=3.24\n",
            "[941 | 583.90] loss=3.29 avg=3.24\n",
            "[942 | 584.41] loss=3.49 avg=3.24\n",
            "[943 | 584.93] loss=3.03 avg=3.24\n",
            "[944 | 585.44] loss=3.10 avg=3.24\n",
            "[945 | 585.96] loss=3.02 avg=3.24\n",
            "[946 | 586.47] loss=3.27 avg=3.24\n",
            "[947 | 586.99] loss=2.68 avg=3.23\n",
            "[948 | 587.51] loss=3.22 avg=3.23\n",
            "[949 | 588.02] loss=3.32 avg=3.23\n",
            "[950 | 588.53] loss=3.37 avg=3.23\n",
            "[951 | 589.04] loss=2.97 avg=3.23\n",
            "[952 | 589.55] loss=3.22 avg=3.23\n",
            "[953 | 590.06] loss=2.81 avg=3.23\n",
            "[954 | 590.57] loss=3.43 avg=3.23\n",
            "[955 | 591.08] loss=3.14 avg=3.23\n",
            "[956 | 591.60] loss=3.24 avg=3.23\n",
            "[957 | 592.11] loss=3.06 avg=3.23\n",
            "[958 | 592.62] loss=3.18 avg=3.23\n",
            "[959 | 593.14] loss=3.00 avg=3.22\n",
            "[960 | 593.64] loss=3.32 avg=3.22\n",
            "[961 | 594.16] loss=2.62 avg=3.22\n",
            "[962 | 594.68] loss=2.85 avg=3.21\n",
            "[963 | 595.19] loss=3.59 avg=3.22\n",
            "[964 | 595.70] loss=3.49 avg=3.22\n",
            "[965 | 596.21] loss=3.36 avg=3.22\n",
            "[966 | 596.73] loss=2.47 avg=3.21\n",
            "[967 | 597.25] loss=3.55 avg=3.22\n",
            "[968 | 597.76] loss=2.82 avg=3.21\n",
            "[969 | 598.27] loss=2.89 avg=3.21\n",
            "[970 | 598.78] loss=2.80 avg=3.21\n",
            "[971 | 599.30] loss=3.13 avg=3.21\n",
            "[972 | 599.81] loss=3.22 avg=3.21\n",
            "[973 | 600.32] loss=2.92 avg=3.20\n",
            "[974 | 600.83] loss=2.67 avg=3.20\n",
            "[975 | 601.34] loss=3.81 avg=3.20\n",
            "[976 | 601.86] loss=2.56 avg=3.20\n",
            "[977 | 602.37] loss=2.47 avg=3.19\n",
            "[978 | 602.89] loss=3.55 avg=3.19\n",
            "[979 | 603.40] loss=3.43 avg=3.20\n",
            "[980 | 603.91] loss=3.42 avg=3.20\n",
            "[981 | 604.42] loss=3.32 avg=3.20\n",
            "[982 | 604.93] loss=3.16 avg=3.20\n",
            "[983 | 605.44] loss=3.38 avg=3.20\n",
            "[984 | 605.95] loss=3.06 avg=3.20\n",
            "[985 | 606.46] loss=3.29 avg=3.20\n",
            "[986 | 606.98] loss=3.43 avg=3.20\n",
            "[987 | 607.49] loss=3.78 avg=3.21\n",
            "[988 | 608.01] loss=3.07 avg=3.21\n",
            "[989 | 608.51] loss=3.47 avg=3.21\n",
            "[990 | 609.03] loss=3.61 avg=3.21\n",
            "[991 | 609.53] loss=2.76 avg=3.21\n",
            "[992 | 610.04] loss=3.38 avg=3.21\n",
            "[993 | 610.55] loss=2.87 avg=3.21\n",
            "[994 | 611.07] loss=2.95 avg=3.20\n",
            "[995 | 611.58] loss=3.27 avg=3.21\n",
            "[996 | 612.08] loss=3.03 avg=3.20\n",
            "[997 | 612.59] loss=2.86 avg=3.20\n",
            "[998 | 613.10] loss=3.11 avg=3.20\n",
            "[999 | 613.61] loss=3.21 avg=3.20\n",
            "Saving checkpoint/all_short_stories/model-1000\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " desired on the body of these five, and they said they wanted to see the head of Mr. Stumpf and Mr. Morgan.\n",
            "\n",
            "\"It would be so,\" said Mrs. Dyer, and then returned to the door.\n",
            "\n",
            "Mr. Morgan had been a pretty boy, and had always made his home away from home.\n",
            "\n",
            "He'd been on the streets a little, and had been playing with his dog. Of course, one day he and the boy and Morgan would come back and live together.\n",
            "\n",
            "When the time came, Mr. Morgan, that little boy, and, what he said to the child, Mr. Stumpf, that evening, as we talked, Mr. Morgan, and the little boy were standing on the street, looking at a window.\n",
            "\n",
            "One of the boys said that Mr. Morgan's name was not Morgan, but B. B. Dick.\n",
            "\n",
            "\"But how did you know it, Mr. Stumpf?\" asked the child.\n",
            "\n",
            "\"I knew it very, very well,\" said Mr. Stumpf. \"I knew his name. I was just glad I caught the boy with him.\"\n",
            "\n",
            "Mr. Morgan had said nothing to the child.\n",
            "\n",
            "After this conversation the child did not wish to hear.\n",
            "\n",
            "That night Mrs. Dyer went upstairs, and was looking up at the window as she passed it.\n",
            "\n",
            "As she walked in the house, a very soft child, who always seemed to speak the last words, came up about her.\n",
            "\n",
            "\"The little boy,\" she said to him, \"was my father,\" and continued: \"He was born a year or two before us, but I think he was born about eight. He lived with a father by the fire, in the little cottage, just before we left these two dogs, and he was a little boy of five years of age, and was adopted by the old woman, Miss Grosvenor. He's my neighbor's dog, Grosvenor, and has never been seen.\"\n",
            "\n",
            "\"Poor girl,\" said Mrs. Dyer, \"I don't know why you've come--why haven't you done any looking for him?\"\n",
            "\n",
            "\"No, never been looking for him,\" said the child, and looking at her, she said: \"I don't know why you've come, or what my neighbor did to you.\"\n",
            "\n",
            "\"Don't you remember, Miss Grosvenor?\"\n",
            "\n",
            "\"Yes,\" said the child. \"She was the neighbor, and the old woman.\"\n",
            "\n",
            "\"And what happened to her?\"\n",
            "\n",
            "\"She's married.\"\n",
            "\n",
            "\"Where are you?\"\n",
            "\n",
            "\"I've just come to find a house for the little boy.\"\n",
            "\n",
            "\"Where?\"\n",
            "\n",
            "\"'Where?'\n",
            "\n",
            "\"My own room, for sure.\"\n",
            "\n",
            "\"Can you tell me where the house is?\"\n",
            "\n",
            "\"Is it in my yard?\"\n",
            "\n",
            "\"Yes, but it's the same as before. You know, I've always been a very poor citizen, and it was a time when my neighbor had all the things she had.\"\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Two people, one white man and one black man, stood in line waiting for a ticket. One of them had a hat. The other had a suit, and two hats. They waited.\n",
            "\n",
            "One of them was sitting with his back to the line. The other was standing next to the counter, leaning forward to look at the ticket.\n",
            "\n",
            "Two men, one white and one black, drew opposite each other. They came down two or three steps behind one another, and came upon each other, and seemed to be in the middle of the lines.\n",
            "\n",
            "\"We want to see \"Nebulism,\" say the two people, and one of them started to speak to the other.\n",
            "\n",
            "\"What is \"Nebulism?\" says the white man, and the black man began to speak to the other.\n",
            "\n",
            "The two people began to talk, and the whites began to talk, and the blacks began to talk.\n",
            "\n",
            "\"If Nebulism happens to happen to us,\" said the white man, \"we will pay the price of his hatred.\"\n",
            "\n",
            "\"You are wrong,\" continued the black man, and started to speak, \"you have no idea what Nebulism is.\"\n",
            "\n",
            "One of the people came forward with his hand on the counter, and said: \"Nebulism--but it's not a sin. It is a crime. We are going to pay the price.\"\n",
            "\n",
            "\"Oh, yes, I am,\" said the person who spoke to them. \"I am a black man. I am a black man. We are going to pay the price if Nebulism happens to happen to us.\"\n",
            "\n",
            "\"It does not matter!\" said the person in the counter. \"We are the only black citizens in America; we\n",
            "\n",
            "[1000 | 628.61] loss=3.13 avg=3.20\n",
            "[1001 | 629.12] loss=3.28 avg=3.20\n",
            "[1002 | 629.63] loss=2.97 avg=3.20\n",
            "[1003 | 630.14] loss=3.66 avg=3.20\n",
            "[1004 | 630.65] loss=3.11 avg=3.20\n",
            "[1005 | 631.16] loss=3.19 avg=3.20\n",
            "[1006 | 631.67] loss=3.11 avg=3.20\n",
            "[1007 | 632.18] loss=3.71 avg=3.20\n",
            "[1008 | 632.69] loss=3.71 avg=3.21\n",
            "[1009 | 633.21] loss=2.93 avg=3.21\n",
            "[1010 | 633.72] loss=3.04 avg=3.21\n",
            "[1011 | 634.24] loss=2.95 avg=3.20\n",
            "[1012 | 634.75] loss=3.70 avg=3.21\n",
            "[1013 | 635.26] loss=3.86 avg=3.21\n",
            "[1014 | 635.77] loss=3.12 avg=3.21\n",
            "[1015 | 636.28] loss=2.61 avg=3.21\n",
            "[1016 | 636.79] loss=3.47 avg=3.21\n",
            "[1017 | 637.30] loss=3.58 avg=3.21\n",
            "[1018 | 637.81] loss=3.34 avg=3.22\n",
            "[1019 | 638.33] loss=3.60 avg=3.22\n",
            "[1020 | 638.85] loss=3.98 avg=3.23\n",
            "[1021 | 639.37] loss=3.27 avg=3.23\n",
            "[1022 | 639.89] loss=3.58 avg=3.23\n",
            "[1023 | 640.40] loss=3.63 avg=3.23\n",
            "[1024 | 640.91] loss=3.32 avg=3.24\n",
            "[1025 | 641.42] loss=4.12 avg=3.24\n",
            "[1026 | 641.93] loss=3.68 avg=3.25\n",
            "[1027 | 642.45] loss=2.96 avg=3.25\n",
            "[1028 | 642.96] loss=3.18 avg=3.25\n",
            "[1029 | 643.48] loss=3.60 avg=3.25\n",
            "[1030 | 643.99] loss=3.53 avg=3.25\n",
            "[1031 | 644.51] loss=3.07 avg=3.25\n",
            "[1032 | 645.03] loss=3.22 avg=3.25\n",
            "[1033 | 645.54] loss=2.87 avg=3.25\n",
            "[1034 | 646.05] loss=2.82 avg=3.24\n",
            "[1035 | 646.57] loss=2.91 avg=3.24\n",
            "[1036 | 647.09] loss=3.43 avg=3.24\n",
            "[1037 | 647.61] loss=3.57 avg=3.24\n",
            "[1038 | 648.12] loss=2.54 avg=3.24\n",
            "[1039 | 648.64] loss=3.15 avg=3.24\n",
            "[1040 | 649.15] loss=3.34 avg=3.24\n",
            "[1041 | 649.67] loss=2.94 avg=3.23\n",
            "[1042 | 650.19] loss=3.26 avg=3.23\n",
            "[1043 | 650.70] loss=3.53 avg=3.24\n",
            "[1044 | 651.21] loss=2.88 avg=3.23\n",
            "[1045 | 651.74] loss=3.26 avg=3.23\n",
            "[1046 | 652.25] loss=3.37 avg=3.23\n",
            "[1047 | 652.77] loss=2.99 avg=3.23\n",
            "[1048 | 653.28] loss=3.57 avg=3.24\n",
            "[1049 | 653.79] loss=3.61 avg=3.24\n",
            "[1050 | 654.31] loss=3.07 avg=3.24\n",
            "[1051 | 654.83] loss=3.29 avg=3.24\n",
            "[1052 | 655.35] loss=3.36 avg=3.24\n",
            "[1053 | 655.87] loss=3.75 avg=3.24\n",
            "[1054 | 656.38] loss=2.94 avg=3.24\n",
            "[1055 | 656.89] loss=2.93 avg=3.24\n",
            "[1056 | 657.40] loss=2.85 avg=3.23\n",
            "[1057 | 657.93] loss=3.31 avg=3.24\n",
            "[1058 | 658.44] loss=3.01 avg=3.23\n",
            "[1059 | 658.95] loss=3.38 avg=3.23\n",
            "[1060 | 659.46] loss=3.89 avg=3.24\n",
            "[1061 | 659.97] loss=2.77 avg=3.24\n",
            "[1062 | 660.49] loss=3.29 avg=3.24\n",
            "[1063 | 661.00] loss=2.99 avg=3.23\n",
            "[1064 | 661.52] loss=3.42 avg=3.24\n",
            "[1065 | 662.04] loss=3.00 avg=3.23\n",
            "[1066 | 662.55] loss=3.51 avg=3.24\n",
            "[1067 | 663.06] loss=2.85 avg=3.23\n",
            "[1068 | 663.57] loss=2.99 avg=3.23\n",
            "[1069 | 664.09] loss=3.50 avg=3.23\n",
            "[1070 | 664.61] loss=3.06 avg=3.23\n",
            "[1071 | 665.12] loss=3.37 avg=3.23\n",
            "[1072 | 665.64] loss=2.78 avg=3.23\n",
            "[1073 | 666.16] loss=3.32 avg=3.23\n",
            "[1074 | 666.67] loss=3.00 avg=3.23\n",
            "[1075 | 667.18] loss=3.16 avg=3.23\n",
            "[1076 | 667.69] loss=3.50 avg=3.23\n",
            "[1077 | 668.21] loss=3.29 avg=3.23\n",
            "[1078 | 668.73] loss=3.38 avg=3.23\n",
            "[1079 | 669.24] loss=2.50 avg=3.22\n",
            "[1080 | 669.75] loss=2.75 avg=3.22\n",
            "[1081 | 670.26] loss=2.70 avg=3.21\n",
            "[1082 | 670.78] loss=2.91 avg=3.21\n",
            "[1083 | 671.30] loss=3.08 avg=3.21\n",
            "[1084 | 671.81] loss=3.38 avg=3.21\n",
            "[1085 | 672.32] loss=2.96 avg=3.21\n",
            "[1086 | 672.84] loss=2.84 avg=3.20\n",
            "[1087 | 673.35] loss=3.24 avg=3.21\n",
            "[1088 | 673.86] loss=2.88 avg=3.20\n",
            "[1089 | 674.37] loss=2.74 avg=3.20\n",
            "[1090 | 674.88] loss=3.40 avg=3.20\n",
            "[1091 | 675.40] loss=3.62 avg=3.20\n",
            "[1092 | 675.92] loss=3.19 avg=3.20\n",
            "[1093 | 676.43] loss=3.25 avg=3.20\n",
            "[1094 | 676.94] loss=3.39 avg=3.21\n",
            "[1095 | 677.45] loss=3.27 avg=3.21\n",
            "[1096 | 677.96] loss=3.46 avg=3.21\n",
            "[1097 | 678.48] loss=3.27 avg=3.21\n",
            "[1098 | 679.00] loss=3.40 avg=3.21\n",
            "[1099 | 679.51] loss=2.70 avg=3.21\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " in some obscure room, which we did not see.\n",
            "\n",
            "All that night we took some of the money with us. After three days they were only in a small hut behind a fence, in which we could not see a single foot.\n",
            "\n",
            "The next morning we went through the whole hut to the foot-well. They were empty; nobody came in.\n",
            "\n",
            "We took our bed-lid with us, laid it out, went over the ground and put it in our pocket. When we walked out of the hut our feet became very uncomfortable, so we went up and looked up at the foot-well, and we saw nothing.\n",
            "\n",
            "After a day or two our legs were swollen, and we could not walk on. They had been so poor, and we had only two dollars on us, and even when they went into the garden there was to be nothing. We tried to live a long time on the money, and when we went out to the field, there was plenty of food, and everything that was poor was eaten up as it had been eaten up by the poor, and every one was starved, and their faces were red from sobs, and they did not know where to find any thing, even to take a look at the money to buy that little dish which we had bought at the village pawn-shop.\n",
            "\n",
            "But one evening there happened a loud knock at the door, and we found it was my husband who rang. He was tall, and he looked like the most famous man in the country; a fine young man, but he had the appearance of a devilish creature. Then he said to himself, \"I am the devil, and I am going out to buy a dish for money.\" So, in a few days I ate very well, and at last my legs became big and swollen, but in spite of all the pain the man got up to the level he was fit and handsome.\n",
            "\n",
            "I went to my father's house to see if, for a long time, was anyone living there. He told me my story to my aunt in a little cottage, and after three months I went in and I found out it was my wife, and she told me all that day, but she felt that she was ill; she was sick and tired, but she could not walk; she was very ill, and every one who ever lived had been sick and tired with her. That's the devil's house. She never left that house, and there were no beggars there.\n",
            "\n",
            "The next day my aunt went in to see my father. He was a pretty old man, and a poor man, but a well-bred one, and he was very wise, and knew everything there was to know. I went in and took my leave. I was a rich and well-mannered man, and when I came home I found my mother and aunt in the house the next day, and while she was there she saw that I was sick and the beggars were getting in more and more. So I went out and saw them all. They were all fine, and I was well-born, and I knew enough. I went up and sat down all night without eating anything.\n",
            "\n",
            "When I came home I felt very happy. I was an old man, and when I came home to see my mother, the things went through me as if I were a living corpse. I was a woman, and I was quite merry, with all my heart and soul, but she was unhappy, and her heart was filled with many things. And when I went to see my aunt I felt like a child, and so I went back to my aunt's house for a day, and I got the beggars out of the way. When I went up to see my father, there were so many beggars that I could not help saying, \"Now, go round the corner, and let them go out.\" So my aunt did as I ordered; she was very nice to me.\n",
            "\n",
            "My aunt kept her eyes fixed on me and kept looking at me to see what I wanted to tell her, for she was very fond of talking about things I would like to tell her. When my mother was sick, I got very good at talking, but her mother, with a very good smile, said I was a poor beggar. I cried and screamed at the same time because I said she was poor, and she said she was much ill, and I said I was sick, and she said I was too sick. So I went to see my aunt, and I felt very merry. I could no longer sit still, for she had so much fatigue that I couldn't breath, and when I reached her I made her happy, and she said: \"Now, do not think too much about me, for your mother is very ill.\"\n",
            "\n",
            "Now my aunt told me that I was a poor beggar, and there was a great deal I wanted to tell her, for I\n",
            "\n",
            "[1100 | 690.80] loss=3.71 avg=3.21\n",
            "[1101 | 691.31] loss=3.46 avg=3.21\n",
            "[1102 | 691.83] loss=2.78 avg=3.21\n",
            "[1103 | 692.34] loss=2.74 avg=3.20\n",
            "[1104 | 692.86] loss=2.87 avg=3.20\n",
            "[1105 | 693.36] loss=3.09 avg=3.20\n",
            "[1106 | 693.87] loss=3.71 avg=3.21\n",
            "[1107 | 694.38] loss=2.59 avg=3.20\n",
            "[1108 | 694.89] loss=3.70 avg=3.20\n",
            "[1109 | 695.40] loss=3.45 avg=3.21\n",
            "[1110 | 695.91] loss=3.65 avg=3.21\n",
            "[1111 | 696.42] loss=3.46 avg=3.21\n",
            "[1112 | 696.95] loss=3.28 avg=3.21\n",
            "[1113 | 697.46] loss=2.91 avg=3.21\n",
            "[1114 | 697.97] loss=3.15 avg=3.21\n",
            "[1115 | 698.48] loss=3.54 avg=3.21\n",
            "[1116 | 698.99] loss=3.24 avg=3.21\n",
            "[1117 | 699.50] loss=2.78 avg=3.21\n",
            "[1118 | 700.01] loss=3.10 avg=3.21\n",
            "[1119 | 700.53] loss=3.20 avg=3.21\n",
            "[1120 | 701.05] loss=2.51 avg=3.20\n",
            "[1121 | 701.55] loss=2.77 avg=3.20\n",
            "[1122 | 702.07] loss=3.01 avg=3.20\n",
            "[1123 | 702.58] loss=3.76 avg=3.20\n",
            "[1124 | 703.09] loss=3.20 avg=3.20\n",
            "[1125 | 703.60] loss=3.43 avg=3.20\n",
            "[1126 | 704.10] loss=3.13 avg=3.20\n",
            "[1127 | 704.61] loss=3.38 avg=3.20\n",
            "[1128 | 705.13] loss=3.03 avg=3.20\n",
            "[1129 | 705.64] loss=2.91 avg=3.20\n",
            "[1130 | 706.15] loss=2.82 avg=3.20\n",
            "[1131 | 706.66] loss=3.29 avg=3.20\n",
            "[1132 | 707.17] loss=3.17 avg=3.20\n",
            "[1133 | 707.68] loss=3.13 avg=3.20\n",
            "[1134 | 708.19] loss=3.10 avg=3.20\n",
            "[1135 | 708.69] loss=3.38 avg=3.20\n",
            "[1136 | 709.21] loss=2.53 avg=3.19\n",
            "[1137 | 709.72] loss=3.50 avg=3.19\n",
            "[1138 | 710.23] loss=2.99 avg=3.19\n",
            "[1139 | 710.74] loss=3.82 avg=3.20\n",
            "[1140 | 711.25] loss=2.94 avg=3.20\n",
            "[1141 | 711.76] loss=3.38 avg=3.20\n",
            "[1142 | 712.27] loss=3.54 avg=3.20\n",
            "[1143 | 712.78] loss=3.07 avg=3.20\n",
            "[1144 | 713.29] loss=3.63 avg=3.20\n",
            "[1145 | 713.80] loss=2.99 avg=3.20\n",
            "[1146 | 714.31] loss=3.29 avg=3.20\n",
            "[1147 | 714.82] loss=3.44 avg=3.20\n",
            "[1148 | 715.34] loss=3.86 avg=3.21\n",
            "[1149 | 715.85] loss=2.79 avg=3.21\n",
            "[1150 | 716.35] loss=2.98 avg=3.20\n",
            "[1151 | 716.86] loss=3.08 avg=3.20\n",
            "[1152 | 717.37] loss=3.48 avg=3.21\n",
            "[1153 | 717.89] loss=3.16 avg=3.21\n",
            "[1154 | 718.40] loss=3.48 avg=3.21\n",
            "[1155 | 718.91] loss=2.92 avg=3.21\n",
            "[1156 | 719.42] loss=3.44 avg=3.21\n",
            "[1157 | 719.93] loss=3.26 avg=3.21\n",
            "[1158 | 720.44] loss=3.57 avg=3.21\n",
            "[1159 | 720.95] loss=3.48 avg=3.21\n",
            "[1160 | 721.46] loss=3.42 avg=3.22\n",
            "[1161 | 721.97] loss=2.83 avg=3.21\n",
            "[1162 | 722.48] loss=3.80 avg=3.22\n",
            "[1163 | 722.99] loss=3.38 avg=3.22\n",
            "[1164 | 723.50] loss=4.07 avg=3.23\n",
            "[1165 | 724.01] loss=3.53 avg=3.23\n",
            "[1166 | 724.52] loss=3.20 avg=3.23\n",
            "[1167 | 725.03] loss=2.46 avg=3.22\n",
            "[1168 | 725.55] loss=3.67 avg=3.23\n",
            "[1169 | 726.05] loss=3.06 avg=3.23\n",
            "[1170 | 726.57] loss=2.74 avg=3.22\n",
            "[1171 | 727.08] loss=3.24 avg=3.22\n",
            "[1172 | 727.60] loss=3.52 avg=3.22\n",
            "[1173 | 728.11] loss=2.86 avg=3.22\n",
            "[1174 | 728.62] loss=3.08 avg=3.22\n",
            "[1175 | 729.13] loss=3.08 avg=3.22\n",
            "[1176 | 729.64] loss=3.34 avg=3.22\n",
            "[1177 | 730.15] loss=3.22 avg=3.22\n",
            "[1178 | 730.66] loss=3.14 avg=3.22\n",
            "[1179 | 731.17] loss=3.07 avg=3.22\n",
            "[1180 | 731.68] loss=2.80 avg=3.21\n",
            "[1181 | 732.19] loss=3.49 avg=3.22\n",
            "[1182 | 732.70] loss=3.85 avg=3.22\n",
            "[1183 | 733.20] loss=3.17 avg=3.22\n",
            "[1184 | 733.71] loss=2.83 avg=3.22\n",
            "[1185 | 734.22] loss=3.61 avg=3.22\n",
            "[1186 | 734.73] loss=3.09 avg=3.22\n",
            "[1187 | 735.25] loss=3.00 avg=3.22\n",
            "[1188 | 735.77] loss=2.87 avg=3.21\n",
            "[1189 | 736.28] loss=3.28 avg=3.22\n",
            "[1190 | 736.79] loss=2.80 avg=3.21\n",
            "[1191 | 737.30] loss=2.89 avg=3.21\n",
            "[1192 | 737.81] loss=2.86 avg=3.20\n",
            "[1193 | 738.32] loss=3.01 avg=3.20\n",
            "[1194 | 738.82] loss=2.47 avg=3.20\n",
            "[1195 | 739.33] loss=3.22 avg=3.20\n",
            "[1196 | 739.84] loss=2.39 avg=3.19\n",
            "[1197 | 740.35] loss=3.41 avg=3.19\n",
            "[1198 | 740.86] loss=3.24 avg=3.19\n",
            "[1199 | 741.37] loss=3.20 avg=3.19\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            ", a man of the highest kind, or an aristocrat, might have been brought to power, but for him it was a man who had gone to India, when it was supposed that the world's greatest conqueror had gone to India, and that his empire had gone to India! A man who had conquered a great people was one who was no longer merely for it, but for the benefit of a great many people, if that be true. He had gone to India, he had not forgotten his countrymen's good work in doing it, he had been an important part, and it was time for him to go home. But there was one woman, a woman of a higher status than himself, who had gone to India and sought to conquer her land, and she was not yet an American. She had gone to India to make America seem like a country in which to be a great nation was a foreign thing from her father's view, she had no business to be known in America; and this person was no longer her husband, and she had returned from India, but she could not become a great citizen to be an American, and her son took her, and in a few days after her departure she began to live in the little villa she knew, in a place where one would expect to keep a visitor, just in case. She went a little while, and that was when her husband met her.\n",
            "\n",
            "Now I have spoken of the woman in the garden that I had earlier alluded to, and she was the daughter of another man. When I talked about my father, I had not intended to show that he had lost much, but to show that his son had become a great-grandson of my father in a different way. My mother, who possessed the world's greatest power, had long been the mother of some of the greatest-grandchildren of this earth; and as the great-grandson of my father, her character, her reputation, her genius, her manners, her skill, had become much better, I thought, as the world's greatest and most advanced mother had become, and now the woman could take her place beside her husband, and I thought equally as much, for she, with her own body, the whole body of the whole great nation, was under the control of my husband, who was my mother. And while he was doing it, I heard him ask my mother, \"Who have you become,\" and she answered me, that I had been a great-grandson of my father. And when they were married, my mind was occupied with the beautiful woman, and it was strange, for she had been beautiful to me in old age, and now she was just as beautiful, the mother of my son, but the mother was no longer her husband. She could not endure it as I had spoken of her, and I did not hesitate to go on going forward with the marriage, but I had many things to fear, for there was a terrible famine, and there was a great war going on in China, and I had many things to fear, much about the wife, and I had many things to fear, more than anything I had before, about them all.\n",
            "\n",
            "I do not wish to go into the causes of the death of my father. But it would seem that he was very poor, and that he lacked any resources of his own. His life was a very poor one; there were so many things in the world. He had many things to live for, things that we ought to have, and what is better than nothing? I would not suppose that he had any hope of any of these things, but his memory was a very bad memory; it is not the memory of a man who has a great wealth; it is the memory of a man who has a very bad life, and yet he has saved so much!\n",
            "\n",
            "When I was a little boy, my father had many friends in the school, and I was not only a poor boy, I was also a very poor, feeble-minded boy. My father was not the sort of person to be found at school without having studied much in the books and papers that he carried about in the room. He took me in his room and made me eat rice cakes and soup, and made me go to school, and his mind was very strong; he did not like to see me do these things. He gave me the best food that he could, and all his friends at school, and taught me all sorts of arts, and all sorts of languages. When I was only a girl, my father would take me to his room, and give me all kinds of books, and he taught me many things, and I was very happy there, and used to love him. He never took me out of the school, but just sat in front of my father, and made me eat rice cakes, and my mother would give me a bowl of soup, and a bowl of rice cakes as gifts. Then whenever I\n",
            "\n",
            "[1200 | 752.60] loss=2.75 avg=3.19\n",
            "[1201 | 753.11] loss=3.44 avg=3.19\n",
            "[1202 | 753.62] loss=3.52 avg=3.19\n",
            "[1203 | 754.13] loss=3.36 avg=3.19\n",
            "[1204 | 754.64] loss=3.15 avg=3.19\n",
            "[1205 | 755.15] loss=3.06 avg=3.19\n",
            "[1206 | 755.67] loss=2.97 avg=3.19\n",
            "[1207 | 756.18] loss=2.92 avg=3.19\n",
            "[1208 | 756.69] loss=2.64 avg=3.18\n",
            "[1209 | 757.21] loss=3.11 avg=3.18\n",
            "[1210 | 757.72] loss=3.19 avg=3.18\n",
            "[1211 | 758.23] loss=2.81 avg=3.18\n",
            "[1212 | 758.74] loss=3.28 avg=3.18\n",
            "[1213 | 759.25] loss=2.74 avg=3.17\n",
            "[1214 | 759.76] loss=2.81 avg=3.17\n",
            "[1215 | 760.28] loss=2.87 avg=3.17\n",
            "[1216 | 760.80] loss=2.96 avg=3.16\n",
            "[1217 | 761.31] loss=2.98 avg=3.16\n",
            "[1218 | 761.82] loss=3.59 avg=3.17\n",
            "[1219 | 762.33] loss=3.77 avg=3.17\n",
            "[1220 | 762.84] loss=2.70 avg=3.17\n",
            "[1221 | 763.35] loss=2.59 avg=3.16\n",
            "[1222 | 763.86] loss=3.00 avg=3.16\n",
            "[1223 | 764.37] loss=3.22 avg=3.16\n",
            "[1224 | 764.88] loss=3.08 avg=3.16\n",
            "[1225 | 765.39] loss=3.36 avg=3.16\n",
            "[1226 | 765.90] loss=2.79 avg=3.16\n",
            "[1227 | 766.41] loss=2.82 avg=3.16\n",
            "[1228 | 766.92] loss=2.85 avg=3.15\n",
            "[1229 | 767.43] loss=3.38 avg=3.15\n",
            "[1230 | 767.94] loss=2.97 avg=3.15\n",
            "[1231 | 768.46] loss=3.62 avg=3.16\n",
            "[1232 | 768.97] loss=2.91 avg=3.16\n",
            "[1233 | 769.49] loss=3.48 avg=3.16\n",
            "[1234 | 770.00] loss=3.07 avg=3.16\n",
            "[1235 | 770.51] loss=2.87 avg=3.15\n",
            "[1236 | 771.02] loss=3.49 avg=3.16\n",
            "[1237 | 771.53] loss=3.72 avg=3.16\n",
            "[1238 | 772.04] loss=3.14 avg=3.16\n",
            "[1239 | 772.55] loss=3.46 avg=3.17\n",
            "[1240 | 773.06] loss=3.31 avg=3.17\n",
            "[1241 | 773.57] loss=3.61 avg=3.17\n",
            "[1242 | 774.09] loss=2.96 avg=3.17\n",
            "[1243 | 774.61] loss=3.28 avg=3.17\n",
            "[1244 | 775.12] loss=2.80 avg=3.17\n",
            "[1245 | 775.62] loss=2.98 avg=3.17\n",
            "[1246 | 776.14] loss=3.38 avg=3.17\n",
            "[1247 | 776.64] loss=3.16 avg=3.17\n",
            "[1248 | 777.16] loss=3.27 avg=3.17\n",
            "[1249 | 777.67] loss=3.34 avg=3.17\n",
            "[1250 | 778.18] loss=3.28 avg=3.17\n",
            "[1251 | 778.69] loss=3.17 avg=3.17\n",
            "[1252 | 779.20] loss=3.14 avg=3.17\n",
            "[1253 | 779.71] loss=3.14 avg=3.17\n",
            "[1254 | 780.22] loss=3.20 avg=3.17\n",
            "[1255 | 780.73] loss=3.13 avg=3.17\n",
            "[1256 | 781.24] loss=3.15 avg=3.17\n",
            "[1257 | 781.75] loss=3.32 avg=3.17\n",
            "[1258 | 782.26] loss=2.72 avg=3.17\n",
            "[1259 | 782.77] loss=3.19 avg=3.17\n",
            "[1260 | 783.28] loss=3.04 avg=3.17\n",
            "[1261 | 783.80] loss=3.08 avg=3.17\n",
            "[1262 | 784.32] loss=3.24 avg=3.17\n",
            "[1263 | 784.83] loss=2.99 avg=3.16\n",
            "[1264 | 785.34] loss=3.53 avg=3.17\n",
            "[1265 | 785.85] loss=3.34 avg=3.17\n",
            "[1266 | 786.36] loss=3.45 avg=3.17\n",
            "[1267 | 786.88] loss=2.95 avg=3.17\n",
            "[1268 | 787.39] loss=3.25 avg=3.17\n",
            "[1269 | 787.90] loss=3.16 avg=3.17\n",
            "[1270 | 788.41] loss=3.44 avg=3.17\n",
            "[1271 | 788.92] loss=3.34 avg=3.18\n",
            "[1272 | 789.43] loss=3.81 avg=3.18\n",
            "[1273 | 789.94] loss=3.00 avg=3.18\n",
            "[1274 | 790.45] loss=2.71 avg=3.18\n",
            "[1275 | 790.96] loss=3.20 avg=3.18\n",
            "[1276 | 791.47] loss=3.16 avg=3.18\n",
            "[1277 | 791.98] loss=2.80 avg=3.17\n",
            "[1278 | 792.49] loss=2.68 avg=3.17\n",
            "[1279 | 793.00] loss=3.23 avg=3.17\n",
            "[1280 | 793.51] loss=3.00 avg=3.17\n",
            "[1281 | 794.02] loss=2.85 avg=3.16\n",
            "[1282 | 794.53] loss=3.71 avg=3.17\n",
            "[1283 | 795.04] loss=3.28 avg=3.17\n",
            "[1284 | 795.55] loss=3.35 avg=3.17\n",
            "[1285 | 796.06] loss=2.45 avg=3.16\n",
            "[1286 | 796.57] loss=2.85 avg=3.16\n",
            "[1287 | 797.08] loss=3.28 avg=3.16\n",
            "[1288 | 797.59] loss=2.71 avg=3.16\n",
            "[1289 | 798.10] loss=3.35 avg=3.16\n",
            "[1290 | 798.61] loss=3.28 avg=3.16\n",
            "[1291 | 799.12] loss=2.95 avg=3.16\n",
            "[1292 | 799.63] loss=3.01 avg=3.16\n",
            "[1293 | 800.14] loss=3.12 avg=3.16\n",
            "[1294 | 800.65] loss=3.55 avg=3.16\n",
            "[1295 | 801.16] loss=3.15 avg=3.16\n",
            "[1296 | 801.67] loss=3.09 avg=3.16\n",
            "[1297 | 802.18] loss=3.07 avg=3.16\n",
            "[1298 | 802.69] loss=2.65 avg=3.15\n",
            "[1299 | 803.20] loss=3.24 avg=3.15\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " waiting, and his legs were covered with blood.\n",
            "\n",
            "His eyes opened suddenly and he looked down at the corpse; at it with a terrible, frightful gaze, and he saw the face, the hair, the teeth on both sides of his face. He heard what he should hear, and he knew, after a long time, that the man was dead!\n",
            "\n",
            "But his thoughts were different--they were not completely in the head; they were more complex. He knew that his eyes were opened, and he could see very clearly, as if he were in a dream, the face of the dead man.\n",
            "\n",
            "But his body had been burned to death with an iron rod in the cave's fire.\n",
            "\n",
            "When the man drew one blood-stained finger from his own, his mind was filled with a vivid experience--he felt that with that, the only man alive would be born.\n",
            "\n",
            "And it happened again. In that instant his mind reeled. His body had been burned to death. All his thoughts were in a dream, and, with that, he became conscious of the presence and the desire of death.\n",
            "\n",
            "But everything remained in his head. He could not remember the face, but at the same time he could sense its hands--his face!\n",
            "\n",
            "He was thinking of the picture which was on the wall: for he did not know how quickly some one would reach the door, and he did not care whether he would reach it by step or by day.\n",
            "\n",
            "A few steps went by, and in a few seconds he was there. He was at the door, holding his own; his finger--the one burning in the cave. He saw the face, the hair, the teeth, and his own eyes. He knew that he could feel the man, and he knew, after what he knew, how he was to feel it--at least, when the body was burned, and all the teeth and the skin were eaten up! He saw the face, he remembered him.\n",
            "\n",
            "Suddenly the head of the man's family moved in and took charge.\n",
            "\n",
            "All his memories of the face and his experience of the man were replaced by a sensation, a sensation which made those who were in the cave shudder.\n",
            "\n",
            "The cave, like a nightmare, would grow darker and darker; indeed, the time would come when all that was sacred and holy--the sacred and holy part of the cave would be wiped away; and that sacred and holy portion of the cave would vanish to its last.\n",
            "\n",
            "Thus, the dream and the dream came crashing down upon him like a crash of lightning upon a road of death, and he was dead.\n",
            "\n",
            "The cave, like a nightmare, would grow colder--it would turn white and dark--there would be a darkness beneath the dark.\n",
            "\n",
            "When he was out of the house, the man brought a bottle of wine--there was no bottle in the whole house. So, too, came his hand, his finger--that in the dream and in his hands and in his eyes.\n",
            "\n",
            "Then--\n",
            "\n",
            "The day was to be very strange, as the man was going out--but here he was--and he was standing under the tree. He had lost his sight. The man had fallen through a tree and it had burned through his eye--he could not see the man, but he saw him coming down.\n",
            "\n",
            "His hand, that in the dream and that in the hand--that was there, he stood up. His palm reached his nose--that was there, he stood up. He was in his eyes. There was a long, black hair on both sides. His ears were straight, the middle portion of his nose had been torn out and covered with blood, his face, too; his face...and his mouth.\n",
            "\n",
            "And there was still no one there--there was no face, no face; there was only fire.\n",
            "\n",
            "His hands were numb.\n",
            "\n",
            "And there was still no one there in the cave,--there was only one.\n",
            "\n",
            "The door swung open, and the door was shut.\n",
            "\n",
            "And in that moment, and on that moment, the man was at the door, holding the hand of the man and the man held it--the hand was frozen to the skin. The man was with it, dead.\n",
            "\n",
            "But, the man was in the door too; he was in the door too, dead.\n",
            "\n",
            "And the door rolled and the door fell.\n",
            "\n",
            "And now the cave, like a nightmare, seemed to be changing into a world of its own. It was white and dark and still! And the man stood there, holding his hand. It was still on his hand, on his hand, still there! He was right at hand--right at hand! This was the man, here the man was holding himself, with his hand still on his hand. This was the one, standing up at the door, staring at the man.\n",
            "\n",
            "[1300 | 814.55] loss=2.91 avg=3.15\n",
            "[1301 | 815.06] loss=3.15 avg=3.15\n",
            "[1302 | 815.57] loss=3.30 avg=3.15\n",
            "[1303 | 816.08] loss=3.71 avg=3.16\n",
            "[1304 | 816.59] loss=3.02 avg=3.16\n",
            "[1305 | 817.10] loss=3.59 avg=3.16\n",
            "[1306 | 817.61] loss=3.24 avg=3.16\n",
            "[1307 | 818.13] loss=2.70 avg=3.16\n",
            "[1308 | 818.64] loss=3.03 avg=3.16\n",
            "[1309 | 819.16] loss=3.34 avg=3.16\n",
            "[1310 | 819.66] loss=2.72 avg=3.15\n",
            "[1311 | 820.17] loss=3.72 avg=3.16\n",
            "[1312 | 820.68] loss=3.35 avg=3.16\n",
            "[1313 | 821.19] loss=3.15 avg=3.16\n",
            "[1314 | 821.70] loss=2.91 avg=3.16\n",
            "[1315 | 822.21] loss=2.87 avg=3.16\n",
            "[1316 | 822.72] loss=2.88 avg=3.15\n",
            "[1317 | 823.24] loss=3.54 avg=3.16\n",
            "[1318 | 823.75] loss=3.35 avg=3.16\n",
            "[1319 | 824.27] loss=2.91 avg=3.16\n",
            "[1320 | 824.78] loss=3.78 avg=3.16\n",
            "[1321 | 825.29] loss=2.94 avg=3.16\n",
            "[1322 | 825.80] loss=2.83 avg=3.16\n",
            "[1323 | 826.31] loss=3.39 avg=3.16\n",
            "[1324 | 826.83] loss=3.01 avg=3.16\n",
            "[1325 | 827.34] loss=3.05 avg=3.16\n",
            "[1326 | 827.85] loss=3.02 avg=3.16\n",
            "[1327 | 828.36] loss=3.48 avg=3.16\n",
            "[1328 | 828.87] loss=3.25 avg=3.16\n",
            "[1329 | 829.38] loss=3.44 avg=3.16\n",
            "[1330 | 829.89] loss=3.42 avg=3.17\n",
            "[1331 | 830.42] loss=2.45 avg=3.16\n",
            "[1332 | 830.92] loss=2.89 avg=3.16\n",
            "[1333 | 831.43] loss=3.24 avg=3.16\n",
            "[1334 | 831.95] loss=3.06 avg=3.16\n",
            "[1335 | 832.47] loss=2.87 avg=3.15\n",
            "[1336 | 832.98] loss=2.85 avg=3.15\n",
            "[1337 | 833.49] loss=3.16 avg=3.15\n",
            "[1338 | 834.00] loss=2.70 avg=3.15\n",
            "[1339 | 834.51] loss=2.95 avg=3.14\n",
            "[1340 | 835.02] loss=2.75 avg=3.14\n",
            "[1341 | 835.53] loss=2.65 avg=3.13\n",
            "[1342 | 836.04] loss=2.93 avg=3.13\n",
            "[1343 | 836.54] loss=2.59 avg=3.13\n",
            "[1344 | 837.05] loss=3.40 avg=3.13\n",
            "[1345 | 837.57] loss=2.97 avg=3.13\n",
            "[1346 | 838.08] loss=3.25 avg=3.13\n",
            "[1347 | 838.58] loss=2.60 avg=3.12\n",
            "[1348 | 839.09] loss=2.71 avg=3.12\n",
            "[1349 | 839.61] loss=3.58 avg=3.12\n",
            "[1350 | 840.12] loss=3.17 avg=3.12\n",
            "[1351 | 840.63] loss=3.70 avg=3.13\n",
            "[1352 | 841.14] loss=2.74 avg=3.13\n",
            "[1353 | 841.64] loss=2.62 avg=3.12\n",
            "[1354 | 842.15] loss=3.49 avg=3.13\n",
            "[1355 | 842.67] loss=2.88 avg=3.12\n",
            "[1356 | 843.17] loss=2.95 avg=3.12\n",
            "[1357 | 843.69] loss=2.69 avg=3.12\n",
            "[1358 | 844.19] loss=3.37 avg=3.12\n",
            "[1359 | 844.71] loss=2.60 avg=3.11\n",
            "[1360 | 845.22] loss=2.95 avg=3.11\n",
            "[1361 | 845.73] loss=3.26 avg=3.11\n",
            "[1362 | 846.24] loss=3.47 avg=3.12\n",
            "[1363 | 846.74] loss=2.96 avg=3.12\n",
            "[1364 | 847.26] loss=3.52 avg=3.12\n",
            "[1365 | 847.77] loss=2.69 avg=3.12\n",
            "[1366 | 848.28] loss=2.59 avg=3.11\n",
            "[1367 | 848.79] loss=3.24 avg=3.11\n",
            "[1368 | 849.30] loss=3.42 avg=3.11\n",
            "[1369 | 849.81] loss=3.23 avg=3.12\n",
            "[1370 | 850.32] loss=3.00 avg=3.11\n",
            "[1371 | 850.83] loss=2.94 avg=3.11\n",
            "[1372 | 851.34] loss=3.26 avg=3.11\n",
            "[1373 | 851.85] loss=2.83 avg=3.11\n",
            "[1374 | 852.36] loss=2.83 avg=3.11\n",
            "[1375 | 852.87] loss=3.47 avg=3.11\n",
            "[1376 | 853.38] loss=2.86 avg=3.11\n",
            "[1377 | 853.89] loss=2.45 avg=3.10\n",
            "[1378 | 854.41] loss=2.99 avg=3.10\n",
            "[1379 | 854.92] loss=3.25 avg=3.10\n",
            "[1380 | 855.43] loss=2.91 avg=3.10\n",
            "[1381 | 855.94] loss=2.84 avg=3.10\n",
            "[1382 | 856.45] loss=2.91 avg=3.10\n",
            "[1383 | 856.96] loss=3.25 avg=3.10\n",
            "[1384 | 857.47] loss=2.96 avg=3.10\n",
            "[1385 | 857.98] loss=3.22 avg=3.10\n",
            "[1386 | 858.49] loss=3.27 avg=3.10\n",
            "[1387 | 859.00] loss=3.33 avg=3.10\n",
            "[1388 | 859.51] loss=2.77 avg=3.10\n",
            "[1389 | 860.02] loss=2.68 avg=3.10\n",
            "[1390 | 860.53] loss=2.93 avg=3.09\n",
            "[1391 | 861.04] loss=3.12 avg=3.09\n",
            "[1392 | 861.55] loss=2.57 avg=3.09\n",
            "[1393 | 862.06] loss=3.63 avg=3.09\n",
            "[1394 | 862.57] loss=3.20 avg=3.10\n",
            "[1395 | 863.08] loss=3.32 avg=3.10\n",
            "[1396 | 863.59] loss=3.09 avg=3.10\n",
            "[1397 | 864.11] loss=3.21 avg=3.10\n",
            "[1398 | 864.62] loss=3.19 avg=3.10\n",
            "[1399 | 865.13] loss=3.63 avg=3.10\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " to go and find out what I wanted.\" The door opened and she heard a familiar voice say: \"You will be at my place later, and I can assure you.\"\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Sarah was lying on the bed of one of the beds in a little room that was very badly pillaged and painted. A few curtains hung round the window, and on the bed which she was lying on was a very beautiful rose. A great, deep blue-bottomed rose was about to rise, and a beautiful little finger, about five feet long, was in the flower at the base, in the centre of the finger. The whole rose was very delicate and delicate, and the little finger, about five inches long by five inches wide, which had only three branches, looked as it did when it was touched.\n",
            "\n",
            "\"Is it not beautiful?\" said Sarah. She looked up at the little rose, and the little finger grew very beautiful. \"There shall never be any more of this,\" said the voice.\n",
            "\n",
            "Sarah had no way of knowing what this was for, since the rose was just before her forehead. She had a long, dark hair, and it was very long and round, and curved. A nice clean beard, of about the same quantity as that under the eyes, stood out from the long hair; and in the middle of the beard was a small, very delicate rose, with two small, very delicate little flowers, and a very thin rose on its breast. It stood so close together with the rose, that the little finger had a beautiful, wonderful effect on the little finger. But it was very thin. It was so delicate that it looked very pale, and did not seem to like anything at all. Sometimes a little finger would drop from the side of the rose, and as the little finger pressed into the finger and was wrapped round it, a great quantity of rose-wax and soap would fall on it. But it was as much as it took to change the appearance of its body. It was very delicate, and delicate as a baby in the womb.\n",
            "\n",
            "Sarah looked around and saw this little finger growing very beautiful; and she was very happy when she looked at her.\n",
            "\n",
            "\"Is there any flower in this little finger? Yes; it is beautiful, and you may look it over and put your finger into it. Do you like it?\" said the little finger, suddenly getting really beautiful. Then it fell to writing on it, \"Sarah!\"\n",
            "\n",
            "Sarah was very happy, and moved all around, and when she looked at Sarah, she saw the little finger with the writing stuck out of it. So she put her finger into the writing, and the little finger became very beautiful. Then it fell to writing on the bed, and the little finger started to look very little; so it fell to writing on the bed and Sarah started to be very happy! And when she started to be very happy, as she sat down to write in her little finger, her heart was very full, and the writing was all very beautiful! She felt her heart beat like to a beat, and she began to really enjoy writing. She began to tell her story and make up her story, and to tell stories, and to have fun! She became very happy! Then she began again to have fun because she was so much attracted to writing, and she was so happy! And Sarah knew that writing was her best pleasure, because it was the best pleasure, and she wrote very beautiful stories!\n",
            "\n",
            "In the summer of 1870 Sarah attended to a friend of Sarah's, Robert P. Whiteham, and as he was attending dinner, when he heard that her friend had lost his life, he said: \"You are all right about Sarah Whiteham, but who is that?\"\n",
            "\n",
            "\"Poor girl!\" said Whiteham, who was a very good man. \"Poor little thing lost, so much so.\"\n",
            "\n",
            "That was about it. On his face it was very sad, and he said: \"Well, I believe it is Mrs. Whiteham, the only one who lost an eye.\"\n",
            "\n",
            "She was very happy and very much excited and much pleased; just as she was when Whiteham was still alive.\n",
            "\n",
            "\"Well, we can't be too sure,\" he said.\n",
            "\n",
            "\"Oh, do not fret, Sarah,\" she said. \"Don't fret, Sarah, just as we are. We were all wrong when our eyes were first sighted. It is not an eye--\"\n",
            "\n",
            "He was very clear on that, and he gave her an old book.\n",
            "\n",
            "\"Good heavens!\" cried Sarah.\n",
            "\n",
            "He read for a long while, and a little while after that he told the story of the eye.\n",
            "\n",
            "\"It was always an eye, the eye of Sarah.\"\n",
            "\n",
            "The eyes of Sarah, as it turns out, were always an eye. \"No,\" she said, \"they were always a eye.\"\n",
            "\n",
            "He had been in this\n",
            "\n",
            "[1400 | 876.40] loss=2.98 avg=3.10\n",
            "[1401 | 876.91] loss=3.17 avg=3.10\n",
            "[1402 | 877.43] loss=2.69 avg=3.10\n",
            "[1403 | 877.94] loss=2.94 avg=3.10\n",
            "[1404 | 878.45] loss=3.01 avg=3.10\n",
            "[1405 | 878.96] loss=3.44 avg=3.10\n",
            "[1406 | 879.47] loss=3.03 avg=3.10\n",
            "[1407 | 879.99] loss=3.23 avg=3.10\n",
            "[1408 | 880.51] loss=3.30 avg=3.10\n",
            "[1409 | 881.01] loss=2.66 avg=3.10\n",
            "[1410 | 881.53] loss=2.68 avg=3.09\n",
            "[1411 | 882.04] loss=3.16 avg=3.10\n",
            "[1412 | 882.55] loss=2.68 avg=3.09\n",
            "[1413 | 883.07] loss=3.16 avg=3.09\n",
            "[1414 | 883.58] loss=3.37 avg=3.09\n",
            "[1415 | 884.09] loss=2.96 avg=3.09\n",
            "[1416 | 884.60] loss=3.56 avg=3.10\n",
            "[1417 | 885.11] loss=2.43 avg=3.09\n",
            "[1418 | 885.62] loss=3.22 avg=3.09\n",
            "[1419 | 886.13] loss=2.65 avg=3.09\n",
            "[1420 | 886.64] loss=3.12 avg=3.09\n",
            "[1421 | 887.15] loss=2.86 avg=3.09\n",
            "[1422 | 887.66] loss=2.82 avg=3.08\n",
            "[1423 | 888.18] loss=2.73 avg=3.08\n",
            "[1424 | 888.70] loss=3.36 avg=3.08\n",
            "[1425 | 889.21] loss=3.10 avg=3.08\n",
            "[1426 | 889.71] loss=3.38 avg=3.09\n",
            "[1427 | 890.23] loss=3.24 avg=3.09\n",
            "[1428 | 890.74] loss=3.04 avg=3.09\n",
            "[1429 | 891.26] loss=2.50 avg=3.08\n",
            "[1430 | 891.77] loss=3.56 avg=3.09\n",
            "[1431 | 892.28] loss=3.01 avg=3.08\n",
            "[1432 | 892.79] loss=3.19 avg=3.09\n",
            "[1433 | 893.30] loss=2.96 avg=3.08\n",
            "[1434 | 893.81] loss=2.95 avg=3.08\n",
            "[1435 | 894.32] loss=2.47 avg=3.08\n",
            "[1436 | 894.83] loss=3.19 avg=3.08\n",
            "[1437 | 895.34] loss=2.58 avg=3.07\n",
            "[1438 | 895.86] loss=3.26 avg=3.08\n",
            "[1439 | 896.38] loss=2.39 avg=3.07\n",
            "[1440 | 896.89] loss=3.67 avg=3.07\n",
            "[1441 | 897.40] loss=3.92 avg=3.08\n",
            "[1442 | 897.91] loss=2.85 avg=3.08\n",
            "[1443 | 898.42] loss=3.46 avg=3.08\n",
            "[1444 | 898.94] loss=3.30 avg=3.09\n",
            "[1445 | 899.46] loss=2.74 avg=3.08\n",
            "[1446 | 899.97] loss=3.46 avg=3.09\n",
            "[1447 | 900.48] loss=3.38 avg=3.09\n",
            "[1448 | 900.99] loss=2.99 avg=3.09\n",
            "[1449 | 901.50] loss=3.17 avg=3.09\n",
            "[1450 | 902.01] loss=2.83 avg=3.09\n",
            "[1451 | 902.52] loss=3.34 avg=3.09\n",
            "[1452 | 903.02] loss=2.81 avg=3.09\n",
            "[1453 | 903.53] loss=3.77 avg=3.09\n",
            "[1454 | 904.04] loss=3.09 avg=3.09\n",
            "[1455 | 904.55] loss=3.38 avg=3.10\n",
            "[1456 | 905.08] loss=2.99 avg=3.10\n",
            "[1457 | 905.59] loss=3.07 avg=3.10\n",
            "[1458 | 906.10] loss=3.50 avg=3.10\n",
            "[1459 | 906.61] loss=2.60 avg=3.09\n",
            "[1460 | 907.12] loss=3.38 avg=3.10\n",
            "[1461 | 907.63] loss=2.98 avg=3.10\n",
            "[1462 | 908.14] loss=2.96 avg=3.09\n",
            "[1463 | 908.65] loss=2.97 avg=3.09\n",
            "[1464 | 909.16] loss=3.54 avg=3.10\n",
            "[1465 | 909.67] loss=3.31 avg=3.10\n",
            "[1466 | 910.18] loss=2.48 avg=3.09\n",
            "[1467 | 910.69] loss=2.88 avg=3.09\n",
            "[1468 | 911.20] loss=3.32 avg=3.09\n",
            "[1469 | 911.71] loss=3.22 avg=3.10\n",
            "[1470 | 912.22] loss=2.99 avg=3.09\n",
            "[1471 | 912.73] loss=3.41 avg=3.10\n",
            "[1472 | 913.24] loss=3.44 avg=3.10\n",
            "[1473 | 913.75] loss=2.86 avg=3.10\n",
            "[1474 | 914.26] loss=3.71 avg=3.10\n",
            "[1475 | 914.77] loss=3.18 avg=3.11\n",
            "[1476 | 915.28] loss=2.54 avg=3.10\n",
            "[1477 | 915.80] loss=3.21 avg=3.10\n",
            "[1478 | 916.32] loss=3.35 avg=3.10\n",
            "[1479 | 916.83] loss=3.41 avg=3.11\n",
            "[1480 | 917.34] loss=2.76 avg=3.10\n",
            "[1481 | 917.85] loss=3.00 avg=3.10\n",
            "[1482 | 918.36] loss=2.69 avg=3.10\n",
            "[1483 | 918.87] loss=3.18 avg=3.10\n",
            "[1484 | 919.38] loss=3.13 avg=3.10\n",
            "[1485 | 919.89] loss=3.35 avg=3.10\n",
            "[1486 | 920.40] loss=2.92 avg=3.10\n",
            "[1487 | 920.91] loss=3.70 avg=3.11\n",
            "[1488 | 921.42] loss=3.20 avg=3.11\n",
            "[1489 | 921.93] loss=3.39 avg=3.11\n",
            "[1490 | 922.44] loss=3.46 avg=3.11\n",
            "[1491 | 922.95] loss=3.60 avg=3.12\n",
            "[1492 | 923.46] loss=3.49 avg=3.12\n",
            "[1493 | 923.97] loss=2.78 avg=3.12\n",
            "[1494 | 924.48] loss=2.56 avg=3.11\n",
            "[1495 | 924.99] loss=3.24 avg=3.11\n",
            "[1496 | 925.50] loss=2.90 avg=3.11\n",
            "[1497 | 926.01] loss=3.45 avg=3.12\n",
            "[1498 | 926.52] loss=3.37 avg=3.12\n",
            "[1499 | 927.03] loss=2.80 avg=3.11\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            ", \"Well, if I were to be anyhow murdered, and the only person who had known me would have been you, I wish that I could tell you that, now, my dear friend, I can say: I can assure you that there's a story in the papers of an Englishman killed in England, and the only thing I've ever heard from him about the murder is that he didn't know anything about it.\" But he didn't understand it at the time. And I still don't.\n",
            "\n",
            "\"What? Why don't you tell her?\"\n",
            "\n",
            "\"Because, dear, at that time I had no more children, and, dear, the children would probably not be living to be children, and, dear, it'd be a good thing her, my boy!\"\n",
            "\n",
            "\"I believe you, sweetie, but,\" I said, and took off the topcoat, \"I think the children may have been at home or gone over, and had been going to town and being merry. I've been thinking about that and will tell you when you can, and if not, I think it'll be a good little story to tell. I've been looking for the children ever since I came out of my parents' house. I've heard that you have had all the children to-night, but I'm a very old girl, and you haven't. I never was born, don't you know, but I took the last bath the minute that there came a noise upon me. I'm quite sure that's not to be seen by anybody now, and a man was shot and killed. Good sir, tell me, I'm sorry for you, but, dear little boy, I am so sorry that I couldn't help it. I couldn't go to school. I couldn't find a wife. I couldn't get into any business. I suppose I am quite ill now, and I'll let you know when you can, and, dear little boy, I beg you to help me, and, dear little boy, tell me!\"\n",
            "\n",
            "\"But you don't believe that. I never was a little girl. It was a matter of life and death. There was no one to-night, I had to go to school, I was a very old man when I came out of my parents' house, and that's all. You see, I was perfectly well, and I was quite well, but here I am, and we're all dead. It wouldn't be possible to tell how many children I took to-sleep here, but they were all perfectly fine, and I was perfectly well, but it's impossible to tell how far they'd come from me, and I was quite poor. You see, I couldn't go to the shops or the farm, I couldn't get into anything. I only talked to the policeman in the street for a little while, and he told me that in those days in England children had to sleep on the floor by themselves, and I was very pretty, but you'll never hear me talk so much again.\"\n",
            "\n",
            "He was going crazy, and I thought he must be insane. I took off the topcoat, pulled the top of my coat, and went inside the house.\n",
            "\n",
            "\"I want to see you, Little Boy, and ask you for the certificate you have given me for my passport. You are in good spirits, but I can't keep you any longer, will we?\"\n",
            "\n",
            "\"I don't think you should answer any questions, Little Boy, but we'll see between two bottles of wine, and I'll keep you ready. Have you two to-night?\"\n",
            "\n",
            "\"No; I'm very ill, but I'll be ready two more nights.\"\n",
            "\n",
            "\"Good God! why did you tell me you were ill?\"\n",
            "\n",
            "\"Because you had so many children to try and keep away from, and I had to get as many of them to-night as I could, so please do not take me at your word that I must go on at all, and tell the police to keep you to-night. Will you come with me to-night?\"\n",
            "\n",
            "\"You have to go all night, and I'll go to-night with you.\"\n",
            "\n",
            "\"I'll go down from this window to-night, you see, and see if I can't find your letter, I beg you. I want to know where you belong, and I'm so sorry for you, Little Boy, for your loss. It would be much better if I could have you in here at once, and then I'll go with you, and tell the police to keep you to-night. I wish you God's love, the Good God! I'll give you the letter, too.\"\n",
            "\n",
            "I answered him, and he went into his dark car, took from it the letter he had hidden in the vestibule, and went away the next morning.\n",
            "\n",
            "I wish it had been sooner\n",
            "\n",
            "[1500 | 938.25] loss=3.74 avg=3.12\n",
            "[1501 | 938.76] loss=2.87 avg=3.12\n",
            "[1502 | 939.28] loss=3.02 avg=3.12\n",
            "[1503 | 939.78] loss=3.28 avg=3.12\n",
            "[1504 | 940.30] loss=3.02 avg=3.12\n",
            "[1505 | 940.80] loss=3.47 avg=3.12\n",
            "[1506 | 941.32] loss=3.22 avg=3.12\n",
            "[1507 | 941.82] loss=3.71 avg=3.13\n",
            "[1508 | 942.34] loss=2.49 avg=3.12\n",
            "[1509 | 942.84] loss=2.93 avg=3.12\n",
            "[1510 | 943.35] loss=2.42 avg=3.11\n",
            "[1511 | 943.86] loss=3.38 avg=3.12\n",
            "[1512 | 944.37] loss=3.04 avg=3.12\n",
            "[1513 | 944.88] loss=2.79 avg=3.11\n",
            "[1514 | 945.39] loss=3.42 avg=3.11\n",
            "[1515 | 945.90] loss=3.44 avg=3.12\n",
            "[1516 | 946.41] loss=3.65 avg=3.12\n",
            "[1517 | 946.92] loss=2.95 avg=3.12\n",
            "[1518 | 947.43] loss=3.25 avg=3.12\n",
            "[1519 | 947.94] loss=3.22 avg=3.12\n",
            "[1520 | 948.45] loss=2.82 avg=3.12\n",
            "[1521 | 948.97] loss=3.03 avg=3.12\n",
            "[1522 | 949.49] loss=3.00 avg=3.12\n",
            "[1523 | 950.00] loss=3.03 avg=3.12\n",
            "[1524 | 950.51] loss=3.16 avg=3.12\n",
            "[1525 | 951.02] loss=3.39 avg=3.12\n",
            "[1526 | 951.53] loss=3.52 avg=3.12\n",
            "[1527 | 952.04] loss=3.16 avg=3.13\n",
            "[1528 | 952.56] loss=3.84 avg=3.13\n",
            "[1529 | 953.07] loss=2.88 avg=3.13\n",
            "[1530 | 953.58] loss=3.07 avg=3.13\n",
            "[1531 | 954.10] loss=2.72 avg=3.13\n",
            "[1532 | 954.62] loss=2.70 avg=3.12\n",
            "[1533 | 955.13] loss=3.19 avg=3.12\n",
            "[1534 | 955.64] loss=3.45 avg=3.12\n",
            "[1535 | 956.15] loss=2.92 avg=3.12\n",
            "[1536 | 956.66] loss=3.46 avg=3.13\n",
            "[1537 | 957.18] loss=2.86 avg=3.12\n",
            "[1538 | 957.69] loss=3.69 avg=3.13\n",
            "[1539 | 958.20] loss=2.83 avg=3.13\n",
            "[1540 | 958.71] loss=2.56 avg=3.12\n",
            "[1541 | 959.22] loss=2.34 avg=3.11\n",
            "[1542 | 959.74] loss=3.13 avg=3.11\n",
            "[1543 | 960.25] loss=3.08 avg=3.11\n",
            "[1544 | 960.77] loss=3.12 avg=3.11\n",
            "[1545 | 961.27] loss=2.98 avg=3.11\n",
            "[1546 | 961.79] loss=3.28 avg=3.11\n",
            "[1547 | 962.30] loss=3.17 avg=3.11\n",
            "[1548 | 962.81] loss=3.15 avg=3.11\n",
            "[1549 | 963.33] loss=3.02 avg=3.11\n",
            "[1550 | 963.85] loss=3.37 avg=3.12\n",
            "[1551 | 964.35] loss=2.33 avg=3.11\n",
            "[1552 | 964.86] loss=3.65 avg=3.11\n",
            "[1553 | 965.37] loss=3.81 avg=3.12\n",
            "[1554 | 965.88] loss=2.42 avg=3.11\n",
            "[1555 | 966.39] loss=2.66 avg=3.11\n",
            "[1556 | 966.90] loss=2.81 avg=3.11\n",
            "[1557 | 967.42] loss=3.82 avg=3.11\n",
            "[1558 | 967.93] loss=3.23 avg=3.11\n",
            "[1559 | 968.44] loss=3.91 avg=3.12\n",
            "[1560 | 968.95] loss=2.63 avg=3.12\n",
            "[1561 | 969.47] loss=3.44 avg=3.12\n",
            "[1562 | 969.98] loss=2.68 avg=3.12\n",
            "[1563 | 970.49] loss=2.58 avg=3.11\n",
            "[1564 | 971.00] loss=2.51 avg=3.10\n",
            "[1565 | 971.51] loss=3.23 avg=3.11\n",
            "[1566 | 972.02] loss=3.04 avg=3.11\n",
            "[1567 | 972.53] loss=3.37 avg=3.11\n",
            "[1568 | 973.04] loss=3.17 avg=3.11\n",
            "[1569 | 973.55] loss=3.35 avg=3.11\n",
            "[1570 | 974.06] loss=3.58 avg=3.12\n",
            "[1571 | 974.57] loss=3.48 avg=3.12\n",
            "[1572 | 975.08] loss=3.01 avg=3.12\n",
            "[1573 | 975.59] loss=3.35 avg=3.12\n",
            "[1574 | 976.10] loss=3.29 avg=3.12\n",
            "[1575 | 976.61] loss=2.72 avg=3.12\n",
            "[1576 | 977.12] loss=2.66 avg=3.11\n",
            "[1577 | 977.63] loss=3.24 avg=3.11\n",
            "[1578 | 978.14] loss=3.14 avg=3.11\n",
            "[1579 | 978.66] loss=2.79 avg=3.11\n",
            "[1580 | 979.16] loss=3.30 avg=3.11\n",
            "[1581 | 979.67] loss=2.77 avg=3.11\n",
            "[1582 | 980.18] loss=3.18 avg=3.11\n",
            "[1583 | 980.69] loss=3.18 avg=3.11\n",
            "[1584 | 981.20] loss=3.07 avg=3.11\n",
            "[1585 | 981.71] loss=2.65 avg=3.11\n",
            "[1586 | 982.22] loss=2.59 avg=3.10\n",
            "[1587 | 982.73] loss=3.36 avg=3.10\n",
            "[1588 | 983.24] loss=3.07 avg=3.10\n",
            "[1589 | 983.75] loss=3.13 avg=3.10\n",
            "[1590 | 984.26] loss=3.84 avg=3.11\n",
            "[1591 | 984.77] loss=3.59 avg=3.12\n",
            "[1592 | 985.28] loss=2.82 avg=3.11\n",
            "[1593 | 985.79] loss=3.20 avg=3.11\n",
            "[1594 | 986.30] loss=3.23 avg=3.12\n",
            "[1595 | 986.82] loss=3.17 avg=3.12\n",
            "[1596 | 987.33] loss=3.83 avg=3.12\n",
            "[1597 | 987.84] loss=2.88 avg=3.12\n",
            "[1598 | 988.35] loss=3.49 avg=3.12\n",
            "[1599 | 988.86] loss=3.01 avg=3.12\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " entessorial, and by the time they knew she was dead, she was well and perfectly dead at the age of twenty-eight.\"\n",
            "\n",
            "\"A year old?\"\n",
            "\n",
            "\"A little boy!\" said Mrs. Peters.\n",
            "\n",
            "\"That is the age Mrs. Loyce is at . . . to put the theory to rest and to show that she is in good health,\" said Sir Walter Scott, adding: \"She is still very much alive, Mrs. Loyce, and well. She has the most beautiful eyes and the most beautiful speech, and she talks like a good person and I think she will live to see her death. In my opinion I believe that is the best way of preserving her life. That is why I am here. I have the utmost sympathy with Mrs. Loyce. I am here with all my heart, with all those who have loved her and cared for her. I also have the greatest sympathy with Mrs. Loyce and her family and all those who have loved her and have felt the need to remember her presence.\"\n",
            "\n",
            "\"Then, dear, it is perfectly certain you are dead?\" asked Mrs. Bredon with a good deal of earnestness.\n",
            "\n",
            "\"I am perfectly certain that I am dead,\" said Sir Walter Scott. He took a red card from the drawer and read it into Madame Loyce.\n",
            "\n",
            "\"That leaves us to- day alone with the questions,\" he said, and he put the card back again. He opened the door and entered the kitchen, looked out through the window, and then he said:\n",
            "\n",
            "\"Now, Mrs. Bredon, you must be dead.\"\n",
            "\n",
            "\"I know this is impossible,\" said the woman, still listening. \"You will be dead in an hour, but I believe you are the one to die the first instant. And do you think it is possible?\" \"Of course I do, Madame Loyce.\"\n",
            "\n",
            "\"Well, why, you remember her presence,\" said the woman. \"She used to be out for long periods of time; you remember her very well. She always used to come in and hang out with gentlemen, and she said things in a tone which would put one to sleep for half an hour.\"\n",
            "\n",
            "All the clerks and servants took their place before a table where there was a very pale, soft, pale-faced, grey-haired girl, who was sitting on an iron bench at a window, smoking a heavy cigar with a little lantern. She was talking with long-drawn words. Her speech was so gentle, so simple and clear, and she did not try to imitate it.\n",
            "\n",
            "\"The young lady,\" said the old lady, \"has been on a pleasant train of mind to such an hour. She knows that time must be a refuge for the very soul of a gentleman. To-day she will be dead; to-morrow, to-morrow, she will not. You mustn't be so sure; it wouldn't be right.\"\n",
            "\n",
            "All the clerks and people were watching Mrs. Bredon. She was in a state of shock. She was an ugly, very ugly thing, that was standing in front of her in very big, very short, white coats, with a long white skirt, a black belt round her waist, a gold-edged hat, and a red cloak lying over her head, a broad, rather rough-edged hat and a gold chain around her neck.\n",
            "\n",
            "\"How does it feel now?\" asked the lady, as she was talking with the old lady.\n",
            "\n",
            "\"Very good,\" said Mrs. Bredon. \"What does it feel like?\"\n",
            "\n",
            "\"It is very much uncomfortable,\" said the old lady, as if she had experienced some strange fear.\n",
            "\n",
            "\"Yes, but how about we talk, Lady Loyce?\" asked the girl.\n",
            "\n",
            "\"She always talks; she is very good at it,\" said the old lady, because the old girl was very clever. The old lady spoke like a man in his own right, as if to ask her a question which he had not the courage to say to herself. All the clerks and people laughed.\n",
            "\n",
            "\"How pleasant it is to talk about Lady Loyce now!\" cried the lady cheerfully; \"even for a minute she is very clear in what makes her like her.\"\n",
            "\n",
            "\"She is like everybody else,\" said the girl, as if she knew how to make her feel. The old lady was silent, and she looked at the young woman in a queer manner, and then began to talk:\n",
            "\n",
            "\"When her hands were about to touch each other they were so tight they were painful.\"\n",
            "\n",
            "Then the old lady started again:\n",
            "\n",
            "\"The old lady she was always very strong, very strong, always in the right and so brave without any excuse! They were always so close, very close, so very close! The old lady was brave, brave she\n",
            "\n",
            "[1600 | 1000.12] loss=3.57 avg=3.13\n",
            "[1601 | 1000.62] loss=3.43 avg=3.13\n",
            "[1602 | 1001.14] loss=3.24 avg=3.13\n",
            "[1603 | 1001.65] loss=3.10 avg=3.13\n",
            "[1604 | 1002.16] loss=2.86 avg=3.13\n",
            "[1605 | 1002.67] loss=2.96 avg=3.13\n",
            "[1606 | 1003.18] loss=2.61 avg=3.12\n",
            "[1607 | 1003.69] loss=3.14 avg=3.12\n",
            "[1608 | 1004.21] loss=3.12 avg=3.12\n",
            "[1609 | 1004.73] loss=3.89 avg=3.13\n",
            "[1610 | 1005.24] loss=2.90 avg=3.13\n",
            "[1611 | 1005.75] loss=3.28 avg=3.13\n",
            "[1612 | 1006.27] loss=2.77 avg=3.13\n",
            "[1613 | 1006.79] loss=3.44 avg=3.13\n",
            "[1614 | 1007.29] loss=3.36 avg=3.13\n",
            "[1615 | 1007.80] loss=3.47 avg=3.13\n",
            "[1616 | 1008.31] loss=2.98 avg=3.13\n",
            "[1617 | 1008.82] loss=3.57 avg=3.14\n",
            "[1618 | 1009.33] loss=3.13 avg=3.14\n",
            "[1619 | 1009.85] loss=3.41 avg=3.14\n",
            "[1620 | 1010.37] loss=3.46 avg=3.14\n",
            "[1621 | 1010.88] loss=2.88 avg=3.14\n",
            "[1622 | 1011.39] loss=3.50 avg=3.14\n",
            "[1623 | 1011.91] loss=2.75 avg=3.14\n",
            "[1624 | 1012.43] loss=3.08 avg=3.14\n",
            "[1625 | 1012.94] loss=3.55 avg=3.14\n",
            "[1626 | 1013.46] loss=3.36 avg=3.15\n",
            "[1627 | 1013.98] loss=3.25 avg=3.15\n",
            "[1628 | 1014.49] loss=2.69 avg=3.14\n",
            "[1629 | 1015.00] loss=3.09 avg=3.14\n",
            "[1630 | 1015.51] loss=2.93 avg=3.14\n",
            "[1631 | 1016.02] loss=2.90 avg=3.14\n",
            "[1632 | 1016.54] loss=2.92 avg=3.13\n",
            "[1633 | 1017.05] loss=2.79 avg=3.13\n",
            "[1634 | 1017.57] loss=2.37 avg=3.12\n",
            "[1635 | 1018.08] loss=3.04 avg=3.12\n",
            "[1636 | 1018.59] loss=3.17 avg=3.12\n",
            "[1637 | 1019.10] loss=2.57 avg=3.12\n",
            "[1638 | 1019.61] loss=3.16 avg=3.12\n",
            "[1639 | 1020.12] loss=3.08 avg=3.12\n",
            "[1640 | 1020.63] loss=2.67 avg=3.11\n",
            "[1641 | 1021.14] loss=3.15 avg=3.11\n",
            "[1642 | 1021.65] loss=2.99 avg=3.11\n",
            "[1643 | 1022.15] loss=2.75 avg=3.11\n",
            "[1644 | 1022.67] loss=2.91 avg=3.11\n",
            "[1645 | 1023.18] loss=3.04 avg=3.11\n",
            "[1646 | 1023.69] loss=3.50 avg=3.11\n",
            "[1647 | 1024.19] loss=3.73 avg=3.12\n",
            "[1648 | 1024.71] loss=3.41 avg=3.12\n",
            "[1649 | 1025.21] loss=2.79 avg=3.12\n",
            "[1650 | 1025.73] loss=3.24 avg=3.12\n",
            "[1651 | 1026.23] loss=3.14 avg=3.12\n",
            "[1652 | 1026.74] loss=2.86 avg=3.11\n",
            "[1653 | 1027.25] loss=3.52 avg=3.12\n",
            "[1654 | 1027.77] loss=3.33 avg=3.12\n",
            "[1655 | 1028.28] loss=2.89 avg=3.12\n",
            "[1656 | 1028.79] loss=2.92 avg=3.12\n",
            "[1657 | 1029.30] loss=3.04 avg=3.12\n",
            "[1658 | 1029.82] loss=3.09 avg=3.12\n",
            "[1659 | 1030.34] loss=3.39 avg=3.12\n",
            "[1660 | 1030.84] loss=3.42 avg=3.12\n",
            "[1661 | 1031.35] loss=3.29 avg=3.12\n",
            "[1662 | 1031.86] loss=3.53 avg=3.13\n",
            "[1663 | 1032.37] loss=3.23 avg=3.13\n",
            "[1664 | 1032.88] loss=2.79 avg=3.12\n",
            "[1665 | 1033.40] loss=3.28 avg=3.13\n",
            "[1666 | 1033.91] loss=2.64 avg=3.12\n",
            "[1667 | 1034.42] loss=2.65 avg=3.12\n",
            "[1668 | 1034.92] loss=3.23 avg=3.12\n",
            "[1669 | 1035.43] loss=3.27 avg=3.12\n",
            "[1670 | 1035.94] loss=2.94 avg=3.12\n",
            "[1671 | 1036.45] loss=3.25 avg=3.12\n",
            "[1672 | 1036.96] loss=3.35 avg=3.12\n",
            "[1673 | 1037.47] loss=3.43 avg=3.12\n",
            "[1674 | 1037.98] loss=2.85 avg=3.12\n",
            "[1675 | 1038.49] loss=3.32 avg=3.12\n",
            "[1676 | 1039.01] loss=3.30 avg=3.13\n",
            "[1677 | 1039.52] loss=3.00 avg=3.12\n",
            "[1678 | 1040.04] loss=3.72 avg=3.13\n",
            "[1679 | 1040.55] loss=2.56 avg=3.12\n",
            "[1680 | 1041.06] loss=2.60 avg=3.12\n",
            "[1681 | 1041.57] loss=3.05 avg=3.12\n",
            "[1682 | 1042.08] loss=2.72 avg=3.11\n",
            "[1683 | 1042.59] loss=2.98 avg=3.11\n",
            "[1684 | 1043.10] loss=2.88 avg=3.11\n",
            "[1685 | 1043.61] loss=2.67 avg=3.11\n",
            "[1686 | 1044.12] loss=2.86 avg=3.10\n",
            "[1687 | 1044.63] loss=2.60 avg=3.10\n",
            "[1688 | 1045.14] loss=2.66 avg=3.09\n",
            "[1689 | 1045.65] loss=2.75 avg=3.09\n",
            "[1690 | 1046.16] loss=3.35 avg=3.09\n",
            "[1691 | 1046.67] loss=3.04 avg=3.09\n",
            "[1692 | 1047.18] loss=2.87 avg=3.09\n",
            "[1693 | 1047.69] loss=3.16 avg=3.09\n",
            "[1694 | 1048.20] loss=3.58 avg=3.10\n",
            "[1695 | 1048.71] loss=2.44 avg=3.09\n",
            "[1696 | 1049.22] loss=3.04 avg=3.09\n",
            "[1697 | 1049.74] loss=2.98 avg=3.09\n",
            "[1698 | 1050.25] loss=3.19 avg=3.09\n",
            "[1699 | 1050.77] loss=3.30 avg=3.09\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            ", he went down like a stone, and then he went all the way back up to the river bank, and had the best day of his life. When I went off to work he would come home and pick me up; but I never did.\n",
            "\n",
            "\"Now what is the meaning of your taking so many prisoners at once?\" you ask.\n",
            "\n",
            "\"They never was very well. For three days I was in a rough state, and when one of those old men came along, he said, 'The English are very merry here,' and put a little stick on me and said, 'They have three days to keep me, and when you come again I shall be well, because this old man is a very merry fellow.' And I sat in my way till I could get him, and then he said to me, 'Come back again, young man.' I was very merry, as soon as I got on; it was the oldest night of my life, and he always went on his merry-go-round. He told me that if I wanted anything, he would fetch me him. Now I never used to get any money, and it was just a stick, and I used to go and say, 'Here, come back, young man.'\n",
            "\n",
            "\"He kept me a good while. I never said a word to him. One evening somebody got up and said, 'There have been three prisoners here over the past year, and you cannot take any of them. What do you suppose they might have done? They were not very happy.\n",
            "\n",
            "\"The man in the stick said, 'I suppose that they used to be miserable prisoners when they lived, or was ever so unhappy;' and I didn't say a word. He put a stick on me and said, 'The English, when they come back, won't buy anything, except a goodly quantity of bread and water, even a little bit of bread and water.' Then I began to tell him how much money I had.\n",
            "\n",
            "\"He kept my money for three years and kept it with me, and never used to give me anything. At last he said, 'You see that the old man got in a trouble, and they never told him, but they all looked at it and looked at it and said, 'This man is a very merry fellow.' I told him, 'What he does this way is very strange; if he does this, he has to be used to that!' And he said, 'One man will put his hand in his eye and watch what we do. Every man and woman has a hand in their eye, and it's a very merry thing that a man has used the eye of all the servants and servants-maids when he is working with that slave's money.' Then he left his hand in his eye-hole, and went back to the river bank.\"\n",
            "\n",
            "When they reached the town of Ewell they brought with them the gold-fish and the other fish they had caught, as usual, and set sail to search. When they reached the town they set out, too, but found no land; but if they had been in a better condition they might have found the owner of that land. When they reached the fishing-house which they had been forced to keep in for that year, it was not a pretty place to have any land, and the land had already been filled up, and the land and the fish were lying in the middle of it.\n",
            "\n",
            "\"They had been very good fishermen all year long, and it was the most charming place they had ever been here, for they got so much food in this wonderful region, that it seemed as if they might live happily ever after, and that made everything delightful, for each of them took an enormous quantity of fish, one after another, and some of them got very rich. The captain of the house, a pretty little woman of twelve, had taken the two fishing-pins which he had brought home, he called the three, and he had filled them with the fish which they had had in his room, and that was their happiness, and they all lived happily ever after.\n",
            "\n",
            "\"'Poor old thing,' I said, 'what do you want from me? I don't have anything, and I want nothing less of you. No longer can you get anything from me, and I have got to give me everything. I've got to get good enough to take over the fishing-house.\n",
            "\n",
            "\"I said to my wife, 'If you want anything, I will give it to you.'\n",
            "\n",
            "\"The captain of the house sent with some money for one hundred of the rods, and they carried it by the river to the house in which the owner of the fish was lodged, and had the money for the fishing-house.\n",
            "\n",
            "\"They returned with a great load of gold-fish, and after a while the captain of the house, who did not know what he was doing, asked the\n",
            "\n",
            "[1700 | 1062.02] loss=3.17 avg=3.09\n",
            "[1701 | 1062.53] loss=3.20 avg=3.09\n",
            "interrupted\n",
            "Saving checkpoint/all_short_stories/model-1702\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vy2eDSQYmrC",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJit5UMJYIBA",
        "colab_type": "text"
      },
      "source": [
        "### Training with concatenation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXnNCqEYX0c4",
        "colab_type": "code",
        "outputId": "7b8ffa1e-41ae-4cb0-e299-0e0386474e94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "## science essays training  - with 345\n",
        "!PYTHONPATH=src ./train.py --dataset /content/drive/My\\ Drive/writrly_proj_files/Atlantic_essays/business --combine 500 --run_name 'atlantic_business_concat_345' --model_name '345M'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0626 13:39:02.292997 140488384042880 deprecation_wrapper.py:119] From /content/gpt-2/src/model.py:147: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
            "\n",
            "W0626 13:39:02.301703 140488384042880 deprecation_wrapper.py:119] From /content/gpt-2/src/memory_saving_gradients.py:13: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n",
            "W0626 13:39:02.395063 140488384042880 deprecation_wrapper.py:119] From ./train.py:87: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "W0626 13:39:02.395398 140488384042880 deprecation_wrapper.py:119] From ./train.py:90: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "2019-06-26 13:39:02.420173: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-06-26 13:39:02.422288: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1329100 executing computations on platform Host. Devices:\n",
            "2019-06-26 13:39:02.422333: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2019-06-26 13:39:02.430583: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\n",
            "2019-06-26 13:39:02.632803: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-26 13:39:02.633348: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1328840 executing computations on platform CUDA. Devices:\n",
            "2019-06-26 13:39:02.633377: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2019-06-26 13:39:02.633625: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-26 13:39:02.634009: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-06-26 13:39:02.644273: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-06-26 13:39:02.835925: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-06-26 13:39:02.925022: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-06-26 13:39:02.953119: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-06-26 13:39:03.159568: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-06-26 13:39:03.276014: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-06-26 13:39:03.677297: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-06-26 13:39:03.677561: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-26 13:39:03.678042: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-26 13:39:03.678421: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
            "2019-06-26 13:39:03.683256: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-06-26 13:39:03.684429: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-06-26 13:39:03.684459: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n",
            "2019-06-26 13:39:03.684480: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n",
            "2019-06-26 13:39:03.691898: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-26 13:39:03.692374: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-26 13:39:03.692753: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14202 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "W0626 13:39:03.693627 140488384042880 deprecation_wrapper.py:119] From ./train.py:91: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0626 13:39:14.086132 140488384042880 deprecation.py:323] From /content/gpt-2/src/sample.py:65: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "W0626 13:39:14.099886 140488384042880 deprecation.py:323] From /content/gpt-2/src/sample.py:16: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0626 13:39:14.101442 140488384042880 deprecation.py:323] From /content/gpt-2/src/sample.py:70: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.random.categorical` instead.\n",
            "W0626 13:39:14.111332 140488384042880 deprecation_wrapper.py:119] From ./train.py:120: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "W0626 13:39:28.879640 140488384042880 deprecation_wrapper.py:119] From ./train.py:143: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n",
            "W0626 13:39:28.882447 140488384042880 deprecation_wrapper.py:119] From ./train.py:146: The name tf.summary.merge is deprecated. Please use tf.compat.v1.summary.merge instead.\n",
            "\n",
            "W0626 13:39:28.883282 140488384042880 deprecation_wrapper.py:119] From ./train.py:148: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
            "\n",
            "W0626 13:39:28.884365 140488384042880 deprecation_wrapper.py:119] From ./train.py:151: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "Loading checkpoint models/345M/model.ckpt\n",
            "W0626 13:39:42.722578 140488384042880 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "Loading dataset...\n",
            "100% 205/205 [02:05<00:00,  1.79it/s]\n",
            "dataset has 391278 tokens\n",
            "Training...\n",
            "2019-06-26 13:42:03.408405: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n",
            "2019-06-26 13:42:04.074512: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "[1 | 12.58] loss=2.82 avg=2.82\n",
            "[2 | 14.10] loss=2.57 avg=2.70\n",
            "[3 | 15.63] loss=2.71 avg=2.70\n",
            "[4 | 17.16] loss=2.53 avg=2.66\n",
            "[5 | 18.69] loss=2.80 avg=2.69\n",
            "[6 | 20.22] loss=2.61 avg=2.67\n",
            "[7 | 21.76] loss=2.71 avg=2.68\n",
            "[8 | 23.30] loss=2.56 avg=2.66\n",
            "[9 | 24.84] loss=2.42 avg=2.63\n",
            "[10 | 26.39] loss=2.64 avg=2.63\n",
            "[11 | 27.93] loss=2.73 avg=2.64\n",
            "[12 | 29.48] loss=2.96 avg=2.67\n",
            "[13 | 31.03] loss=2.86 avg=2.69\n",
            "[14 | 32.60] loss=2.78 avg=2.69\n",
            "[15 | 34.15] loss=2.99 avg=2.71\n",
            "[16 | 35.72] loss=2.75 avg=2.72\n",
            "[17 | 37.28] loss=2.58 avg=2.71\n",
            "[18 | 38.85] loss=2.90 avg=2.72\n",
            "[19 | 40.43] loss=2.54 avg=2.71\n",
            "[20 | 42.01] loss=2.68 avg=2.71\n",
            "[21 | 43.59] loss=2.59 avg=2.70\n",
            "[22 | 45.17] loss=2.53 avg=2.69\n",
            "[23 | 46.76] loss=2.53 avg=2.69\n",
            "[24 | 48.34] loss=2.65 avg=2.68\n",
            "[25 | 49.93] loss=3.01 avg=2.70\n",
            "[26 | 51.52] loss=2.39 avg=2.69\n",
            "[27 | 53.12] loss=2.59 avg=2.68\n",
            "[28 | 54.71] loss=2.64 avg=2.68\n",
            "[29 | 56.32] loss=2.57 avg=2.68\n",
            "[30 | 57.92] loss=2.92 avg=2.68\n",
            "[31 | 59.53] loss=2.46 avg=2.68\n",
            "[32 | 61.14] loss=2.74 avg=2.68\n",
            "[33 | 62.75] loss=2.84 avg=2.68\n",
            "[34 | 64.36] loss=2.57 avg=2.68\n",
            "[35 | 65.98] loss=2.99 avg=2.69\n",
            "[36 | 67.61] loss=2.76 avg=2.69\n",
            "[37 | 69.24] loss=2.36 avg=2.68\n",
            "[38 | 70.88] loss=2.35 avg=2.67\n",
            "[39 | 72.51] loss=2.74 avg=2.67\n",
            "[40 | 74.15] loss=3.14 avg=2.69\n",
            "[41 | 75.79] loss=2.61 avg=2.69\n",
            "[42 | 77.44] loss=3.00 avg=2.69\n",
            "[43 | 79.09] loss=2.63 avg=2.69\n",
            "[44 | 80.75] loss=2.82 avg=2.70\n",
            "[45 | 82.40] loss=2.87 avg=2.70\n",
            "[46 | 84.05] loss=2.74 avg=2.70\n",
            "[47 | 85.70] loss=2.42 avg=2.69\n",
            "[48 | 87.35] loss=2.67 avg=2.69\n",
            "[49 | 89.00] loss=2.61 avg=2.69\n",
            "[50 | 90.64] loss=2.32 avg=2.68\n",
            "[51 | 92.27] loss=2.86 avg=2.69\n",
            "[52 | 93.91] loss=2.78 avg=2.69\n",
            "[53 | 95.53] loss=2.64 avg=2.69\n",
            "[54 | 97.16] loss=2.45 avg=2.68\n",
            "[55 | 98.79] loss=2.74 avg=2.68\n",
            "[56 | 100.41] loss=2.91 avg=2.69\n",
            "[57 | 102.04] loss=2.62 avg=2.69\n",
            "[58 | 103.66] loss=2.67 avg=2.69\n",
            "[59 | 105.28] loss=2.40 avg=2.68\n",
            "[60 | 106.89] loss=2.91 avg=2.69\n",
            "[61 | 108.50] loss=2.77 avg=2.69\n",
            "[62 | 110.11] loss=2.73 avg=2.69\n",
            "[63 | 111.72] loss=2.41 avg=2.68\n",
            "[64 | 113.33] loss=2.64 avg=2.68\n",
            "[65 | 114.94] loss=2.56 avg=2.68\n",
            "[66 | 116.54] loss=2.93 avg=2.68\n",
            "[67 | 118.15] loss=2.27 avg=2.68\n",
            "[68 | 119.75] loss=3.04 avg=2.68\n",
            "[69 | 121.36] loss=2.81 avg=2.69\n",
            "[70 | 122.96] loss=2.56 avg=2.68\n",
            "[71 | 124.57] loss=2.76 avg=2.68\n",
            "[72 | 126.17] loss=2.50 avg=2.68\n",
            "[73 | 127.77] loss=2.68 avg=2.68\n",
            "[74 | 129.38] loss=2.32 avg=2.67\n",
            "[75 | 130.99] loss=2.38 avg=2.67\n",
            "[76 | 132.59] loss=2.40 avg=2.66\n",
            "[77 | 134.20] loss=3.32 avg=2.68\n",
            "[78 | 135.80] loss=2.49 avg=2.67\n",
            "[79 | 137.40] loss=3.23 avg=2.68\n",
            "[80 | 139.01] loss=2.70 avg=2.68\n",
            "[81 | 140.61] loss=2.52 avg=2.68\n",
            "[82 | 142.22] loss=2.49 avg=2.68\n",
            "[83 | 143.83] loss=3.04 avg=2.68\n",
            "[84 | 145.44] loss=2.62 avg=2.68\n",
            "[85 | 147.05] loss=2.75 avg=2.68\n",
            "[86 | 148.66] loss=2.59 avg=2.68\n",
            "[87 | 150.27] loss=2.63 avg=2.68\n",
            "[88 | 151.89] loss=2.90 avg=2.68\n",
            "[89 | 153.50] loss=2.50 avg=2.68\n",
            "[90 | 155.11] loss=2.50 avg=2.68\n",
            "[91 | 156.72] loss=2.31 avg=2.67\n",
            "[92 | 158.35] loss=2.78 avg=2.67\n",
            "[93 | 159.97] loss=2.47 avg=2.67\n",
            "[94 | 161.59] loss=2.20 avg=2.66\n",
            "[95 | 163.20] loss=2.74 avg=2.66\n",
            "[96 | 164.82] loss=2.19 avg=2.66\n",
            "[97 | 166.44] loss=2.75 avg=2.66\n",
            "[98 | 168.06] loss=2.68 avg=2.66\n",
            "[99 | 169.69] loss=2.95 avg=2.66\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "As I said, the first part of my review contains spoilers for the movie. Please don't read if you haven't seen the movie. My reviews for each individual movie are listed under the titles, and I'll take care of those later in the review.\n",
            "\n",
            "1. Guardians of the Galaxy (2015)\n",
            "\n",
            "If the first-look teaser for the upcoming Guardians of the Galaxy sequel, Avengers: Infinity War, was anything like the look of the first teaser trailer, it would be enough to make anyone want to skip out on the movie entirely. But that's not what the first-look teaser trailer for Guardians of the Galaxy: Age of Ultron did, and we were pleasantly surprised by how different Ultron's look was from that of the original character. After all, Ultron used to be a villain in this franchise, so it might be weird for a guy who likes cats and coffee and eating kitties to go after those who want to make movies, right? So he might have seemed a little… more like a hero, but maybe he wasn't exactly a hero. Of course, this is only a teaser. There's nothing that's ever released in full that confirms the movie's plot and characters. But the first look trailer showed off a new character who'll play a major role in Age of Ultron: Drax the Destroyer, a supercomputer who is the leader of Thanos' forces in this movie. That's right, a new Thanos to be named after the villain we saw in the first teaser trailer. That's right, Thanos, one of the MCU's most iconic villains, is going to be voiced by an actual human being like Tom Hiddleston. Oh, and the one-in-a-billion character we probably know has to be Thanos, too. This is all the more reason why fans won't care about the plot of Age of Ultron: Marvel Studios is using the Infinity War storyline to try and expand in a new movie while getting into their own Marvel TV show and movies and being even better, as well. And when the Marvel TV show is out, it should probably be in January, too.\n",
            "\n",
            "2. The Force Awakens (2015)\n",
            "\n",
            "The Force Awakens, which we already reviewed, was another film that made me question Marvel Studios films, movies I usually liked or didn't care so much about. This movie was kind of like a bad Pixar movie for the first two years it was released, with the cast all getting their comeuppance and the story getting really, really bad. It wasn't until the third season that I liked it again, and it did pretty well to establish itself as a fun movie as well. However, while the film is really about Rey (Daisy Ridley), it feels rushed, with too much action and too much CG. I thought it was a bit rushed in the second half, where we don't get to see an entire crew of crew members from the Millennium Falcon. (Also, because I didn't watch it at the time, I can't say what the scene was called. So I can't say that it was about the Millennium Falcon.) This movie is, as the title suggests, a prequel to the first movie, the Star Wars trilogy. So while this movie was more fun in the first two weeks it was out, most fans will be left off and confused when the movie comes out in 2017.\n",
            "\n",
            "3. Iron Man 2 (2005)\n",
            "\n",
            "Tony Stark was kind of like the bad guy in Iron Man. As the only character in the Marvel Cinematic Universe, he's generally seen as an angry, overprotective, and even a little bit cold villain. So the second movie in the Marvel Cinematic Universe, Iron Man 2, was kind of a letdown too. It was good, but it wasn't good enough. In fact, the movie was kind of boring; it had a lot of action, but nothing new to it. It was one of the few movies in the Marvel Studios movies where the characters had a bit of fun. However, even though Iron Man 2 was basically a reboot of the movie, it does have a lot of fun in it. I really liked the character of Tony Stark, but what I didn't like about the film was the villain, Black Bolt, and the whole Guardians of the Galaxy film, which was boring.\n",
            "\n",
            "4. The Avengers: Age of Ultron (2015)\n",
            "\n",
            "While they're a Marvel Studios movie, The Avengers: Age of Ultron doesn't really have anything to do with Marvel comic book stories. In this film, Chris Evans plays Captain America, one of two super-strength heroes who was created by Nick Fury, the world's greatest scientist and the leader of a secret government. One of the best things about this film is its cast and characters: Chris Pratt, who plays the superhero sidekick Iron Man, is one of the scariest people I've ever seen in a role like that, and the rest of the cast of Avengers is great,\n",
            "\n",
            "[100 | 196.64] loss=2.80 avg=2.66\n",
            "[101 | 198.27] loss=2.79 avg=2.67\n",
            "[102 | 199.90] loss=2.42 avg=2.66\n",
            "[103 | 201.53] loss=2.69 avg=2.66\n",
            "[104 | 203.16] loss=2.17 avg=2.66\n",
            "[105 | 204.79] loss=2.28 avg=2.65\n",
            "[106 | 206.42] loss=2.54 avg=2.65\n",
            "[107 | 208.05] loss=2.83 avg=2.65\n",
            "[108 | 209.67] loss=2.33 avg=2.65\n",
            "[109 | 211.30] loss=2.73 avg=2.65\n",
            "[110 | 212.92] loss=2.36 avg=2.64\n",
            "[111 | 214.54] loss=2.58 avg=2.64\n",
            "[112 | 216.17] loss=2.76 avg=2.64\n",
            "[113 | 217.79] loss=2.89 avg=2.65\n",
            "[114 | 219.41] loss=2.55 avg=2.65\n",
            "[115 | 221.03] loss=2.66 avg=2.65\n",
            "[116 | 222.66] loss=2.92 avg=2.65\n",
            "[117 | 224.28] loss=2.73 avg=2.65\n",
            "[118 | 225.90] loss=2.51 avg=2.65\n",
            "[119 | 227.53] loss=2.51 avg=2.65\n",
            "[120 | 229.15] loss=2.59 avg=2.65\n",
            "[121 | 230.77] loss=2.23 avg=2.64\n",
            "[122 | 232.39] loss=2.70 avg=2.64\n",
            "[123 | 234.02] loss=2.72 avg=2.64\n",
            "[124 | 235.64] loss=2.45 avg=2.64\n",
            "[125 | 237.26] loss=2.53 avg=2.64\n",
            "[126 | 238.88] loss=2.93 avg=2.64\n",
            "[127 | 240.51] loss=2.40 avg=2.64\n",
            "[128 | 242.13] loss=2.66 avg=2.64\n",
            "[129 | 243.76] loss=2.66 avg=2.64\n",
            "[130 | 245.39] loss=2.53 avg=2.64\n",
            "[131 | 247.02] loss=3.05 avg=2.64\n",
            "[132 | 248.64] loss=3.00 avg=2.65\n",
            "[133 | 250.26] loss=2.47 avg=2.65\n",
            "[134 | 251.89] loss=2.68 avg=2.65\n",
            "[135 | 253.52] loss=2.75 avg=2.65\n",
            "[136 | 255.14] loss=2.61 avg=2.65\n",
            "[137 | 256.77] loss=2.29 avg=2.64\n",
            "[138 | 258.38] loss=2.66 avg=2.64\n",
            "[139 | 260.01] loss=2.48 avg=2.64\n",
            "[140 | 261.64] loss=2.61 avg=2.64\n",
            "[141 | 263.27] loss=2.62 avg=2.64\n",
            "[142 | 264.90] loss=2.48 avg=2.64\n",
            "[143 | 266.53] loss=2.59 avg=2.64\n",
            "[144 | 268.15] loss=2.33 avg=2.63\n",
            "[145 | 269.78] loss=2.90 avg=2.64\n",
            "[146 | 271.41] loss=2.24 avg=2.63\n",
            "[147 | 273.04] loss=2.17 avg=2.63\n",
            "[148 | 274.67] loss=2.66 avg=2.63\n",
            "[149 | 276.29] loss=2.97 avg=2.63\n",
            "[150 | 277.92] loss=2.59 avg=2.63\n",
            "[151 | 279.55] loss=2.04 avg=2.62\n",
            "[152 | 281.18] loss=2.39 avg=2.62\n",
            "[153 | 282.81] loss=2.47 avg=2.62\n",
            "[154 | 284.44] loss=2.63 avg=2.62\n",
            "[155 | 286.07] loss=2.36 avg=2.61\n",
            "[156 | 287.69] loss=2.90 avg=2.62\n",
            "[157 | 289.32] loss=2.53 avg=2.62\n",
            "[158 | 290.95] loss=2.32 avg=2.61\n",
            "[159 | 292.58] loss=2.78 avg=2.62\n",
            "[160 | 294.21] loss=2.25 avg=2.61\n",
            "[161 | 295.84] loss=2.40 avg=2.61\n",
            "[162 | 297.47] loss=2.76 avg=2.61\n",
            "[163 | 299.10] loss=2.26 avg=2.61\n",
            "[164 | 300.73] loss=2.62 avg=2.61\n",
            "[165 | 302.36] loss=2.52 avg=2.60\n",
            "[166 | 303.99] loss=2.79 avg=2.61\n",
            "[167 | 305.62] loss=2.53 avg=2.61\n",
            "[168 | 307.25] loss=2.76 avg=2.61\n",
            "[169 | 308.88] loss=3.09 avg=2.61\n",
            "[170 | 310.51] loss=2.64 avg=2.61\n",
            "[171 | 312.14] loss=2.63 avg=2.61\n",
            "[172 | 313.77] loss=2.63 avg=2.61\n",
            "[173 | 315.40] loss=2.85 avg=2.62\n",
            "[174 | 317.03] loss=2.61 avg=2.62\n",
            "[175 | 318.66] loss=2.22 avg=2.61\n",
            "[176 | 320.29] loss=2.44 avg=2.61\n",
            "[177 | 321.92] loss=2.23 avg=2.61\n",
            "[178 | 323.55] loss=2.66 avg=2.61\n",
            "[179 | 325.18] loss=2.28 avg=2.60\n",
            "[180 | 326.81] loss=3.16 avg=2.61\n",
            "[181 | 328.44] loss=2.52 avg=2.61\n",
            "[182 | 330.07] loss=2.39 avg=2.61\n",
            "[183 | 331.70] loss=2.90 avg=2.61\n",
            "[184 | 333.32] loss=3.10 avg=2.62\n",
            "[185 | 334.96] loss=2.40 avg=2.61\n",
            "[186 | 336.59] loss=2.09 avg=2.61\n",
            "[187 | 338.22] loss=2.29 avg=2.60\n",
            "[188 | 339.85] loss=2.77 avg=2.60\n",
            "[189 | 341.48] loss=2.59 avg=2.60\n",
            "[190 | 343.11] loss=2.23 avg=2.60\n",
            "[191 | 344.74] loss=2.84 avg=2.60\n",
            "[192 | 346.37] loss=2.67 avg=2.60\n",
            "[193 | 348.00] loss=2.75 avg=2.61\n",
            "[194 | 349.63] loss=2.69 avg=2.61\n",
            "[195 | 351.26] loss=2.13 avg=2.60\n",
            "[196 | 352.89] loss=2.70 avg=2.60\n",
            "[197 | 354.51] loss=2.57 avg=2.60\n",
            "[198 | 356.14] loss=2.89 avg=2.60\n",
            "[199 | 357.77] loss=2.59 avg=2.60\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " year that the economy expanded at a 1.23 percent annual rate.\n",
            "\n",
            "The federal government is expected to spend an additional $1.4 trillion over the next decade on programs, such as farm subsidies, on-the-job training, public works, infrastructure, research and development or national defense, according to a recent report.\n",
            "\n",
            "But the economy is expected — and the country is, and always has been, a small one. The Bureau of Economic Analysis projects that the economy will continue to grow at just a little 4 percent a year through 2060 and, even if today's figures hold, the nation will become a smaller and smaller part of the globe than it is today.\n",
            "\n",
            "The chart shows the size of each one of the five major countries that make up the U.S. workforce, grouped together for comparison sake. As you can see, the size of the American workforce is smaller than it has been historically or at any point in recent history. And the size of the American economy is larger than it has been in any decade in most measures: In 2012, the government projected that it would get just 3.7 percent of the U.S. economy, down from 4.6 percent in 2000. (It is still a bigger number than the overall economy as a whole, which will account for 22.3 percent of the economy.)\n",
            "\n",
            "Why? The federal government has more money to spend to help companies grow and businesses contribute to the national economy than to pay workers or to support public services. That has caused some companies to hire workers less, and to cut back hiring. There is growing evidence, for example, that companies are cutting back hiring in search of efficiencies, particularly as they seek to reduce costs. (The data are a bit cloudy, but a good place to start would be by looking at the trend of job cuts since 2000, in particular by companies that have been large-size companies, such as Apple and UPS, as well as smaller companies, such as McDonald's and Target.)\n",
            "\n",
            "For Americans, the most common explanation for the size of the country is that, like other countries around the world, the U.S. is a small economy. In the chart, the large red circles are the U.S. as a whole, and the small blue bar is the U.S., by percentiles. (The small blue bars show the U.S. as a whole, and the other bars show the U.S., for all other countries.)\n",
            "\n",
            "But a closer look at the data tells a different story. While the size of the U.S. economy is larger now than at any time since the 1970s, it has been smaller a lot over that span. (It is now smaller than at any point in all of recorded history.) Some of this is because the U.S. economy is, for the most part, a small one. Americans work fewer hours. They are working shorter hours. And, to some extent, they are working too much. The size of the U.S. economy has contracted substantially due both to lower than expected GDP growth and higher unemployment.\n",
            "\n",
            "The data also show that the government and private companies are largely the same all across the country. The size of each large economy is the same, which may be surprising, especially given that the largest ones are now growing faster than they were in the 1990s. (Large economies typically grow faster because of better management at the national level and private activity, which helps create a smaller economy.) Large firms can add more jobs than small ones because they are better able to absorb and create. And small firms often have the money to hire more people than large ones. (Large firms are often able to hire workers in the private sector, such as in the service occupations.) Smaller firms have money to hire people in the public sector, too, because larger firms — on average — are able to hire more people than smaller ones.\n",
            "\n",
            "By some measures, the size of the U.S. economy has shrunk dramatically in recent years. The economy shrank by 1.8 percent, the fastest since the late 1980s. But the economy grew by some other things as well, including consumer spending and business investment, which are strong and healthy economic engines and the ones that support a larger economy. (The federal government is already spending almost more money than it makes in revenue.)\n",
            "\n",
            "And while the size of the economy is smaller in the short run, the size of the US economy is growing much more rapidly and substantially. The chart above uses growth from the Bureau of Economic Analysis rather than from the Congressional Budget Office's most recent survey. The growth is in terms of real GDP and nominal GDP, or the share of gross domestic product that is spent on domestic purchases. The growth is stronger in terms of real GDP and larger in terms of nominal GDP: Real GDP is the share of gross domestic product that is spent on domestic purchases, while nominal is when you adjust all figures for inflation.\n",
            "\n",
            "In the long run,\n",
            "\n",
            "[200 | 381.66] loss=2.38 avg=2.60\n",
            "[201 | 383.28] loss=2.63 avg=2.60\n",
            "[202 | 384.90] loss=2.64 avg=2.60\n",
            "[203 | 386.52] loss=2.66 avg=2.60\n",
            "[204 | 388.14] loss=2.14 avg=2.60\n",
            "[205 | 389.76] loss=2.23 avg=2.59\n",
            "[206 | 391.39] loss=1.99 avg=2.59\n",
            "[207 | 393.01] loss=2.41 avg=2.58\n",
            "[208 | 394.63] loss=2.37 avg=2.58\n",
            "[209 | 396.25] loss=2.01 avg=2.58\n",
            "[210 | 397.87] loss=2.51 avg=2.58\n",
            "[211 | 399.49] loss=2.37 avg=2.57\n",
            "[212 | 401.11] loss=2.26 avg=2.57\n",
            "[213 | 402.73] loss=2.62 avg=2.57\n",
            "[214 | 404.35] loss=2.34 avg=2.57\n",
            "[215 | 405.97] loss=2.93 avg=2.57\n",
            "[216 | 407.60] loss=2.95 avg=2.58\n",
            "[217 | 409.22] loss=2.70 avg=2.58\n",
            "[218 | 410.85] loss=2.23 avg=2.57\n",
            "[219 | 412.48] loss=2.85 avg=2.58\n",
            "[220 | 414.11] loss=2.38 avg=2.57\n",
            "[221 | 415.73] loss=2.10 avg=2.57\n",
            "[222 | 417.35] loss=2.49 avg=2.57\n",
            "[223 | 418.98] loss=2.31 avg=2.57\n",
            "[224 | 420.60] loss=2.47 avg=2.56\n",
            "[225 | 422.23] loss=2.42 avg=2.56\n",
            "[226 | 423.86] loss=2.96 avg=2.57\n",
            "[227 | 425.49] loss=2.54 avg=2.57\n",
            "[228 | 427.12] loss=2.54 avg=2.57\n",
            "[229 | 428.75] loss=2.06 avg=2.56\n",
            "[230 | 430.38] loss=2.29 avg=2.56\n",
            "[231 | 432.01] loss=2.43 avg=2.56\n",
            "[232 | 433.64] loss=2.66 avg=2.56\n",
            "[233 | 435.27] loss=2.24 avg=2.55\n",
            "[234 | 436.90] loss=2.79 avg=2.56\n",
            "[235 | 438.53] loss=2.67 avg=2.56\n",
            "[236 | 440.16] loss=2.91 avg=2.56\n",
            "[237 | 441.79] loss=2.29 avg=2.56\n",
            "[238 | 443.43] loss=2.59 avg=2.56\n",
            "[239 | 445.06] loss=2.19 avg=2.55\n",
            "[240 | 446.69] loss=2.98 avg=2.56\n",
            "[241 | 448.32] loss=1.77 avg=2.55\n",
            "[242 | 449.95] loss=2.45 avg=2.55\n",
            "[243 | 451.58] loss=2.48 avg=2.55\n",
            "[244 | 453.21] loss=2.27 avg=2.55\n",
            "[245 | 454.84] loss=2.81 avg=2.55\n",
            "[246 | 456.47] loss=2.13 avg=2.54\n",
            "[247 | 458.11] loss=2.44 avg=2.54\n",
            "[248 | 459.74] loss=2.50 avg=2.54\n",
            "[249 | 461.37] loss=3.11 avg=2.55\n",
            "[250 | 463.00] loss=2.44 avg=2.55\n",
            "[251 | 464.62] loss=2.25 avg=2.54\n",
            "[252 | 466.26] loss=2.79 avg=2.55\n",
            "[253 | 467.89] loss=2.32 avg=2.54\n",
            "[254 | 469.52] loss=2.89 avg=2.55\n",
            "[255 | 471.14] loss=2.44 avg=2.55\n",
            "[256 | 472.77] loss=2.76 avg=2.55\n",
            "[257 | 474.40] loss=2.37 avg=2.55\n",
            "[258 | 476.03] loss=2.19 avg=2.54\n",
            "[259 | 477.66] loss=2.31 avg=2.54\n",
            "[260 | 479.28] loss=2.95 avg=2.55\n",
            "[261 | 480.91] loss=2.44 avg=2.54\n",
            "[262 | 482.54] loss=2.28 avg=2.54\n",
            "[263 | 484.17] loss=2.13 avg=2.54\n",
            "[264 | 485.80] loss=2.61 avg=2.54\n",
            "[265 | 487.43] loss=2.51 avg=2.54\n",
            "[266 | 489.05] loss=2.69 avg=2.54\n",
            "[267 | 490.68] loss=2.81 avg=2.54\n",
            "[268 | 492.31] loss=2.72 avg=2.54\n",
            "[269 | 493.94] loss=1.66 avg=2.53\n",
            "[270 | 495.57] loss=2.63 avg=2.54\n",
            "[271 | 497.20] loss=2.36 avg=2.53\n",
            "[272 | 498.82] loss=2.03 avg=2.53\n",
            "[273 | 500.45] loss=2.03 avg=2.52\n",
            "[274 | 502.08] loss=2.39 avg=2.52\n",
            "[275 | 503.71] loss=2.32 avg=2.52\n",
            "[276 | 505.34] loss=2.23 avg=2.52\n",
            "[277 | 506.97] loss=2.61 avg=2.52\n",
            "[278 | 508.59] loss=2.99 avg=2.52\n",
            "[279 | 510.21] loss=2.56 avg=2.52\n",
            "[280 | 511.84] loss=2.89 avg=2.53\n",
            "[281 | 513.47] loss=2.11 avg=2.52\n",
            "[282 | 515.09] loss=2.48 avg=2.52\n",
            "[283 | 516.71] loss=2.57 avg=2.52\n",
            "[284 | 518.34] loss=2.51 avg=2.52\n",
            "[285 | 519.96] loss=2.46 avg=2.52\n",
            "[286 | 521.59] loss=2.04 avg=2.52\n",
            "[287 | 523.22] loss=2.43 avg=2.52\n",
            "[288 | 524.84] loss=2.49 avg=2.52\n",
            "[289 | 526.47] loss=2.52 avg=2.52\n",
            "[290 | 528.09] loss=2.32 avg=2.51\n",
            "[291 | 529.72] loss=2.41 avg=2.51\n",
            "[292 | 531.34] loss=2.12 avg=2.51\n",
            "[293 | 532.98] loss=2.57 avg=2.51\n",
            "[294 | 534.60] loss=2.64 avg=2.51\n",
            "[295 | 536.23] loss=2.16 avg=2.51\n",
            "[296 | 537.85] loss=1.53 avg=2.50\n",
            "[297 | 539.49] loss=2.28 avg=2.49\n",
            "[298 | 541.11] loss=2.50 avg=2.49\n",
            "[299 | 542.74] loss=2.49 avg=2.49\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            ". The number of U.S. workers participating in the program is expected to reach 15 million by 2020.\n",
            "\n",
            "In addition to raising wages, the Affordable Care Act also includes a new step in the process, known as the HSA. This step, which was introduced in 2014, makes it easier for employers and employees to share payroll and benefits.\n",
            "\n",
            "Employers, by law, must offer health care coverage in the form of a basic health-care plan to every employee. However, because employers can opt out of offering health-care coverage, the federal government must give employees a plan that doesn't include any insurance on the employer side. An employer may choose between three options: an \"individual coverage\" plan with no private insurance; an employee-only plan that offers private insurance in addition to health benefits, but doesn't have coverage for covered benefits; and a group-policy plan with some private insurance, but with no coverage for covered benefits.\n",
            "\n",
            "A new law, the American Jobs Act of 2017, took effect in September 2017. Under the new law, employers who provide health care to employees are able to request that employees obtain a new plan from a federal agency. Under the new law, companies who provide health care to employees without providing coverage for covered benefits would have to notify employees that they were breaking the law by not providing health care to employees as required under the ACA. Employees who do enroll in a plan through the ACA could have their coverage terminated if premiums or deductibles rose beyond a certain amount. Employer-sponsored health plans would have to provide at least the same level of coverage as a standard employer plan, and the employer could choose whether it also offered a high-deductible, low-cost silver plan to employees.\n",
            "\n",
            "These changes to the workplace insurance market are one of several steps in President Trump's promised infrastructure spending plan to help grow the economy—including infrastructure investments that would boost both growth and the national economy. In October 2017, the president proposed the Southern Infrastructure Investment Bank (SIIB), a new initiative designed to help revitalize the nation's crumbling infrastructure. The new infrastructure bank would be composed of three members: the State, Federal, and United States Treasury, and would provide private-sector investment in infrastructure along the United States and Mississippi Rivers, the eastern seaboard, and parts of Central and South America.\n",
            "\n",
            "SIIB participants would have to include federal, state, and local governments in the group, including a requirement to invest at least 25 percent of their gross domestic product (GDP) in infrastructure; another 25 percent on infrastructure with a target for a top rate of at least 35 percent; and a 10 percent tax deduction for the construction of federal-aid infrastructure projects. SIIB participants could also use state funds to invest in roads, railways, transit, waterways, levees, and other infrastructure projects.\n",
            "\n",
            "This announcement is a major step towards improving America's outdated infrastructure in the United States. It could also be a wakeup call to federal officials to improve the infrastructure funding process—or, perhaps, an invitation to Washington to re-engineer the process so that it is a more just and fair one.\n",
            "\n",
            "If the administration is serious about improving infrastructure spending, it should include this type of investment in its infrastructure plan.\n",
            "\n",
            "If the president is serious about improving infrastructure spending, it should include this type of investment in its infrastructure plan.\n",
            "\n",
            "The president's infrastructure plan does a good job of recognizing the national need for new infrastructure, and it emphasizes infrastructure investments in high-demand regions, like the United States and Mississippi Rivers. The National Association of Manufacture, along with many other business, labor, and faith-based organizations, has called for the United States to make infrastructure investments to help spur job creation and growth.\n",
            "\n",
            "The White House says it wants to increase infrastructure spending by $700 billion over the next decade. So if the president were serious about infrastructure spending, it's not that surprising that he also wants to improve access to health care for workers, especially in the face of the ACA's requirement that everyone have at least basic coverage.\n",
            "\n",
            "That said, the cost of health care, in particular, needs to be held down for workers and their families. According to a report on the Affordable Care Act, the number of people without health coverage is projected to rise by about 4 million between 2018 and 2026. In the first six months of 2017, roughly 4.1 million people lost their health coverage. In contrast, in January in 2017, 11.2 million people gained coverage, bringing the total to about 20 million, mostly through the law.\n",
            "\n",
            "This is another step in the path to improving access to health insurance for workers and for families. But Trump and Congress have come a long way.\n",
            "\n",
            "The president's infrastructure plans are long overdue. If the president is serious about improving infrastructure spending, he and Congress have the political will and the political will to act. The path to a better, fairer, and more efficient infrastructure and the economy will be one\n",
            "\n",
            "[300 | 566.76] loss=2.49 avg=2.49\n",
            "[301 | 568.38] loss=2.79 avg=2.50\n",
            "[302 | 570.01] loss=2.20 avg=2.49\n",
            "[303 | 571.63] loss=2.40 avg=2.49\n",
            "[304 | 573.26] loss=2.57 avg=2.49\n",
            "[305 | 574.89] loss=2.44 avg=2.49\n",
            "[306 | 576.51] loss=2.44 avg=2.49\n",
            "[307 | 578.13] loss=2.87 avg=2.50\n",
            "[308 | 579.76] loss=2.43 avg=2.50\n",
            "[309 | 581.38] loss=1.97 avg=2.49\n",
            "[310 | 583.01] loss=2.17 avg=2.49\n",
            "[311 | 584.64] loss=1.96 avg=2.48\n",
            "[312 | 586.26] loss=2.76 avg=2.48\n",
            "[313 | 587.88] loss=2.03 avg=2.48\n",
            "[314 | 589.50] loss=1.72 avg=2.47\n",
            "[315 | 591.12] loss=2.68 avg=2.47\n",
            "[316 | 592.75] loss=2.12 avg=2.47\n",
            "[317 | 594.37] loss=2.96 avg=2.48\n",
            "[318 | 596.00] loss=2.61 avg=2.48\n",
            "[319 | 597.62] loss=2.65 avg=2.48\n",
            "[320 | 599.23] loss=2.29 avg=2.48\n",
            "[321 | 600.85] loss=2.25 avg=2.47\n",
            "[322 | 602.48] loss=2.23 avg=2.47\n",
            "[323 | 604.10] loss=2.09 avg=2.47\n",
            "[324 | 605.72] loss=1.21 avg=2.45\n",
            "[325 | 607.35] loss=2.13 avg=2.45\n",
            "[326 | 608.97] loss=2.21 avg=2.45\n",
            "[327 | 610.59] loss=2.67 avg=2.45\n",
            "[328 | 612.22] loss=2.39 avg=2.45\n",
            "[329 | 613.85] loss=2.84 avg=2.45\n",
            "[330 | 615.47] loss=2.15 avg=2.45\n",
            "[331 | 617.09] loss=2.69 avg=2.45\n",
            "[332 | 618.71] loss=2.60 avg=2.46\n",
            "[333 | 620.35] loss=2.58 avg=2.46\n",
            "[334 | 621.98] loss=2.13 avg=2.45\n",
            "[335 | 623.60] loss=2.57 avg=2.45\n",
            "[336 | 625.23] loss=2.93 avg=2.46\n",
            "[337 | 626.86] loss=3.04 avg=2.47\n",
            "[338 | 628.49] loss=2.34 avg=2.46\n",
            "[339 | 630.12] loss=2.87 avg=2.47\n",
            "[340 | 631.75] loss=2.95 avg=2.47\n",
            "[341 | 633.38] loss=2.37 avg=2.47\n",
            "[342 | 635.01] loss=2.60 avg=2.47\n",
            "[343 | 636.64] loss=2.33 avg=2.47\n",
            "[344 | 638.27] loss=2.54 avg=2.47\n",
            "[345 | 639.90] loss=3.13 avg=2.48\n",
            "[346 | 641.53] loss=2.90 avg=2.48\n",
            "[347 | 643.15] loss=2.43 avg=2.48\n",
            "[348 | 644.78] loss=1.49 avg=2.47\n",
            "[349 | 646.40] loss=2.33 avg=2.47\n",
            "[350 | 648.03] loss=2.46 avg=2.47\n",
            "[351 | 649.66] loss=2.40 avg=2.47\n",
            "[352 | 651.29] loss=2.37 avg=2.47\n",
            "[353 | 652.92] loss=2.65 avg=2.47\n",
            "[354 | 654.55] loss=2.23 avg=2.47\n",
            "[355 | 656.19] loss=2.63 avg=2.47\n",
            "[356 | 657.82] loss=2.10 avg=2.47\n",
            "[357 | 659.46] loss=2.76 avg=2.47\n",
            "[358 | 661.09] loss=2.78 avg=2.47\n",
            "[359 | 662.72] loss=2.45 avg=2.47\n",
            "[360 | 664.36] loss=2.19 avg=2.47\n",
            "[361 | 665.99] loss=2.21 avg=2.47\n",
            "[362 | 667.62] loss=2.69 avg=2.47\n",
            "[363 | 669.25] loss=2.39 avg=2.47\n",
            "[364 | 670.87] loss=1.96 avg=2.46\n",
            "[365 | 672.50] loss=2.45 avg=2.46\n",
            "[366 | 674.13] loss=2.40 avg=2.46\n",
            "[367 | 675.76] loss=2.43 avg=2.46\n",
            "[368 | 677.39] loss=2.61 avg=2.46\n",
            "[369 | 679.02] loss=2.42 avg=2.46\n",
            "[370 | 680.65] loss=2.04 avg=2.46\n",
            "[371 | 682.28] loss=2.76 avg=2.46\n",
            "[372 | 683.90] loss=2.22 avg=2.46\n",
            "[373 | 685.53] loss=2.23 avg=2.46\n",
            "[374 | 687.16] loss=1.21 avg=2.44\n",
            "[375 | 688.79] loss=2.70 avg=2.45\n",
            "[376 | 690.42] loss=1.90 avg=2.44\n",
            "[377 | 692.05] loss=2.45 avg=2.44\n",
            "[378 | 693.67] loss=2.84 avg=2.45\n",
            "[379 | 695.30] loss=2.11 avg=2.44\n",
            "[380 | 696.93] loss=1.46 avg=2.43\n",
            "[381 | 698.57] loss=2.06 avg=2.43\n",
            "[382 | 700.19] loss=1.97 avg=2.42\n",
            "[383 | 701.82] loss=2.38 avg=2.42\n",
            "[384 | 703.45] loss=2.38 avg=2.42\n",
            "[385 | 705.08] loss=2.32 avg=2.42\n",
            "[386 | 706.72] loss=2.58 avg=2.42\n",
            "[387 | 708.35] loss=2.03 avg=2.42\n",
            "[388 | 709.98] loss=2.33 avg=2.42\n",
            "[389 | 711.61] loss=2.24 avg=2.42\n",
            "[390 | 713.24] loss=2.46 avg=2.42\n",
            "[391 | 714.86] loss=2.55 avg=2.42\n",
            "[392 | 716.50] loss=1.68 avg=2.41\n",
            "[393 | 718.13] loss=2.76 avg=2.41\n",
            "[394 | 719.76] loss=2.55 avg=2.42\n",
            "[395 | 721.39] loss=2.58 avg=2.42\n",
            "[396 | 723.01] loss=3.09 avg=2.42\n",
            "[397 | 724.64] loss=2.62 avg=2.43\n",
            "[398 | 726.27] loss=1.89 avg=2.42\n",
            "[399 | 727.90] loss=2.30 avg=2.42\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " life. I used to drive a truck with my dad every weekend, getting to the warehouse by 11, driving home by 5, because I didn't want to miss my dad working.\n",
            "\n",
            "That experience made me want to work at that kind of location. I didn't realize it at the time, but we have all felt that way in our life—like, I'll work in a shoe store and then I'll work at a shoe store and in the day I'll miss my dad working. So that's the thing that helped. That experience was the best thing ever for me.\n",
            "\n",
            "And also to think that there are places that I need to learn how to do this stuff—because I can't do it anywhere else—that was a good thing in a time when I didn't want to work anywhere and didn't want to go work at the shoe store. And that wasn't the first place I wanted to go. That was a really good idea. So all of these reasons I had, they made me a better person. So there's some good and some bad. There's some good and some bad. This is kind of the worst one.\n",
            "\n",
            "JH: I know your family has been in the business for decades. How old are you?\n",
            "\n",
            "MB: I'm 46 years old. I'm from Kansas.\n",
            "\n",
            "JH: Are you aware of the history of the shoe warehouse? What was the history of this thing that you did?\n",
            "\n",
            "MB: Not as much as you might think. I don't think it had much to do with my family having been in it for fifty-three years before I went to college. But it was my dad getting us started doing this. He always told us, if we get into stuff as teenagers, one of the first things that you're going to do, and what you're going to do, is take care of the stuff. So I did.\n",
            "\n",
            "I worked the warehouse for five years. And one of the things I used to notice about this particular job—and this hasn't changed much—but one of the things I used to notice is, every time somebody would come in, I would take a look at them and ask the questions. I would ask questions, 'Cause I knew how many hours were they working, but also because I wanted to find out, Why are they here? And just because I knew it, couldn't help me, so I would find out more. And there would always be new things that I don't even know. So even if you didn't know—and obviously I don't know—You just had to guess—It would be a thing you always did. Like—I will make the guess wrong because that's how they work—I'll guess 'cause I really want to know more. So it always gave me that chance.\n",
            "\n",
            "My dad was always the person that would tell you—and make sure that you knew—And so, even in my second year, you know, when I left college and I was starting my own business, I would always be in touch with him. Like, 'Oh my gosh, my friend is working here today.' Or 'Oh, my friend is working today and I have to tell her, you know, I'm so sorry, but you have to go.' And he would be like, 'Look, I know you want to work at the shoe store and you don't want to, but it's the warehouse for the shoe store. And for the warehouse, you just have to know the different jobs, and what sort of work you want to do.' That was the way he told me everything. Because he knew what I was doing, to a point. It was just, Like, he knew the different jobs I could do. It was his background so he would be like, 'No, we can't. But look, here's how we do it, and here's what you do.' And they just did this shit every day.\n",
            "\n",
            "When we weren't working, my stepdad would always go to my step dad's, like my dad's sister [was) and say—you know, he never talked like, 'Hi, I'm just dropping in to see this lady.' He never spoke like that, but just things like that. He would talk to my dad and say, 'Hey, I don't know why I came in. I just dropped in. But I love the warehouse, I've never been there before.' And you know, just like that they would walk in, and it would be just like his home, you know? This was after he retired in the late '80s so it wasn't like he just moved into a friend's house. Because it was all his stuff. So all his stuff was, like, that warehouse.\n",
            "\n",
            "And he says…I'll do this. You know the way you do it and you take a look and you don't know how it will come out,\n",
            "\n",
            "[400 | 751.80] loss=2.34 avg=2.42\n",
            "[401 | 753.43] loss=2.15 avg=2.42\n",
            "[402 | 755.06] loss=2.62 avg=2.42\n",
            "[403 | 756.69] loss=2.30 avg=2.42\n",
            "[404 | 758.32] loss=2.59 avg=2.42\n",
            "[405 | 759.94] loss=2.26 avg=2.42\n",
            "[406 | 761.57] loss=2.52 avg=2.42\n",
            "[407 | 763.20] loss=2.64 avg=2.42\n",
            "[408 | 764.82] loss=1.97 avg=2.42\n",
            "[409 | 766.44] loss=2.38 avg=2.42\n",
            "[410 | 768.07] loss=2.07 avg=2.41\n",
            "[411 | 769.69] loss=1.59 avg=2.40\n",
            "[412 | 771.31] loss=2.22 avg=2.40\n",
            "[413 | 772.94] loss=2.21 avg=2.40\n",
            "[414 | 774.56] loss=1.51 avg=2.39\n",
            "[415 | 776.18] loss=2.00 avg=2.39\n",
            "[416 | 777.81] loss=2.56 avg=2.39\n",
            "[417 | 779.43] loss=2.50 avg=2.39\n",
            "[418 | 781.06] loss=2.34 avg=2.39\n",
            "[419 | 782.68] loss=1.95 avg=2.38\n",
            "[420 | 784.31] loss=2.27 avg=2.38\n",
            "[421 | 785.94] loss=2.34 avg=2.38\n",
            "[422 | 787.57] loss=2.17 avg=2.38\n",
            "[423 | 789.19] loss=2.19 avg=2.38\n",
            "[424 | 790.82] loss=0.89 avg=2.36\n",
            "[425 | 792.45] loss=3.11 avg=2.37\n",
            "[426 | 794.08] loss=0.74 avg=2.36\n",
            "[427 | 795.70] loss=2.52 avg=2.36\n",
            "[428 | 797.33] loss=2.42 avg=2.36\n",
            "[429 | 798.95] loss=2.24 avg=2.36\n",
            "[430 | 800.58] loss=2.77 avg=2.36\n",
            "[431 | 802.21] loss=2.06 avg=2.36\n",
            "[432 | 803.84] loss=2.38 avg=2.36\n",
            "[433 | 805.46] loss=2.43 avg=2.36\n",
            "[434 | 807.09] loss=2.29 avg=2.36\n",
            "[435 | 808.71] loss=2.67 avg=2.36\n",
            "[436 | 810.34] loss=2.49 avg=2.36\n",
            "[437 | 811.97] loss=2.44 avg=2.36\n",
            "[438 | 813.60] loss=2.84 avg=2.37\n",
            "[439 | 815.23] loss=2.51 avg=2.37\n",
            "[440 | 816.86] loss=0.71 avg=2.35\n",
            "[441 | 818.49] loss=2.51 avg=2.35\n",
            "[442 | 820.12] loss=2.45 avg=2.36\n",
            "[443 | 821.74] loss=2.28 avg=2.35\n",
            "[444 | 823.37] loss=1.87 avg=2.35\n",
            "[445 | 825.00] loss=2.26 avg=2.35\n",
            "[446 | 826.63] loss=2.45 avg=2.35\n",
            "[447 | 828.26] loss=1.92 avg=2.35\n",
            "[448 | 829.89] loss=2.63 avg=2.35\n",
            "[449 | 831.52] loss=1.89 avg=2.34\n",
            "[450 | 833.15] loss=2.71 avg=2.35\n",
            "[451 | 834.78] loss=2.20 avg=2.35\n",
            "[452 | 836.41] loss=2.62 avg=2.35\n",
            "[453 | 838.04] loss=2.06 avg=2.35\n",
            "[454 | 839.67] loss=2.36 avg=2.35\n",
            "[455 | 841.30] loss=1.96 avg=2.34\n",
            "[456 | 842.93] loss=2.73 avg=2.35\n",
            "[457 | 844.56] loss=2.10 avg=2.34\n",
            "[458 | 846.19] loss=3.54 avg=2.36\n",
            "[459 | 847.82] loss=2.51 avg=2.36\n",
            "[460 | 849.45] loss=2.09 avg=2.35\n",
            "[461 | 851.08] loss=0.71 avg=2.34\n",
            "[462 | 852.72] loss=2.49 avg=2.34\n",
            "[463 | 854.35] loss=2.72 avg=2.34\n",
            "[464 | 855.99] loss=2.65 avg=2.35\n",
            "[465 | 857.62] loss=0.57 avg=2.33\n",
            "[466 | 859.25] loss=2.83 avg=2.33\n",
            "[467 | 860.88] loss=2.49 avg=2.33\n",
            "[468 | 862.51] loss=2.41 avg=2.34\n",
            "[469 | 864.14] loss=2.05 avg=2.33\n",
            "[470 | 865.78] loss=1.02 avg=2.32\n",
            "[471 | 867.41] loss=2.18 avg=2.32\n",
            "[472 | 869.04] loss=1.26 avg=2.31\n",
            "[473 | 870.67] loss=2.43 avg=2.31\n",
            "[474 | 872.30] loss=2.52 avg=2.31\n",
            "[475 | 873.93] loss=2.31 avg=2.31\n",
            "[476 | 875.56] loss=2.47 avg=2.31\n",
            "[477 | 877.19] loss=1.86 avg=2.31\n",
            "[478 | 878.82] loss=2.42 avg=2.31\n",
            "[479 | 880.46] loss=2.52 avg=2.31\n",
            "[480 | 882.09] loss=2.25 avg=2.31\n",
            "[481 | 883.72] loss=2.58 avg=2.31\n",
            "[482 | 885.35] loss=1.87 avg=2.31\n",
            "[483 | 886.98] loss=2.03 avg=2.31\n",
            "[484 | 888.61] loss=2.78 avg=2.31\n",
            "[485 | 890.24] loss=2.26 avg=2.31\n",
            "[486 | 891.87] loss=2.23 avg=2.31\n",
            "[487 | 893.50] loss=2.01 avg=2.31\n",
            "[488 | 895.14] loss=2.80 avg=2.31\n",
            "[489 | 896.77] loss=2.97 avg=2.32\n",
            "[490 | 898.40] loss=2.53 avg=2.32\n",
            "[491 | 900.03] loss=1.93 avg=2.32\n",
            "[492 | 901.66] loss=2.30 avg=2.32\n",
            "[493 | 903.29] loss=2.23 avg=2.32\n",
            "[494 | 904.92] loss=2.45 avg=2.32\n",
            "[495 | 906.55] loss=1.76 avg=2.31\n",
            "[496 | 908.18] loss=2.16 avg=2.31\n",
            "[497 | 909.81] loss=2.28 avg=2.31\n",
            "[498 | 911.44] loss=2.48 avg=2.31\n",
            "[499 | 913.07] loss=2.36 avg=2.31\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " did not respond to a request for comment.\n",
            "\n",
            "But after years of lobbying, the Obama administration last year announced the creation of the Federal Aviation Administration's Federal Aviation Partnership, an initiative aimed at \"developing, sharing, and optimizing aviation network standards and infrastructure.\"\n",
            "\n",
            "With such an effort underway, why has the number of passenger-aircraft fatalities in the United States increased so dramatically over the past two decades?\n",
            "\n",
            "The first explanation for the increase has to do with better regulation, which has significantly reduced the amount of time airplanes spend flying overhead. In 1983, the federal minimum safe cruising altitude for passenger aircraft—the point at which a plane cannot exceed 30 miles per hour—was 225 feet. Last year, that number had been raised to 290 feet, according to Federal Aviation Administration data. That raised the bar for all aircraft, forcing operators to adapt to higher traffic levels. When federal aviation agencies set minimum levels for the safety of air traffic—usually known as the 'airports operator standard' or 'no-fly' zone—they were also taking into account additional factors that might increase the risks for pilots. For example, operators might require more frequent landing and takeoff. There was a time when the United States imposed a 'no-fly zone' on Cuba because of the communist country's ballistic missile program.\n",
            "\n",
            "In addition, advances in technology that make it harder for pilots to avoid crashes and more likely for passengers to get injured when flying overhead—particularly in aircraft that have smaller propellers and engines—have affected the way the air is delivered to the front and sides of aircraft. In a crash, a wingtip strikes the ground, producing a 'tearing' motion. This produces a lot of drag, which pushes the air at the front of the aircraft, which can create nose hits. This can also cause the front of the aircraft to tip over or become trapped in the ground, reducing the amount of lateral acceleration it experiences to move the air. In a plane like a Piper Cub, the front of the aircraft is heavier than the back, which causes the plane to tip over in rough terrain. This results in a downward-accelerating spin to the wing. This causes more damage to the side of the aircraft. It can also cause the front of the aircraft to tilt backward and hit the ground. This results in a downward-accelerating spin to the wing, which compresses the air in the side of the aircraft. This results in more damage to the side of the aircraft.\n",
            "\n",
            "In the decades since the advent of the first commercial airplane, more companies built passenger-carrying aircraft than ever before. However, this influx of new airplanes meant that operators have to accommodate ever more planes, more people, and with more turbulence in their skies. That requires more pilots, more aircraft to run the controls, and with them more fuel, requiring more fuel cells to carry more load. The increase in fuel prices and the need to increase productivity have pushed up the cost of operating airplanes.\n",
            "\n",
            "A number of factors seem to be at work. The first, and most obvious, is what is called the 'drag coefficient,' which describes the tendency of a body of fluid to roll along a straight surface when traveling in the opposite direction. The drag coefficient works out like this. Suppose a cylinder contains about 90 percent water and only 10 percent carbon dioxide. When you put a fluid of the same viscosity in the cylinder, the air flow will be the same. The point being, a lighter body of water absorbs more heat, and the air will rise. However, there is a small amount of water that floats on top of that. If you let the water to rise up, you cause the water to roll off the edge of the cylinder at high speed, which is what we see when you let a heavier body of air ascend at the same angle in the cylinder in the future.\n",
            "\n",
            "By increasing the load on an airplane, it can raise the drag coefficient by making more air flow along the fuselage (making it feel heavier), and this causes more of the air to flow up and down, causing it to feel heavier. Increasing the fuel weight also can raise the drag coefficient. One study found that the average increase in fuel weight accounted for about one-tenth of the increase in the drag coefficient over the course of a 30-year period.\n",
            "\n",
            "Finally, there is the fact that the nature of high-speed flight makes it not only harder to avoid hitting objects in the atmosphere, but also harder to stay in one place for long periods of time. Pilots may need to stay aloft longer to avoid hitting the ground, and may have to maneuver to avoid hitting the ground—also making it easier to flip back and forth. That is easier said than done, as pilots know.\n",
            "\n",
            "These and other factors combine to explain why more airplanes mean more passengers—more deaths and injuries—and therefore more profits, which is also why the stock market has soared in recent years. But while the number of fatalities in the passenger\n",
            "\n",
            "[500 | 936.96] loss=2.19 avg=2.31\n",
            "[501 | 938.58] loss=2.49 avg=2.31\n",
            "[502 | 940.21] loss=2.25 avg=2.31\n",
            "[503 | 941.83] loss=2.56 avg=2.31\n",
            "[504 | 943.45] loss=2.03 avg=2.31\n",
            "[505 | 945.08] loss=2.08 avg=2.31\n",
            "[506 | 946.70] loss=2.61 avg=2.31\n",
            "[507 | 948.32] loss=2.49 avg=2.31\n",
            "[508 | 949.95] loss=2.24 avg=2.31\n",
            "[509 | 951.57] loss=2.48 avg=2.31\n",
            "[510 | 953.19] loss=2.07 avg=2.31\n",
            "[511 | 954.81] loss=2.68 avg=2.32\n",
            "[512 | 956.44] loss=2.67 avg=2.32\n",
            "[513 | 958.06] loss=2.67 avg=2.32\n",
            "[514 | 959.68] loss=2.39 avg=2.32\n",
            "[515 | 961.30] loss=1.67 avg=2.32\n",
            "[516 | 962.92] loss=2.33 avg=2.32\n",
            "[517 | 964.54] loss=2.34 avg=2.32\n",
            "[518 | 966.16] loss=2.74 avg=2.32\n",
            "[519 | 967.79] loss=2.20 avg=2.32\n",
            "[520 | 969.42] loss=2.08 avg=2.32\n",
            "[521 | 971.05] loss=2.16 avg=2.32\n",
            "[522 | 972.68] loss=2.54 avg=2.32\n",
            "[523 | 974.30] loss=2.66 avg=2.32\n",
            "[524 | 975.93] loss=2.68 avg=2.33\n",
            "[525 | 977.56] loss=2.27 avg=2.32\n",
            "[526 | 979.20] loss=1.89 avg=2.32\n",
            "[527 | 980.83] loss=1.82 avg=2.32\n",
            "[528 | 982.46] loss=2.16 avg=2.31\n",
            "[529 | 984.09] loss=2.69 avg=2.32\n",
            "[530 | 985.72] loss=2.86 avg=2.32\n",
            "[531 | 987.35] loss=1.91 avg=2.32\n",
            "[532 | 988.98] loss=2.23 avg=2.32\n",
            "[533 | 990.61] loss=1.80 avg=2.31\n",
            "[534 | 992.24] loss=0.93 avg=2.30\n",
            "[535 | 993.87] loss=2.14 avg=2.30\n",
            "[536 | 995.51] loss=2.46 avg=2.30\n",
            "[537 | 997.14] loss=2.14 avg=2.30\n",
            "[538 | 998.77] loss=2.39 avg=2.30\n",
            "[539 | 1000.40] loss=2.13 avg=2.30\n",
            "[540 | 1002.03] loss=1.92 avg=2.29\n",
            "[541 | 1003.66] loss=0.35 avg=2.27\n",
            "[542 | 1005.30] loss=2.85 avg=2.28\n",
            "[543 | 1006.93] loss=1.90 avg=2.28\n",
            "[544 | 1008.55] loss=2.36 avg=2.28\n",
            "[545 | 1010.19] loss=2.37 avg=2.28\n",
            "[546 | 1011.82] loss=1.96 avg=2.27\n",
            "[547 | 1013.45] loss=2.32 avg=2.27\n",
            "[548 | 1015.08] loss=1.68 avg=2.27\n",
            "[549 | 1016.71] loss=1.88 avg=2.26\n",
            "[550 | 1018.34] loss=2.70 avg=2.27\n",
            "[551 | 1019.97] loss=2.82 avg=2.27\n",
            "[552 | 1021.60] loss=2.27 avg=2.27\n",
            "[553 | 1023.23] loss=2.36 avg=2.28\n",
            "[554 | 1024.86] loss=2.33 avg=2.28\n",
            "[555 | 1026.49] loss=1.84 avg=2.27\n",
            "[556 | 1028.12] loss=2.25 avg=2.27\n",
            "[557 | 1029.75] loss=2.44 avg=2.27\n",
            "[558 | 1031.38] loss=2.91 avg=2.28\n",
            "[559 | 1033.00] loss=1.75 avg=2.27\n",
            "[560 | 1034.63] loss=2.38 avg=2.27\n",
            "[561 | 1036.26] loss=1.78 avg=2.27\n",
            "[562 | 1037.90] loss=1.76 avg=2.26\n",
            "[563 | 1039.52] loss=1.94 avg=2.26\n",
            "[564 | 1041.15] loss=2.64 avg=2.27\n",
            "[565 | 1042.79] loss=1.92 avg=2.26\n",
            "[566 | 1044.42] loss=2.45 avg=2.26\n",
            "[567 | 1046.05] loss=2.61 avg=2.27\n",
            "[568 | 1047.68] loss=2.03 avg=2.26\n",
            "[569 | 1049.30] loss=1.02 avg=2.25\n",
            "[570 | 1050.93] loss=1.64 avg=2.25\n",
            "[571 | 1052.56] loss=2.32 avg=2.25\n",
            "[572 | 1054.19] loss=2.49 avg=2.25\n",
            "[573 | 1055.82] loss=1.84 avg=2.25\n",
            "[574 | 1057.45] loss=2.15 avg=2.24\n",
            "[575 | 1059.08] loss=2.26 avg=2.24\n",
            "[576 | 1060.71] loss=2.38 avg=2.25\n",
            "[577 | 1062.34] loss=1.75 avg=2.24\n",
            "[578 | 1063.97] loss=1.80 avg=2.24\n",
            "[579 | 1065.59] loss=3.02 avg=2.24\n",
            "[580 | 1067.23] loss=2.21 avg=2.24\n",
            "[581 | 1068.86] loss=2.24 avg=2.24\n",
            "[582 | 1070.49] loss=2.45 avg=2.25\n",
            "[583 | 1072.12] loss=1.86 avg=2.24\n",
            "[584 | 1073.75] loss=1.72 avg=2.24\n",
            "[585 | 1075.38] loss=2.26 avg=2.24\n",
            "[586 | 1077.01] loss=2.41 avg=2.24\n",
            "[587 | 1078.64] loss=2.04 avg=2.24\n",
            "[588 | 1080.27] loss=2.45 avg=2.24\n",
            "[589 | 1081.90] loss=1.55 avg=2.23\n",
            "[590 | 1083.52] loss=0.78 avg=2.22\n",
            "[591 | 1085.15] loss=1.67 avg=2.21\n",
            "[592 | 1086.78] loss=1.55 avg=2.21\n",
            "[593 | 1088.41] loss=2.34 avg=2.21\n",
            "[594 | 1090.04] loss=1.56 avg=2.20\n",
            "[595 | 1091.67] loss=2.50 avg=2.20\n",
            "[596 | 1093.30] loss=2.03 avg=2.20\n",
            "[597 | 1094.93] loss=1.98 avg=2.20\n",
            "[598 | 1096.55] loss=1.86 avg=2.20\n",
            "[599 | 1098.18] loss=0.25 avg=2.18\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " found the opportunity to give her daughter a better start in life because of the school. (She had no children of her own by then.) After graduating from college, Ms. López-Ortiz worked as a waitress in a restaurant and then as a waitress in a fast-food restaurant before eventually finding work working for a nonprofit helping children with disabilities. She moved to the city and took a job working for a public safety officer for San Diego Unified School district, where the pay was $13.50 an hour. During that time, she says, she did not earn more than $1,000, but \"I had no money other than how I lived.\"\n",
            "\n",
            "She also spent time in foster homes, where her father kept her in the home he shared with other families in South Central Los Angeles. One of those times, she says, he choked her with a belt. \"I remember getting up and saying I don't want to go out, and he saying I'm a slave—and he was punching me in the stomach,\" she says. \"I remember him saying to me, 'You're so pretty, honey—you can make me a man, can you?' I said, 'No, dad—I don't want to live like a woman.' \"\n",
            "\n",
            "It is not that Ms. López-Ortiz did not have friends; her friends were women of color, and she always found ways to make friends with different groups. She also discovered the strength of her faith, and this was not so different from any other time in her life. \"When I came to this place of being a slave inside a man's body, there was something inside of me that was strong and confident, and I believed that that strength, that confidence, would carry me through anything that I faced,\" she says. But there were times, she says, when her faith had to be tested. It was during this time, after years of living as a woman, that she became aware of her inner conflict. Although she had trusted her father to keep her in the house, now she had to make a choice: Should she keep her own home and take money from him to pay for rent or should she leave and have to rely on the help of the public safety officer?\n",
            "\n",
            "\"I didn't know if I could trust the system. But I also had this inner strength that the system was there for me.\"\n",
            "\n",
            "In 2010, shortly after receiving her GED and a state job offer, Ms. López-Ortiz moved into an apartment with her cousin, a friend from school who worked in an immigration-service agency. (Her mother lives in the same apartment complex.) She began to feel she had found a home. After moving into an apartment with a man from another family, who turned out to also be a public-safety officer, she described her experience to a mentor, who encouraged her to seek help. She did so, but was still ashamed of who she was and felt she was not ready to share her story. In 2013, she was diagnosed with gender dysphoria, a highly stigmatized mental condition for which there is no approved treatment and, while significant, may never be significant enough to justify medical treatments. The next year, after a lengthy legal battle, she received a diagnosis of gender identity disorder diagnosis number 2, which stands for gender identity disorder and transsexualism.\n",
            "\n",
            "These diagnoses made it possible for Ms. López-Ortiz to continue to live as a woman, but she had limited access to legal services because of her sex-reassignment surgery; because of the stigma attached to gender reassignment, she was unable to obtain a birth certificate from her birth woman. When she finally received a birth certificate listing her biological sex, an adjustment of sorts took place in her life. Her confidence increased, as did her sense of purpose. Today, Ms. López-Ortiz is a high-school junior and works for the Los Angeles Unified School District. (She asked that her last name and hometown not be used out of concern that discussing her work in her neighborhood could expose her as trans.)\n",
            "\n",
            "Despite the hurdles she faced, Ms. López-Ortiz is determined to continue to be herself. She is currently working on a master's in management at the University of Southern California. And while she has spoken out before about the challenges of coming out to her family, now that she knows what it will mean for them, she knows that if she doesn't act now, something terrible may happen to herself and her children. \"As I was struggling in the closet, I was very vulnerable to things happening to me out in the world,\" she says. \"I had to be very careful about what I did and didn’t say. … The worst thing that could happen to me is to have a family member or friend find out and be hurt. I wanted to protect them, even though they’re my family, and if I had\n",
            "\n",
            "[600 | 1122.13] loss=2.96 avg=2.18\n",
            "[601 | 1123.75] loss=2.25 avg=2.18\n",
            "[602 | 1125.38] loss=2.15 avg=2.18\n",
            "[603 | 1127.01] loss=2.16 avg=2.18\n",
            "[604 | 1128.63] loss=1.82 avg=2.18\n",
            "[605 | 1130.25] loss=2.06 avg=2.18\n",
            "[606 | 1131.87] loss=1.09 avg=2.17\n",
            "[607 | 1133.49] loss=2.56 avg=2.17\n",
            "[608 | 1135.12] loss=2.20 avg=2.17\n",
            "[609 | 1136.74] loss=2.23 avg=2.17\n",
            "[610 | 1138.36] loss=2.92 avg=2.18\n",
            "[611 | 1139.98] loss=1.51 avg=2.17\n",
            "[612 | 1141.60] loss=2.14 avg=2.17\n",
            "[613 | 1143.21] loss=2.66 avg=2.18\n",
            "[614 | 1144.84] loss=2.92 avg=2.19\n",
            "[615 | 1146.46] loss=2.29 avg=2.19\n",
            "[616 | 1148.08] loss=1.98 avg=2.18\n",
            "[617 | 1149.70] loss=2.33 avg=2.19\n",
            "[618 | 1151.33] loss=2.14 avg=2.19\n",
            "[619 | 1152.96] loss=2.08 avg=2.18\n",
            "[620 | 1154.59] loss=2.47 avg=2.19\n",
            "[621 | 1156.21] loss=2.22 avg=2.19\n",
            "[622 | 1157.84] loss=2.16 avg=2.19\n",
            "[623 | 1159.47] loss=2.66 avg=2.19\n",
            "[624 | 1161.09] loss=1.59 avg=2.19\n",
            "[625 | 1162.72] loss=2.53 avg=2.19\n",
            "[626 | 1164.35] loss=2.27 avg=2.19\n",
            "[627 | 1165.98] loss=2.45 avg=2.19\n",
            "[628 | 1167.60] loss=2.99 avg=2.20\n",
            "[629 | 1169.24] loss=2.19 avg=2.20\n",
            "[630 | 1170.87] loss=2.12 avg=2.20\n",
            "[631 | 1172.50] loss=2.08 avg=2.20\n",
            "[632 | 1174.13] loss=1.89 avg=2.20\n",
            "[633 | 1175.76] loss=2.30 avg=2.20\n",
            "[634 | 1177.38] loss=2.09 avg=2.20\n",
            "[635 | 1179.01] loss=1.96 avg=2.19\n",
            "[636 | 1180.64] loss=2.49 avg=2.20\n",
            "[637 | 1182.27] loss=2.36 avg=2.20\n",
            "[638 | 1183.90] loss=2.76 avg=2.20\n",
            "[639 | 1185.53] loss=2.37 avg=2.21\n",
            "[640 | 1187.16] loss=2.76 avg=2.21\n",
            "[641 | 1188.79] loss=2.33 avg=2.21\n",
            "[642 | 1190.42] loss=2.39 avg=2.21\n",
            "[643 | 1192.05] loss=2.47 avg=2.22\n",
            "[644 | 1193.69] loss=2.00 avg=2.21\n",
            "[645 | 1195.32] loss=2.38 avg=2.22\n",
            "[646 | 1196.95] loss=0.45 avg=2.20\n",
            "[647 | 1198.58] loss=2.13 avg=2.20\n",
            "[648 | 1200.21] loss=2.45 avg=2.20\n",
            "[649 | 1201.85] loss=2.51 avg=2.20\n",
            "[650 | 1203.48] loss=2.97 avg=2.21\n",
            "[651 | 1205.11] loss=2.14 avg=2.21\n",
            "[652 | 1206.74] loss=2.09 avg=2.21\n",
            "[653 | 1208.36] loss=2.40 avg=2.21\n",
            "[654 | 1209.98] loss=2.49 avg=2.21\n",
            "[655 | 1211.61] loss=2.43 avg=2.22\n",
            "[656 | 1213.24] loss=0.26 avg=2.20\n",
            "[657 | 1214.87] loss=1.86 avg=2.19\n",
            "[658 | 1216.50] loss=2.21 avg=2.19\n",
            "[659 | 1218.13] loss=2.25 avg=2.19\n",
            "[660 | 1219.76] loss=2.72 avg=2.20\n",
            "[661 | 1221.39] loss=2.52 avg=2.20\n",
            "[662 | 1223.02] loss=2.32 avg=2.20\n",
            "[663 | 1224.65] loss=2.22 avg=2.20\n",
            "[664 | 1226.27] loss=2.18 avg=2.20\n",
            "[665 | 1227.90] loss=2.51 avg=2.21\n",
            "[666 | 1229.53] loss=2.16 avg=2.21\n",
            "[667 | 1231.16] loss=2.69 avg=2.21\n",
            "[668 | 1232.79] loss=1.35 avg=2.20\n",
            "[669 | 1234.42] loss=2.04 avg=2.20\n",
            "[670 | 1236.04] loss=2.60 avg=2.20\n",
            "[671 | 1237.67] loss=1.97 avg=2.20\n",
            "[672 | 1239.30] loss=2.74 avg=2.21\n",
            "[673 | 1240.93] loss=2.11 avg=2.21\n",
            "[674 | 1242.56] loss=1.99 avg=2.20\n",
            "[675 | 1244.20] loss=1.50 avg=2.20\n",
            "[676 | 1245.82] loss=1.44 avg=2.19\n",
            "[677 | 1247.45] loss=2.75 avg=2.20\n",
            "[678 | 1249.08] loss=2.21 avg=2.20\n",
            "[679 | 1250.71] loss=2.26 avg=2.20\n",
            "[680 | 1252.34] loss=1.92 avg=2.19\n",
            "[681 | 1253.97] loss=2.72 avg=2.20\n",
            "[682 | 1255.60] loss=2.31 avg=2.20\n",
            "[683 | 1257.23] loss=2.13 avg=2.20\n",
            "[684 | 1258.86] loss=2.10 avg=2.20\n",
            "[685 | 1260.49] loss=2.80 avg=2.20\n",
            "[686 | 1262.12] loss=2.00 avg=2.20\n",
            "[687 | 1263.76] loss=0.37 avg=2.18\n",
            "[688 | 1265.39] loss=1.97 avg=2.18\n",
            "[689 | 1267.02] loss=2.38 avg=2.18\n",
            "[690 | 1268.65] loss=2.12 avg=2.18\n",
            "[691 | 1270.28] loss=1.89 avg=2.18\n",
            "[692 | 1271.91] loss=2.55 avg=2.18\n",
            "[693 | 1273.54] loss=2.27 avg=2.18\n",
            "[694 | 1275.17] loss=2.12 avg=2.18\n",
            "[695 | 1276.80] loss=1.87 avg=2.18\n",
            "[696 | 1278.43] loss=2.26 avg=2.18\n",
            "[697 | 1280.06] loss=2.05 avg=2.18\n",
            "[698 | 1281.69] loss=1.11 avg=2.17\n",
            "[699 | 1283.32] loss=2.44 avg=2.17\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "const’s position is that he’s an independent contractor; that he’s not an employee. In any event, “when you take into account that it costs him less to do this for someone, the cost is there,” she said.\n",
            "\n",
            "The company then made the decision to cut the price by $50,000, because it believed the person would have to be more willing to spend the money than it is now. They did that because the hourly rate is less than what a night stay does at an Airbnb, Marr said.\n",
            "\n",
            "“If we were going to provide a full-service hotel in San Francisco,” he said, referring to the cost of living differences between cities, and the cost of providing amenities like free Wi-Fi,” the “full-body” massage that’s similar to nipos, and other services and amenities that would be out of the standard rates, he said. He described how his room — which is also an Airbnb listing for $4,300 — is “full” of gadgets; one wall is covered with photos of his kids and a laptop. There is a bed with mattresses on it and a small coffee maker, an odd object in a room that is generally not common, he said.\n",
            "\n",
            "“All this stuff we’re not giving away,” he added,” and how that could make people more likely to leave if they don’t have more money. “That’s why the increase in [refunds] was so drastic. … It was like there was a knife coming out of the back of my mind.”\n",
            "\n",
            "That said, it’s true that many hotels and vacation rental–company owners are struggling right now, the company that owns the Travel Channel’s “Grit and Grind” has been having trouble hiring locals and getting people to stay in certain areas of the city, and the vacancy rate for rentals across the country has been well below 6 percent, according to Zillow. “We’re experiencing great waves of economic change that are making the city of San Francisco even more of a player than it has been in recent memory,” Marr told me.\n",
            "\n",
            "But this isn’t so much the housing crisis that is causing people to vacation in other places; it’s the economy that makes it difficult. According to the Bureau of Labor Statistics, the median household income in San Francisco increased 1.1 percent last year, well above the national rate of 1.4 percent. (The national figure was $54,300 in September, up from $53,900 a year ago.) But the influx of Airbnb travelers is also squeezing out a lot of traditional hoteliers. The Bay Area’s largest hotel chain has said it is planning to close about two-thirds of its San Francisco properties by 2017. As the city has struggled to cope, Airbnb has responded with a lot of advertising and tips on how to make it seem like you don’t need a hotel. Some of the tips seem to have worked, as the Bay Area’s Airbnb rate has jumped from 1.1 in November 2016 to 1.5 in October 2017.\n",
            "\n",
            "Even with the uptick in Airbnb rentals, the city faces a much larger budget issue. “The problem with San Francisco right now is it’s still trying to balance the budget,” said Chris Sacca, a former staff director for the state Legislature who now runs the Sacca Group consultancy. “And that’s not getting any easier if you look at the federal budget.”\n",
            "\n",
            "Sacca is talking about the budget, as well, because the Trump administration has indicated it might cut funding for the state and local government. As for the budget, San Francisco had a $14.9 billion deficit in fiscal 2018, and San Jose had a $23 billion gap, according to S&P Global Ratings.\n",
            "\n",
            "The mayor’s office said this week that it’s planning to borrow money to pay down the $4.4 billion bill that the state and federal governments put on its books. It’s also talking with its lenders about raising additional funds. If no one else, it’ll probably use the money it raises to build something called “Carnegie View,” which would be a public-transportation system in which the city, rather than the public or private entities that pay for it, would foot the rest of the tab.\n",
            "\n",
            "But that would be a big sacrifice for a city that is already burdened with sky-high property taxes and one of the highest minimum wages in the country. Even with tax reform, the city would still have a $5.9 billion deficit in fiscal 2019 and $7.3 billion on its balance sheet by 2021. San Francisco also has a $22.3 billion budget deficit\n",
            "\n",
            "[700 | 1307.26] loss=2.61 avg=2.18\n",
            "[701 | 1308.89] loss=2.88 avg=2.18\n",
            "[702 | 1310.52] loss=2.03 avg=2.18\n",
            "[703 | 1312.14] loss=2.37 avg=2.18\n",
            "[704 | 1313.76] loss=1.83 avg=2.18\n",
            "[705 | 1315.39] loss=2.33 avg=2.18\n",
            "[706 | 1317.01] loss=2.15 avg=2.18\n",
            "[707 | 1318.63] loss=2.34 avg=2.18\n",
            "[708 | 1320.25] loss=2.08 avg=2.18\n",
            "[709 | 1321.88] loss=1.87 avg=2.18\n",
            "[710 | 1323.50] loss=2.10 avg=2.18\n",
            "[711 | 1325.12] loss=2.41 avg=2.18\n",
            "[712 | 1326.74] loss=2.29 avg=2.18\n",
            "[713 | 1328.36] loss=2.28 avg=2.18\n",
            "[714 | 1329.98] loss=2.33 avg=2.18\n",
            "[715 | 1331.61] loss=1.80 avg=2.18\n",
            "[716 | 1333.23] loss=2.26 avg=2.18\n",
            "[717 | 1334.85] loss=2.52 avg=2.18\n",
            "[718 | 1336.49] loss=2.71 avg=2.19\n",
            "[719 | 1338.11] loss=2.23 avg=2.19\n",
            "[720 | 1339.73] loss=1.33 avg=2.18\n",
            "[721 | 1341.36] loss=2.11 avg=2.18\n",
            "[722 | 1342.98] loss=1.47 avg=2.17\n",
            "[723 | 1344.61] loss=1.95 avg=2.17\n",
            "[724 | 1346.24] loss=2.77 avg=2.18\n",
            "[725 | 1347.87] loss=2.89 avg=2.18\n",
            "[726 | 1349.50] loss=1.97 avg=2.18\n",
            "[727 | 1351.12] loss=1.82 avg=2.18\n",
            "[728 | 1352.75] loss=2.46 avg=2.18\n",
            "[729 | 1354.38] loss=2.14 avg=2.18\n",
            "[730 | 1356.01] loss=1.69 avg=2.18\n",
            "[731 | 1357.62] loss=2.44 avg=2.18\n",
            "[732 | 1359.25] loss=1.59 avg=2.17\n",
            "[733 | 1360.88] loss=1.51 avg=2.17\n",
            "[734 | 1362.49] loss=2.55 avg=2.17\n",
            "[735 | 1364.12] loss=1.19 avg=2.16\n",
            "[736 | 1365.74] loss=2.67 avg=2.17\n",
            "[737 | 1367.37] loss=2.27 avg=2.17\n",
            "[738 | 1369.00] loss=1.96 avg=2.16\n",
            "[739 | 1370.63] loss=2.41 avg=2.17\n",
            "[740 | 1372.26] loss=2.32 avg=2.17\n",
            "[741 | 1373.89] loss=2.21 avg=2.17\n",
            "[742 | 1375.52] loss=1.85 avg=2.17\n",
            "[743 | 1377.15] loss=2.12 avg=2.17\n",
            "[744 | 1378.78] loss=2.50 avg=2.17\n",
            "[745 | 1380.41] loss=2.19 avg=2.17\n",
            "[746 | 1382.05] loss=2.31 avg=2.17\n",
            "[747 | 1383.68] loss=2.69 avg=2.18\n",
            "[748 | 1385.30] loss=2.77 avg=2.18\n",
            "[749 | 1386.94] loss=2.42 avg=2.18\n",
            "[750 | 1388.57] loss=2.51 avg=2.19\n",
            "[751 | 1390.20] loss=2.53 avg=2.19\n",
            "[752 | 1391.83] loss=2.62 avg=2.19\n",
            "[753 | 1393.46] loss=1.90 avg=2.19\n",
            "[754 | 1395.09] loss=1.73 avg=2.19\n",
            "[755 | 1396.72] loss=2.56 avg=2.19\n",
            "[756 | 1398.35] loss=2.44 avg=2.19\n",
            "[757 | 1399.98] loss=2.04 avg=2.19\n",
            "[758 | 1401.61] loss=2.95 avg=2.20\n",
            "[759 | 1403.24] loss=2.63 avg=2.20\n",
            "[760 | 1404.88] loss=1.37 avg=2.20\n",
            "[761 | 1406.51] loss=1.79 avg=2.19\n",
            "[762 | 1408.14] loss=2.29 avg=2.19\n",
            "[763 | 1409.77] loss=2.04 avg=2.19\n",
            "[764 | 1411.41] loss=1.70 avg=2.19\n",
            "[765 | 1413.04] loss=2.29 avg=2.19\n",
            "[766 | 1414.66] loss=2.18 avg=2.19\n",
            "[767 | 1416.30] loss=2.35 avg=2.19\n",
            "[768 | 1417.92] loss=2.15 avg=2.19\n",
            "[769 | 1419.56] loss=2.43 avg=2.19\n",
            "[770 | 1421.19] loss=2.78 avg=2.20\n",
            "[771 | 1422.82] loss=2.11 avg=2.20\n",
            "[772 | 1424.44] loss=1.81 avg=2.19\n",
            "[773 | 1426.08] loss=1.85 avg=2.19\n",
            "[774 | 1427.70] loss=2.24 avg=2.19\n",
            "[775 | 1429.33] loss=2.57 avg=2.19\n",
            "[776 | 1430.96] loss=1.46 avg=2.19\n",
            "[777 | 1432.59] loss=1.39 avg=2.18\n",
            "[778 | 1434.23] loss=2.11 avg=2.18\n",
            "[779 | 1435.85] loss=2.09 avg=2.18\n",
            "[780 | 1437.48] loss=2.81 avg=2.18\n",
            "[781 | 1439.11] loss=1.52 avg=2.18\n",
            "[782 | 1440.74] loss=1.45 avg=2.17\n",
            "[783 | 1442.37] loss=2.38 avg=2.17\n",
            "[784 | 1444.00] loss=2.10 avg=2.17\n",
            "[785 | 1445.62] loss=2.88 avg=2.18\n",
            "[786 | 1447.24] loss=2.19 avg=2.18\n",
            "[787 | 1448.87] loss=2.20 avg=2.18\n",
            "[788 | 1450.50] loss=2.17 avg=2.18\n",
            "[789 | 1452.12] loss=2.07 avg=2.18\n",
            "[790 | 1453.75] loss=2.20 avg=2.18\n",
            "[791 | 1455.38] loss=0.22 avg=2.16\n",
            "[792 | 1457.01] loss=2.36 avg=2.16\n",
            "[793 | 1458.64] loss=1.97 avg=2.16\n",
            "[794 | 1460.27] loss=1.93 avg=2.15\n",
            "[795 | 1461.90] loss=2.59 avg=2.16\n",
            "[796 | 1463.53] loss=1.89 avg=2.16\n",
            "[797 | 1465.15] loss=1.33 avg=2.15\n",
            "[798 | 1466.78] loss=2.06 avg=2.15\n",
            "[799 | 1468.40] loss=2.13 avg=2.15\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "t:”It’s more of a symbiotic relationship.” It allows people to have a much larger volume of food that is distributed in an efficient fashion. “I don’t know if that makes you happier if you have so much more food coming into your home, but it certainly doesn’t make you sad.”\n",
            "\n",
            "Siegel: What about people who use lots of the same produce? For instance someone making a good deal of money, they might buy strawberries, and then in the summer they might buy carrots and celery. Why is it that this sort of trade-off means some people actually end up with less fruit and vegetables than they would have otherwise?\n",
            "\n",
            "Vaughan: Sometimes you’ve got to go a little further with incentives in order to create a trade-off between people’s ability to consume food and their desire to have more to eat. And people know this. They know those of us in the public-health profession, and our families know this. We’ll sometimes push harder than we would to increase our consumption of fruit and vegetables, and many people would rather have more than less. But there is a trade-off there.\n",
            "\n",
            "Siegel: Is there a way to make incentives work better in the environment?\n",
            "\n",
            "Vaughan: There is a way to make it better that I think is likely to make a lot of people happy—but it takes a shift in the way we think about incentives in the environment.\n",
            "\n",
            "First of all, we need to understand that incentives work for the environment. The only thing they’ll do is increase the wealth of those who get the incentives, rather than reduce it for those who don’t respond. And so, the way you might try to increase the wealth of the wealthy is to give them more money. But that might not work if you don’t take into account the fact that they will have an incentive not to respond, which might encourage them to not respond even if the money isn’t there.\n",
            "\n",
            "The second factor is that there are things people can do at the level of their communities that aren’t necessarily as efficient as if they had an incentive. People can do things like put solar panels on their roofs. Maybe they need solar panels that aren’t solar panels. Maybe if they can get someone to buy the electricity from them, they are more likely to use it for other things.\n",
            "\n",
            "But what people are actually likely to do is to use the energy that they get from the sun—to cook. So they are able to use some of the excess energy that they get from the sun, and use it for other things. They are less aware of the value of all the things that they are using their excess electricity for when it comes to cooking, and that could be good. It could also be bad. We need to think about that a little bit.\n",
            "\n",
            "It’s not just that people could use the extra money. They have other financial interests, like family members. They see a rise in their rent because someone has solar panels. They see an increase in their health care costs because a family can use more of their electricity when it is cheaper. So it is not only that they could use the energy that they get from the sun; it is also that they could use more of it.\n",
            "\n",
            "This second line of reasoning—that people would use the extra income in other ways—can have a big impact, because with that they might be able to buy more health care equipment, more computers, and, in the case of family-run businesses, more food.\n",
            "\n",
            "Siegel: Some people argue that there probably is a trade-off between the incentive to use solar panels and other kinds of energy because of the environmental costs. For instance, if you put solar panels in your house, you might pay for a lot of the power at a price that is a little more affordable for the environment on the whole.\n",
            "\n",
            "Vaughan: There are environmental costs in terms of whether the electricity is being wasted and whatnot, which you need to balance that with your other wants that you might want to do for instance eat some vegetables, because, for instance, some people might eat more vegetables, some people might not.\n",
            "\n",
            "Siegel: There was one thing I found absolutely fascinating about this paper—and I think it’s not surprising or surprising—and I wasn’t aware of it, but there was one area in which the researchers in the study were the most precise. They really carefully made their calculation of the effects of what they call “negative income tax” after they used the term “negative income tax” when they were talking about an earlier time. Negative income taxes are, in fact, a concept that is not as old as the U.S. In the paper, they talk about “non-viable programs.” What they mean\n",
            "\n",
            "[800 | 1492.27] loss=1.85 avg=2.14\n",
            "[801 | 1493.90] loss=1.96 avg=2.14\n",
            "[802 | 1495.52] loss=1.84 avg=2.14\n",
            "[803 | 1497.15] loss=1.54 avg=2.13\n",
            "[804 | 1498.78] loss=2.22 avg=2.13\n",
            "[805 | 1500.41] loss=1.75 avg=2.13\n",
            "[806 | 1502.04] loss=2.54 avg=2.13\n",
            "[807 | 1503.67] loss=2.51 avg=2.14\n",
            "[808 | 1505.29] loss=1.92 avg=2.14\n",
            "[809 | 1506.91] loss=1.98 avg=2.13\n",
            "[810 | 1508.53] loss=2.12 avg=2.13\n",
            "[811 | 1510.16] loss=2.94 avg=2.14\n",
            "[812 | 1511.78] loss=2.48 avg=2.15\n",
            "[813 | 1513.40] loss=2.13 avg=2.15\n",
            "[814 | 1515.03] loss=2.21 avg=2.15\n",
            "[815 | 1516.65] loss=2.34 avg=2.15\n",
            "[816 | 1518.28] loss=1.74 avg=2.14\n",
            "[817 | 1519.90] loss=2.04 avg=2.14\n",
            "[818 | 1521.52] loss=2.11 avg=2.14\n",
            "[819 | 1523.15] loss=2.09 avg=2.14\n",
            "[820 | 1524.76] loss=2.14 avg=2.14\n",
            "[821 | 1526.38] loss=2.92 avg=2.15\n",
            "[822 | 1528.01] loss=1.94 avg=2.15\n",
            "[823 | 1529.62] loss=1.98 avg=2.15\n",
            "[824 | 1531.25] loss=1.87 avg=2.14\n",
            "[825 | 1532.87] loss=1.62 avg=2.14\n",
            "[826 | 1534.50] loss=2.29 avg=2.14\n",
            "[827 | 1536.13] loss=2.30 avg=2.14\n",
            "[828 | 1537.76] loss=2.27 avg=2.14\n",
            "[829 | 1539.39] loss=2.39 avg=2.14\n",
            "[830 | 1541.02] loss=1.45 avg=2.14\n",
            "[831 | 1542.65] loss=1.92 avg=2.14\n",
            "[832 | 1544.28] loss=1.79 avg=2.13\n",
            "[833 | 1545.91] loss=1.97 avg=2.13\n",
            "[834 | 1547.54] loss=2.18 avg=2.13\n",
            "[835 | 1549.17] loss=1.62 avg=2.13\n",
            "[836 | 1550.81] loss=2.26 avg=2.13\n",
            "[837 | 1552.44] loss=1.76 avg=2.12\n",
            "[838 | 1554.06] loss=1.29 avg=2.12\n",
            "[839 | 1555.70] loss=1.82 avg=2.11\n",
            "[840 | 1557.32] loss=1.75 avg=2.11\n",
            "[841 | 1558.95] loss=2.94 avg=2.12\n",
            "[842 | 1560.58] loss=2.47 avg=2.12\n",
            "[843 | 1562.21] loss=2.50 avg=2.12\n",
            "[844 | 1563.84] loss=0.42 avg=2.11\n",
            "[845 | 1565.47] loss=1.41 avg=2.10\n",
            "[846 | 1567.10] loss=2.13 avg=2.10\n",
            "[847 | 1568.73] loss=1.98 avg=2.10\n",
            "[848 | 1570.36] loss=1.39 avg=2.09\n",
            "[849 | 1571.99] loss=1.81 avg=2.09\n",
            "[850 | 1573.62] loss=3.24 avg=2.10\n",
            "[851 | 1575.25] loss=1.99 avg=2.10\n",
            "[852 | 1576.87] loss=2.05 avg=2.10\n",
            "[853 | 1578.50] loss=1.86 avg=2.10\n",
            "[854 | 1580.13] loss=1.87 avg=2.09\n",
            "[855 | 1581.76] loss=2.49 avg=2.10\n",
            "[856 | 1583.39] loss=1.91 avg=2.10\n",
            "[857 | 1585.02] loss=2.07 avg=2.10\n",
            "[858 | 1586.65] loss=1.51 avg=2.09\n",
            "[859 | 1588.28] loss=2.06 avg=2.09\n",
            "[860 | 1589.91] loss=2.00 avg=2.09\n",
            "[861 | 1591.54] loss=2.13 avg=2.09\n",
            "[862 | 1593.17] loss=1.81 avg=2.09\n",
            "[863 | 1594.80] loss=2.32 avg=2.09\n",
            "[864 | 1596.43] loss=2.05 avg=2.09\n",
            "[865 | 1598.06] loss=2.09 avg=2.09\n",
            "[866 | 1599.69] loss=2.13 avg=2.09\n",
            "[867 | 1601.32] loss=1.92 avg=2.09\n",
            "[868 | 1602.95] loss=1.55 avg=2.08\n",
            "[869 | 1604.59] loss=2.24 avg=2.08\n",
            "[870 | 1606.22] loss=2.47 avg=2.09\n",
            "[871 | 1607.85] loss=1.95 avg=2.09\n",
            "[872 | 1609.48] loss=2.56 avg=2.09\n",
            "[873 | 1611.10] loss=1.85 avg=2.09\n",
            "[874 | 1612.73] loss=2.06 avg=2.09\n",
            "[875 | 1614.36] loss=1.98 avg=2.09\n",
            "[876 | 1615.99] loss=2.39 avg=2.09\n",
            "[877 | 1617.62] loss=2.48 avg=2.09\n",
            "[878 | 1619.25] loss=2.63 avg=2.10\n",
            "[879 | 1620.88] loss=1.70 avg=2.10\n",
            "[880 | 1622.51] loss=1.93 avg=2.09\n",
            "[881 | 1624.14] loss=1.83 avg=2.09\n",
            "[882 | 1625.77] loss=2.57 avg=2.10\n",
            "[883 | 1627.39] loss=1.68 avg=2.09\n",
            "[884 | 1629.02] loss=2.15 avg=2.09\n",
            "[885 | 1630.65] loss=1.50 avg=2.09\n",
            "[886 | 1632.28] loss=1.87 avg=2.08\n",
            "[887 | 1633.91] loss=2.32 avg=2.09\n",
            "[888 | 1635.54] loss=2.82 avg=2.09\n",
            "[889 | 1637.17] loss=2.44 avg=2.10\n",
            "[890 | 1638.80] loss=1.97 avg=2.10\n",
            "[891 | 1640.43] loss=1.61 avg=2.09\n",
            "[892 | 1642.06] loss=1.35 avg=2.08\n",
            "[893 | 1643.69] loss=1.98 avg=2.08\n",
            "[894 | 1645.31] loss=2.47 avg=2.09\n",
            "[895 | 1646.94] loss=0.18 avg=2.07\n",
            "[896 | 1648.57] loss=1.51 avg=2.06\n",
            "[897 | 1650.20] loss=1.10 avg=2.05\n",
            "[898 | 1651.83] loss=2.24 avg=2.05\n",
            "[899 | 1653.46] loss=1.94 avg=2.05\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "oney’s last hurrah, during which he co-founded his brother-in-law’s insurance practice, he told his son, David, he didn’t think he would be around to see it.\n",
            "\n",
            "In a statement, the law firm that retained the Gertelsman family’s personal attorney, Michael Gelles, said that’s true’s, but nothing close to the whole story. In the decades since the collapse of the family firm, it has remained active in local, state, and federal politics. It represented President George W. Bush in his reelection campaign, and President Trump in his campaign to fill Attorney General Jeff Sessions’s old Senate seat. Gellesman’s sister is now the U.S. Senator from Alabama.\n",
            "\n",
            "That’s no accident. “This company has been in business for 60 years, and I did not build it, nor was my children built,” Trump said during his Wednesday briefing. “They used their tax dollars to build a club for rich people and a club for people who couldn’t afford it.”\n",
            "\n",
            "But a closer look at the history of the Gellersmans shows other aspects of their family’s existence that are much less cheerful reading—and less American than crème brûlée. These include a Southern-fried sense of modesty; a Southern-fried obsession with money; and, as Donald Trump has explained, an almost Southern sense of humor. (His son, Barron, has a more serious side as well: During a recent interview on Jimmy Kimmel Live!, he joked, “I’ve never been a member of the New York Yankees, so this is a shock for me,” adding, “I was going to say the Yankees, but Donald told me to apologize because I’ve never even been to Yankee Stadium.)\n",
            "\n",
            "The Gellersmans’ wealth came from a unique combination of circumstances—first, their father, Samuel Gellers, a manufacturer of tires, made his fortune manufacturing shoes; then, through inheritance, the other family owned many hotels. William Gellers was the great-grandson of the original manufacturers of shoes, who were founded by a man named Thomas Pomeroy in Williamsburg, Brooklyn. William Sr. became the general manager of the family business, which he passed to his son.\n",
            "\n",
            "The Gellersmans were well known in the business world. Their company produced shoes for such manufacturers as Louis J. Watson, of Watson & Runway, and the Gap Corporation. (The Gap Museum’s collection lists G&S as one of the companies it credits with inventing the camera.) William Jr., one of the most famous boys in New York City, was a star performer as the title character in the hit TV series “The Brady Bunch” from 1938 to 1944. In the 1950s, when the Gellersmans moved their headquarters to West Orange, New Jersey, their photo remained on the wall in the hotel lobby.\n",
            "\n",
            "The Gellersmans’ closeness to Williamsburg, and to New York City in general, made William Sr. a particularly effective salesman. He always had a good reason to boast about the success of his business, and G&S, by far the biggest manufacturer of shoes in the world, outsold all but the biggest manufacturers. William Jr. recalled that his father’s greatest business success was “I don’t know how many stores I sold in New York in one day.” He was always telling people, “I’ve sold a lot of stores, but I don’t care, I don’t care what they’re selling, they’”” were his most important sales pitches.\n",
            "\n",
            "The Gellersmans knew they had to attract new customers to survive. They took out ads in the local papers—one headline read: “Hollywood of the West Palm Beach Area—Beef With Gertrude and William Gellers.” The ads were so effective that, decades later, the family did not spend much time with the Hollywood sign. Instead, the family bought and renamed a large tract of land along the Miami River, named after their daughter. The name Gertrude Gellers Farms, later renumbered Gertrude & William Gellers, was adopted by the Trump construction company and sold everywhere else in the construction industry.\n",
            "\n",
            "It is hard to overstate Gertrude Gellers’ impact on the course of American history. Born into wealth during the Great Migration, she was brought up not only with the traditions of the genteel antebellum South, but also the aspirations of a new kind of working woman—one who saw upward mobility for herself in the form of the family business and the institutions it supported. It was\n",
            "\n",
            "[900 | 1677.38] loss=1.77 avg=2.05\n",
            "[901 | 1679.01] loss=1.68 avg=2.05\n",
            "[902 | 1680.64] loss=2.16 avg=2.05\n",
            "[903 | 1682.26] loss=2.11 avg=2.05\n",
            "[904 | 1683.89] loss=2.07 avg=2.05\n",
            "[905 | 1685.51] loss=1.91 avg=2.05\n",
            "[906 | 1687.13] loss=2.31 avg=2.05\n",
            "[907 | 1688.76] loss=2.24 avg=2.05\n",
            "[908 | 1690.37] loss=0.20 avg=2.03\n",
            "[909 | 1691.99] loss=1.54 avg=2.03\n",
            "[910 | 1693.61] loss=1.82 avg=2.03\n",
            "[911 | 1695.23] loss=1.93 avg=2.03\n",
            "[912 | 1696.86] loss=1.89 avg=2.02\n",
            "[913 | 1698.49] loss=0.16 avg=2.01\n",
            "[914 | 1700.11] loss=2.45 avg=2.01\n",
            "[915 | 1701.74] loss=1.45 avg=2.00\n",
            "[916 | 1703.36] loss=2.05 avg=2.00\n",
            "[917 | 1704.98] loss=3.10 avg=2.02\n",
            "[918 | 1706.60] loss=2.69 avg=2.02\n",
            "[919 | 1708.23] loss=0.18 avg=2.00\n",
            "[920 | 1709.86] loss=2.74 avg=2.01\n",
            "[921 | 1711.49] loss=1.95 avg=2.01\n",
            "[922 | 1713.12] loss=1.39 avg=2.00\n",
            "[923 | 1714.75] loss=2.30 avg=2.01\n",
            "[924 | 1716.37] loss=1.46 avg=2.00\n",
            "[925 | 1718.00] loss=2.51 avg=2.01\n",
            "[926 | 1719.63] loss=1.74 avg=2.00\n",
            "[927 | 1721.26] loss=1.95 avg=2.00\n",
            "[928 | 1722.89] loss=2.05 avg=2.00\n",
            "[929 | 1724.52] loss=1.51 avg=2.00\n",
            "[930 | 1726.15] loss=2.24 avg=2.00\n",
            "[931 | 1727.78] loss=2.56 avg=2.01\n",
            "[932 | 1729.41] loss=1.71 avg=2.00\n",
            "[933 | 1731.04] loss=1.64 avg=2.00\n",
            "[934 | 1732.67] loss=2.51 avg=2.01\n",
            "[935 | 1734.30] loss=2.10 avg=2.01\n",
            "[936 | 1735.93] loss=2.45 avg=2.01\n",
            "[937 | 1737.56] loss=1.81 avg=2.01\n",
            "[938 | 1739.19] loss=1.99 avg=2.01\n",
            "[939 | 1740.81] loss=2.02 avg=2.01\n",
            "[940 | 1742.44] loss=1.84 avg=2.01\n",
            "[941 | 1744.07] loss=1.01 avg=2.00\n",
            "[942 | 1745.71] loss=1.65 avg=1.99\n",
            "[943 | 1747.34] loss=2.58 avg=2.00\n",
            "[944 | 1748.97] loss=1.86 avg=2.00\n",
            "[945 | 1750.61] loss=1.98 avg=2.00\n",
            "[946 | 1752.24] loss=1.75 avg=2.00\n",
            "[947 | 1753.87] loss=2.55 avg=2.00\n",
            "[948 | 1755.50] loss=0.23 avg=1.98\n",
            "[949 | 1757.12] loss=2.55 avg=1.99\n",
            "[950 | 1758.76] loss=1.99 avg=1.99\n",
            "[951 | 1760.39] loss=2.25 avg=1.99\n",
            "[952 | 1762.02] loss=2.17 avg=1.99\n",
            "[953 | 1763.64] loss=1.99 avg=1.99\n",
            "[954 | 1765.27] loss=2.12 avg=1.99\n",
            "[955 | 1766.89] loss=1.78 avg=1.99\n",
            "[956 | 1768.52] loss=2.43 avg=2.00\n",
            "[957 | 1770.16] loss=1.83 avg=2.00\n",
            "[958 | 1771.78] loss=2.12 avg=2.00\n",
            "[959 | 1773.41] loss=1.24 avg=1.99\n",
            "[960 | 1775.04] loss=1.95 avg=1.99\n",
            "[961 | 1776.67] loss=0.44 avg=1.97\n",
            "[962 | 1778.30] loss=1.65 avg=1.97\n",
            "[963 | 1779.93] loss=2.15 avg=1.97\n",
            "[964 | 1781.56] loss=1.63 avg=1.97\n",
            "[965 | 1783.20] loss=1.33 avg=1.96\n",
            "[966 | 1784.83] loss=1.60 avg=1.96\n",
            "[967 | 1786.46] loss=1.48 avg=1.95\n",
            "[968 | 1788.10] loss=1.85 avg=1.95\n",
            "[969 | 1789.73] loss=1.59 avg=1.95\n",
            "[970 | 1791.37] loss=1.68 avg=1.95\n",
            "[971 | 1793.00] loss=2.22 avg=1.95\n",
            "[972 | 1794.63] loss=2.28 avg=1.95\n",
            "[973 | 1796.26] loss=0.37 avg=1.94\n",
            "[974 | 1797.89] loss=1.94 avg=1.94\n",
            "[975 | 1799.52] loss=2.42 avg=1.94\n",
            "[976 | 1801.15] loss=3.28 avg=1.95\n",
            "[977 | 1802.78] loss=3.14 avg=1.97\n",
            "[978 | 1804.40] loss=1.57 avg=1.96\n",
            "[979 | 1806.03] loss=0.52 avg=1.95\n",
            "[980 | 1807.67] loss=2.26 avg=1.95\n",
            "[981 | 1809.30] loss=1.93 avg=1.95\n",
            "[982 | 1810.93] loss=1.96 avg=1.95\n",
            "[983 | 1812.56] loss=1.93 avg=1.95\n",
            "[984 | 1814.20] loss=2.08 avg=1.95\n",
            "[985 | 1815.83] loss=1.60 avg=1.95\n",
            "[986 | 1817.46] loss=1.33 avg=1.94\n",
            "[987 | 1819.09] loss=2.37 avg=1.95\n",
            "[988 | 1820.71] loss=1.64 avg=1.94\n",
            "[989 | 1822.34] loss=2.09 avg=1.95\n",
            "[990 | 1823.97] loss=2.49 avg=1.95\n",
            "[991 | 1825.61] loss=1.41 avg=1.95\n",
            "[992 | 1827.23] loss=2.36 avg=1.95\n",
            "[993 | 1828.86] loss=2.09 avg=1.95\n",
            "[994 | 1830.49] loss=1.79 avg=1.95\n",
            "[995 | 1832.12] loss=1.13 avg=1.94\n",
            "[996 | 1833.75] loss=1.64 avg=1.94\n",
            "[997 | 1835.38] loss=0.99 avg=1.93\n",
            "[998 | 1837.01] loss=1.45 avg=1.92\n",
            "[999 | 1838.64] loss=2.02 avg=1.92\n",
            "Saving checkpoint/atlantic_business_concat_345/model-1000\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " never a bad idea. “I would just do it,” he said. “I think it’s a good idea.”\n",
            "\n",
            "What he didn’t mention was that he would be taking time off. So he began writing his thesis on entrepreneurship.\n",
            "\n",
            "Fadulu thought he was nuts, but this idea—writing a thesis on entrepreneurship—took off. He started teaching himself online courses, and eventually landed himself a job at StartupsOhio, a Columbus-area accelerator that helps start-ups raise capital. He’s now the chairman of StartupsOhio’s board and had never worked for an employee-owned startup before.\n",
            "\n",
            "The concept of employee-owned enterprises (to use the acronym) came from the American-Statesman’s Cornhusker Army of the 1860s. It was there that farmers and soldiers opened their own cooperatives and businesses. But when the idea of employee-owned companies caught on—as it should have, in today’s highly competitive, competitive world—the employee ownership model caught on as well.\n",
            "\n",
            "At a conference in 2011, a professor named Brian Feldman, a founding partner at Next Generation Ventures, described an employee-owned company as one owned by a highly skilled worker who wants to invest in things other than the business, like education. He’s talking about a business that hires an intern to manage information technology. The employee then purchases the IT equipment (usually hardware) and tools (typically software) that he uses in his day-to-day business.\n",
            "\n",
            "In addition to improving employees’ lives, employee-owned businesses can be a way for an employer to compete on cost without increasing wages. A recent study by the Center for Business and Economic Research at the University of Pennsylvania looked at the costs and benefits of employee-owned companies in the U.S. Between 2001 and 2012, the study found—according to the Internal Revenue Service—employee-owned businesses paid an average of 4.5 times the wages and 4.0 times the benefits of companies that relied on employees to run the business.\n",
            "\n",
            "What’s more, the study found, employee-owned companies were less likely than those owned by individual entrepreneurs to experience a reduction in productivity or sales volume. In addition, the study found that fewer companies remained employee-owned into their later years. By the early stages of an employee-owned business, median costs typically have risen at an annual average rate of 4 percent per year. And as such, an industry with a low worker base tends to have low average costs. In a study published in the same issue of the Economic Journal in 2013, the authors of the study write: “In contrast to companies that are owned by individual workers, which are owned and operated collectively, employee-owned companies can be individually owned or privately owned, which creates a separate ownership structure for the owners.”\n",
            "\n",
            "This is the model on which Mark Cuban’s new startup, Hitting the Wall Street , is modeled. The founder of the hedge fund Elliott Management, Cuban, along with his father, James Cuban, founded Elliott Liberty, one of the largest private-equity firms. A hedge fund is a hedge fund. But unlike a hedge fund, an investor has a clear plan and is accountable to investors about how to spend their money. Elliott is also structured as a publicly-traded company, meaning that if investors decide that it is no longer profitable, they can sell their shares. So far, so good news for Cuban. He recently sold about $15 million of his own Elliott Liberty shares at a bargain-basement price of just $12 per share, as reported by CNBC.\n",
            "\n",
            "But now one thing is different: The firm is publicly traded, for-profit. The Cubanes no longer own any controlling interests in the company. So, like an employee-owned venture capital firm, the company has to be public for investors to invest in it, and so it is. As a result, the firm has become the first publicly traded investment-management firm to engage in for-profit hedge-fund business.\n",
            "\n",
            "In another study published in the American Economic Review this past May, researchers examined how hedge funds are affected by regulation. They looked at how fees and other costs and restrictions on the company, such as the requirement that it is publicly traded, affect hedge funds’ ability to speculate and invest in the firm. They found that the fees that hedge funds have to pay to run hedge funds can have a significant impact on their performance.” They found that, for a benchmark fee of $1,000 ($300 per employee), public company hedge funds’ returns on their investments fell by an average of .6 percent a year. “These results suggest that, when hedge funds are subject to regulations that limit their choices, their investment returns suffer, and, even though these returns would be higher if hedge funds were not subject to regulatory oversight, they actually decline,�\n",
            "\n",
            "[1000 | 1874.38] loss=1.32 avg=1.92\n",
            "[1001 | 1876.05] loss=2.04 avg=1.92\n",
            "interrupted\n",
            "Saving checkpoint/atlantic_business_concat_345/model-1002\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWzs6nQ-VW20",
        "colab_type": "code",
        "outputId": "85a02727-31e9-4295-e43b-eb530cee8f65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "## technology essays training  - with 345\n",
        "!PYTHONPATH=src ./train.py --dataset /content/drive/My\\ Drive/writrly_proj_files/Atlantic_essays/technology --combine 500 --run_name 'atlantic_technology_concat_345' --model_name '345M'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0626 14:13:30.483827 139983816439680 deprecation_wrapper.py:119] From /content/gpt-2/src/model.py:147: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
            "\n",
            "W0626 14:13:30.508350 139983816439680 deprecation_wrapper.py:119] From /content/gpt-2/src/memory_saving_gradients.py:13: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n",
            "W0626 14:13:30.612340 139983816439680 deprecation_wrapper.py:119] From ./train.py:87: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "W0626 14:13:30.612653 139983816439680 deprecation_wrapper.py:119] From ./train.py:90: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "2019-06-26 14:13:30.618807: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-06-26 14:13:30.619067: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x283b100 executing computations on platform Host. Devices:\n",
            "2019-06-26 14:13:30.619097: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2019-06-26 14:13:30.638534: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\n",
            "2019-06-26 14:13:30.812598: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-26 14:13:30.813139: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x283a840 executing computations on platform CUDA. Devices:\n",
            "2019-06-26 14:13:30.813195: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2019-06-26 14:13:30.813454: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-26 14:13:30.813802: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-06-26 14:13:30.814340: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-06-26 14:13:30.815920: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-06-26 14:13:30.817329: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-06-26 14:13:30.818044: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-06-26 14:13:30.819929: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-06-26 14:13:30.821198: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-06-26 14:13:30.824872: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-06-26 14:13:30.825026: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-26 14:13:30.825456: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-26 14:13:30.825757: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
            "2019-06-26 14:13:30.825837: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-06-26 14:13:30.826770: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-06-26 14:13:30.826795: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n",
            "2019-06-26 14:13:30.826805: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n",
            "2019-06-26 14:13:30.827111: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-26 14:13:30.827524: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-26 14:13:30.828549: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14202 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "W0626 14:13:30.829456 139983816439680 deprecation_wrapper.py:119] From ./train.py:91: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0626 14:13:41.134307 139983816439680 deprecation.py:323] From /content/gpt-2/src/sample.py:65: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "W0626 14:13:41.148002 139983816439680 deprecation.py:323] From /content/gpt-2/src/sample.py:16: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0626 14:13:41.149624 139983816439680 deprecation.py:323] From /content/gpt-2/src/sample.py:70: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.random.categorical` instead.\n",
            "W0626 14:13:41.159183 139983816439680 deprecation_wrapper.py:119] From ./train.py:120: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "W0626 14:13:55.822647 139983816439680 deprecation_wrapper.py:119] From ./train.py:143: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n",
            "W0626 14:13:55.825336 139983816439680 deprecation_wrapper.py:119] From ./train.py:146: The name tf.summary.merge is deprecated. Please use tf.compat.v1.summary.merge instead.\n",
            "\n",
            "W0626 14:13:55.826121 139983816439680 deprecation_wrapper.py:119] From ./train.py:148: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
            "\n",
            "W0626 14:13:55.826808 139983816439680 deprecation_wrapper.py:119] From ./train.py:151: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "Loading checkpoint models/345M/model.ckpt\n",
            "W0626 14:14:08.333047 139983816439680 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "Loading dataset...\n",
            "100% 229/229 [02:20<00:00,  1.72it/s]\n",
            "dataset has 461098 tokens\n",
            "Training...\n",
            "2019-06-26 14:16:48.029671: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n",
            "2019-06-26 14:16:48.703795: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "[1 | 12.96] loss=3.22 avg=3.22\n",
            "[2 | 14.55] loss=3.06 avg=3.14\n",
            "[3 | 16.14] loss=3.06 avg=3.11\n",
            "[4 | 17.73] loss=3.00 avg=3.09\n",
            "[5 | 19.33] loss=3.15 avg=3.10\n",
            "[6 | 20.94] loss=3.10 avg=3.10\n",
            "[7 | 22.55] loss=2.83 avg=3.06\n",
            "[8 | 24.17] loss=3.18 avg=3.08\n",
            "[9 | 25.79] loss=3.06 avg=3.07\n",
            "[10 | 27.41] loss=2.59 avg=3.02\n",
            "[11 | 29.04] loss=3.09 avg=3.03\n",
            "[12 | 30.68] loss=2.89 avg=3.02\n",
            "[13 | 32.31] loss=2.56 avg=2.98\n",
            "[14 | 33.95] loss=3.00 avg=2.98\n",
            "[15 | 35.60] loss=2.65 avg=2.96\n",
            "[16 | 37.25] loss=2.91 avg=2.95\n",
            "[17 | 38.91] loss=3.01 avg=2.96\n",
            "[18 | 40.57] loss=3.14 avg=2.97\n",
            "[19 | 42.23] loss=3.06 avg=2.97\n",
            "[20 | 43.91] loss=2.86 avg=2.97\n",
            "[21 | 45.57] loss=2.99 avg=2.97\n",
            "[22 | 47.24] loss=3.12 avg=2.98\n",
            "[23 | 48.90] loss=2.93 avg=2.97\n",
            "[24 | 50.56] loss=3.13 avg=2.98\n",
            "[25 | 52.22] loss=2.89 avg=2.98\n",
            "[26 | 53.87] loss=2.99 avg=2.98\n",
            "[27 | 55.51] loss=3.03 avg=2.98\n",
            "[28 | 57.15] loss=3.15 avg=2.99\n",
            "[29 | 58.78] loss=2.55 avg=2.97\n",
            "[30 | 60.42] loss=2.92 avg=2.97\n",
            "[31 | 62.05] loss=2.56 avg=2.95\n",
            "[32 | 63.69] loss=3.11 avg=2.96\n",
            "[33 | 65.31] loss=2.90 avg=2.96\n",
            "[34 | 66.93] loss=3.12 avg=2.96\n",
            "[35 | 68.55] loss=2.42 avg=2.94\n",
            "[36 | 70.17] loss=2.94 avg=2.94\n",
            "[37 | 71.79] loss=2.66 avg=2.94\n",
            "[38 | 73.41] loss=2.75 avg=2.93\n",
            "[39 | 75.02] loss=3.31 avg=2.94\n",
            "[40 | 76.62] loss=3.24 avg=2.95\n",
            "[41 | 78.23] loss=2.70 avg=2.94\n",
            "[42 | 79.84] loss=2.80 avg=2.94\n",
            "[43 | 81.45] loss=2.74 avg=2.93\n",
            "[44 | 83.05] loss=2.98 avg=2.93\n",
            "[45 | 84.66] loss=3.03 avg=2.94\n",
            "[46 | 86.26] loss=2.63 avg=2.93\n",
            "[47 | 87.87] loss=2.92 avg=2.93\n",
            "[48 | 89.47] loss=2.93 avg=2.93\n",
            "[49 | 91.07] loss=2.94 avg=2.93\n",
            "[50 | 92.68] loss=2.50 avg=2.92\n",
            "[51 | 94.27] loss=3.02 avg=2.92\n",
            "[52 | 95.87] loss=2.92 avg=2.92\n",
            "[53 | 97.48] loss=2.89 avg=2.92\n",
            "[54 | 99.08] loss=2.84 avg=2.92\n",
            "[55 | 100.68] loss=3.16 avg=2.92\n",
            "[56 | 102.29] loss=2.87 avg=2.92\n",
            "[57 | 103.89] loss=3.34 avg=2.93\n",
            "[58 | 105.50] loss=3.24 avg=2.94\n",
            "[59 | 107.11] loss=2.53 avg=2.93\n",
            "[60 | 108.72] loss=2.74 avg=2.93\n",
            "[61 | 110.33] loss=3.45 avg=2.94\n",
            "[62 | 111.94] loss=3.11 avg=2.94\n",
            "[63 | 113.56] loss=3.28 avg=2.95\n",
            "[64 | 115.18] loss=2.58 avg=2.94\n",
            "[65 | 116.80] loss=2.43 avg=2.93\n",
            "[66 | 118.42] loss=3.45 avg=2.94\n",
            "[67 | 120.04] loss=2.87 avg=2.94\n",
            "[68 | 121.70] loss=2.88 avg=2.94\n",
            "[69 | 123.34] loss=2.79 avg=2.93\n",
            "[70 | 124.96] loss=3.36 avg=2.94\n",
            "[71 | 126.59] loss=3.04 avg=2.95\n",
            "[72 | 128.21] loss=2.42 avg=2.94\n",
            "[73 | 129.84] loss=2.76 avg=2.93\n",
            "[74 | 131.47] loss=2.23 avg=2.92\n",
            "[75 | 133.10] loss=3.31 avg=2.93\n",
            "[76 | 134.72] loss=2.84 avg=2.92\n",
            "[77 | 136.35] loss=2.65 avg=2.92\n",
            "[78 | 137.98] loss=2.44 avg=2.91\n",
            "[79 | 139.61] loss=3.32 avg=2.92\n",
            "[80 | 141.23] loss=3.25 avg=2.92\n",
            "[81 | 142.87] loss=2.66 avg=2.92\n",
            "[82 | 144.50] loss=3.14 avg=2.92\n",
            "[83 | 146.13] loss=2.84 avg=2.92\n",
            "[84 | 147.76] loss=3.04 avg=2.92\n",
            "[85 | 149.40] loss=3.11 avg=2.93\n",
            "[86 | 151.01] loss=2.77 avg=2.92\n",
            "[87 | 152.64] loss=2.87 avg=2.92\n",
            "[88 | 154.26] loss=3.00 avg=2.92\n",
            "[89 | 155.89] loss=3.03 avg=2.93\n",
            "[90 | 157.52] loss=2.91 avg=2.93\n",
            "[91 | 159.14] loss=2.51 avg=2.92\n",
            "[92 | 160.76] loss=2.72 avg=2.92\n",
            "[93 | 162.38] loss=3.09 avg=2.92\n",
            "[94 | 164.00] loss=2.65 avg=2.91\n",
            "[95 | 165.61] loss=2.97 avg=2.92\n",
            "[96 | 167.23] loss=3.28 avg=2.92\n",
            "[97 | 168.85] loss=2.85 avg=2.92\n",
            "[98 | 170.46] loss=2.71 avg=2.92\n",
            "[99 | 172.07] loss=3.27 avg=2.92\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " designed for the general public), but we can't ignore the fact that the design is not for everybody.\n",
            "\n",
            "What's more important, though, is the company that builds it, and its ability to make money: Google Fiber. A $1,000 installation and a monthly cable bill can get you pretty much unlimited coverage, since the company is able to reach millions of its customers at speeds of thousands of miles per hour.\n",
            "\n",
            "Google has no problem selling broadband as a service, and has taken no position on the question of whether a service like Google Fiber should require a cable. In fact, it has promoted it as such, saying that it will help people \"unlock\" their homes by giving them faster Internet access. And it has pushed a business model that claims Google Fiber is the ultimate Internet access alternative.\n",
            "\n",
            "But when it comes to fiber, Google is clearly the underdog in this fight. You may think Google Fiber is just another piece of infrastructure that's built by a company. You'd be wrong.\n",
            "\n",
            "A little over a year ago, Google Fiber announced plans to replace fiber optics in its fiber-optic networks with fiber-optic cable (though the company has had problems with these deployments.) Google Fiber plans to eventually use fiber and WiFi to connect to its main network, but unlike with the main network, the main network will not be directly connected to its residential network, though Google Fiber has stated publicly that it will eventually be.\n",
            "\n",
            "What's so strange about Google Fiber, though? For one (and only one) reason: Google says it has its own fiber-optic cable system, but it won't go into service until March 2018 after the end of an $8 billion acquisition in which it buys a minority stake. Google plans to have it up and running as soon as it starts receiving customers. And Google already does everything it can to assure that the company's fiber service is the best it can do for people, even if it's not necessarily a good deal for other customers. It takes its time in building and installing the networks and also makes sure every home has access to the service before it comes online. This isn't to say it's perfect—it's only one company with its own cable and Internet service, and it hasn't always been a good fit for consumers—but it's better than nothing.\n",
            "\n",
            "It also may be a little unfair. The Fiber team has said so explicitly about the network being for Google customers, and the team has also said a lot about wanting to help Google get its network ready for wider rollout. The problem, though, is that this isn't just about the Fiber team. Comcast has been on a mission to get every household connected to the Internet at least once, but Google Fiber is a different animal. If Google Fiber really is Google Fiber, though, it is clearly going for a bigger share of the pie. Comcast has its own fiber network in the works that it says will be ready in mid-2018 (which does indeed appear to be coming later than Google), but Comcast and AT&T are trying to do this at vastly different scales and costs, and they both want to be sure that their networks come with the best experience for Google Fiber.\n",
            "\n",
            "If Google Fiber wins (as I imagine it's going to, because the company has already won), it could have a big impact on local competition. This was one of Google Fiber's core business models, so why not try to do the same with fiber? Sure, Google Fiber wouldn't be able to control the local fiber monopoly; but with fiber, Google could compete on the same level as Google Fiber. (You might argue that Google Fiber shouldn't have been able do the fiber business at all, but that's another issue.)\n",
            "\n",
            "That said, I wonder if Google Fiber will have a major impact on some people's lives. Fiber is great for making broadband faster and more reliable (though I'm glad Google Fiber is getting on that bandwagon). It also lets people live like they're doing in the real world, and that's a good thing. But I don't know if Google Fiber is going to do that in a way that makes residents feel at home, or even if Google Fiber can have a major impact on the lives of some of its customers. (At the time of writing, there were still no plans for Fiber in Memphis, Tennessee.)\n",
            "\n",
            "Google can certainly have an impact on the way customers live—we'll just have to see. And yes, it could make Fiber a little cheaper here, as the price can be cut by 25 percent to the average consumer. But even at $1,000, it's still a lot. Google seems to be making a big effort to take away its competitors' monopoly in both the home and the digital world. It seems to be hoping that having an edge will help, at best, and that people will end up being more satisfied. And if that doesn't work out—and it may not—then maybe Google Fiber has its own agenda.\n",
            "\n",
            "I\n",
            "\n",
            "[100 | 198.89] loss=2.66 avg=2.92\n",
            "[101 | 200.51] loss=2.86 avg=2.92\n",
            "[102 | 202.14] loss=2.60 avg=2.91\n",
            "[103 | 203.77] loss=2.70 avg=2.91\n",
            "[104 | 205.40] loss=2.59 avg=2.90\n",
            "[105 | 207.03] loss=2.59 avg=2.90\n",
            "[106 | 208.66] loss=3.07 avg=2.90\n",
            "[107 | 210.29] loss=2.58 avg=2.90\n",
            "[108 | 211.92] loss=2.58 avg=2.89\n",
            "[109 | 213.55] loss=2.67 avg=2.89\n",
            "[110 | 215.18] loss=2.70 avg=2.89\n",
            "[111 | 216.81] loss=2.79 avg=2.88\n",
            "[112 | 218.43] loss=2.65 avg=2.88\n",
            "[113 | 220.06] loss=3.39 avg=2.89\n",
            "[114 | 221.69] loss=2.70 avg=2.89\n",
            "[115 | 223.32] loss=3.13 avg=2.89\n",
            "[116 | 224.96] loss=2.43 avg=2.88\n",
            "[117 | 226.59] loss=2.79 avg=2.88\n",
            "[118 | 228.22] loss=3.34 avg=2.89\n",
            "[119 | 229.86] loss=2.93 avg=2.89\n",
            "[120 | 231.49] loss=2.47 avg=2.88\n",
            "[121 | 233.13] loss=2.87 avg=2.88\n",
            "[122 | 234.76] loss=2.68 avg=2.88\n",
            "[123 | 236.40] loss=2.47 avg=2.87\n",
            "[124 | 238.03] loss=2.44 avg=2.87\n",
            "[125 | 239.67] loss=2.92 avg=2.87\n",
            "[126 | 241.31] loss=2.67 avg=2.87\n",
            "[127 | 242.94] loss=3.21 avg=2.87\n",
            "[128 | 244.58] loss=3.05 avg=2.87\n",
            "[129 | 246.22] loss=2.57 avg=2.87\n",
            "[130 | 247.84] loss=2.91 avg=2.87\n",
            "[131 | 249.48] loss=2.63 avg=2.87\n",
            "[132 | 251.11] loss=3.19 avg=2.87\n",
            "[133 | 252.75] loss=2.98 avg=2.87\n",
            "[134 | 254.36] loss=2.66 avg=2.87\n",
            "[135 | 255.99] loss=2.87 avg=2.87\n",
            "[136 | 257.63] loss=2.47 avg=2.86\n",
            "[137 | 259.25] loss=2.50 avg=2.86\n",
            "[138 | 260.88] loss=2.75 avg=2.86\n",
            "[139 | 262.51] loss=2.49 avg=2.85\n",
            "[140 | 264.14] loss=3.08 avg=2.86\n",
            "[141 | 265.77] loss=2.59 avg=2.85\n",
            "[142 | 267.40] loss=2.52 avg=2.85\n",
            "[143 | 269.03] loss=2.35 avg=2.84\n",
            "[144 | 270.67] loss=2.92 avg=2.84\n",
            "[145 | 272.29] loss=2.69 avg=2.84\n",
            "[146 | 273.92] loss=3.21 avg=2.84\n",
            "[147 | 275.54] loss=2.74 avg=2.84\n",
            "[148 | 277.17] loss=2.60 avg=2.84\n",
            "[149 | 278.78] loss=2.69 avg=2.84\n",
            "[150 | 280.40] loss=2.75 avg=2.84\n",
            "[151 | 282.03] loss=2.86 avg=2.84\n",
            "[152 | 283.66] loss=3.16 avg=2.84\n",
            "[153 | 285.28] loss=3.12 avg=2.84\n",
            "[154 | 286.91] loss=2.91 avg=2.85\n",
            "[155 | 288.54] loss=2.64 avg=2.84\n",
            "[156 | 290.16] loss=2.77 avg=2.84\n",
            "[157 | 291.79] loss=3.24 avg=2.85\n",
            "[158 | 293.42] loss=2.44 avg=2.84\n",
            "[159 | 295.03] loss=2.61 avg=2.84\n",
            "[160 | 296.66] loss=2.61 avg=2.84\n",
            "[161 | 298.28] loss=3.06 avg=2.84\n",
            "[162 | 299.91] loss=2.21 avg=2.83\n",
            "[163 | 301.54] loss=3.08 avg=2.83\n",
            "[164 | 303.17] loss=2.89 avg=2.84\n",
            "[165 | 304.80] loss=3.04 avg=2.84\n",
            "[166 | 306.43] loss=3.34 avg=2.84\n",
            "[167 | 308.06] loss=2.59 avg=2.84\n",
            "[168 | 309.68] loss=2.90 avg=2.84\n",
            "[169 | 311.30] loss=2.39 avg=2.84\n",
            "[170 | 312.93] loss=2.85 avg=2.84\n",
            "[171 | 314.56] loss=2.70 avg=2.83\n",
            "[172 | 316.19] loss=3.11 avg=2.84\n",
            "[173 | 317.82] loss=2.84 avg=2.84\n",
            "[174 | 319.44] loss=2.63 avg=2.84\n",
            "[175 | 321.07] loss=2.63 avg=2.83\n",
            "[176 | 322.71] loss=2.82 avg=2.83\n",
            "[177 | 324.34] loss=2.43 avg=2.83\n",
            "[178 | 325.97] loss=2.45 avg=2.82\n",
            "[179 | 327.60] loss=2.46 avg=2.82\n",
            "[180 | 329.23] loss=2.81 avg=2.82\n",
            "[181 | 330.86] loss=2.86 avg=2.82\n",
            "[182 | 332.49] loss=3.43 avg=2.83\n",
            "[183 | 334.12] loss=2.99 avg=2.83\n",
            "[184 | 335.74] loss=2.85 avg=2.83\n",
            "[185 | 337.38] loss=3.43 avg=2.84\n",
            "[186 | 339.01] loss=2.25 avg=2.83\n",
            "[187 | 340.64] loss=3.05 avg=2.83\n",
            "[188 | 342.28] loss=2.83 avg=2.83\n",
            "[189 | 343.91] loss=2.82 avg=2.83\n",
            "[190 | 345.54] loss=2.40 avg=2.83\n",
            "[191 | 347.17] loss=2.39 avg=2.82\n",
            "[192 | 348.79] loss=3.12 avg=2.82\n",
            "[193 | 350.43] loss=3.03 avg=2.83\n",
            "[194 | 352.06] loss=2.62 avg=2.82\n",
            "[195 | 353.69] loss=2.71 avg=2.82\n",
            "[196 | 355.32] loss=2.62 avg=2.82\n",
            "[197 | 356.95] loss=2.70 avg=2.82\n",
            "[198 | 358.57] loss=2.67 avg=2.82\n",
            "[199 | 360.20] loss=2.87 avg=2.82\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " have helped to turn public-policy debates around from the left-liberal to the right-conservative, so I want to focus in particular on what I've called the 'liberal/conservative dichotomy.' I'm going to use the term 'liberal' to refer to a more positive definition of what liberalism means, and 'conservative' to refer to more negative ones.\n",
            "\n",
            "There are a few big differences between the two. One is that liberals, unlike conservatives, are relatively willing to give up some social liberalism (i.e., the pursuit of justice and social equality); in other words, in the liberal tradition in particular, the goal is to make the world a better place and we should strive to make people happy and happy. Liberal policy doesn't necessarily require a massive expansion of state power because the world is actually quite safe with everyone peacefully living together, or it's not a concern to try to ensure that everyone has a decent standard of living; and I'm not sure that conservative policy is necessarily aimed at making the world a bad place.\n",
            "\n",
            "Another big difference is that liberal and conservative values are not as closely aligned—and thus more difficult to divide into good and bad—as left-right ones. For one thing, liberal values tend to be about a general concern about the world and society, while the conservative ones focus on certain topics. Liberal societies (and I include liberal countries like the United Kingdom and most Western European countries, in an attempt to make one another seem more liberal than they really are) support free speech and freedom of association, while conservative societies tend to favor narrow rights and restrictions where society can make people suffer.\n",
            "\n",
            "Another difference between the two is that liberal beliefs and practices differ from conservative ones. There's much more of a connection between liberals and conservatives, as the liberals are often seen as doing a lot more in many cases. For example: In a typical liberal country, citizens are often treated equally by the government (it's called equality), there are no laws that regulate the activities of private entities (like the government), and governments have more freedom to act like states, as opposed to being ruled and controlled by one group of politicians (though it's not very common, for example). However, if you're a conservative and a liberal, you don't get the same rights, and the government has more power, which means things can get more political in some instances and less in others. The only real difference is that liberals tend to believe in basic human rights and conservatives tend to be on the side of liberty and limited government (as opposed to a general belief that the government should be the least intrusive authority on citizen's life and liberties), making them harder to divide.\n",
            "\n",
            "But another big difference is that conservatives tend to embrace certain social and economic principles: If you believe in individual responsibility and personal responsibility, you're more likely to support lower taxes and less government regulation. If you believe in markets and free markets, or that labor is more important than profits, and that some are \"morally just,\" you're more likely to support lower government taxes and less regulation of individuals. There's a strong cultural, not just empirical, basis for this: Conservatives are more likely to believe in values like hard work and thrift, rather than economic theories like free markets and open markets. I think this is mainly because conservative Americans have been under more attack by liberals than either are willing to acknowledge, which can leave conservatives feeling the economic and fiscal burdens of left-right polarization more than liberals.\n",
            "\n",
            "There's even a real cultural gap. If liberals accept some of the values of conservative countries—that there is a market where everyone should be able to reach their ultimate goal of perfection, that certain individuals have the right to a certain standard of living in the society, and that government isn't the perfect solution—then the conservative argument for keeping government out of the economy, or protecting individual liberties, becomes harder to make. If you're a liberal, then liberal values are more in line with liberal values than is conservative.\n",
            "\n",
            "So why are conservatives so hostile to liberalism? Some of it has to do with the fact that it's the most liberal of the liberal forces, while conservatives are conservative in many ways. Some of it has to do with the fact that conservatives think the world should be a good place and that conservatives are better in the world. Some of it has to do with the fact that in many ways, conservatives hold the view that government should be run with the lowest and least intrusive authority, a viewpoint reflected by the belief that government has more freedom than it does. But the bigger reason is that liberals and conservatives share many values, including social liberalism, but that the liberal vision of social justice and government intervention is far more positive than conservatives' approach. And with this, I think we can call it the conservative/liberal dichotomy.\n",
            "\n",
            "What are these more positive values? Let's start with what liberals do, rather than conservatives, when it comes to social and economic issues. Conservatives think the government gets most of the benefits. Liberals think more\n",
            "\n",
            "[200 | 384.11] loss=3.34 avg=2.82\n",
            "[201 | 385.73] loss=2.69 avg=2.82\n",
            "[202 | 387.35] loss=2.37 avg=2.82\n",
            "[203 | 388.98] loss=2.95 avg=2.82\n",
            "[204 | 390.60] loss=2.71 avg=2.82\n",
            "[205 | 392.23] loss=2.93 avg=2.82\n",
            "[206 | 393.85] loss=2.69 avg=2.82\n",
            "[207 | 395.47] loss=2.85 avg=2.82\n",
            "[208 | 397.10] loss=3.10 avg=2.82\n",
            "[209 | 398.72] loss=2.73 avg=2.82\n",
            "[210 | 400.35] loss=3.03 avg=2.82\n",
            "[211 | 401.97] loss=2.68 avg=2.82\n",
            "[212 | 403.59] loss=3.18 avg=2.83\n",
            "[213 | 405.22] loss=2.46 avg=2.82\n",
            "[214 | 406.84] loss=3.12 avg=2.82\n",
            "[215 | 408.46] loss=2.97 avg=2.83\n",
            "[216 | 410.08] loss=3.09 avg=2.83\n",
            "[217 | 411.70] loss=2.53 avg=2.83\n",
            "[218 | 413.32] loss=3.16 avg=2.83\n",
            "[219 | 414.95] loss=2.56 avg=2.83\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kgZGSI8NX0pw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## science essays training  - with 345\n",
        "!PYTHONPATH=src ./train.py --dataset /content/drive/My\\ Drive/writrly_proj_files/Atlantic_essays/science --combine 5000 --run_name 'atlantic_science_concat_345' --model_name '345M'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GA2IH_PaWtoD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## education essays training - with 345\n",
        "!PYTHONPATH=src ./train.py --dataset /content/drive/My\\ Drive/writrly_proj_files/Atlantic_essays/education --combine 500  --run_name 'atlantic_education_concat_345' --model_name '345M'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PV7CC8KOWtbf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## politics essays training - with 345\n",
        "!PYTHONPATH=src ./train.py --dataset /content/drive/My Drive/writrly_proj_files/Atlantic_essays/politics --combine 500 --run_name 'atlantic_politics_concat_345' --model_name '345M'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dy4KClI7WtPQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## entertainment essays training - with 345\n",
        "!PYTHONPATH=src ./train.py --dataset /content/drive/My\\ Drive/writrly_proj_files/Atlantic_essays/entertainment --combine 500 --run_name 'atlantic_entertainment_concat_345' --model_name '345M'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mB-4-DZ1Ws4x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## ideas essays training - with 345\n",
        "!PYTHONPATH=src ./train.py --dataset /content/drive/My\\ Drive/writrly_proj_files/Atlantic_essays/ideas --combine 500 --run_name 'atlantic_ideas_concat_345' --model_name '345M'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHlL8xtQXWah",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## international essays training - with 345\n",
        "!PYTHONPATH=src ./train.py --dataset /content/drive/My Drive/writrly_proj_files/Atlantic_essays/international --combine 500 --run_name 'atlantic_international_concat_345' --model_name '345M'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jg6MPO81XWOf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## health essays training - with 345\n",
        "!PYTHONPATH=src ./train.py --dataset /content/drive/My Drive/writrly_proj_files/Atlantic_essays/health --combine 500 --run_name 'atlantic_health_concat_345' --model_name '345M'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVQC0yMtXWCU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWCUWl8jXV1W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vS1RJJDFOPnb",
        "colab_type": "text"
      },
      "source": [
        "### Save our checkpoints to start training again later"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JretqG1zOXdi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp -r /content/gpt-2/checkpoint/ /content/drive/My\\ Drive/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6D-i7vERWbNS",
        "colab_type": "text"
      },
      "source": [
        "Load your trained model for use in sampling below (117M or 345M)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTyAWMgIvg9R",
        "colab_type": "code",
        "outputId": "75c3d13e-676e-4173-bda2-ac969e48c668",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!cp -r /content/gpt-2/checkpoint/NR_train_personal/* /content/gpt-2/models/117M_NR/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cp: target '/content/gpt-2/models/117M_NR/' is not a directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VeETvWvrbKga",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp -r /content/gpt-2/checkpoint/run1/* /content/gpt-2/models/117M/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "np0r6qfXBeUX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp -r /content/gpt-2/checkpoint/run1/* /content/gpt-2/models/345M/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmnSrXqtfRbq",
        "colab_type": "text"
      },
      "source": [
        "Generate conditional samples from the model given a prompt you provide -  change top-k hyperparameter if desired (default is 40),  if you're using 345M, add \"--model-name 345M\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "utJj-iY4gHwE",
        "colab_type": "code",
        "outputId": "693043d9-a238-413f-ebc1-ac7ba3c992d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1686
        }
      },
      "source": [
        "!python3 src/interactive_conditional_samples.py --top_k 40 --model_name \"117M\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0620 16:35:08.869527 140272897111936 deprecation_wrapper.py:119] From /content/gpt-2/src/model.py:147: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
            "\n",
            "W0620 16:35:09.208445 140272897111936 deprecation_wrapper.py:119] From src/interactive_conditional_samples.py:55: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "2019-06-20 16:35:09.210092: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\n",
            "2019-06-20 16:35:09.252310: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-20 16:35:09.252799: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-06-20 16:35:09.253079: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-06-20 16:35:09.254332: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-06-20 16:35:09.255536: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-06-20 16:35:09.255866: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-06-20 16:35:09.257227: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-06-20 16:35:09.258320: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-06-20 16:35:09.261517: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-06-20 16:35:09.261684: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-20 16:35:09.262161: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-20 16:35:09.262583: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
            "2019-06-20 16:35:09.268017: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-06-20 16:35:09.268250: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x25d3480 executing computations on platform Host. Devices:\n",
            "2019-06-20 16:35:09.268293: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2019-06-20 16:35:09.389097: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-20 16:35:09.389646: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x25d2a00 executing computations on platform CUDA. Devices:\n",
            "2019-06-20 16:35:09.389675: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2019-06-20 16:35:09.389956: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-20 16:35:09.390311: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-06-20 16:35:09.390402: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-06-20 16:35:09.390473: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-06-20 16:35:09.390499: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-06-20 16:35:09.390531: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-06-20 16:35:09.390555: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-06-20 16:35:09.390576: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-06-20 16:35:09.390599: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-06-20 16:35:09.390695: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-20 16:35:09.391096: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-20 16:35:09.391461: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
            "2019-06-20 16:35:09.391521: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-06-20 16:35:09.392507: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-06-20 16:35:09.392532: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n",
            "2019-06-20 16:35:09.392545: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n",
            "2019-06-20 16:35:09.392835: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-20 16:35:09.393229: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-20 16:35:09.393582: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:40] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2019-06-20 16:35:09.393624: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14202 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "W0620 16:35:09.394258 140272897111936 deprecation_wrapper.py:119] From src/interactive_conditional_samples.py:56: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0620 16:35:09.395797 140272897111936 deprecation_wrapper.py:119] From src/interactive_conditional_samples.py:58: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
            "\n",
            "W0620 16:35:09.398805 140272897111936 deprecation_wrapper.py:119] From /content/gpt-2/src/model.py:148: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "W0620 16:35:13.149915 140272897111936 deprecation.py:323] From /content/gpt-2/src/sample.py:65: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "W0620 16:35:13.164368 140272897111936 deprecation.py:323] From /content/gpt-2/src/sample.py:16: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0620 16:35:13.166012 140272897111936 deprecation.py:323] From /content/gpt-2/src/sample.py:70: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.random.categorical` instead.\n",
            "W0620 16:35:13.175179 140272897111936 deprecation_wrapper.py:119] From src/interactive_conditional_samples.py:66: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "W0620 16:35:13.297903 140272897111936 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "Model prompt >>> Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/contextlib.py\", line 99, in __exit__\n",
            "    self.gen.throw(type, value, traceback)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 5652, in get_controller\n",
            "    yield g\n",
            "  File \"src/interactive_conditional_samples.py\", line 71, in interact_model\n",
            "    raw_text = input(\"Model prompt >>> \")\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"src/interactive_conditional_samples.py\", line 89, in <module>\n",
            "    fire.Fire(interact_model)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/fire/core.py\", line 127, in Fire\n",
            "    component_trace = _Fire(component, args, context, name)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/fire/core.py\", line 366, in _Fire\n",
            "    component, remaining_args)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/fire/core.py\", line 542, in _CallCallable\n",
            "    result = fn(*varargs, **kwargs)\n",
            "  File \"src/interactive_conditional_samples.py\", line 86, in interact_model\n",
            "    print(\"=\" * 80)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1619, in __exit__\n",
            "    close_thread.join(30.0)\n",
            "  File \"/usr/lib/python3.6/threading.py\", line 1060, in join\n",
            "    self._wait_for_tstate_lock(timeout=max(timeout, 0))\n",
            "  File \"/usr/lib/python3.6/threading.py\", line 1072, in _wait_for_tstate_lock\n",
            "    elif lock.acquire(block, timeout):\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tDuzlLYqEHJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LeDhY97XMDXn",
        "colab_type": "text"
      },
      "source": [
        "To check flag descriptions, use:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBaj2L_KMAgb",
        "colab_type": "code",
        "outputId": "72a53834-322a-4249-8923-4c54266d423c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        }
      },
      "source": [
        "!python3 src/interactive_conditional_samples.py -- --help"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0620 16:30:15.744753 139747760654208 deprecation_wrapper.py:119] From /content/gpt-2/src/model.py:147: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
            "\n",
            "Type:        function\n",
            "String form: <function interact_model at 0x7f198d753d08>\n",
            "File:        /content/gpt-2/src/interactive_conditional_samples.py\n",
            "Line:        11\n",
            "Docstring:   Interactively run the model\n",
            ":model_name=117M : String, which model to use\n",
            ":seed=None : Integer seed for random number generators, fix seed to reproduce\n",
            " results\n",
            ":nsamples=1 : Number of samples to return total\n",
            ":batch_size=1 : Number of batches (only affects speed/memory).  Must divide nsamples.\n",
            ":length=None : Number of tokens in generated text, if None (default), is\n",
            " determined by model hyperparameters\n",
            ":temperature=1 : Float value controlling randomness in boltzmann\n",
            " distribution. Lower temperature results in less random completions. As the\n",
            " temperature approaches zero, the model will become deterministic and\n",
            " repetitive. Higher temperature results in more random completions.\n",
            ":top_k=0 : Integer value controlling diversity. 1 means only 1 word is\n",
            " considered for each step (token), resulting in deterministic completions,\n",
            " while 40 means 40 words are considered at each step. 0 (default) is a\n",
            " special setting meaning no restrictions. 40 generally is a good value.\n",
            ":top_p=0.0 : Float value controlling diversity. Implements nucleus sampling,\n",
            " overriding top_k if set to a value > 0. A good setting is 0.9.\n",
            "\n",
            "Usage:       interactive_conditional_samples.py [MODEL_NAME] [SEED] [NSAMPLES] [BATCH_SIZE] [LENGTH] [TEMPERATURE] [TOP_K] [TOP_P]\n",
            "             interactive_conditional_samples.py [--model-name MODEL_NAME] [--seed SEED] [--nsamples NSAMPLES] [--batch-size BATCH_SIZE] [--length LENGTH] [--temperature TEMPERATURE] [--top-k TOP_K] [--top-p TOP_P]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8rSqkGxg5OK",
        "colab_type": "text"
      },
      "source": [
        "Generate unconditional samples from the model,  if you're using 345M, add \"--model-name 345M\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LaQUEnRxWc3c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python3 src/generate_unconditional_samples.py --model_name \"345M\" | tee /tmp/samples"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VM1Hag-JL3Bt",
        "colab_type": "text"
      },
      "source": [
        "To check flag descriptions, use:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sdxfye-SL66I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python3 src/generate_unconditional_samples.py -- --help"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
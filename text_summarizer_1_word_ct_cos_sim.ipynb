{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text_summarizer_1_word_ct_cos_sim.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.2"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mowillia/phantom_pen/blob/master/text_summarizer_1_word_ct_cos_sim.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_SNKAHLSh_P",
        "colab_type": "text"
      },
      "source": [
        "### Text Summarizer (#1) - Word Count and Cosine Similarity\n",
        "**(June 17, 2019)**\n",
        "\n",
        "Extractive Text Summarizer described in https://towardsdatascience.com/understand-text-summarization-and-create-your-own-summarizer-in-python-b26a9f09fc70"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Idb2ujXGSh_R",
        "colab_type": "code",
        "outputId": "3514efb0-8f82-4a6c-daed-a411a05a885f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "#!/usr/bin/env python\n",
        "# coding: utf-8\n",
        "\n",
        "import nltk\n",
        "import textwrap\n",
        "\n",
        "import nltk.data # natural language tool kit\n",
        "\n",
        "# for tokenizing sentences according by the words\n",
        "from nltk.tokenize import WhitespaceTokenizer\n",
        "\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize # $ pip install nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.cluster.util import cosine_distance\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        " "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_G3eTCMXK69T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filename  = '/content/sample_essay.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WA5wQEJaSh_Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Function that outputs paragraphs from text file\n",
        "def text_to_para(filename):\n",
        "    \n",
        "    para_list = open(filename).read().splitlines()\n",
        "    \n",
        "    para_list[:] = (value for value in para_list if value != '')\n",
        "    \n",
        "    return para_list\n",
        "\n",
        "## function that outputs the sentences in a paragraph\n",
        "def sents(para): \n",
        "    \n",
        "    return sent_tokenize(para)\n",
        "\n",
        "### function takes in a file and outputs a sentence length trajectory\n",
        "\n",
        "## vector of sentences in a piece \n",
        "def raw_sents(filename):\n",
        "    \n",
        "    sent = []\n",
        "    \n",
        "    paragraphs = text_to_para(filename)[:]\n",
        "    \n",
        "    for paragraph in paragraphs:\n",
        "        sent += sents(paragraph)\n",
        "        \n",
        "    return sent"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzUnpgi7Sh_c",
        "colab_type": "code",
        "outputId": "a6d569db-9e64-4951-dc3a-c410bbe318cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "raw_sents(filename)[55]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\"If you listen to Yunior on where you should put your eye on the text, you will miss the whole book.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnm71MVMSh_g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sentence_similarity(sent1, sent2, stopwords=None):\n",
        "    if stopwords is None:\n",
        "        stopwords = []\n",
        " \n",
        "    sent1 = [w.lower() for w in sent1]\n",
        "    sent2 = [w.lower() for w in sent2]\n",
        " \n",
        "    all_words = list(set(sent1 + sent2))\n",
        " \n",
        "    vector1 = [0] * len(all_words)\n",
        "    vector2 = [0] * len(all_words)\n",
        " \n",
        "    # build the vector for the first sentence\n",
        "    for w in sent1:\n",
        "        if w in stopwords:\n",
        "            continue\n",
        "        vector1[all_words.index(w)] += 1\n",
        " \n",
        "    # build the vector for the second sentence\n",
        "    for w in sent2:\n",
        "        if w in stopwords:\n",
        "            continue\n",
        "        vector2[all_words.index(w)] += 1\n",
        " \n",
        "    return 1 - cosine_distance(vector1, vector2)\n",
        " \n",
        "def build_similarity_matrix(sentences, stop_words):\n",
        "    # Create an empty similarity matrix\n",
        "    similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
        " \n",
        "    for idx1 in range(len(sentences)):\n",
        "        for idx2 in range(len(sentences)):\n",
        "            if idx1 == idx2: #ignore if both are same sentences\n",
        "                continue \n",
        "            similarity_matrix[idx1][idx2] = sentence_similarity(sentences[idx1], sentences[idx2], stop_words)\n",
        "\n",
        "    return similarity_matrix\n",
        "\n",
        "\n",
        "def generate_summary(file_name, top_n=5):\n",
        "    stop_words = stopwords.words('english')\n",
        "    summarize_text = []\n",
        "\n",
        "    # Step 1 - Read text anc split it\n",
        "    sentences =  raw_sents(file_name)\n",
        "\n",
        "    # Step 2 - Generate Similary Martix across sentences\n",
        "    sentence_similarity_martix = build_similarity_matrix(sentences, stop_words)\n",
        "\n",
        "    # Step 3 - Rank sentences in similarity martix\n",
        "    sentence_similarity_graph = nx.from_numpy_matrix(sentence_similarity_martix)\n",
        "    scores = nx.pagerank(sentence_similarity_graph)\n",
        "\n",
        "    # Step 4 - Sort the rank and pick top sentences\n",
        "    ranked_sentence = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)    \n",
        "    #print(\"Indexes of top ranked_sentence order are \", ranked_sentence)    \n",
        "\n",
        "    for i in range(top_n):\n",
        "        summarize_text.append(\"\".join(ranked_sentence[i][1]))\n",
        "\n",
        "    # Step 5 - Offcourse, output the summarize texr\n",
        "    print(\"Summarize Text: \\n\",textwrap.fill(\" \".join(summarize_text), 50))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNABSs6wSh_j",
        "colab_type": "code",
        "outputId": "0494eb42-9178-4b29-9803-66d3177d5c77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        }
      },
      "source": [
        "generate_summary(filename, 5)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Summarize Text: \n",
            " And although my appreciation of her writing\n",
            "tempered as I grew older, unlike much of the\n",
            "culture which now categorically vilifies Rand, I\n",
            "still saw a considerable potency and relevance in\n",
            "what she had written. Most learned truths about\n",
            "the world are confused and complicated, bearing\n",
            "Bohr’s hallmark of a deep truth in which even\n",
            "their seemingly antithetical statements are also\n",
            "somehow true of the world. In explaining his work,\n",
            "Díaz said the real story of A Brief and Wondrous\n",
            "Life of Oscar Wao could not be gleaned by\n",
            "following precisely where Yunior led or to where\n",
            "the intellectual insecurity Yunior deliberately\n",
            "creates in the reader may push you [2]: And so\n",
            "even after I grew out (or, thought I grew out) of\n",
            "the self-absorption which initially attached me to\n",
            "her fiction, I still remained intrigued by the\n",
            "effect it had on people. It rather has more to do\n",
            "with how people understand themselves and the need\n",
            "for that understanding to be expressed, if not\n",
            "necessarily according to what one is, most\n",
            "certainly according to what one is not.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wqrgSyczIyh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
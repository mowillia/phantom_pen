{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gpt2_training_clean.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "UJit5UMJYIBA"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mowillia/phantom_pen/blob/master/gpt2_training_clean.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pzxl1vYX-1kk",
        "colab_type": "text"
      },
      "source": [
        "### GPT 2 Training -- Google Colab\n",
        "\n",
        "\n",
        "This notebook contains code used to train the models for phantom pen.\n",
        "\n",
        "[Modified from [ak9250's guide](https://github.com/ak9250/gpt-2-colab/blob/master/GPT_2.ipynb) to training GPT-2 on Nshepperd's gpt-2 fork]\n",
        "\n",
        "\n",
        "**General Note:** This notebook will not automatically run on your (the reader's) computer. Instead, use it as a guide for writing a similar notebook that links to your appropriate text corpora directory and google drive.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGVz-6MvQmoS",
        "colab_type": "text"
      },
      "source": [
        "#### Preaparing for Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iW0abT07ZkhZ",
        "colab_type": "text"
      },
      "source": [
        "1. Ensure that GPU is enabled in Colab. Go to Edit->Notebook Settings-> Hardware Accelerator -> GPU\n",
        "\n",
        "\n",
        "2. Since Colab resets after 12 hours, copy the current notebook to your Google Drive. File -> Save a copy in drive.\n",
        "\n",
        "**Important:** The model saves its training parameters in \"checkpoints\". Due to the 12 hour reset time, you should make sure to save your model checkpoints before the 12 hour mark and, most importantly, copy those checkpoints to your personal drive. After Colab resets, you can copy the checkpoints back into Colab and start training again from the previous checkpoint. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLXW02eIYpcB",
        "colab_type": "text"
      },
      "source": [
        "3. Clone and cd into the repository, mowillia's fork https://github.com/mowillia/gpt-2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICYu3w9hIJkC",
        "colab_type": "code",
        "outputId": "d1b538ff-6abb-499f-99f6-bb6aafff56f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "!git clone https://github.com/mowillia/gpt-2.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'gpt-2'...\n",
            "remote: Enumerating objects: 297, done.\u001b[K\n",
            "remote: Total 297 (delta 0), reused 0 (delta 0), pack-reused 297\u001b[K\n",
            "Receiving objects: 100% (297/297), 4.40 MiB | 14.71 MiB/s, done.\n",
            "Resolving deltas: 100% (162/162), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4alKLBnoKBcE",
        "colab_type": "text"
      },
      "source": [
        "4. Change directory to the gpt-2 folder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6eEIs3ApZUVO",
        "colab_type": "code",
        "outputId": "bfc77f71-9452-40d7-c86a-a80713de1fc8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd gpt-2"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gpt-2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLJZmeQBKFIh",
        "colab_type": "text"
      },
      "source": [
        "5. Check the GPU status"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMzufBi55Xfb",
        "colab_type": "code",
        "outputId": "3255379d-5f9a-4f2d-eb6e-706a94b1e90c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "#check GPU status\n",
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Jun 27 15:42:44 2019       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 418.67       Driver Version: 410.79       CUDA Version: 10.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   46C    P8    16W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYOyKVqRKISi",
        "colab_type": "text"
      },
      "source": [
        "6. Second check to ensure GPU is being used "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLclw6Y95bG4",
        "colab_type": "code",
        "outputId": "3869d58e-79b2-4f12-f1e4-06c3945c7f03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import torch\n",
        "# Checking if GPU is available\n",
        "work_with_gpu = torch.cuda.is_available()\n",
        "if(work_with_gpu):\n",
        "    print('Using GPU!')\n",
        "else: \n",
        "    print('No GPU available, using CPU; Consider using short texts.')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using GPU!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qtn1qZPgZLb0",
        "colab_type": "text"
      },
      "source": [
        "7. Install the requirements for training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "434oOx0bZH6J",
        "colab_type": "code",
        "outputId": "2b1e53d6-0507-4d59-c593-31c64d60e9f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582
        }
      },
      "source": [
        "!pip3 install -r requirements.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fire>=0.1.3 (from -r requirements.txt (line 1))\n",
            "  Downloading https://files.pythonhosted.org/packages/5a/b7/205702f348aab198baecd1d8344a90748cb68f53bdcd1cc30cbc08e47d3e/fire-0.1.3.tar.gz\n",
            "Collecting regex==2017.4.5 (from -r requirements.txt (line 2))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/62/c0c0d762ffd4ffaf39f372eb8561b8d491a11ace5a7884610424a8b40f95/regex-2017.04.05.tar.gz (601kB)\n",
            "\u001b[K     |████████████████████████████████| 604kB 9.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests==2.21.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 3)) (2.21.0)\n",
            "Collecting tqdm==4.31.1 (from -r requirements.txt (line 4))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6c/4b/c38b5144cf167c4f52288517436ccafefe9dc01b8d1c190e18a6b154cd4a/tqdm-4.31.1-py2.py3-none-any.whl (48kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 23.5MB/s \n",
            "\u001b[?25hCollecting toposort==1.5 (from -r requirements.txt (line 5))\n",
            "  Downloading https://files.pythonhosted.org/packages/e9/8a/321cd8ea5f4a22a06e3ba30ef31ec33bea11a3443eeb1d89807640ee6ed4/toposort-1.5-py2.py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from fire>=0.1.3->-r requirements.txt (line 1)) (1.12.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (2019.6.16)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (3.0.4)\n",
            "Building wheels for collected packages: fire, regex\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/2a/1a/4d/6b30377c3051e76559d1185c1dbbfff15aed31f87acdd14c22\n",
            "  Building wheel for regex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/75/07/38/3c16b529d50cb4e0cd3dbc7b75cece8a09c132692c74450b01\n",
            "Successfully built fire regex\n",
            "Installing collected packages: fire, regex, tqdm, toposort\n",
            "  Found existing installation: tqdm 4.28.1\n",
            "    Uninstalling tqdm-4.28.1:\n",
            "      Successfully uninstalled tqdm-4.28.1\n",
            "Successfully installed fire-0.1.3 regex-2017.4.5 toposort-1.5 tqdm-4.31.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tqdm"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvUQhgK3PQ4L",
        "colab_type": "text"
      },
      "source": [
        "8. Mount drive to access google drive for saving and accessing checkpoints later. Have to log in to your google account"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNpf6R4ahYSN",
        "colab_type": "code",
        "outputId": "69649b5f-76df-4c64-e053-a19063ae62c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1hrgeKFYsuE",
        "colab_type": "text"
      },
      "source": [
        "9. Download the model data. You have two choices the 117M model and the 345M model. The program phantom pen uses both models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "A498TySgHYyF",
        "outputId": "35e2abe5-98d2-43f8-9b79-ddeb9a4b3bae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "!python3 download_model.py 117M"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\rFetching checkpoint:   0%|                                              | 0.00/77.0 [00:00<?, ?it/s]\rFetching checkpoint: 1.00kit [00:00, 566kit/s]                                                      \n",
            "Fetching encoder.json: 1.04Mit [00:00, 54.7Mit/s]                                                   \n",
            "Fetching hparams.json: 1.00kit [00:00, 923kit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 498Mit [00:09, 52.1Mit/s]                                  \n",
            "Fetching model.ckpt.index: 6.00kit [00:00, 5.53Mit/s]                                               \n",
            "Fetching model.ckpt.meta: 472kit [00:00, 53.5Mit/s]                                                 \n",
            "Fetching vocab.bpe: 457kit [00:00, 52.4Mit/s]                                                       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UDpEGjfO8Q2",
        "colab_type": "code",
        "outputId": "33faac78-57ce-4317-95f5-47371c112ae2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "#!python3 download_model.py 345M"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\rFetching checkpoint:   0%|                                              | 0.00/77.0 [00:00<?, ?it/s]\rFetching checkpoint: 1.00kit [00:00, 814kit/s]                                                      \n",
            "\rFetching encoder.json:   0%|                                           | 0.00/1.04M [00:00<?, ?it/s]\rFetching encoder.json: 1.04Mit [00:00, 54.1Mit/s]                                                   \n",
            "Fetching hparams.json: 1.00kit [00:00, 1.02Mit/s]                                                   \n",
            "Fetching model.ckpt.data-00000-of-00001: 1.42Git [00:21, 65.4Mit/s]                                 \n",
            "Fetching model.ckpt.index: 11.0kit [00:00, 7.13Mit/s]                                               \n",
            "Fetching model.ckpt.meta: 927kit [00:00, 52.4Mit/s]                                                 \n",
            "Fetching vocab.bpe: 457kit [00:00, 46.1Mit/s]                                                       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zq-YwRnNOBYO",
        "colab_type": "text"
      },
      "source": [
        "10. Export particular Python encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7oJPQtdLbbeK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!export PYTHONIOENCODING=UTF-8"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KzSbAvePgsI",
        "colab_type": "text"
      },
      "source": [
        "11. Fetch checkpoints if you have them saved in google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQ_-Css-r5fI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fetch the checkpoints \n",
        "!cp -r /content/drive/My\\ Drive/checkpoint/ /content/gpt-2/ "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRo7ZSZCPliY",
        "colab_type": "text"
      },
      "source": [
        "12. Copy the corpora you will use from training from your google drive to the content folder of colab. [Below is the code used to copy from my own directory. Yours would be different.]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_P3KCCx91uG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get full essays\n",
        "!cp -r /content/drive/My\\ Drive/writrly_proj_files/Full_Essays/* /content/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0p--9zwqQRTc",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Let's get our train on! In this case the file is A Tale of Two Cities (Charles Dickens) from Project Gutenberg. To change the dataset GPT-2 models will fine-tune on, change this URL to another .txt file, and change corresponding part of the next cell. Note that you can use small datasets if you want but you will have to be sure not to run the fine-tuning for too long or you will overfit badly. Roughly, expect interesting results within minutes to hours in the 1-10s of megabyte ballpark, and below this you may want to stop the run early as fine-tuning can be very fast."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwJqVDNPMF-E",
        "colab_type": "text"
      },
      "source": [
        "### Training the 117M Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79Ta79LrQsSh",
        "colab_type": "text"
      },
      "source": [
        "Now we will train the model on various corpora. Phantom Pen uses 11 different corpora and we train a 117M model on each one. We title the `run_name` according to the chosen corpora, we have set the model_name to be `117M` so the program knows which pretrained version of GPT-2 to use, and we end the training after 1000 steps.\n",
        "\n",
        "**Training Tip:** We are fine-tuning the pretrained model parameters. Using small data sets (~2 MB in size) such as the ones used in Phantom Pen (excluding the gutenberg corpus which is ~20MB) is allowed but it is important not to let the training run too long or the program will overfit to the training text. I have found that the choice of 1000 steps is good for producing reasonable results. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Z5zfRrkSNBT7",
        "colab": {}
      },
      "source": [
        "## business essays training - with 117M\n",
        "!PYTHONPATH=src ./train.py --dataset /content/all_business.txt --run_name 'atlantic_business' --model_name '117M' --counter_end 1000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CQVtUs0FNBUB",
        "colab": {}
      },
      "source": [
        "## technology essays training  - with 117M\n",
        "!PYTHONPATH=src ./train.py --dataset /content/all_technology.txt --run_name 'atlantic_technology' --model_name '117M' --counter_end 1000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XxZInCQ_NBUF",
        "colab": {}
      },
      "source": [
        "## science essays training  - with 117M\n",
        "!PYTHONPATH=src ./train.py --dataset /content/all_science.txt --run_name 'atlantic_science' --model_name '117M' --counter_end 1000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "O-DYIzDONBUK",
        "colab": {}
      },
      "source": [
        "## education essays training - with 117M\n",
        "!PYTHONPATH=src ./train.py --dataset /content/all_education.txt --run_name 'atlantic_education' --model_name '117M' --counter_end 1000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dQVI4Np2NBUQ",
        "colab": {}
      },
      "source": [
        "## politics essays training - with 117M\n",
        "!PYTHONPATH=src ./train.py --dataset /content/all_politics.txt --run_name 'atlantic_politics' --model_name '117M' --counter_end 1000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WQgbQObNNBUS",
        "colab": {}
      },
      "source": [
        "## entertainment essays training - with 117M\n",
        "!PYTHONPATH=src ./train.py --dataset /content/all_entertainment.txt --run_name 'atlantic_entertainment' --model_name '117M' --counter_end 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cI0hH-MVNBUX",
        "colab": {}
      },
      "source": [
        "## ideas essays training - with 117M\n",
        "!PYTHONPATH=src ./train.py --dataset /content/all_ideas.txt --run_name 'atlantic_ideas' --model_name '117M' --counter_end 1000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TuqO8VEHNBUa",
        "colab": {}
      },
      "source": [
        "## international essays training - with 117M\n",
        "!PYTHONPATH=src ./train.py --dataset /content/all_international.txt --run_name 'atlantic_international' --model_name '117M' --counter_end 1000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xEGyqbX6NBUe",
        "colab": {}
      },
      "source": [
        "## health essays training - with 117M\n",
        "!PYTHONPATH=src ./train.py --dataset /content/all_health.txt --run_name 'atlantic_health' --model_name '117M' --counter_end 1000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "U8qgaQSwNBUk",
        "colab": {}
      },
      "source": [
        "## gutenberg training - with 117M\n",
        "!PYTHONPATH=src ./train.py --dataset /content/all_gutenberg.txt --run_name 'gutenberg' --model_name '117M' --counter_end 1000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WUEmD09LNBUq",
        "colab": {}
      },
      "source": [
        "## short story training - with 117M\n",
        "!PYTHONPATH=src ./train.py --dataset /content/all_short_stories.txt --run_name 'all_short_stories' --model_name '117M' --counter_end 1000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDFQZfANYm_T",
        "colab_type": "text"
      },
      "source": [
        "### Training the 345M Model\n",
        "\n",
        "[Repeat previous training with 345M model]\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEn_ihcGI00T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## business essays training - with 345M\n",
        "!PYTHONPATH=src ./train.py --dataset /content/all_business.txt --run_name 'atlantic_business_345' --model_name '345M' --counter_end 1000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axNGwoKZ8UGF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## technology essays training  - with 345M\n",
        "!PYTHONPATH=src ./train.py --dataset /content/all_technology.txt --run_name 'atlantic_technology_345' --model_name '345M' --counter_end 1000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUzko5uO8UQT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## science essays training  - with 345M\n",
        "!PYTHONPATH=src ./train.py --dataset /content/all_science.txt --run_name 'atlantic_science_345' --model_name '345M' --counter_end 1000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpK_8Ln-8UVa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## education essays training - with 345M\n",
        "!PYTHONPATH=src ./train.py --dataset /content/all_education.txt --run_name 'atlantic_education_345' --model_name '345M' --counter_end 1000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WoTB9Vwl8Uk4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## politics essays training - with 345M\n",
        "!PYTHONPATH=src ./train.py --dataset /content/all_politics.txt --run_name 'atlantic_politics_345' --model_name '345M' --counter_end 1000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRUjEIu68Url",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## entertainment essays training - with 345M\n",
        "!PYTHONPATH=src ./train.py --dataset /content/all_entertainment.txt --run_name 'atlantic_entertainment_345' --model_name '345M' --counter_end 1000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXCgZs5p8Uo5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## ideas essays training - with 345M\n",
        "!PYTHONPATH=src ./train.py --dataset /content/all_ideas.txt --run_name 'atlantic_ideas_345' --model_name '345M' --counter_end 1000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHsAW2op8Uh9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## international essays training - with 345M\n",
        "!PYTHONPATH=src ./train.py --dataset /content/all_international.txt --run_name 'atlantic_international_345' --model_name '345M' --counter_end 1000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uly598Th8UfE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## health essays training - with 345M\n",
        "!PYTHONPATH=src ./train.py --dataset /content/all_health.txt --run_name 'atlantic_health_345' --model_name '345M' --counter_end 1000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDv9jHF88UNz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## gutenberg training - with 345M\n",
        "!PYTHONPATH=src ./train.py --dataset /content/all_gutenberg.txt --run_name 'gutenberg_345' --model_name '345M' --counter_end 1000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDxmeWIewzvH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## short story training - with 345M\n",
        "!PYTHONPATH=src ./train.py --dataset /content/all_short_stories.txt --run_name 'all_short_stories_345' --model_name '345M' --counter_end 1000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vS1RJJDFOPnb",
        "colab_type": "text"
      },
      "source": [
        "### Saving and Loading Checkpoints\n",
        "\n",
        "After training the model, we need to save them to our google drive, after which we can load them for additional training or for sample generation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JretqG1zOXdi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## saves checpoints\n",
        "## Note: Saving takes a long time (at least an hour) for the 345M model\n",
        "!cp -r /content/gpt-2/checkpoint/ /content/drive/My\\ Drive/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6D-i7vERWbNS",
        "colab_type": "text"
      },
      "source": [
        "Load one of the trained models from above for sampling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTyAWMgIvg9R",
        "colab_type": "code",
        "outputId": "75c3d13e-676e-4173-bda2-ac969e48c668",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "## copies checkpoint to model folder so we can use the model for generation\n",
        "## We are using atlantic_business as an example\n",
        "!cp -r /content/gpt-2/checkpoint/atlantic_business/* /content/gpt-2/models/atlantic_business/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cp: target '/content/gpt-2/models/117M_NR/' is not a directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmnSrXqtfRbq",
        "colab_type": "text"
      },
      "source": [
        "Generate conditional samples from the model given a prompt you provide -  change top-k hyperparameter if desired (default is 40)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Zd8adGWJOaJv",
        "colab": {}
      },
      "source": [
        "!python3 src/interactive_conditional_samples.py --top_k 40 --model_name \"atlantic_business\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LeDhY97XMDXn",
        "colab_type": "text"
      },
      "source": [
        "To check flag descriptions, use:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBaj2L_KMAgb",
        "colab_type": "code",
        "outputId": "72a53834-322a-4249-8923-4c54266d423c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        }
      },
      "source": [
        "!python3 src/interactive_conditional_samples.py -- --help"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0620 16:30:15.744753 139747760654208 deprecation_wrapper.py:119] From /content/gpt-2/src/model.py:147: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
            "\n",
            "Type:        function\n",
            "String form: <function interact_model at 0x7f198d753d08>\n",
            "File:        /content/gpt-2/src/interactive_conditional_samples.py\n",
            "Line:        11\n",
            "Docstring:   Interactively run the model\n",
            ":model_name=117M : String, which model to use\n",
            ":seed=None : Integer seed for random number generators, fix seed to reproduce\n",
            " results\n",
            ":nsamples=1 : Number of samples to return total\n",
            ":batch_size=1 : Number of batches (only affects speed/memory).  Must divide nsamples.\n",
            ":length=None : Number of tokens in generated text, if None (default), is\n",
            " determined by model hyperparameters\n",
            ":temperature=1 : Float value controlling randomness in boltzmann\n",
            " distribution. Lower temperature results in less random completions. As the\n",
            " temperature approaches zero, the model will become deterministic and\n",
            " repetitive. Higher temperature results in more random completions.\n",
            ":top_k=0 : Integer value controlling diversity. 1 means only 1 word is\n",
            " considered for each step (token), resulting in deterministic completions,\n",
            " while 40 means 40 words are considered at each step. 0 (default) is a\n",
            " special setting meaning no restrictions. 40 generally is a good value.\n",
            ":top_p=0.0 : Float value controlling diversity. Implements nucleus sampling,\n",
            " overriding top_k if set to a value > 0. A good setting is 0.9.\n",
            "\n",
            "Usage:       interactive_conditional_samples.py [MODEL_NAME] [SEED] [NSAMPLES] [BATCH_SIZE] [LENGTH] [TEMPERATURE] [TOP_K] [TOP_P]\n",
            "             interactive_conditional_samples.py [--model-name MODEL_NAME] [--seed SEED] [--nsamples NSAMPLES] [--batch-size BATCH_SIZE] [--length LENGTH] [--temperature TEMPERATURE] [--top-k TOP_K] [--top-p TOP_P]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8rSqkGxg5OK",
        "colab_type": "text"
      },
      "source": [
        "Generate unconditional samples from the model,  if you're using 345M, add \"--model-name 345M\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LaQUEnRxWc3c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python3 src/generate_unconditional_samples.py --model_name \"345M\" | tee /tmp/samples"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VM1Hag-JL3Bt",
        "colab_type": "text"
      },
      "source": [
        "To check flag descriptions, use:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sdxfye-SL66I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python3 src/generate_unconditional_samples.py -- --help"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
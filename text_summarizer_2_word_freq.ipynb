{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text_summarizer_2_word_freq.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.2"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mowillia/phantom_pen/blob/master/text_summarizer_2_word_freq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xU-yhA_lJdRH",
        "colab_type": "text"
      },
      "source": [
        "### Text Summarizer (#2) - Word Frequencies\n",
        "**(June 17, 2019)**\n",
        "\n",
        "Extractive Text Summarizer described in https://stackabuse.com/text-summarization-with-nltk-in-python/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQMDQqhCJdRe",
        "colab_type": "code",
        "outputId": "4fe640ad-fab3-40cf-bd2e-105a6d8b4452",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "#!/usr/bin/env python\n",
        "# coding: utf-8\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import textwrap\n",
        "\n",
        "# used in the count of words\n",
        "import string\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "import nltk.data # natural language tool kit\n",
        "\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize, WhitespaceTokenizer # $ pip install nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.cluster.util import cosine_distance\n",
        "import networkx as nx"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RiD1BiDgJdSK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Function that outputs paragraphs from text file\n",
        "def text_to_para(filename):\n",
        "    \n",
        "    para_list = open(filename).read().splitlines()\n",
        "    \n",
        "    para_list[:] = (value for value in para_list if value != '')\n",
        "    \n",
        "    return para_list\n",
        "\n",
        "## function that outputs the sentences in a paragraph\n",
        "def sents(para): \n",
        "    \n",
        "    return sent_tokenize(para)\n",
        "\n",
        "### function takes in a file and outputs a sentence length trajectory\n",
        "\n",
        "## vector of sentences in a piece \n",
        "def raw_sents(filename):\n",
        "    \n",
        "    sent = []\n",
        "    \n",
        "    paragraphs = text_to_para(filename)\n",
        "    \n",
        "    for paragraph in paragraphs:\n",
        "        sent += sents(paragraph)\n",
        "        \n",
        "    return sent\n",
        "\n",
        "# Takes in a sentence string and outputs the words in the sentence as a list\n",
        "def words_sent(sentence): \n",
        "    \n",
        "    # selects out words in sentence and takes the punctuation as well\n",
        "    word_list_punct = WhitespaceTokenizer().tokenize(sentence)\n",
        "    \n",
        "    # removed the punctuation in word list\n",
        "    word_list = [elem.translate(str.maketrans('', '', string.punctuation)) \n",
        "                 for elem in word_list_punct]  \n",
        "    return word_list\n",
        "\n",
        "\n",
        "## vector of words in a piece\n",
        "def raw_words(filename):\n",
        "    \n",
        "    word = []\n",
        "    \n",
        "    paragraphs = text_to_para(filename)\n",
        "    \n",
        "    for paragraph in paragraphs:\n",
        "        for sent in sents(paragraph):\n",
        "            word += words_sent(sent)\n",
        "        \n",
        "    return word\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3Tc0dxhKgcR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define filename; from google drive\n",
        "filename = '/content/sample_essay.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxiV_sYlJdSi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create list of sentences\n",
        "sentence_list = raw_sents(filename)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "boz2d7erJdTL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# list of stop \"words\" in english\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "# computes the number of occurence of each word\n",
        "\n",
        "# create empty dictionary where words and number of occurrence are stored\n",
        "word_frequencies = {}  \n",
        "\n",
        "# loop through words in the text\n",
        "for word in raw_words(filename):  \n",
        "    \n",
        "    # if word is not in stop words, \n",
        "    # word becomes key for dictionary and its value is incremented by 1    \n",
        "    if word not in stopwords:\n",
        "        if word not in word_frequencies.keys():\n",
        "            word_frequencies[word] = 1\n",
        "        else:\n",
        "            word_frequencies[word] += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxt2ZNasJdVD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# computes the frequency of occurence of each word \n",
        "# by normalizing by the the maximum number of occurences\n",
        "\n",
        "maximum_frequncy = max(word_frequencies.values())\n",
        "\n",
        "# loop through words in dictionary\n",
        "for word in word_frequencies.keys():  \n",
        "    \n",
        "    # normalize by max frequency\n",
        "    word_frequencies[word] = (word_frequencies[word]/maximum_frequncy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKIwPQ26JdVH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# computes the score for a sentence by adding the \n",
        "# frequencies for each word in the sentence\n",
        "\n",
        "#  create empty dictionary where keys are sentences and values are scores\n",
        "sentence_scores = {}  \n",
        "\n",
        "# loop through sentences\n",
        "for sent in sentence_list:  \n",
        "    \n",
        "    #tokenize sentences into words \n",
        "    for word in nltk.word_tokenize(sent.lower()):\n",
        "        \n",
        "        # word occurs in the frequency key dictionary\n",
        "        if word in word_frequencies.keys():\n",
        "            \n",
        "            # only consider sentences with less than 30 words\n",
        "            if len(sent.split(' ')) < 30:\n",
        "                \n",
        "                # if sentence doesn't exist we add it to the dictionary as a value \n",
        "                # and add the frequency-score of the first word as a value\n",
        "                if sent not in sentence_scores.keys():\n",
        "                    sentence_scores[sent] = word_frequencies[word]\n",
        "                    \n",
        "                # if sentence already exists we add to its score the frequency-score\n",
        "                # of the next word\n",
        "                else:\n",
        "                    sentence_scores[sent] += word_frequencies[word]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIfN5UIpJdVb",
        "colab_type": "code",
        "outputId": "7b914cb4-c9c7-4b09-f54a-846f2807bdbf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        }
      },
      "source": [
        "#\n",
        "import heapq  \n",
        "\n",
        "summary_sentences = heapq.nlargest(7, sentence_scores, key=sentence_scores.get)\n",
        "\n",
        "summary = ' '.join(summary_sentences)  \n",
        "print(textwrap.fill(summary, 50))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "How could so many identify with her work even\n",
            "amidst the clear space between Rand’s truth and\n",
            "the lived truth of people’s lives? Frustration,\n",
            "confusion, and hopelessness are the negative end\n",
            "results when one applies narratives outside of the\n",
            "simple regimes of applicability in which these\n",
            "stories were first created. In spite of the name,\n",
            "this framing has nothing superficially — and\n",
            "perhaps everything deeply — to do with the history\n",
            "of racism in this country. What unites autocrats\n",
            "and novelists is the simultaneous — and thus\n",
            "dangerous — fungibility and potency of their\n",
            "narratives. And what they take from the\n",
            "audience — validation of themselves — is what the\n",
            "audience believes, inaccurately, “Los dictadores”\n",
            "are providing to them. I imagine that many people\n",
            "see Díaz’s writing as an outcropping of the\n",
            "current ethnic and cultural zeitgeist that grips\n",
            "this country. But what audiences are really\n",
            "receiving is a constructed reality which fails\n",
            "constantly to mesh sensibly with an outside world.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SoW_ow4KJdW_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text_summarizer_3_word_embed.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.2"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mowillia/phantom_pen/blob/master/text_summarizer_3_word_embed.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MaxoiIQ6JhaA",
        "colab_type": "text"
      },
      "source": [
        "### Text Summarizer (#3)- Word Embeddings\n",
        "**(June 17, 2019)**\n",
        "\n",
        "Extractive Text Summarizer described in https://www.analyticsvidhya.com/blog/2018/11/introduction-text-summarization-textrank-python/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPwP-4ObJhaB",
        "colab_type": "code",
        "outputId": "5a5f9fc8-8370-4e40-e369-d3879f66a041",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import textwrap\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "import nltk\n",
        "import nltk.data # natural language tool kit\n",
        "from nltk.tokenize import sent_tokenize # $ pip install nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "import networkx as nx"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /Users/Williams/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82sGA6nZJhaH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Function that outputs paragraphs from text file\n",
        "def text_to_para(filename):\n",
        "    \n",
        "    para_list = open(filename).read().splitlines()\n",
        "    \n",
        "    para_list[:] = (value for value in para_list if value != '')\n",
        "    \n",
        "    return para_list\n",
        "\n",
        "## function that outputs the sentences in a paragraph\n",
        "def sents(para): \n",
        "    \n",
        "    return sent_tokenize(para)\n",
        "\n",
        "### function takes in a file and outputs a sentence length trajectory\n",
        "\n",
        "## vector of sentences in a piece \n",
        "def raw_sents(filename):\n",
        "    \n",
        "    sent = []\n",
        "    \n",
        "    paragraphs = text_to_para(filename)[:]\n",
        "    \n",
        "    for paragraph in paragraphs:\n",
        "        sent += sents(paragraph)\n",
        "        \n",
        "    return sent"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9h3L1mS9JhaL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filename  = '/content/sample_essay.txt'\n",
        "sentences = raw_sents(filename);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wk1I0677JhaO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Extract word vectors\n",
        "word_embeddings = {}\n",
        "f = open('/content/glove.6B.100d.txt', encoding='utf-8')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    word_embeddings[word] = coefs\n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEukXHiGJhaS",
        "colab_type": "code",
        "outputId": "6f139d96-65e8-4999-fb81-947e61f5ba98",
        "colab": {}
      },
      "source": [
        "len(word_embeddings)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "400000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwO7i46pJhaY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# remove punctuations, numbers and special characters\n",
        "clean_sentences = pd.Series(sentences).str.replace(\"[^a-zA-Z]\", \" \")\n",
        "\n",
        "# make alphabets lowercase\n",
        "clean_sentences = [s.lower() for s in clean_sentences]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnNQTu8GJhae",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#english stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "# function to remove stopwords\n",
        "def remove_stopwords(sen):\n",
        "    sen_new = \" \".join([i for i in sen if i not in stop_words])\n",
        "    return sen_new\n",
        "\n",
        "# remove stopwords from the sentences\n",
        "clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iB02-YAtJhah",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create sentence vectors from word embeddings\n",
        "sentence_vectors = []\n",
        "for i in clean_sentences:\n",
        "    if len(i) != 0:\n",
        "        v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
        "    else:\n",
        "        v = np.zeros((100,))\n",
        "    sentence_vectors.append(v)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUly1CrPJhal",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# similarity matrix\n",
        "sim_mat = np.zeros([len(sentences), len(sentences)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99fwkEj7Jhar",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# compute similarity matrix between sentences with cosine similarity\n",
        "for i in range(len(sentences)):\n",
        "    for j in range(len(sentences)):\n",
        "        if i != j:\n",
        "            sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100), sentence_vectors[j].reshape(1,100))[0,0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhS6VWlyJhaw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# apply page ranking to get ranking of sentences\n",
        "nx_graph = nx.from_numpy_matrix(sim_mat)\n",
        "scores = nx.pagerank(nx_graph)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXZKdAlYJha0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function to get ranked sentences\n",
        "ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cV7NRuxJha4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ranked_sentences;"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkGzUOwgJha7",
        "colab_type": "code",
        "outputId": "e6ee763c-0abc-4a0b-a4a6-24eb50dd91e6",
        "colab": {}
      },
      "source": [
        "# Extract top 10 sentences as the summary\n",
        "for i in range(5):\n",
        "  print(textwrap.fill(ranked_sentences[i][1], 50))\n",
        "    #print(ranked_sentences[i][1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Adept writers can do this with such softness of touch that at the end of their work you’re led to believe that what you have learned is absolutely true and would be true regardless of what you have just read.\n",
            "And while I have many times been on the receiving end of being limited by such a perspective on life, it seems that I, like most people cannot help adopting it when it is convenient.\n",
            "Because it was not exactly that Rand was attempting to manipulate people’s insecurities and anger — although that was precisely what she was doing — but that the language in which she was writing was so polemical and confident — so, one might say, autocratic — that it seemed specifically geared to give stability to those who most wanted some foundational principles by which to live a life.\n",
            "For Americans, she painted the world as black and white, and thereby gave voice to the simple stories people wanted to believe, the stories they perhaps had always somehow believed but had never been able to articulate themselves, stories which were the foundation for how they saw themselves and their country.\n",
            "Surely something which resonated with so many people could not in fact be completely untrue of the world, and thus, if nothing else, Rand was saying something about how people (perhaps specifically Americans) like to see themselves.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28R0vLO2JhbB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-xy9fOPJhbE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
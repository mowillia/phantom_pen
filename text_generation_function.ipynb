{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text_generation_function.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mowillia/phantom_pen/blob/master/text_generation_function.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6i603UsTcWG",
        "colab_type": "text"
      },
      "source": [
        "### Text Generation -- Google Colab\n",
        "\n",
        "This notebook implements the various generation functions of the Phantom Pen application. These functions include\n",
        "\n",
        "- Simple Generate\n",
        "\n",
        "- Classify and Generate\n",
        "\n",
        "- Classify, Extract, and Generate\n",
        "\n",
        "\n",
        "This code runs most efficiently when GPU is enabled. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZTbj1G831jy",
        "colab_type": "code",
        "outputId": "53f58eeb-aed1-42a5-af08-4c477245a5c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import pandas as pd\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "\n",
        "import nltk\n",
        "import nltk.data # natural language tool kit\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize # $ pip install nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.cluster.util import cosine_distance\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "\n",
        "from sklearn.externals import joblib\n",
        "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "import json\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "import textwrap\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
            "  warnings.warn(msg, category=DeprecationWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JDZMoCAgH5pZ",
        "colab_type": "code",
        "outputId": "b8662860-763f-45d8-83d4-a14c64b5dd4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        }
      },
      "source": [
        "#check GPU status\n",
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Jun 30 07:42:07 2019       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 418.67       Driver Version: 410.79       CUDA Version: 10.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   67C    P8    32W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AhBa1DGa463",
        "colab_type": "code",
        "outputId": "cb73b4a2-e96f-4907-8916-a69e4738eaff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "# clone mowillia repository to get access to encoder, decoder, sample files\n",
        "!git clone https://github.com/mowillia/gpt-2.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'gpt-2'...\n",
            "remote: Enumerating objects: 315, done.\u001b[K\n",
            "remote: Total 315 (delta 0), reused 0 (delta 0), pack-reused 315\u001b[K\n",
            "Receiving objects: 100% (315/315), 4.40 MiB | 16.16 MiB/s, done.\n",
            "Resolving deltas: 100% (172/172), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ew5EsK01oje",
        "colab_type": "code",
        "outputId": "c151f670-0096-40c9-8edd-8bd854a215b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# change directory to GPT2\n",
        "cd gpt-2"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gpt-2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2xexp901lmY",
        "colab_type": "code",
        "outputId": "54908b72-b0aa-455a-8ae9-b851e92f56d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582
        }
      },
      "source": [
        "# ensures we load packages needed for the function of program\n",
        "!pip3 install -r requirements.txt\n",
        "!pip install regex"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fire>=0.1.3 (from -r requirements.txt (line 1))\n",
            "  Downloading https://files.pythonhosted.org/packages/5a/b7/205702f348aab198baecd1d8344a90748cb68f53bdcd1cc30cbc08e47d3e/fire-0.1.3.tar.gz\n",
            "Collecting regex==2017.4.5 (from -r requirements.txt (line 2))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/62/c0c0d762ffd4ffaf39f372eb8561b8d491a11ace5a7884610424a8b40f95/regex-2017.04.05.tar.gz (601kB)\n",
            "\u001b[K     |████████████████████████████████| 604kB 10.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests==2.21.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 3)) (2.21.0)\n",
            "Collecting tqdm==4.31.1 (from -r requirements.txt (line 4))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6c/4b/c38b5144cf167c4f52288517436ccafefe9dc01b8d1c190e18a6b154cd4a/tqdm-4.31.1-py2.py3-none-any.whl (48kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 19.4MB/s \n",
            "\u001b[?25hCollecting toposort==1.5 (from -r requirements.txt (line 5))\n",
            "  Downloading https://files.pythonhosted.org/packages/e9/8a/321cd8ea5f4a22a06e3ba30ef31ec33bea11a3443eeb1d89807640ee6ed4/toposort-1.5-py2.py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from fire>=0.1.3->-r requirements.txt (line 1)) (1.12.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (2019.6.16)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (1.24.3)\n",
            "Building wheels for collected packages: fire, regex\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/2a/1a/4d/6b30377c3051e76559d1185c1dbbfff15aed31f87acdd14c22\n",
            "  Building wheel for regex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/75/07/38/3c16b529d50cb4e0cd3dbc7b75cece8a09c132692c74450b01\n",
            "Successfully built fire regex\n",
            "Installing collected packages: fire, regex, tqdm, toposort\n",
            "  Found existing installation: tqdm 4.28.1\n",
            "    Uninstalling tqdm-4.28.1:\n",
            "      Successfully uninstalled tqdm-4.28.1\n",
            "Successfully installed fire-0.1.3 regex-2017.4.5 toposort-1.5 tqdm-4.31.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tqdm"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GE32M--lbh3V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Run this cell to mount your Google Drive.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKYTUeEc1xWV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Exporting Python encoding\n",
        "!export PYTHONIOENCODING=UTF-8"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KL60BaAO_fH8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# copy trained classifier and essay text data\n",
        "!cp -r /content/drive/My\\ Drive/writrly_proj_files/data_pickle_files/* /content/\n",
        "\n",
        "# copy models from checkpoint to models folder; necessary for generation\n",
        "!cp -r /content/drive/My\\ Drive/checkpoint/* /content/gpt-2/models"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bm-mgTiU31Ze",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# copy encoder\n",
        "!cp -r /content/drive/My\\ Drive/writrly_proj_files/gpt2_support_files/encoder.py /content/gpt-2/\n",
        "\n",
        "# copy sample\n",
        "!cp -r /content/drive/My\\ Drive/writrly_proj_files/gpt2_support_files/sample.py /content/gpt-2/\n",
        "\n",
        "# copy model\n",
        "!cp -r /content/drive/My\\ Drive/writrly_proj_files/gpt2_support_files/model.py /content/gpt-2/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMexd6lD7l2H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# loads the encoder, sample(r) and model for generation function\n",
        "## note: these files must be in your content directory\n",
        "import encoder, sample, model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rl3cdjcP2ZKz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## dictionary for 345 model\n",
        "\n",
        "model_dict = {'science': 'atlantic_science_345', \n",
        "              'entertainment': 'atlantic_entertainment_345',\n",
        "              'education': 'atlantic_education_345', \n",
        "              'politics': 'atlantic_politics_345', \n",
        "              'technology':'atlantic_technology_345',\n",
        "             'health': 'atlantic_health_345',\n",
        "             'ideas': 'atlantic_ideas_345',\n",
        "             'international': 'atlantic_international_345',\n",
        "             'business':'atlantic_business_345',\n",
        "             'short_story': 'all_short_stories_345'}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_V10HhrWsCu",
        "colab_type": "text"
      },
      "source": [
        "### Simple Generate Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mbs--NGl1xdD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## def print string given input\n",
        "\n",
        "def simple_gen(input_string, lens, temp, model_choice):\n",
        "    \n",
        "    model_name=model_choice\n",
        "    seed=None\n",
        "    raw_text = input_string\n",
        "    length=lens\n",
        "    temperature=temp #set to 1.0 for highest diversity\n",
        "    top_k=40 #set to 40\n",
        "    top_p=0.9 #set to 0.9\n",
        "    \n",
        "    \"\"\"\n",
        "    Interactively run the model\n",
        "    :model_name=117M : String, which model to use\n",
        "    :seed=None : Integer seed for random number generators, fix seed to reproduce\n",
        "     results\n",
        "    :length=None : Number of tokens in generated text, if None (default), is\n",
        "     determined by model hyperparameters\n",
        "    :temperature=1 : Float value controlling randomness in boltzmann\n",
        "     distribution. Lower temperature results in less random completions. As the\n",
        "     temperature approaches zero, the model will become deterministic and\n",
        "     repetitive. Higher temperature results in more random completions.\n",
        "    :top_k=0 : Integer value controlling diversity. 1 means only 1 word is\n",
        "     considered for each step (token), resulting in deterministic completions,\n",
        "     while 40 means 40 words are considered at each step. 0 (default) is a\n",
        "     special setting meaning no restrictions. 40 generally is a good value.\n",
        "    :top_p=0.0 : Float value controlling diversity. Implements nucleus sampling,\n",
        "     overriding top_k if set to a value > 0. A good setting is 0.9.\n",
        "    \"\"\"\n",
        "    \n",
        "    # produce only a single batch\n",
        "    batch_size = 1\n",
        "\n",
        "    # create encoder based on chosen model\n",
        "    enc = encoder.get_encoder(model_name)\n",
        "    \n",
        "    # select hyperparameters based on model\n",
        "    hparams = model.default_hparams()\n",
        "    \n",
        "    with open(os.path.join('models', model_name, 'hparams.json')) as f:\n",
        "        hparams.override_from_dict(json.load(f))\n",
        "\n",
        "    if length is None:\n",
        "        length = hparams.n_ctx // 2\n",
        "    elif length > hparams.n_ctx:\n",
        "        raise ValueError(\"Can't get samples longer than window size: %s\" % hparams.n_ctx)\n",
        "\n",
        "    with tf.Session(graph=tf.Graph()) as sess:\n",
        "        context = tf.placeholder(tf.int32, [batch_size, None])\n",
        "        np.random.seed(seed)\n",
        "        tf.set_random_seed(seed)\n",
        "        output = sample.sample_sequence(\n",
        "            hparams=hparams, length=length,\n",
        "            context=context,\n",
        "            batch_size=batch_size,\n",
        "            temperature=temperature, top_k=top_k, top_p=top_p\n",
        "        )\n",
        "\n",
        "        saver = tf.train.Saver()\n",
        "        ckpt = tf.train.latest_checkpoint(os.path.join('models', model_name))\n",
        "        saver.restore(sess, ckpt)\n",
        "    \n",
        "        # encodes raw text for processing\n",
        "        context_tokens = enc.encode(raw_text)\n",
        "\n",
        "        # processes text through sampling program\n",
        "        out = sess.run(output, feed_dict={context: [context_tokens for _ in range(batch_size)]})[:, len(context_tokens):]\n",
        "        \n",
        "        # decodes output back into text\n",
        "        text = enc.decode(out[0])\n",
        "\n",
        "    return(text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isqrujpqTFis",
        "colab_type": "code",
        "outputId": "35ea4483-ccae-4c84-f429-d07243beac78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "Beginning_String = '\\n\\n\\n\\nPhysics is a quite cool subject.'\n",
        "\n",
        "start_time = time.time()\n",
        "text_result = simple_gen(input_string = Beginning_String, \n",
        "              lens = 800, \n",
        "              temp = 1.0, \n",
        "              model_choice = 'atlantic_technology_345')\n",
        "\n",
        "print('Run time:', str(time.time()-start_time)+' secs')\n",
        "print('     ')\n",
        "print(textwrap.fill(text_result, 60))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Run time: 30.29616117477417 secs\n",
            "     \n",
            " Almost anything can be described using physics. There are\n",
            "the elementary particles, which are called protons and\n",
            "neutrons, and electrons. There are the masses of these\n",
            "particles and their interaction, called electric charge. And\n",
            "there are the forces that cause the interaction between the\n",
            "particles and their surroundings.   Physics describes how a\n",
            "particle interacts with an environment, and can explain,\n",
            "among other things, how gravity works.   But it is the\n",
            "atomic world that physics tells us how atoms interact with\n",
            "one another. That is the world physicists study, and that is\n",
            "the real world in which humanity lives.   We could all live\n",
            "in a quantum superposition of carbon and oxygen—which never\n",
            "gets enough heat to grow—but that would be a nightmare of\n",
            "itself. In an ideal world, atoms would be totally stable, in\n",
            "all sense of the word. Our air, water, and land would remain\n",
            "air, water, and land. Our food would remain food. A bath of\n",
            "nectar, a steady rain, a fountaining machine, and a few\n",
            "other functions in our everyday life would operate like\n",
            "natural events—expecting nothing.  But our world is not like\n",
            "that. We live in an unstable mess, in which countless\n",
            "different types of parts can and do get mixed up in very\n",
            "complicated ways. What I mean is that we are simply not sure\n",
            "what is safe. A bath of nectar—whether it is perfumed in\n",
            "beeswax, corn starch, or coconut oil—might seem a logical\n",
            "approach to protecting against dengue fever or a fever. A\n",
            "gentle breeze might produce feverish coughs, nosebleeds, or\n",
            "other symptoms.   Still, some places already have strict\n",
            "precautions, like in Brazil or in Cambodia. Bad things can\n",
            "happen. A promising new vaccine, for example, might produce\n",
            "human-disease reactions. And even though the water here is\n",
            "safe, the very fact that I live in a swamp suggests that it\n",
            "is also a den of disease.  Our bodies, for example, are\n",
            "built to work with water. The muscles in our neck and\n",
            "shoulders contract if we cough, and our heart takes in blood\n",
            "flow when we sneeze. But for a whole host of reasons, our\n",
            "bodies are built for air. They are also sensitive to cold\n",
            "temperatures, which is why gloves, hat, and masks are highly\n",
            "recommended for work and play.  Alone, our bodies are\n",
            "probably pretty good at adapting to cold. Our skin holds\n",
            "water that our body needs to stay alive. That adaptation\n",
            "allows our bodies to regulate temperature to keep us warm.\n",
            "But as soon as we breathe in cold air, our bodies become\n",
            "more sensitive to the way that temperature changes our\n",
            "bodies, keeping our bodies from working at optimal\n",
            "temperature. We can only adapt to a certain temperature by\n",
            "increasing the surface area of our body. But on a frigid day\n",
            "in October, sweat rolls down our backs and onto our face.\n",
            "Because our bodies are made to be able to handle cold, our\n",
            "bodies are also designed to work very well at freezing\n",
            "temperatures. So the skin on our hands and forearms becomes\n",
            "more active at very low temperatures. As we warm up, this is\n",
            "broken down. Our hands become much more rigid, which\n",
            "protects our hands from cold temperatures. We even have some\n",
            "people with arthritis in their fingers caused by the fact\n",
            "that they are more sensitive to cold temperatures than our\n",
            "bodies would otherwise be. That stiffness limits the\n",
            "flexibility in our joints that allows our hands to stay\n",
            "warm.   People do things for many reasons: for recreation or\n",
            "for certain types of work that requires special training or\n",
            "pain tolerance. But the sole practical thing we can do is\n",
            "walk in the world around us and do our jobs. And when we\n",
            "choose to do that, as we tend to do around the office or in\n",
            "the wild, we also have to make sure our bodies are adapted\n",
            "to handle temperature changes, just as our bodies are\n",
            "adapted to working at lower temperatures. That work is all\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcqzgeAS2My3",
        "colab_type": "code",
        "outputId": "b709727c-1606-416e-8ccf-4f3e9a5ad608",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "Beginning_String = '\\n\\n\\n\\nToday is the last day of the rest of your life.'\n",
        "\n",
        "start_time = time.time()\n",
        "text_result = simple_gen(input_string = Beginning_String, \n",
        "              lens = 800, \n",
        "              temp = 1.0, \n",
        "              model_choice = 'atlantic_ideas_345')\n",
        "\n",
        "print('Run time:', str(time.time()-start_time)+' secs')\n",
        "print('     ')\n",
        "print(textwrap.fill(text_result, 60))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Run time: 37.91648316383362 secs\n",
            "     \n",
            " You'll never forget it. But before you die, why not reflect\n",
            "on the fleeting moment when you did?  We keep counting down\n",
            "to the election of Donald J. Trump, the horrible corruption\n",
            "of our democracy, and I can barely keep track of how many\n",
            "people are signing my petition, lest we all end up on the\n",
            "same shortlist for execution. It begins:       To spend too\n",
            "much time imagining Donald Trump or Ivanka Trump or\n",
            "Kellyanne Conway or the warm, fuzzy feeling of having made a\n",
            "massive life decision, or, you know, actually doing\n",
            "something productive. Which is to say, much of today. Which\n",
            "means that we can step back from the inescapable and turn\n",
            "our minds to the juicy details of the administration itself.\n",
            "(I spend a large part of my Friday reading New York\n",
            "magazine; I know it's nice. Get it?) For me, at least, the\n",
            "most important thing about the new health care bill is that\n",
            "it includes a big bang stabilization fund, funding for which\n",
            "is already guaranteed by Congress, which is a nice way of\n",
            "putting the mind at ease that people will eventually be\n",
            "furloughed just because Trump thought it was a good idea to\n",
            "blow a few budget holes up in his Twitter feed.)  Last week,\n",
            "Kellyanne Conway promised to give the administration an\n",
            "award if its agenda goes really well. She later clarified: I\n",
            "mean, honestly, the president would be honored, I think, in\n",
            "some way. He certainly got some good news. Last week,\n",
            "Kellyanne Conway promised to give the administration an\n",
            "award if its agenda goes really well. She later clarified: I\n",
            "mean, honestly, the president would be honored, I think, in\n",
            "some way. He certainly got some good news. Last week, the\n",
            "Department of Health and Human Services revised its own tax\n",
            "preferences for insurance plans sold on the federal\n",
            "marketplace to allow older adults and people with pre-\n",
            "existing conditions to more easily deduct up to $2,000 of\n",
            "out-of-pocket expenses. This was good news to people who had\n",
            "been paying into the HHS Modernization Tax Credit, the\n",
            "refundable tax credit that Republicans cut from the tax code\n",
            "last year. It was good news to Jared Polis, the newly minted\n",
            "ranking Democrat on the tax policy subcommittee. A taxpayer\n",
            "with a taxable income of $123,000 would, theoretically, be\n",
            "able to claim up to $1,000 in credit per month. That seems\n",
            "like a lot of cash, but in 2018 it would raise about $8,000,\n",
            "per person. (Your tax bill may be lower because you paid a\n",
            "penalty; you only have to pay income tax on the money after\n",
            "you receive your refund.) It will probably come out to be\n",
            "less than $1,000 in your pocketbook, but it seems worth it\n",
            "to President Donald Trump. For Conway, the administration\n",
            "got good news. The new tax policy. For Polis, the\n",
            "administration got good news. The credit is good news. This\n",
            "may be Trump himself saying all of this, but it makes a\n",
            "difference.       To be clear, I am not suggesting that\n",
            "Donald Trump, by volunteering to be President of the United\n",
            "States, is actually giving his word to ever be a big guy\n",
            "with thick arms and a strong arm. Donald Trump, on the other\n",
            "hand, does not usually do that. He likes his hands a little\n",
            "bigger. When he says he is willing to be \"President of the\n",
            "Trumps,\" he generally means he is willing to make an\n",
            "appearance onstage in front of the cameras.  And yet tonight\n",
            "was a very big Donald Trump Donald’s speech. He began by\n",
            "making a very strong case that he is the best and truest\n",
            "representative of every single American sentiment about the\n",
            "Trumps—by promising to be the best, most authentic version\n",
            "of him that the Trumps could find, to the max. He boasted of\n",
            "how he has managed to keep up with the Kardashians, who are\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ly9xJsZOWPhe",
        "colab_type": "text"
      },
      "source": [
        "### Pickle Files and Functions for Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBQYo2ay3UnE",
        "colab_type": "text"
      },
      "source": [
        "#### Former Pickle File Load\n",
        "\n",
        "Loads Pickle file for classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVsG9gulY0hP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## cleaning text for classification\n",
        "\n",
        "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
        "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "        text: a string\n",
        "        \n",
        "        return: modified initial string\n",
        "    \"\"\"\n",
        "    text = BeautifulSoup(text, \"lxml\").text # HTML decoding\n",
        "    text = text.lower() # lowercase text\n",
        "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text\n",
        "    text = BAD_SYMBOLS_RE.sub('', text) # delete symbols which are in BAD_SYMBOLS_RE from text\n",
        "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # delete stopwors from text\n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4V4K7DkwnnBI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "\n",
        "# import data frame\n",
        "masterDF = pd.read_csv('/content/master_df.csv')\n",
        "\n",
        "# clean essay element\n",
        "essay = masterDF['essay']\n",
        "essay = [clean_text(elem) for elem in essay]\n",
        "masterDF['essay'] = essay\n",
        "\n",
        "# Split data into training and test sets \n",
        "train_x, test_x, train_y, test_y = train_test_split(masterDF['essay'], masterDF['topic'])\n",
        "\n",
        "# label encode the target variable \n",
        "# Encode labels with value between 0 and n_classes-1.\n",
        "encoder = preprocessing.LabelEncoder()\n",
        "train_y = encoder.fit_transform(train_y)\n",
        "test_y = encoder.fit_transform(test_y)\n",
        "\n",
        "# create a count vectorizer object \n",
        "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
        "count_vect.fit(masterDF['essay'])\n",
        "\n",
        "# transform the training and validation data using count vectorizer object\n",
        "xtrain_count =  count_vect.transform(train_x)\n",
        "xtest_count =  count_vect.transform(test_x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5XKnFTEp2maV",
        "colab_type": "code",
        "outputId": "539913db-bef1-4978-a937-9874dcd27c5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "from sklearn.externals import joblib\n",
        "\n",
        "#reverse encode\n",
        "reverse_encode = ['business',\n",
        " 'education',\n",
        " 'entertainment',\n",
        " 'health',\n",
        " 'ideas',\n",
        " 'international',\n",
        " 'politics',\n",
        " 'science',\n",
        " 'short_story',\n",
        " 'technology']\n",
        "\n",
        "# select file\n",
        "joblib_file = \"/content/logreg_wordcount_model.pkl\"\n",
        "\n",
        "#load from file\n",
        "file_model = joblib.load(joblib_file)\n",
        "\n",
        "k =130\n",
        "\n",
        "#Calculate accuracy and predictions\n",
        "predictions = file_model.predict(xtest_count)\n",
        "print('Accuracy:', metrics.accuracy_score(predictions, test_y))\n",
        "predict_key = file_model.predict(xtest_count[k])[0]\n",
        "print('Prediction: ', reverse_encode[predict_key])\n",
        "print('Actual: ', reverse_encode[test_y[k]])\n",
        "print('Probability: ', max(file_model.predict_proba(xtest_count[k])[0]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9540441176470589\n",
            "Prediction:  technology\n",
            "Actual:  technology\n",
            "Probability:  0.9939616411216464\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "br8scUvq0xgn",
        "colab_type": "code",
        "outputId": "5e703c32-09b4-4f92-92f7-c35195f9fdf0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "#confusion_matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "confusion_matrix(predictions, test_y) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[42,  0,  0,  0,  0,  0,  0,  0,  0,  3],\n",
              "       [ 1, 71,  0,  0,  1,  0,  0,  0,  0,  0],\n",
              "       [ 1,  0, 60,  0,  0,  0,  0,  0,  0,  0],\n",
              "       [ 0,  0,  0, 49,  0,  0,  0,  1,  0,  0],\n",
              "       [ 1,  0,  0,  1, 51,  1,  1,  0,  1,  1],\n",
              "       [ 0,  0,  0,  0,  0, 38,  1,  0,  0,  0],\n",
              "       [ 0,  0,  0,  0,  3,  0, 45,  0,  0,  1],\n",
              "       [ 0,  0,  0,  0,  0,  0,  0, 73,  0,  0],\n",
              "       [ 0,  0,  0,  0,  0,  0,  0,  0, 42,  0],\n",
              "       [ 0,  0,  0,  1,  0,  0,  0,  0,  0, 54]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68CD5MpErhJ1",
        "colab_type": "code",
        "outputId": "5293cef2-2666-460b-df15-da5086199174",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Testing prediction\n",
        "free_x = 'I am a politician and I love talking about economics and crime!. President Obama is a man who once wanted to be a writer.'\n",
        "free_x = clean_text(free_x)\n",
        "free_vect = count_vect.transform([free_x])\n",
        "reverse_encode[file_model.predict(free_vect)[0]]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'international'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZyQ-3bIIocOM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load master data frame \n",
        "masterDF = pd.read_csv('/content/master_df.csv')\n",
        "\n",
        "# clean essay element\n",
        "essay = masterDF['essay']\n",
        "essay = [clean_text(elem) for elem in essay]\n",
        "masterDF['essay'] = essay\n",
        "\n",
        "# create a count vectorizer object \n",
        "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
        "count_vect.fit(masterDF['essay'])\n",
        "\n",
        "# predicts class of string or file\n",
        "def predict_class(free_x):\n",
        "  \n",
        "  #clean text\n",
        "  free_x = clean_text(free_x)\n",
        "  \n",
        "  free_vect = count_vect.transform([free_x])\n",
        "\n",
        "  prediction = reverse_encode[file_model.predict(free_vect)[0]]\n",
        "\n",
        "  return prediction, max(file_model.predict_proba(free_vect)[0])\n",
        "\n",
        "def predict_class_file(filename):\n",
        "  \n",
        "  with open(filename, 'r') as file:\n",
        "      free_x = file.read().replace('\\n', '')  \n",
        "  \n",
        "  # clean text \n",
        "  free_x = clean_text(free_x)\n",
        "  \n",
        "  free_vect = count_vect.transform([free_x])\n",
        "\n",
        "  prediction = reverse_encode[file_model.predict(free_vect)[0]]\n",
        "\n",
        "  return prediction, max(file_model.predict_proba(free_vect)[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7IRP5QQuvaxM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Testing Prediction on sample essay\n",
        "\n",
        "## text_string\n",
        "with open('/content/sample_essay.txt', 'r') as file:\n",
        "    text_file = file.read().replace('\\n', '')\n",
        "\n",
        "predict_class(text_file)    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwYsFBUXafEO",
        "colab_type": "text"
      },
      "source": [
        "### Functions for extractive summary from text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SWF5joSaeJ8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sentence_similarity(sent1, sent2, stopwords=None):\n",
        "    if stopwords is None:\n",
        "        stopwords = []\n",
        " \n",
        "    sent1 = [w.lower() for w in sent1]\n",
        "    sent2 = [w.lower() for w in sent2]\n",
        " \n",
        "    all_words = list(set(sent1 + sent2))\n",
        " \n",
        "    vector1 = [0] * len(all_words)\n",
        "    vector2 = [0] * len(all_words)\n",
        " \n",
        "    # build the vector for the first sentence\n",
        "    for w in sent1:\n",
        "        if w in stopwords:\n",
        "            continue\n",
        "        vector1[all_words.index(w)] += 1\n",
        " \n",
        "    # build the vector for the second sentence\n",
        "    for w in sent2:\n",
        "        if w in stopwords:\n",
        "            continue\n",
        "        vector2[all_words.index(w)] += 1\n",
        " \n",
        "    return 1 - cosine_distance(vector1, vector2)\n",
        " \n",
        "def build_similarity_matrix(sentences, stop_words):\n",
        "    # Create an empty similarity matrix\n",
        "    similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
        " \n",
        "    for idx1 in range(len(sentences)):\n",
        "        for idx2 in range(len(sentences)):\n",
        "            if idx1 == idx2: #ignore if both are same sentences\n",
        "                continue \n",
        "            similarity_matrix[idx1][idx2] = sentence_similarity(sentences[idx1], sentences[idx2], stop_words)\n",
        "\n",
        "    return similarity_matrix\n",
        "\n",
        "\n",
        "def generate_summary(file_name, top_n):\n",
        "    stop_words = stopwords.words('english')\n",
        "    summarize_text = []\n",
        "\n",
        "    # Step 1 - Read text anc split it\n",
        "    sentences =  raw_sents(file_name)\n",
        "\n",
        "    # Step 2 - Generate Similary Martix across sentences\n",
        "    sentence_similarity_martix = build_similarity_matrix(sentences, stop_words)\n",
        "\n",
        "    # Step 3 - Rank sentences in similarity martix\n",
        "    sentence_similarity_graph = nx.from_numpy_matrix(sentence_similarity_martix)\n",
        "    scores = nx.pagerank(sentence_similarity_graph)\n",
        "\n",
        "    # Step 4 - Sort the rank and pick top sentences\n",
        "    ranked_sentence = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)    \n",
        "    #print(\"Indexes of top ranked_sentence order are \", ranked_sentence)    \n",
        "\n",
        "    for i in range(top_n):\n",
        "        summarize_text.append(\"\".join(ranked_sentence[i][1]))\n",
        "\n",
        "    # Step 5 - Offcourse, output the summarize texr\n",
        "    #print(\"Summarize Text: \\n\",textwrap.fill(\" \".join(summarize_text), 50))\n",
        "    return(\" \".join(summarize_text))\n",
        "  \n",
        "def generate_summary_text(text, top_n):\n",
        "    stop_words = stopwords.words('english')\n",
        "    summarize_text = []\n",
        "\n",
        "    # Step 1 - Read text anc split it\n",
        "    sentences =  sent_tokenize(text)\n",
        "\n",
        "    # Step 2 - Generate Similary Martix across sentences\n",
        "    sentence_similarity_martix = build_similarity_matrix(sentences, stop_words)\n",
        "\n",
        "    # Step 3 - Rank sentences in similarity martix\n",
        "    sentence_similarity_graph = nx.from_numpy_matrix(sentence_similarity_martix)\n",
        "    scores = nx.pagerank(sentence_similarity_graph)\n",
        "\n",
        "    # Step 4 - Sort the rank and pick top sentences\n",
        "    ranked_sentence = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)    \n",
        "    #print(\"Indexes of top ranked_sentence order are \", ranked_sentence)    \n",
        "\n",
        "    for i in range(top_n):\n",
        "        summarize_text.append(\"\".join(ranked_sentence[i][1]))\n",
        "\n",
        "    # Step 5 - Offcourse, output the summarize texr\n",
        "    #print(\"Summarize Text: \\n\",textwrap.fill(\" \".join(summarize_text), 50))\n",
        "    return(\" \".join(summarize_text))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGQlBdjJ5Up7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sentence_similarity(sent1, sent2, stopwords=None):\n",
        "    if stopwords is None:\n",
        "        stopwords = []\n",
        " \n",
        "    sent1 = [w.lower() for w in sent1]\n",
        "    sent2 = [w.lower() for w in sent2]\n",
        " \n",
        "    all_words = list(set(sent1 + sent2))\n",
        " \n",
        "    vector1 = [0] * len(all_words)\n",
        "    vector2 = [0] * len(all_words)\n",
        " \n",
        "    # build the vector for the first sentence\n",
        "    for w in sent1:\n",
        "        if w in stopwords:\n",
        "            continue\n",
        "        vector1[all_words.index(w)] += 1\n",
        " \n",
        "    # build the vector for the second sentence\n",
        "    for w in sent2:\n",
        "        if w in stopwords:\n",
        "            continue\n",
        "        vector2[all_words.index(w)] += 1\n",
        " \n",
        "    return 1 - cosine_distance(vector1, vector2)\n",
        " \n",
        "def build_similarity_matrix(sentences, stop_words):\n",
        "    # Create an empty similarity matrix\n",
        "    similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
        " \n",
        "    for idx1 in range(len(sentences)):\n",
        "        for idx2 in range(len(sentences)):\n",
        "            if idx1 == idx2: #ignore if both are same sentences\n",
        "                continue \n",
        "            similarity_matrix[idx1][idx2] = sentence_similarity(sentences[idx1], sentences[idx2], stop_words)\n",
        "\n",
        "    return similarity_matrix\n",
        "\n",
        "\n",
        "def generate_summary(file_name, top_n):\n",
        "    stop_words = stopwords.words('english')\n",
        "    summarize_text = []\n",
        "\n",
        "    # Step 1 - Read text anc split it\n",
        "    sentences =  raw_sents(file_name)\n",
        "\n",
        "    # Step 2 - Generate Similary Martix across sentences\n",
        "    sentence_similarity_martix = build_similarity_matrix(sentences, stop_words)\n",
        "\n",
        "    # Step 3 - Rank sentences in similarity martix\n",
        "    sentence_similarity_graph = nx.from_numpy_matrix(sentence_similarity_martix)\n",
        "    scores = nx.pagerank(sentence_similarity_graph)\n",
        "\n",
        "    # Step 4 - Sort the rank and pick top sentences\n",
        "    ranked_sentence = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)    \n",
        "    #print(\"Indexes of top ranked_sentence order are \", ranked_sentence)    \n",
        "\n",
        "    for i in range(top_n):\n",
        "        summarize_text.append(\"\".join(ranked_sentence[i][1]))\n",
        "\n",
        "    # Step 5 - Offcourse, output the summarize texr\n",
        "    #print(\"Summarize Text: \\n\",textwrap.fill(\" \".join(summarize_text), 50))\n",
        "    return(\" \".join(summarize_text))\n",
        "  \n",
        "def generate_summary_text(text, top_n):\n",
        "    stop_words = stopwords.words('english')\n",
        "    summarize_text = []\n",
        "\n",
        "    # Step 1 - Read text anc split it\n",
        "    sentences =  sent_tokenize(text)\n",
        "\n",
        "    # Step 2 - Generate Similary Martix across sentences\n",
        "    sentence_similarity_martix = build_similarity_matrix(sentences, stop_words)\n",
        "\n",
        "    # Step 3 - Rank sentences in similarity martix\n",
        "    sentence_similarity_graph = nx.from_numpy_matrix(sentence_similarity_martix)\n",
        "    scores = nx.pagerank(sentence_similarity_graph)\n",
        "\n",
        "    # Step 4 - Sort the rank and pick top sentences\n",
        "    ranked_sentence = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)    \n",
        "    #print(\"Indexes of top ranked_sentence order are \", ranked_sentence)    \n",
        "\n",
        "    for i in range(top_n):\n",
        "        summarize_text.append(ranked_sentence[i][1])\n",
        "\n",
        "    # Step 5 - Offcourse, output the summarize texr\n",
        "    #print(\"Summarize Text: \\n\",textwrap.fill(\" \".join(summarize_text), 50))\n",
        "    return(summarize_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VNIhRcGZ-XR",
        "colab_type": "text"
      },
      "source": [
        "### Classify, Extract, and Generate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KbW1r-fvaB5u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create text to text transformer \n",
        "# with model chosen from text similarity or length \n",
        "\n",
        "def class_extract_gen_str(input_text,  T):\n",
        "  \n",
        "  # predicts class of input text\n",
        "  old_class, old_prob = predict_class(input_text)\n",
        "  \n",
        "  # text_length\n",
        "  length = min([950, len(word_tokenize(input_text))])\n",
        "  \n",
        "  # summary string\n",
        "  summ_string = generate_summary_text(text = input_text, top_n=2)\n",
        "  \n",
        "  # return new text\n",
        "  new_string =  simple_gen(input_string = '\\n\\n\\n\\n'+summ_string, \n",
        "              lens = length, \n",
        "              temp = T, \n",
        "              model_choice = model_dict[old_class])  \n",
        "  \n",
        "  # predicts probability of class of new string\n",
        "  new_class, new_prob = predict_class(new_string)\n",
        "  \n",
        "  # computes cosine similarity\n",
        "  vect = TfidfVectorizer(min_df=1)\n",
        "  tfidf = vect.fit_transform([input_text,new_string])\n",
        "  \n",
        "  return remove_end_punct(new_string), [old_class, old_prob], [new_class, new_prob], (tfidf * tfidf.T).A, summ_string                                   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThW1vDAK2CLJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create text to text transformer \n",
        "# with model chosen from text similarity or length \n",
        "\n",
        "def class_extract_gen(filename,  T):\n",
        "  \n",
        "  with open(filename, 'r') as file:\n",
        "      input_text = file.read()\n",
        "      \n",
        "  # predicts class of input text\n",
        "  old_class, old_prob = predict_class(input_text)\n",
        "  \n",
        "  # text_length\n",
        "  length = min([950, len(word_tokenize(input_text))])\n",
        "  \n",
        "  # summary string\n",
        "  summ_string = generate_summary_text(text = input_text, top_n=2)  \n",
        "  \n",
        "  # return new text\n",
        "  new_string =  simple_gen(input_string = '\\n\\n\\n\\n'+summ_string, \n",
        "              lens = length, \n",
        "              temp = T, \n",
        "              model_choice = model_dict[old_class])  \n",
        "  \n",
        "  # predicts probability of class of new string\n",
        "  new_class, new_prob = predict_class(new_string)\n",
        "  \n",
        "  # computes cosine similarity\n",
        "  vect = TfidfVectorizer(min_df=1)\n",
        "  tfidf = vect.fit_transform([input_text,new_string])\n",
        "  \n",
        "  \n",
        "  return remove_end_punct(new_string), [old_class, old_prob], [new_class, new_prob], (tfidf * tfidf.T).A, summ_string"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nYUq2Zg2C2i",
        "colab_type": "code",
        "outputId": "56d3005a-4218-4e81-ef49-ac20f18b8dc7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "start_time = time.time()\n",
        "# example implementation\n",
        "result = class_extract_gen('/content/essay_sample_texts/sample_essay.txt', 1.0)\n",
        "print('Run time:', str(time.time()-start_time)+' secs')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Run time: 45.80405926704407 secs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anvJ2Fe02Clk",
        "colab_type": "code",
        "outputId": "778120b9-2394-4425-9e3a-9cf1c21290aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print('Old class and probability:', result[1][0], result[1][1])\n",
        "print('    ')\n",
        "print('New class and probability:', result[2][0], result[2][1])\n",
        "print('    ')\n",
        "print('Cosine Similarity Matrix:', result[3])\n",
        "print('    ')\n",
        "print('Summary:', textwrap.fill(result[4], 60))\n",
        "print('(******)')\n",
        "print('    ')\n",
        "print(textwrap.fill(result[0], 60))\n",
        "print('    ')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Old class and probability: ideas 0.5128328716855207\n",
            "    \n",
            "New class and probability: ideas 0.9128141176680875\n",
            "    \n",
            "Cosine Similarity Matrix: [[1.         0.83843083]\n",
            " [0.83843083 1.        ]]\n",
            "    \n",
            "Summary: And although my appreciation of her writing tempered as I\n",
            "grew older, unlike much of the culture which now\n",
            "categorically vilifies Rand, I still saw a considerable\n",
            "potency and relevance in what she had written. Most learned\n",
            "truths about the world are confused and complicated, bearing\n",
            "Bohr’s hallmark of a deep truth in which even their\n",
            "seemingly antithetical statements are also somehow true of\n",
            "the world.\n",
            "(******)\n",
            "    \n",
            " What a vast world we live in, how many people and things we\n",
            "know, how many sides of each story we have to encounter. But\n",
            "we always must admit that we know so little—that even we\n",
            "never really see the whole story—which is why it is\n",
            "necessary to be convinced and enamored by her writings and\n",
            "performances. There is a vivid, tingling, creative life\n",
            "here, whether we like it or not, and the evidence of what\n",
            "Rand wrote were books that danced in our lap, littering the\n",
            "pages with small essays. This was also what I now recognize\n",
            "as a personal asset to the Humanist community.  I have met\n",
            "some of her admirers, among them teachers, relatives, and\n",
            "college professors. Like any friend, they have been by their\n",
            "self-absorbed, aspiring, and wonderful egos, even when they\n",
            "are not her readers or admirers. The fact that I wanted to\n",
            "help a solitary world-renowned, self-absorbed, and wonderful\n",
            "egomania get filled my wheelhouse. But I also don’t know\n",
            "anyone who would ever buy a book by Rand, as a reward for a\n",
            "hard day’s work at Rand Tower. I had to make a distinction,\n",
            "a very important one, between scholarly and influential\n",
            "work, and a bunch of essayists who I have the hard duty to\n",
            "think are a lot more interesting than academic or\n",
            "influential. I might try to do my best to keep it that way.\n",
            "Rand’s sources were sources. These were the people who knew\n",
            "her, and were completely uninvolved in her intellectual\n",
            "life. There are one and the same, and enormous, gaps in\n",
            "knowledge, and source gaps. Who these people were has not\n",
            "been totally made clear, and I would go so far as to say as\n",
            "a matter of the philosophical ethics of scholarly work that\n",
            "we as a profession would all be better off without them, if\n",
            "they were willing to be so public about it. None of them\n",
            "are. But I will be honest, and I think a good person would\n",
            "be happy to be so honest. But I’ve seen it, and I will admit\n",
            "to knowing, having met Rand at some point, that she was a\n",
            "very particular and very serious person, who wrote a lot\n",
            "about many kinds of things, many places, and living herself.\n",
            "“If a man has a wife, the wife must be free to choose which\n",
            "to obey,” said Jesus, “or he has no wife.”  The most recent\n",
            "release from our archives is a conceptual-philosophical\n",
            "survey of the possibilities of human moral agency. To be\n",
            "sure, philosophical movements have been telling us for\n",
            "decades that the preservation of the species is beyond the\n",
            "scope of most human capabilities. An important branch of\n",
            "philosophical inquiry today is the study of the theory of\n",
            "punishment, as applied to humans. It is a subtler field than\n",
            "the interrogation of natural law by the legal profession; it\n",
            "is much less tractable than the ultimate concerns of ethical\n",
            "decision making, even at the margins of society; and because\n",
            "the purpose of the study is the moral understanding of the\n",
            "particular situation in which a person finds himself, it\n",
            "stands no chance of success in the current culture war,\n",
            "which sees all debate about the future of life as a battle\n",
            "between good and evil.  What is needed is a small,\n",
            "tentative, and in many ways inconsequential series of steps,\n",
            "which includes a theoretical survey of the possible\n",
            "solutions to various human problems, and an ethical\n",
            "rethinking of the means by which they are meant to be\n",
            "accomplished. If this series of steps achieves some effect,\n",
            "then the philosophical cause will be made whole.  I like the\n",
            "series “Weird Al’s” and “Evan’s” as podcasts, for they have\n",
            "forced me to pay attention to some of the philosophical\n",
            "threads that run through their thought. One of the flaws of\n",
            "the intellectual history of the last several decades is that\n",
            "most philosophers of science and reason have not, to date,\n",
            "identified themselves as thinkers in their field, and\n",
            "neglected to include their own writings in their analysis.\n",
            "The results of our current series are more than a means to a\n",
            "higher purpose; they are a component of an intellectual\n",
            "work, building upon the work that the humanistic ethical\n",
            "tradition has produced for many decades, in the most\n",
            "important philosophical circles of the past half century.\n",
            "Perhaps it is fitting that this should be the first major\n",
            "exploration of the subject that the series “Weird Al” and\n",
            "“Evan” have undertaken.\n",
            "    \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ICeRxud4IlQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function that removes ending punctuations\n",
        "def remove_end_punct(string):\n",
        "    reverse_string = string[::-1]\n",
        "  \n",
        "    i1 = reverse_string.find('.')\n",
        "    i2 = reverse_string.find('?')\n",
        "    i3 = reverse_string.find('!')\n",
        "  \n",
        "    if i1 == -1:\n",
        "        i1 = 1000\n",
        "    if i2 == -1:\n",
        "        i2 = 1000\n",
        "    if i3 == -1:\n",
        "        i3 = 10000\n",
        "    \n",
        "    ifinal = min([i1, i2, i3])\n",
        "\n",
        "    return string[:len(string)-ifinal]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GA0GDt-W7Bw-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# insert string\n",
        "with open('/content/sample_essay.txt', 'r') as file:\n",
        "    input_text = file.read().replace('\\n', '')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSFJ0J0LXn1o",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "### Classify and Generate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNkaITs-XniG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create text to text transformer \n",
        "# with model chosen from text similarity or length \n",
        "\n",
        "def class_and_gen_str(input_text, length,  T):\n",
        "  \n",
        "#   with open(filename, 'r') as file:\n",
        "#       input_text = file.read()\n",
        "      \n",
        "  # predicts class of input text\n",
        "  old_class, old_prob = predict_class(input_text)\n",
        "  \n",
        "#   # summary string\n",
        "#   summ_string = generate_summary_text(text = input_text, top_n=2)  \n",
        "  \n",
        "  # return new text\n",
        "  new_string =  simple_gen(input_string = '\\n\\n\\n\\n'+input_text, \n",
        "              lens = length, \n",
        "              temp = T, \n",
        "              model_choice = model_dict[old_class])  \n",
        "  \n",
        "  # predicts probability of class of new string\n",
        "  new_class, new_prob = predict_class(new_string)\n",
        "  \n",
        "  return remove_end_punct(new_string), [old_class, old_prob], [new_class, new_prob]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbO-ctNNXnOf",
        "colab_type": "code",
        "outputId": "0bc8846e-b079-416d-d0a5-0a064d0b8938",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "start_time = time.time()\n",
        "# example implementation\n",
        "result = class_and_gen_str('Can you write an essay about physics?', 500, 1.0)\n",
        "print('Run time:', str(time.time()-start_time)+' secs')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Run time: 22.83637046813965 secs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhJ_rs3_Xm4k",
        "colab_type": "code",
        "outputId": "db596be1-d914-4820-b225-8c9f1950536b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 748
        }
      },
      "source": [
        "print('Old class and probability:', result[1][0], result[1][1])\n",
        "print('    ')\n",
        "print('New class and probability:', result[2][0], result[2][1])\n",
        "print('    ')\n",
        "print(textwrap.fill(result[0], 60))\n",
        "print('    ')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Old class and probability: science 0.1295174474194357\n",
            "    \n",
            "New class and probability: science 0.48487098061126166\n",
            "    \n",
            " Not exactly. That's because physics was, at least from the\n",
            "1930s to the 1950s, a lab exercise. It wasn't technically\n",
            "possible to make useful articles about physics. The Field\n",
            "Museum’s Modern Physics Lab wasn’t set up to produce\n",
            "important scientific papers. Instead, it was intended to\n",
            "test new ideas. “It’s hardly surprising,” Marcello Cahn, a\n",
            "theoretical physicist at ETH Zurich, told me, that his own\n",
            "experiments ended up “undecidable.”  In the 1960s, a new\n",
            "wave of physicists, including those from the University of\n",
            "Zurich, asked the same question. They decided to combine an\n",
            "experiment that was easy and dangerous with something that\n",
            "could be much harder: solving real-world problems. Their aim\n",
            "was to try and produce modern-day articles on a single\n",
            "issue—the existence of dark energy.  Cahn said that he used\n",
            "to regularly argue with physics teachers about the current\n",
            "state of physics, and that the argument evolved into “Doing\n",
            "Something Different.” The idea was that experiments where\n",
            "light of different frequencies is used can come out better,\n",
            "without having to actually observe the data.  The problem is\n",
            "that it’s not that easy to do modern-day papers on quantum\n",
            "gravity. First, anyone interested in quantum gravity still\n",
            "has to decide whether they want to do any research at all on\n",
            "dark energy. One of the authors of the paper, Carlos\n",
            "Popovic, told me that it’s “a rigorous decision and should\n",
            "not be taken lightly.” This means that the problem of\n",
            "modern-day physics is not just incredibly hard, but, rather,\n",
            "that no serious physicist has even attempted it.  Second,\n",
            "and more importantly, anyone interested in modern-day\n",
            "physics is basically stuck with today’s leading questions,\n",
            "rather than the answers. It means that if there is a\n",
            "breakthrough in modern-day physics, someone will always be\n",
            "writing about it, even though the idea has already been\n",
            "backproposed for years. And it means that these young people\n",
            "will never be able to use their experience to improve the\n",
            "scientific papers that are currently under way.    When\n",
            "Leonardo da Vinci invented the world’s first model of a\n",
            "human heart, at least one group of scientists had their\n",
            "sights set on the elusive Heart of Christ.\n",
            "    \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
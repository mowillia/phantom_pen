<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

    <title>Phantom Pen</title>

    <!-- Bootstrap CSS CDN -->
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.1.0/css/bootstrap.min.css" integrity="sha384-9gVQ4dYFwwWSjIDZnLEWnxCjeSWFphJiwGPXr1jddIhOegiu1FwO5qRGvFXOdJZ4" crossorigin="anonymous">
    <!-- Our Custom CSS -->
    <!-- For referencing style sheet locally-->
    <link rel="stylesheet" href="style_extra.css">
    <link rel="stylesheet" href="personal_style_extra.css">
    <link rel="icon" href="writrly_favicon.png">

    
<!--    for referencing style sheet through application-->
    <link rel= "stylesheet" type= "text/css" href= "static/css/personal_style_extra.css">
    <link rel="stylesheet" type= "text/css" href= "static/css/style_extra.css">
    <link rel="icon" href="static/images/writrly_favicon.png">

<!-- Font Awesome JS -->
    <script defer src="https://use.fontawesome.com/releases/v5.0.13/js/solid.js" integrity="sha384-tzzSw1/Vo+0N5UhStP3bvwWPq+uvzCMfrN1fEFe+xBmv1C/AtVX5K0uZtmcHitFZ" crossorigin="anonymous"></script>
    <script defer src="https://use.fontawesome.com/releases/v5.0.13/js/fontawesome.js" integrity="sha384-6OIrr52G08NpOFSZdxxz1xdNSndlD4vdcf/q2myIUVO0VsqaGHJsB0RaBE01VTOY" crossorigin="anonymous"></script>

<!--   Favicon for webpage -->
  <link rel="icon" href="static/images/writrly_favicon.png">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.4.0/jquery.min.js"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.0/js/bootstrap.min.js"></script>
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.1/jquery.min.js"></script>
</head>

<body>
    <div class="wrapper">
        
        <!-- Sidebar  -->
        <nav id="sidebar">
            <div class="sidebar-header">
                <h4><i>PHANTOM PEN</i></h4>
            </div>

            <ul class="list-unstyled components">
<!--                <p>Dummy Heading</p>-->

                <li>
                    <a href="{{ url_for('index') }}">Home</a>
                </li>
                <li>
                    <a href="{{ url_for('usage') }}">Usage Tips</a>
                </li>                
                <li>
                    <a href="{{ url_for('about') }}">Background</a>
                </li>

                <li>
                    <a href="{{ url_for('corpora') }}">Corpora</a>
                </li>
                <li>
                    <a href="{{ url_for('ethics') }}">Ethics</a>
                </li>
                
            </ul>

            <ul class="list-unstyled CTAs">
<!--
                <li>
                    <a href="https://bootstrapious.com/tutorial/files/sidebar.zip" class="download">Download source</a>
                </li>
-->
                <li>
                    <a href="https://github.com/mowillia" class="article">Github Repo</a>
                </li>
                <li>
                    <a href="{{ url_for('slides') }}" class="download">Slides</a>
                </li>                   
            </ul>
        </nav>
    
        <!-- Page Content  -->
        <div id="content" >

              <nav class="navbar navbar-expand-lg navbar-light bg-light">
                <div class="container-fluid">

                    <button type="button" id="sidebarCollapse" class="navbar-btn">
                        <span style = "color:  #fff;"></span>
                        <span style = "color:  #fff;"></span>
                        <span style = "color:  #fff;"></span>
                    </button>
                    <button class="btn btn-dark d-inline-block d-lg-none ml-auto" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
                        <i class="fas fa-align-justify"></i>
                    </button>

<!--

-->
                </div>
            </nav>

            
            
            <div class="container">
                
              <h3 style = "font-family: GillSans, 'Gill Sans MT', Calibri, sans-serif;  text-transform: uppercase;  font-size: 18pt" ><i>When The Machines Can Write</i></h3>  <h5 style = "font-family: GillSans, 'Gill Sans MT', Calibri, sans-serif; text-transform: uppercase; font-size: 12px">How is this program producing original text? </h5>
                        <p>
                        In creating this program I chose <i>Open AI</i>'s GPT-2 to build the language model due to the transformer's ability recognize and reproduce the semantic structure of the training corpus. There are many other text generation models, and we can understand how and why the transformer is special by comparing it to less sophisticated alternatives.
                        </p>
                <div class="row">
                    
                    <div class="col-sm-6"  style = "font-size: 14px">
                        <h5><i>What's up with WhatsApp</i></h5> 
                        <p>
                            It is quite easy to create a stupid text generator. Given a large body of text, you represent the text as a sequential list of words. Next, you create a table where the list of words defines both the rows and columns. The elements of the table are how often a column word follows a row word. Normalizing across each row, you can then generate text by beginning with one word and probabilistically sampling from the previously created frequency table. 
                        </p>
                        <p>

                            Text messaging applications (like WhatsApp) use this type of text-generation in their auto-suggest feature. The "large body of text" they use to generate their suggestions consists of all your previous texts. Consequently, these suggestions tend to be generic when you first start using your phone, but improve (up to a point) after longer use. 
                        </p>

                       <p>
                           In more technical language, the application is using a <a href = "https://en.wikipedia.org/wiki/Markov_chain" style = "color: dodgerblue"><b>Markov Chain</b></a> to implement text-generation, the key property being that the next state (i.e., the predicted word, two-words, or three-words) depends only on the previous state (i.e., the written word, two-word, or three-words) and hence has no memory of what came before. 
                        </p>                        
                        
 
                    </div>
                    <div class="col-sm-6"  style = "font-size: 14px">
                      <div class = 'col text-center'>
                        <img src="static/images/whatapp.jpg" alt="WhatsAPP" style = "width: 75%" >
                          <figcaption><i>Auto-complete in WhatsApp uses a Markov Model keyed on one or a few of the previous words.</i></figcaption>
                        </div>
                    </div>

                </div>
                <div class="row">
                    
                    <div class="col-sm-6"  style = "font-size: 14px">
                        <div class = 'col text-center'>
                        <img src="static/images/rnn.png" alt="RNN" style = "width:85%" >
                            <figcaption><i>Comparison between Vanilla-NN and RNN architectures</i></figcaption>
                        </div>
                        <p><br>
                            There is one major problem with a text generator that only uses the previous two or three words to predict a subsequent word: It does not do a good job of considering the context (semantic or structural) in which words appear.  It is this context which allows humans to create meaning from strings of words, and a text generator that cannot reproduce such context ultimately begins producing meaningless sentences. The context-independence of basic text-generators is why continuously following the first recommended word in a text-messaging app (circa 2019) often leads to gibberish. </p>
                        
 
                    </div>
                    <div class="col-sm-6"  style = "font-size: 14px">
                        <h5><i>How Smart is Gmail Smart Compose</i></h5> 

                        <p>
                            In order to recognize and reproduce the context of word, a model requires memory. The <b>Recurrent Neural Network (RNN)</b> is one such model. It is the foundational architecture used in GMail's Smart Compose in order to predict the next few words of an email text given the email subject and the previous email body <a href = "https://ai.googleblog.com/2018/05/smart-compose-using-neural-networks-to.html" style = "color:dodgerblue"><b>(Smart Compose: Using Neural Networks to Help Write Emails)</b></a>.  The RNN differs from a basic neural network in that the former does not merely feed outputs from hidden layers forward throughout the network. Instead, in an RNN outputs are fed back into the cell that produced them so the output is determined not only by a single input but the entire history of previous inputs. Ultimately, this feature gives the network its memory and allows it to embody and reflect the coherence of sequential information like written texts. 
                        </p>
                           
                    </div>

                </div>
                <div class="row">
                    
                    <div class="col-sm-6"  style = "font-size: 14px">
                        <h5><i>Why <i>Open AI</i> isn't so Open</i></h5>
                        <p>
                            The key feature that makes RNNs work better than simple Markov Chains as text generators is the RNNs ability to remember long-strings of texts that preceded one word. However, because of the way the neural network is optimized (i.e., through gradient descent), this memory is quite limited.</p>
                        
                        <p> 
                            In 2017, researchers at <a href = "https://ai.google/research/pubs/pub46201" style = "color: dodgerblue"><b>Google Brain</b></a> improved upon the RNNs by suggesting a return to the fundamentals of neural networks. They proposed a neural network architecture without outputs being fed back into their inputs, but instead with a property called "<a href = "https://skymind.ai/wiki/attention-mechanism-memory-network" style = "color: dodgerblue"><b>attention</b></a>" being the main implement.  In attention, a word is encoded into a numerical vector along with the words that appear around it, and this vector is then processed through the network. Consequently, neural networks with attention are able to recognize the semantic context of words such that upon inserting the string "I lost my keys. Where are they?", the network properly recognizes that "they" refers to "keys." 
                        </p>

                        
                        <p>
                            Open AI capitalized on Google Brain's work by building transformers of their own, ultimately culminating in the <a href = "https://www.google.com/search?q=Open+AI+GPT2&oq=Open+AI+GPT2&aqs=chrome..69i57j0l5.2409j0j9&sourceid=chrome&ie=UTF-8" style = "color:dodgerblue"><b>Generalized Pre Trained Transformer-2 (GPT-2)</b></a> released in February 2019 and used in this application. 
                        </p>
                        <p>The program in this application features two versions of Open AI's GPT-2: a 117 million parameter model and a 345 million parameter model. </p>

                        
 
                    </div>
                    <div class="col-sm-6"  style = "font-size: 14px">
                        <div class = 'col text-center'>
                        <img src="static/images/openai_2.png" alt="openai" style = "width:100%" >
                            <figcaption><i>Text written by Open AI's full GPT-2 given a prompt</i></figcaption>
                        </div>
<!--
                        <p>
                            RNNs produce reasonably good text generators in the sense that they are much better than simple frequency-table based models, but after a few sentences the text generated from an RNN loses its coherence and one can easily recognize that it was not generated by a human with mastery of the language. This is not so with a properly trained neural network with a transformer architecture.
                            
                        </p>
-->
                        <p><br>
                             The more parameters in the model the greater its ability to reproduce the semantic and structural properties of the text it has been trained on. Using the 345 million parameter model, you find that it does a good job of producing original and coherent text, but there is even an unreleased better version of the model with 1.5 billion parameters. By Open AI's estimation, this model does <i>too good</i> of a job at reproducing realistic text, and thus easily lends itself to abuse. Consequently, they have not released it to the public. 
  
                        </p>
                        <p>
                        (See the <a href="{{ url_for('ethics') }}" style = "color:dodgerblue"><b>Ethics</b></a> page for a longer discussion of the ethical issues associated with this application.)
                        </p>
                    </div>

                </div>                
            </div>
            
        </div>
        
    </div>

    <!-- jQuery CDN - Slim version (=without AJAX) -->
    <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
    <!-- Popper.JS -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.0/umd/popper.min.js" integrity="sha384-cs/chFZiN24E4KMATLdqdvsezGxaGsi4hLGOzlXwp5UZB1LY//20VyM2taTB4QvJ" crossorigin="anonymous"></script>
    <!-- Bootstrap JS -->
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.1.0/js/bootstrap.min.js" integrity="sha384-uefMccjFJAIv6A+rW+L4AHf99KvxDjWSu1z9VI8SKNVmz4sk7buKt/6v9KI65qnm" crossorigin="anonymous"></script>

    <script type="text/javascript">
        $(document).ready(function () {
            $('#sidebarCollapse').on('click', function () {
                $('#sidebar').toggleClass('active');
                $(this).toggleClass('active');
            });
        });
    </script>

    
</body>

</html>
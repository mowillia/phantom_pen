# -*- coding: utf-8 -*-
"""web_scraping_short_stories

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ajw7QNYP-QT9RQV4MkBquosIIc9MMpCi
"""

import requests
from bs4 import BeautifulSoup
import numpy as np
import pandas as pd
import os


# Run this cell to mount your Google Drive.
from google.colab import drive
drive.mount('/content/drive')

# gdrive location
gdrive_dir = '/content/drive/My Drive/writrly_proj_files/short_stories/'

"""### Web scraping short stories"""

### Getting base url

# url definition
url_orig = "https://americanliterature.com/100-great-short-stories"

# url home
url_home = "https://americanliterature.com/"

# Request
r1 = requests.get(url_orig)

# We'll save in coverpage the cover page content
coverpage = r1.content

# Soup creation
soup1 = BeautifulSoup(coverpage, 'html5lib')

# News identification
coverpage_news = soup1.find_all('span', )

# remove non interestoing metadata
coverpage_news = coverpage_news[11:]

# Getting url lists

url_list = []
for k in range(len(coverpage_news)):

  # create full url
  url_orig = coverpage_news[k].find('a')['href']
  url_list.append(url_home+url_orig)

## Collecting all essays
for k in range(len(coverpage_news)):

  # url 
  string = url_list[k]
  slash_locs = [i for i, x in enumerate(string) if x == "/"]

  # indices for the slashes
  ilast = slash_locs[-1]
  i1last = slash_locs[-2]
  i2last = slash_locs[-3]
  author = string[i2last+1:i1last]
  title = string[ilast+1:]

  # Reading the content (it is divided in paragraphs)
  article = requests.get(string)
  article_content = article.content
  soup_article = BeautifulSoup(article_content, 'html5lib')
  test_p = soup_article.find_all('p')

  # Unifying the paragraphs
  list_paragraphs = []
  for p in range(len(test_p)):
      paragraph = test_p[p].get_text()

      if paragraph != '':
        if 'Return to' not in paragraph and 'Or read more' not in paragraph:
          if 'Visit' not in paragraph:
            list_paragraphs.append(paragraph)

        final_article = "\n\n".join(list_paragraphs)

#   print('Title: ', title)
#   print('  ')
#   print('Author: ', author)
#   print('  ')
#   print('Paragraphs: ',list_paragraphs)

  # new file name
  new_file_name = title+'_'+author

  with open(gdrive_dir+"/"+new_file_name+".txt", 'w') as f:
    f.write(final_article)

# -*- coding: utf-8 -*-
"""web_scraping_atlantic_subjs

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18c0cKyAtFsxHq8WMoi7wcZoprxh8BB9x
"""

import requests
from bs4 import BeautifulSoup
import numpy as np
import pandas as pd
import time
import os

import pandas as pd

# Run this cell to mount your Google Drive.
from google.colab import drive
drive.mount('/content/drive')

# gdrive location
gdrive_dir = '/content/drive/My Drive/Atlantic_essays/'

tags = ['entertainment',
       'health',
       'international',
       'education',
       'business',
       'ideas',
       'politics',
       'science']

"""**Web scraper for all subject essays (besides technology)**"""

# Web scraper for all essays besides technology

for tag in tags:

  for m in range(1,15):

    # url definition
    url_num = "https://www.theatlantic.com/"+tag+"/?page="+str(m)

    # url definition
    url_base = "https://www.theatlantic.com/"+tag+"/"

    # Request
    r1 = requests.get(url_num)

    # We'll save in coverpage the cover page content
    coverpage = r1.content

    # Soup creation
    soup1 = BeautifulSoup(coverpage, 'html5lib')

    # News identification
    coverpage_news = soup1.find_all('li', class_='article blog-article ')

    for k in range(len(coverpage_news)):

      # create full urle
      url_end = coverpage_news[k].find('a')['href']
      if tag in url_end:
        url_end = url_end.replace("/"+tag+"/","")
        full_url = url_base+url_end

        # title of article
        title = coverpage_news[k].find('h2').get_text()
        title = title.replace("\n", "")
        title = title.replace('    ', "")

        # date of article
        date = coverpage_news[k].find('time').get_text()
        date = date.replace("\n", "")
        date = date.replace('    ', "")

        # author of article
        author = coverpage_news[k].find("li").get_text()

        # Reading the content (it is divided in paragraphs)
        article = requests.get(full_url)
        article_content = article.content
        soup_article = BeautifulSoup(article_content, 'html5lib')
        test_p = soup_article.find_all('p')

        article_descr = []
        # beginning description of article
        if len(soup_article.find_all('p', class_='c-dek'))!=0:
          article_descr = soup_article.find_all('p', class_='c-dek')[0].get_text()
          
        article_descr_list = []  
        if len(soup_article.find_all('p', class_='dek'))!=0:
          article_descr = soup_article.find_all('p', class_='dek')
          article_descr_list = [elem.get_text() for elem in article_descr]

        article_end = []  
        # ending tagline of article
        if len(soup_article.find_all('p', class_='c-letters-cta__text'))!=0:
          article_end = soup_article.find_all('p', class_='c-letters-cta__text')[0].get_text()

        # copyright of article
        article_copyr = soup_article.find_all('p', class_='c-footer__copyright')[0].get_text()

        # footer logo
        article_footer = soup_article.find_all('p', class_='c-footer__logo')[0].get_text()

         # recirculation link
        article_recirculation = soup_article.find_all('p', class_='c-recirculation-link')
        recirc_list = [elem.get_text() for elem in   article_recirculation]
        
        article_recirculation = soup_article.find_all('p', class_='c-recirculation-link')
        recirc_list = [elem.get_text() for elem in   article_recirculation]

        # master_list of miscellanea to remove from article
        mstr_list = [article_descr,
                     article_end,
                     article_copyr,
                     article_footer]+recirc_list+article_descr_list

        # Unifying the paragraphs
        list_paragraphs = []
        for p in range(len(test_p)):
            paragraph = test_p[p].get_text()
            
            if len(mstr_list)==0:
                list_paragraphs.append(paragraph)

                final_article = "\n\n".join(list_paragraphs)
               
            else: 
              if paragraph not in mstr_list:
                list_paragraphs.append(paragraph)

                final_article = "\n\n".join(list_paragraphs)              
        
        # remove forward slashed from title
        if '/' in title:
          title = title.replace("/", "")
          
        # new file name
        new_file_name = title+'_'+author+'_'+date

        with open(gdrive_dir +tag+"/"+new_file_name+".txt", 'w') as f:
          f.write(final_article)

"""**Web scraper for technology**"""

# webscraper for technology

tag = 'technology'

for m in range(1,15):

  # url definition
  url_num = "https://www.theatlantic.com/"+tag+"/?page="+str(m)

  # url definition
  url_base = "https://www.theatlantic.com/"+tag+"/"

  # Request
  r1 = requests.get(url_num)

  # We'll save in coverpage the cover page content
  coverpage = r1.content

  # Soup creation
  soup1 = BeautifulSoup(coverpage, 'html5lib')

  # News identification
  coverpage_news = soup1.find_all('li', class_='c-most-popular__item')

  for k in range(len(coverpage_news)):

    # create full urle
    full_url = coverpage_news[k].find('a')['href']
    if tag in full_url:

      # title of article
      title = coverpage_news[k].find('h3').get_text()
      title = title.replace("\n", "")
      title = title.replace('    ', "")
      title = title.replace('  ', "")  
      
      # author of article
      author = coverpage_news[k].find("address").get_text()

      # Reading the content (it is divided in paragraphs)
      article = requests.get(full_url)
      article_content = article.content
      soup_article = BeautifulSoup(article_content, 'html5lib')
      test_p = soup_article.find_all('p')
      
      # date of article
      if len(soup_article.find_all('li', class_ ='date'))!=0:

        date= soup_article.find_all('li', class_ ='date')[0].get_text()
        date = date.replace("\n", "")
        date = date.replace('  ', "")
        
      elif len(soup_article.find_all('time', class_ ='c-dateline'))!=0:

        date= soup_article.find_all('time', class_ ='c-dateline')[0].get_text()
        date = date.replace('  ', "")      
      

      article_descr = []
      # beginning description of article
      if len(soup_article.find_all('p', class_='c-dek'))!=0:
        article_descr = soup_article.find_all('p', class_='c-dek')[0].get_text()

      article_descr_list = []  
      if len(soup_article.find_all('p', class_='dek'))!=0:
        article_descr = soup_article.find_all('p', class_='dek')
        article_descr_list = [elem.get_text() for elem in article_descr]

      article_end = []  
      # ending tagline of article
      if len(soup_article.find_all('p', class_='c-letters-cta__text'))!=0:
        article_end = soup_article.find_all('p', class_='c-letters-cta__text')[0].get_text()

      # copyright of article
      article_copyr = soup_article.find_all('p', class_='c-footer__copyright')[0].get_text()

      # footer logo
      article_footer = soup_article.find_all('p', class_='c-footer__logo')[0].get_text()

       # recirculation link
      article_recirculation = soup_article.find_all('p', class_='c-recirculation-link')
      recirc_list = [elem.get_text() for elem in   article_recirculation]

      article_recirculation = soup_article.find_all('p', class_='c-recirculation-link')
      recirc_list = [elem.get_text() for elem in   article_recirculation]

      # master_list of miscellanea to remove from article
      mstr_list = [article_descr,
                   article_end,
                   article_copyr,
                   article_footer]+recirc_list+article_descr_list

      # Unifying the paragraphs
      list_paragraphs = []
      for p in range(len(test_p)):
          paragraph = test_p[p].get_text()

          if len(mstr_list)==0:
              list_paragraphs.append(paragraph)

              final_article = "\n\n".join(list_paragraphs)

          else: 
            if paragraph not in mstr_list:
              list_paragraphs.append(paragraph)

              final_article = "\n\n".join(list_paragraphs)              

      # remove forward slashed from title
      if '/' in title:
        title = title.replace("/", "")

      # new file name
      new_file_name = title+'_'+author+'_'+date

      with open(gdrive_dir +tag+"/"+new_file_name+".txt", 'w') as f:
        f.write(final_article)
